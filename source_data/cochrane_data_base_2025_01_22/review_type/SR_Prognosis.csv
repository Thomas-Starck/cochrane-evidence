Cochrane Review ID,Author(s),Title,Source,Year,Abstract,Issue,Publisher,ISSN,Keywords,DOI,URL,Cochrane Review Group Code
"CD014966.PUB2","Jayanti, S; Beruni, NA; Chui, JN; Deng, D; Liang, A; Chong, AS; Craig, JC; Foster, B; Howell, M; Kim, S; Mannon, RB; Sapir-Pichhadze, R; Scholes-Robertson, NJ; Strauss, AT; Jaure, A; West, L; Cooper, TE; Wong, G","Sex and gender as predictors for allograft and patient‐relevant outcomes after kidney transplantation","Cochrane Database of Systematic Reviews","2024","Abstract - Background Sex, as a biological construct, and gender, defined as the cultural attitudes and behaviours attributed by society, may be associated with allograft loss, death, cancer, and rejection. Other factors, such as recipient age and donor sex, may modify the association between sex/gender and post‐transplant outcomes. Objectives We sought to evaluate the prognostic effects of recipient sex and, separately, gender as independent predictors of graft loss, death, cancer, and allograft rejection following kidney or simultaneous pancreas‐kidney (SPK) transplantation. We aimed to evaluate this prognostic effect by defining the relationship between recipient sex or gender and post‐transplantation outcomes identifying reasons for variations between sexes and genders, and then quantifying the magnitude of this relationship. Search methods We searched MEDLINE and EMBASE databases from inception up to 12 April 2023, through contact with the Cochrane Kidney and Transplant Information Specialist, using search terms relevant to this review and no language restrictions. Selection criteria Cohort, case‐control, or cross‐sectional studies were included if sex or gender were the primary exposure and clearly defined. Studies needed to focus on our defined outcomes post‐transplantation. Sex was defined as the chromosomal, gonadal, and anatomical characteristics associated with the biological sex, and we used the terms “males” and “females”. Gender was defined as the attitudes and behaviours that a given culture associates with a person’s biological sex, and we used the terms “men” and “women”. Data collection and analysis Two authors independently assessed the references for eligibility, extracted the data and assessed the risk of bias using the Quality in Prognosis Studies (QUIPS) tool. Whenever appropriate, we performed random‐effects meta‐analyses to estimate the mean difference in outcomes. The outcomes of interest included the Standardised Outcomes in Nephrology‐Kidney Transplant (SONG‐Tx) core outcomes, allograft loss, death, cancer (overall incidence and site‐specific) and acute or chronic graft rejection. Main results Fifty‐three studies (2,144,613 patients; range 59 to 407,963) conducted between 1990 and 2023 were included. Sixteen studies were conducted in the Americas, 12 in Europe, 11 in the Western Pacific, four in the Eastern Mediterranean, three in Africa, two in Southeast Asia, and five across multiple regions. All but one study focused on sex rather than gender as the primary exposure of interest. The number identified as male was 54%; 49 studies included kidney transplant recipients, and four studies included SPK transplant recipients. Twenty‐four studies included adults and children, 25 studies included only adults, and four studies included only children. Data from 33 studies were included in the meta‐analyses. Among these, six studies presented unadjusted hazard ratios (HRs) that assessed the effect of recipient sex on kidney allograft loss. The other studies reported risk ratios (RRs) for the pre‐defined outcomes. Notably, the decision to restrict the meta‐analyses to unadjusted estimates arose from the variation in covariate adjustment methods across studies, lacking a common set of adjusted variables. Only three studies considered the modifying effect of recipient age on graft loss or death, which is likely crucial to evaluating sex differences in post‐transplant outcomes. No studies considered the modifying effect of recipient age on cancer incidence or allograft rejection risk. In low certainty evidence, compared with male recipients, being female may make little or no difference in kidney allograft loss post‐transplantation (7 studies, 5843 patients: RR 0.91, 95% CI 0.73 to 1.12; I 2  = 73%). This was also observed in studies that included time‐to‐event analyses (6 studies, 238,937 patients; HR 1.07, 95% CI, 0.95 to 1.20; I 2  = 44%). Two recent large registry‐based cohort studies that considered the modifying effects of donor sex and recipient age showed that female recipients under 45 years of age had significantly higher graft loss rates than age‐matched male recipients in the setting of a male donor. In contrast, female recipients 60 years and older had lower graft loss rates than age‐matched male recipients, regardless of donor sex. Compared with male recipients, being female may make little or no difference in death up to 30 years post‐transplantation; however, the evidence is very uncertain (13 studies, 60,818 patients: RR 0.94, 95% CI 0.81 to 1.09; I 2  = 92%). Studies that considered the modifying effect of recipient age and donor sex showed that female recipients had a higher excess death risk than males under 45 years of age in the setting of a male donor. Compared with male recipients, being female may make little or no difference in cancer incidence up to 20 years post‐transplantation; however, the evidence is very uncertain (7 studies, 25,076 patients; RR 0.84, 95% CI 0.70 to 1.01; I 2  = 60%). Compared with male recipients, being female may make little or no difference in the incidence of acute and chronic kidney allograft rejection up to 15 years post‐transplantation (9 studies, 6158 patients: RR 0.89, 95% CI 0.75 to 1.05; I 2  =54%; low certainty evidence). One study assessed gender and reported that when compared with men, women experienced better five‐year survival in high (HR 0.71, 95% CI 0.59 to 0.87) and middle‐income areas (HR 0.82, 95% CI 0.74 to 0.92), with no difference in low‐income areas (HR 0.85, 95% CI 0.72 to 1.01). There was considerable uncertainty regarding any association between sex or gender and post‐transplant patient‐relevant outcomes. This was primarily due to clinical and methodological heterogeneity. The observed clinical heterogeneity between studies could be attributed to diverse patient characteristics within sample populations. As a result of limited sex‐stratified demographic data being provided, further investigation of this heterogeneity was constrained. However, factors contributing to this finding may include recipient age, donor age, types, and sex. Methodological heterogeneity was noted with the interchangeable use of sex and gender, outcome misclassification, the use of different measures of effects, inconsistent covariate profiles, and disregard for important effect modification. Authors' conclusions There is very low to low certainty evidence to suggest there are no differences in kidney and pancreas allograft survival, patient survival, cancer, and acute and chronic allograft rejection between male and female kidney and SPK transplant recipients. Plain language summary What are the effects of sex and gender on outcomes after kidney transplantation? Key messages • There may be no differences in the loss of a transplanted kidney (a surgical procedure where a kidney is transferred from one person ‐ the donor ‐ to another person ‐ the recipient), death, acute or chronic rejection (the body's immune system attacking the transplanted kidney), or the incidence of cancer between male and female kidney transplant recipients (the definition of “male” or “female” based on biological differences). • Only one study investigated gender (defined as the sociocultural identity of individuals), and no conclusions could be made about the effect of gender on kidney transplant outcomes. What is the issue? For a person with kidney failure (a condition where the kidneys no longer function well enough to keep a person alive), kidney transplantation (a surgical procedure where a kidney is transferred from one person ‐ the donor ‐ to another person ‐ the recipient) significantly improves a person's survival and quality of life. As a result, it is essential that all factors that impact important outcomes, such as the survival of the transplanted kidney and the overall person's survival, are thoroughly assessed. Recipient sex, which is defined as “male” or “female” based on biological differences, may impact these outcomes through an immunological process affecting graft rejection. Gender, defined as the sociocultural identity of individuals, may influence factors such as medication adherence, which could also influence relevant outcomes. However, it is not clear currently if there is an impact. What did we want to find out? We wanted to find out if a person's sex or gender influences post‐transplant outcomes such as rejection of the kidney (caused by the body's immune system attacking the transplanted kidney), loss of the transplanted kidney, development of cancer, or death. What did we do? We searched the medical literature primarily for cohort studies (studies that follow people over a period of time), case‐control studies (studies comparing two groups of people with and without our specific outcomes) and cross‐sectional studies (data collected on many individuals at a single point in time) that focused on how sex and gender impact on graft survival, death, cancer incidence and rejection after kidney and simultaneous pancreas‐kidney transplantation. We did not include studies that did not define sex and gender clearly. We compared and summarised the results of the studies and rated our confidence in the information based on factors such as study methods and size. What did we find? We found 53 studies with 2,940,273 patients, with the number ranging from 59 to 407,963, and of these, 46% were female and 54% male. Sixteen studies were conducted in the Americas, 12 in Europe, 11 in the Western Pacific, four in the Eastern Mediterranean, three in Africa, two in Southeast Asia and five across multiple regions. Compared to males, being female may make little or no difference to the loss of the transplanted kidney, death, those diagnosed with cancer, or the incidence of either acute or chronic rejection. We found only one study that focused on recipient gender and, therefore, cannot make any conclusions on the effect of gender on outcomes following kidney transplantation. What are the limitations of the evidence? Our confidence in the evidence is low because most studies did not define sex or gender separately, and the terms were often used interchangeably. Studies were conducted over a very wide time frame (from 1990 to 2023), and definitions, particularly for acute rejection, have changed over this 30‐year period. How up to date is this review? We searched databases up until 12 April 2023.","12","John Wiley & Sons, Ltd","1465-1858","*Graft Rejection; *Graft Survival; *Kidney Transplantation; Allografts; Bias; Female; Humans; Male; Neoplasms [mortality, surgery]; Sex Factors","10.1002/14651858.CD014966.pub2","http://dx.doi.org/10.1002/14651858.CD014966.pub2","Kidney and Transplant"
"CD016013.PUB2","Franco, JVA; Bongaerts, B; Metzendorf, MI; Risso, A; Guo, Y; Peña Silva, L; Boeckmann, M; Schlesinger, S; Damen, JAAG; Richter, B; Baddeley, A; Bastard, M; Carlqvist, A; Garcia-Casal, MN; Hemmingsen, B; Mavhunga, F; Manne-Goehler, J; Viney, K; Lethaby, A","Diabetes as a risk factor for tuberculosis disease","Cochrane Database of Systematic Reviews","2024","Abstract - Background Tuberculosis (TB) is amongst the leading causes of death from an infectious disease, with an estimated 1.3 million deaths from TB in 2022. Approximately 25% of the global population is estimated to be infected with the TB bacterium, giving rise to 10.6 million episodes of TB disease in 2022. The prevalence of diabetes influences TB incidence and TB mortality. It is associated not only with an increased risk of TB disease but also death during TB treatment, TB relapse after treatment completion and multidrug‐resistant TB. Since 2011, the World Health Organization (WHO) has recommended collaborative TB and diabetes activities as outlined in the Collaborative Framework for Care and Control of TB and Diabetes. Objectives To determine the prognostic value of diabetes mellitus (DM) in the general population of adults, adolescents and children for predicting tuberculosis disease. Search methods We searched the literature databases MEDLINE (via PubMed) and WHO Global Index Medicus, and the WHO International Clinical Trials Registry Platform (ICTRP) on 3 May 2023 (date of last search for all databases); we placed no restrictions on the language of publication. Selection criteria We included retrospective and prospective cohort studies, irrespective of publication status or language. The target population comprised adults, adolescents and children from diverse settings, encompassing outpatient and inpatient cohorts, with varying comorbidities and risk of exposure to tuberculosis. Data collection and analysis We used standard Cochrane methodology and the Quality In Prognosis Studies (QUIPS) tool. Prognostic factors assessed at enrolment/baseline included diabetes, as defined by the individual studies, encompassing patient‐reported status, abstracted from medical records or claims data, or diagnosed by plasma glucose/glycosylated haemoglobin. The primary outcome was the incidence of tuberculosis disease. The secondary outcome was recurrent TB disease. We performed a random‐effects meta‐analysis for the adjusted hazard ratios, risk ratios, or odds ratios, employing the restricted maximum likelihood estimation. We rated the certainty of the evidence using the GRADE approach. Main results We included 48 cohort studies with over 61 million participants from the six WHO regions. However, the representation was variable as eight population‐based studies were from South Korea and 19 from China, with overlapping study periods, and only one from the African region (Ethiopia). All studies included adults, and nine studies also included children and adolescents. Most studies diagnosed DM based on clinical records, including fasting blood glucose levels or glucose‐lowering treatments. The studies did not distinguish between type 1 and type 2 DM; only one study focused on type 1 DM. Diagnosis and exclusion of TB were performed using culture or molecular WHO‐recommended rapid diagnostic tests (mWRD) in only 12 studies, which could have biassed the effect estimate. The median follow‐up time was five years (interquartile range 1.5 to 10, range 1 to 16.9), and the studies primarily reported an adjusted hazard ratio from a multivariable Cox‐proportional hazard model. Hazard Ratios (HR) The HR estimates represent the highest certainty of the evidence, explored through sensitivity analyses and excluding studies at high risk of bias. We present 95% confidence intervals (CI) and prediction intervals, which show between‐study heterogeneity represented in measuring the variability of effect sizes (i.e. the interval within which the effect size of a new study would fall considering the same population of studies included in the meta‐analysis). DM may increase the risk of tuberculosis disease (HR 1.90, 95% CI 1.51 to 2.40; prediction interval 0.83 to 4.39; 10 studies; 11,713,023 participants). The certainty of the evidence is low, due to a moderate risk of bias across studies and inconsistency. Considering a risk without diabetes of 129 cases per 100,000 population, this represents 102 more (59 to 153 more) cases per 100,000. When stratified by follow‐up time, the results are more consistent across < 10 years follow‐up (HR 1.52, 95% CI 1.47 to 1.57; prediction interval 1.45 to 1.59; 7 studies; 10,380,872 participants). This results in a moderate certainty of the evidence due to a moderate risk of bias across studies. However, at 10 or more years of follow‐up, the estimates yield a wider CI and a higher HR (HR 2.44, 95% CI 1.22 to 4.88; prediction interval 0.09 to 69.12; 3 studies; 1,332,151 participants). The certainty of the evidence is low due to the moderate risk of bias and inconsistency. Odds Ratio (OR) DM may increase the odds of tuberculosis disease (OR 1.61, 95% CI 1.27 to 2.04; prediction interval 0.96 to 2.70; 4 studies; 167,564 participants). Stratification by follow‐up time was not possible as all studies had a follow‐up < 10 years. The certainty of the evidence is low due to a moderate risk of bias and inconsistency. Risk Ratio (RR) The RR estimates represent the highest certainty of the evidence, explored through sensitivity analyses and excluding studies at high risk of bias. DM probably increases the risk of tuberculosis disease (RR 1.60, 95% CI 1.42 to 1.80; prediction interval 1.38 to 1.85; 6 studies; 44,058,675 participants). Stratification by follow‐up time was not possible as all studies had a follow‐up < 10 years. The certainty of the evidence is moderate due to a moderate risk of bias. Authors' conclusions Diabetes probably increases the risk of developing TB disease in the short term (< 10 years) and may also increase the risk in the long term (≥ 10 years). As glycaemic control and access to care may be potential effect modifiers of the association between diabetes and the risk of TB disease, the overall estimates should be interpreted with caution when applied locally. Policies targeted at reducing the burden of diabetes are needed to contribute to the aims of ending TB. Large population‐based cohorts, including those derived from high‐quality national registries of exposures (diabetes) and outcomes (TB disease), are needed to provide estimates with a high certainty of evidence of this risk across different settings and populations, including low‐ and middle‐income countries from different WHO regions. Moreover, studies including children and adolescents and currently recommended methods for diagnosing TB would provide more up‐to‐date information relevant to practice and policy. Funding World Health Organization (203256442) Registration PROSPERO registration: CRD42023408807 Plain language summary Does diabetes increase the risk of tuberculosis? Key messages Having diabetes probably increases the risk of developing tuberculosis disease to about double the population risk (1.5 to 2.4 times increased risk). These results apply to the general adult population; the risk for other groups, such as adolescents and children, is unclear. What is tuberculosis? Tuberculosis (TB) is an infection caused by the bacterium  Mycobacterium tuberculosis . It primarily affects the lungs but can also affect other sites in the body. TB spreads through the air when a person with TB disease coughs or sneezes. Some people do not develop symptoms after infection, while others develop the disease and symptoms, such as persistent cough, weight loss, fever and night sweats. This is called  TB disease . The diagnosis of TB disease usually involves the administration of a molecular test or culture that detects the bacteria in a bodily secretion (such as sputum (mucus and saliva)) after consultation with a doctor. TB is a major health concern globally; over 10 million people develop TB disease, and approximately 1.3 million die from the disease every year. What is diabetes? Diabetes is a chronic condition characterised by abnormally high blood glucose (sugar) due toinadequate insulin production by the pancreas or the body's inability to properly utilise the insulin it produces, or a combination of both. There are many different types of diabetes; the most frequent is called 'type 2 diabetes mellitus'. Insulin is a hormone that regulates blood sugar. This leads to symptoms such as thirst, frequent urination, tiredness, and slow healing of wounds. Without proper management, diabetes can lead to complications such as heart disease, kidney damage, nerve problems, and eye issues. Managing diabetes involves healthy eating, staying active, taking medicine, and monitoring health parameters to prevent complications. What did we want to find out? We wanted to estimate the risk of developing TB disease for people with diabetes compared to those without diabetes. What did we do? We looked at studies that included people with and without diabetes over time and compared how frequently each group developed tuberculosis. What did we find? We included 48 studies with over 61 million participants from the six WHO regions. However, the representation was variable as we found eight whole‐population studies from South Korea, 19 from China, and only one from the African region (Ethiopia). Most studies were in adults, four in children and three in children and adults. On average, the studies followed people for five years. We found that people with diabetes were at 1.5 to 2.4 times higher risk of developing tuberculosis compared to those without diabetes. What are the limitations of the evidence? Many of the studies had limitations. One problem was that many of them used sputum microscopy to diagnose TB in people with symptoms which might have missed some diagnoses. More accurate methods, such as culture or rapid diagnostic tests, currently exist and would miss fewer cases of TB. Moreover, well‐defined diagnostic criteria for diabetes, including the type of diabetes and how well‐controlled the glucose levels are, are needed to estimate the risk associated with the condition accurately. How up‐to‐date is this review? This evidence is up‐to‐date as of 3 May 2023.","8","John Wiley & Sons, Ltd","1465-1858","*Diabetes Mellitus [epidemiology]; *Tuberculosis [epidemiology]; Adolescent; Adult; Child; Humans; Incidence; Prognosis; Risk Factors","10.1002/14651858.CD016013.pub2","http://dx.doi.org/10.1002/14651858.CD016013.pub2","Central Editorial Service"
"CD015890.PUB2","Franco, JVA; Bongaerts, B; Metzendorf, MI; Risso, A; Guo, Y; Peña Silva, L; Boeckmann, M; Schlesinger, S; Damen, JAAG; Richter, B; Baddeley, A; Bastard, M; Carlqvist, A; Garcia-Casal, MN; Hemmingsen, B; Mavhunga, F; Manne-Goehler, J; Viney, K; Bellorini, J","Undernutrition as a risk factor for tuberculosis disease","Cochrane Database of Systematic Reviews","2024","Abstract - Background Tuberculosis (TB) is a leading cause of mortality due to an infectious disease, with an estimated 1.6 million deaths due to TB in 2022. Approximately 25% of the global population has TB infection, giving rise to 10.6 million episodes of TB disease in 2022. Undernutrition is a key risk factor for TB and was linked to an estimated 2.2 million TB episodes in 2022, as outlined in the World Health Organization (WHO) Global Tuberculosis Report. Objectives To determine the prognostic value of undernutrition in the general population of adults, adolescents, and children for predicting tuberculosis disease over any time period. Search methods We searched the literature databases MEDLINE (via PubMed) and WHO Global Index Medicus, as well as the WHO International Clinical Trials Registry Platform (ICTRP) on 3 May 2023 (date of last search for all databases). We placed no restrictions on the language of publication. Selection criteria We included retrospective and prospective cohort studies, irrespective of publication status or language. The target population comprised adults, adolescents, and children from diverse settings, encompassing outpatient and inpatient cohorts, with varying comorbidities and risk of exposure to tuberculosis. Data collection and analysis We used standard Cochrane methodology and the Quality In Prognosis Studies (QUIPS) tool to assess the risk of bias of the studies. Prognostic factors included undernutrition, defined as wasting, stunting, and underweight, with specific measures such as body mass index (BMI) less than two standard deviations below the median for children and adolescents and low BMI scores (< 18.5) for adults and adolescents. Prognostication occurred at enrolment/baseline. The primary outcome was the incidence of TB disease. The secondary outcome was recurrent TB disease. We performed a random‐effects meta‐analysis for the adjusted hazard ratios (HR), risk ratios (RR), or odds ratios (OR), employing the restricted maximum likelihood estimation. We rated the certainty of the evidence using the GRADE approach. Main results We included 51 cohort studies with over 27 million participants from the six WHO regions. Sixteen large population‐based studies were conducted in China, Singapore, South Korea, and the USA, and 25 studies focused on people living with HIV, which were mainly conducted in the African region. Most studies were in adults, four in children, and three in children and adults. Undernutrition as an exposure was usually defined according to standard criteria; however, the diagnosis of TB did not include a confirmatory culture or molecular diagnosis using a WHO‐approved rapid diagnostic test in eight studies. The median follow‐up time was 3.5 years, and the studies primarily reported an adjusted hazard ratio from a multivariable Cox‐proportional hazard model. Hazard ratios (HR) The HR estimates represent the highest certainty of the evidence, explored through sensitivity analyses and excluding studies at high risk of bias. We present 95% confidence intervals (CI) and prediction intervals, which present between‐study heterogeneity represented in a measurement of the variability of effect sizes (i.e. the interval within which the effect size of a new study would fall considering the same population of studies included in the meta‐analysis). Undernutrition may increase the risk of TB disease (HR 2.23, 95% CI 1.83 to 2.72; prediction interval 0.98 to 5.05; 23 studies; 2,883,266 participants). The certainty of the evidence is low due to a moderate risk of bias across studies and inconsistency. When stratified by follow‐up time, the results are more consistent across < 10 years follow‐up (HR 2.02, 95% CI 1.74 to 2.34; prediction interval 1.20 to 3.39; 22 studies; 2,869,077 participants). This results in a moderate certainty of evidence due to a moderate risk of bias across studies. However, at 10 or more years of follow‐up, we found only one study with a wider CI and higher HR (HR 12.43, 95% CI 5.74 to 26.91; 14,189 participants). The certainty of the evidence is low due to the moderate risk of bias and indirectness. Odds ratio (OR) Undernutrition may increase the odds of TB disease, but the results are uncertain (OR 1.56, 95% CI 1.13 to 2.17; prediction interval 0.61 to 3.99; 8 studies; 173,497 participants). Stratification by follow‐up was not possible as all studies had a follow‐up of < 10 years. The certainty of the evidence is very low due to the high risk of bias and inconsistency. Contour‐enhanced funnel plots were not reported due to the few studies included. Risk ratio (RR) Undernutrition may increase the risk of TB disease (RR 1.95, 95% CI 1.72 to 2.20; prediction interval 1.49 to 2.55; 4 studies; 1,475,867 participants). Stratification by follow‐up was not possible as all studies had a follow‐up of < 10 years. The certainty of the evidence is low due to the high risk of bias. Contour‐enhanced funnel plots were not reported due to the few studies included. Authors' conclusions Undernutrition probably increases the risk of TB two‐fold in the short term (< 10 years) and may also increase the risk in the long term (> 10 years). Policies targeted towards the reduction of the burden of undernutrition are not only needed to alleviate human suffering due to undernutrition and its many adverse consequences, but are also an important part of the critical measures for ending the TB epidemic by 2030. Large population‐based cohorts, including those derived from high‐quality national registries of exposures (undernutrition) and outcomes (TB disease), are needed to provide high‐certainty estimates of this risk across different settings and populations, including low and middle‐income countries from different WHO regions. Moreover, studies including children and adolescents and state‐of‐the‐art methods for diagnosing TB would provide more up‐to‐date information relevant to practice and policy. Funding World Health Organization (203256442). Registration PROSPERO registration:  CRD42023408807 Protocol:   https://doi.org/10.1002/14651858.CD015890 Plain language summary Does undernutrition increase the risk of tuberculosis? Key messages Having undernutrition doubles the risk of getting tuberculosis. These results apply to the general adult population; the risk for other groups, such as adolescents and children, is uncertain. What is tuberculosis? Tuberculosis (TB) is an infection caused by a bacteria ( Mycobacterium tuberculosis ). It primarily affects the lungs but can also affect other sites in the body. TB spreads through the air when an infected person coughs or sneezes. Some people do not develop symptoms after infection, while others progress with the disease and develop symptoms, such as persistent cough, which may be bloody, weight loss, fever, and night sweats. After a TB diagnosis, this is called  TB disease . The diagnosis of TB disease usually involves a molecular test or culture that detects the bacteria in a bodily secretion (such as sputum) after consultation with a doctor. TB is a major health concern globally; over 10 million people develop TB, and over 1.5 million die from the disease every year. What is undernutition? Undernutrition is a condition where the body does not get enough essential nutrients, affecting a person's health. Body mass index (BMI) is a measure that takes into account both weight and height. It helps identify if someone is underweight, which is one of the definitions of undernutrition. For children, we use ""weight‐for‐height"" to assess growth. It compares a child's weight to what is typical for their height, helping to gauge if they are thriving or facing nutritional challenges. What did we want to find out? We wanted to estimate the risk of getting TB disease for people with undernutrition compared to those without undernutrition. What did we do? We looked at studies that included people with and without undernutrition over time and compared how frequently each group developed tuberculosis. What did we find? We included 51 studies with over 27 million participants from the six World Health Organization (WHO) regions. Sixteen large population‐based studies were conducted in China, Singapore, South Korea, and the USA, and 25 studies focused on people living with HIV, which were mainly conducted in the African region. Most studies were in adults, four in children, and three in children and adults combined. On average, the studies followed people for 3.5 years. We found that having undernutrition doubled the risk of getting sick due to tuberculosis compared to those who had not. What are the limitations of the evidence? Many of the studies had some limitations. One problem was that many of them used sputum microscopy to diagnose TB in people with symptoms, which might have missed some diagnoses. More accurate methods, such as culture or rapid diagnostic tests, currently exist and would miss less TB. How up‐to‐date is this review? This evidence is up‐to‐date as of 3 May 2023.","6","John Wiley & Sons, Ltd","1465-1858","*Malnutrition [complications, epidemiology]; *Tuberculosis [epidemiology]; Adolescent; Adult; Child; Humans; Prognosis; Prospective Studies; Retrospective Studies; Risk Factors","10.1002/14651858.CD015890.pub2","http://dx.doi.org/10.1002/14651858.CD015890.pub2","Central Editorial Service"
"CD013606.PUB2","Reeve, K; On, BI; Havla, J; Burns, J; Gosteli-Peter, MA; Alabsawi, A; Alayash, Z; Götschi, A; Seibold, H; Mansmann, U; Held, U","Prognostic models for predicting clinical disease progression, worsening and activity in people with multiple sclerosis","Cochrane Database of Systematic Reviews","2023","Abstract - Background Multiple sclerosis (MS) is a chronic inflammatory disease of the central nervous system that affects millions of people worldwide. The disease course varies greatly across individuals and many disease‐modifying treatments with different safety and efficacy profiles have been developed recently. Prognostic models evaluated and shown to be valid in different settings have the potential to support people with MS and their physicians during the decision‐making process for treatment or disease/life management, allow stratified and more precise interpretation of interventional trials, and provide insights into disease mechanisms. Many researchers have turned to prognostic models to help predict clinical outcomes in people with MS; however, to our knowledge, no widely accepted prognostic model for MS is being used in clinical practice yet. Objectives To identify and summarise multivariable prognostic models, and their validation studies for quantifying the risk of clinical disease progression, worsening, and activity in adults with MS. Search methods We searched MEDLINE, Embase, and the Cochrane Database of Systematic Reviews from January 1996 until July 2021. We also screened the reference lists of included studies and relevant reviews, and references citing the included studies. Selection criteria We included all statistically developed multivariable prognostic models aiming to predict clinical disease progression, worsening, and activity, as measured by disability, relapse, conversion to definite MS, conversion to progressive MS, or a composite of these in adult individuals with MS. We also included any studies evaluating the performance of (i.e. validating) these models. There were no restrictions based on language, data source, timing of prognostication, or timing of outcome. Data collection and analysis Pairs of review authors independently screened titles/abstracts and full texts, extracted data using a piloted form based on the Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies (CHARMS), assessed risk of bias using the Prediction Model Risk Of Bias Assessment Tool (PROBAST), and assessed reporting deficiencies based on the checklist items in Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD). The characteristics of the included models and their validations are described narratively. We planned to meta‐analyse the discrimination and calibration of models with at least three external validations outside the model development study but no model met this criterion. We summarised between‐study heterogeneity narratively but again could not perform the planned meta‐regression. Main results We included 57 studies, from which we identified 75 model developments, 15 external validations corresponding to only 12 (16%) of the models, and six author‐reported validations. Only two models were externally validated multiple times. None of the identified external validations were performed by researchers independent of those that developed the model. The outcome was related to disease progression in 39 (41%), relapses in 8 (8%), conversion to definite MS in 17 (18%), and conversion to progressive MS in 27 (28%) of the 96 models or validations. The disease and treatment‐related characteristics of included participants, and definitions of considered predictors and outcome, were highly heterogeneous amongst the studies. Based on the publication year, we observed an increase in the percent of participants on treatment, diversification of the diagnostic criteria used, an increase in consideration of biomarkers or treatment as predictors, and increased use of machine learning methods over time. Usability and reproducibility All identified models contained at least one predictor requiring the skills of a medical specialist for measurement or assessment. Most of the models (44; 59%) contained predictors that require specialist equipment likely to be absent from primary care or standard hospital settings. Over half (52%) of the developed models were not accompanied by model coefficients, tools, or instructions, which hinders their application, independent validation or reproduction. The data used in model developments were made publicly available or reported to be available on request only in a few studies (two and six, respectively). Risk of bias We rated all but one of the model developments or validations as having high overall risk of bias. The main reason for this was the statistical methods used for the development or evaluation of prognostic models; we rated all but two of the included model developments or validations as having high risk of bias in the analysis domain. None of the model developments that were externally validated or these models' external validations had low risk of bias. There were concerns related to applicability of the models to our research question in over one‐third (38%) of the models or their validations. Reporting deficiencies Reporting was poor overall and there was no observable increase in the quality of reporting over time. The items that were unclearly reported or not reported at all for most of the included models or validations were related to sample size justification, blinding of outcome assessors, details of the full model or how to obtain predictions from it, amount of missing data, and treatments received by the participants. Reporting of preferred model performance measures of discrimination and calibration was suboptimal. Authors' conclusions The current evidence is not sufficient for recommending the use of any of the published prognostic prediction models for people with MS in clinical routine today due to lack of independent external validations. The MS prognostic research community should adhere to the current reporting and methodological guidelines and conduct many more state‐of‐the‐art external validation studies for the existing or newly developed models. Plain language summary Which models exist for prediction of future disease outcomes in people with multiple sclerosis? Why is it important to study multiple sclerosis? Multiple sclerosis (MS) is a chronic disease of the brain, spine, and nerves. Millions of people worldwide suffer from this disease, but the disease and how it progresses can be very different from person to person. Although MS cannot be cured, different treatments are available that can help reduce symptoms and slow the worsening of the disease. These treatments work differently, with some having more severe side effects than others. Understanding the severity of an individual’s MS is important to patients and medical professionals. Why are prognostic models important in the context of multiple sclerosis? Prognostic models help patients and medical professionals understand how sick an individual is and will become. This understanding can support patients during life and treatment choices. Prognostic models can also help medical professionals make decisions about how to best treat an individual, better understand the disease, or to develop treatments. Prognostic models for MS might involve combining a range of different pieces of information about an individual to predict how their MS will continue to develop. Important pieces of information to include in a prognostic model could be, for example, information on personal characteristics (such as age, sex, body mass index), information on their behaviour (such as whether they smoke), and information about their MS (such as how long they have had the disease). Other clinical features or measurements may also be important. What did we want to find out? We wanted to search for and find all prognostic models that combine multiple pieces of information to predict how MS will continue to develop and worsen in adults. What did we do? We used different techniques to search for all studies that described prognostic models, which combine multiple pieces of information, developed in the context of MS. We were interested in studies showing how these prognostic models were developed, as well as studies evaluating how well they actually work in practice. Once we found all relevant studies, we summarised them and evaluated how well they reported their results and how well they were conducted. What did we find? We found 57 studies that described prognostic models combining multiple pieces of information to predict how MS will continue to develop and worsen in adults. These studies described the development of 75 different prognostic models. There were 15 instances in which the performance of specific prognostic models was evaluated. We found that prognostic models focus on different outcomes; 41% looked at disease progression, 8% at relapses, 18% at moving from a first attack to definite MS, and 28% at moving from the early stages of MS to progressive MS. The prognostic models we found were very different from one another in many ways. The patients they used to develop the models, for example, were very different in terms of treatments. In addition, the pieces of information they used to predict the course of MS were very different from one another. We found that prognostic models have changed over time regarding the diagnosis of MS and increase in use of treatment, the information observed with new techniques, or new modelling approaches. We also found that using these prognostic models requires information about the individual that would require a medical specialist and often specialist equipment, both of which may not be available in many clinics and hospitals. What are the limitations of the evidence? We found problems with most studies, meaning that we may not be able to trust their results. Common problems involved data and statistical methods used across studies. Additionally, many of the studies report results that may be very different if the prognostic models are applied to a new set of people with MS. We also found that the studies did a poor job of describing their methods and reporting their findings. What does this mean? The studies we found show that the evidence on prognostic models for predicting how MS will continue to develop and worsen in adults is not yet well‐developed. New research is needed that focusses on using methods recommended in guidelines to develop prognostic models and evaluate their performance. This research should also focus on describing their methods and results well, so that other researchers and medical professionals can use them for research and clinical practice.","9","John Wiley & Sons, Ltd","1465-1858","*Multiple Sclerosis; Adult; Disease Progression; Humans; Prognosis; Reproducibility of Results; Systematic Reviews as Topic","10.1002/14651858.CD013606.pub2","http://dx.doi.org/10.1002/14651858.CD013606.pub2","Multiple Sclerosis and Rare Diseases of the CNS"
"CD014885.PUB2","Mohanannair Geethadevi, G; Quinn, TJ; George, J; Anstey, KJ; Bell, JS; Sarwar, MR; Cross, AJ","Multi‐domain prognostic models used in middle‐aged adults without known cognitive impairment for predicting subsequent dementia","Cochrane Database of Systematic Reviews","2023","Abstract - Background Dementia, a global health priority, has no current cure. Around 50 million people worldwide currently live with dementia, and this number is expected to treble by 2050. Some health conditions and lifestyle behaviours can increase or decrease the risk of dementia and are known as 'predictors' .  Prognostic models combine such predictors to measure the risk of future dementia. Models that can accurately predict future dementia would help clinicians select high‐risk adults in middle age and implement targeted risk reduction. Objectives Our primary objective was to identify multi‐domain prognostic models used in middle‐aged adults (aged 45 to 65 years) for predicting dementia or cognitive impairment. Eligible multi‐domain prognostic models involved two or more of the modifiable dementia predictors identified in a 2020 Lancet Commission report and a 2019 World Health Organization (WHO) report (less education, hearing loss, traumatic brain injury, hypertension, excessive alcohol intake, obesity, smoking, depression, social isolation, physical inactivity, diabetes mellitus, air pollution, poor diet, and cognitive inactivity). Our secondary objectives were to summarise the prognostic models, to appraise their predictive accuracy (discrimination and calibration) as reported in the development and validation studies, and to identify the implications of using dementia prognostic models for the management of people at a higher risk for future dementia. Search methods We searched MEDLINE, Embase, PsycINFO, CINAHL, and ISI Web of Science Core Collection from inception until 6 June 2022. We performed forwards and backwards citation tracking of included studies using the Web of Science platform.  Selection criteria We included development and validation studies of multi‐domain prognostic models. The minimum eligible follow‐up was five years. Our primary outcome was an incident clinical diagnosis of dementia based on validated diagnostic criteria, and our secondary outcome was dementia or cognitive impairment determined by any other method. Data collection and analysis Two review authors independently screened the references, extracted data using a template based on the CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS), and assessed risk of bias and applicability of included studies using the Prediction model Risk Of Bias ASsessment Tool (PROBAST). We synthesised the C‐statistics of models that had been externally validated in at least three comparable studies.  Main results We identified 20 eligible studies; eight were development studies and 12 were validation studies. There were 14 unique prognostic models: seven models with validation studies and seven models with development‐only studies. The models included a median of nine predictors (range 6 to 34); the median number of modifiable predictors was five (range 2 to 11). The most common modifiable predictors in externally validated models were diabetes, hypertension, smoking, physical activity, and obesity. In development‐only models, the most common modifiable predictors were obesity, diabetes, hypertension, and smoking. No models included hearing loss or air pollution as predictors. Nineteen studies had a high risk of bias according to the PROBAST assessment, mainly because of inappropriate analysis methods, particularly lack of reported calibration measures. Applicability concerns were low for 12 studies, as their population, predictors, and outcomes were consistent with those of interest for this review. Applicability concerns were high for nine studies, as they lacked baseline cognitive screening or excluded an age group within the range of 45 to 65 years. Only one model, Cardiovascular Risk Factors, Ageing, and Dementia (CAIDE), had been externally validated in multiple studies, allowing for meta‐analysis. The CAIDE model included eight predictors (four modifiable predictors): age, education, sex, systolic blood pressure, body mass index (BMI), total cholesterol, physical activity and APOEƐ4 status. Overall, our confidence in the prediction accuracy of CAIDE was very low; our main reasons for downgrading the certainty of the evidence were high risk of bias across all the studies, high concern of applicability, non‐overlapping confidence intervals (CIs), and a high degree of heterogeneity. The summary C‐statistic was 0.71 (95% CI 0.66 to 0.76; 3 studies; very low‐certainty evidence) for the incident clinical diagnosis of dementia, and 0.67 (95% CI 0.61 to 0.73; 3 studies; very low‐certainty evidence) for dementia or cognitive impairment based on cognitive scores. Meta‐analysis of calibration measures was not possible, as few studies provided these data. Authors' conclusions We identified 14 unique multi‐domain prognostic models used in middle‐aged adults for predicting subsequent dementia. Diabetes, hypertension, obesity, and smoking were the most common modifiable risk factors used as predictors in the models. We performed meta‐analyses of C‐statistics for one model (CAIDE), but the summary values were unreliable. Owing to lack of data, we were unable to meta‐analyse the calibration measures of CAIDE. This review highlights the need for further robust external validations of multi‐domain prognostic models for predicting future risk of dementia in middle‐aged adults. Plain language summary What tools exist to assess the presence of multiple risk factors for dementia in middle‐aged people, and can they correctly predict future dementia? Key messages • We found 14 tools used in middle‐aged people to predict future dementia.  • Seven studies tested a prediction tool named Cardiovascular Risk Factors, Ageing, and Dementia (CAIDE).  • The benefits of using these tools to predict dementia later in life are unclear, because the studies provided little high‐quality evidence. What is dementia?  Dementia refers to a group of brain conditions that commonly affect older people and lead to progressive problems with memory, problem‐solving, or performing everyday activities. People with certain health conditions or behaviours in middle age – such as high blood pressure, excessive alcohol intake, smoking, depression, low levels of exercise, or poor diet – have a higher chance of developing dementia in later life. We classify these health conditions or behaviours as 'modifiable risk factors' for dementia, because measures such as lifestyle changes can reduce them. What are prediction tools?  To develop prediction tools, researchers observe a group of people over years to see how many with such risk factors develop dementia. The tools assign a higher risk score to people who have a higher chance of getting dementia later in life, based on the presence or absence of risk factors in middle age. Why do we use tools that assess risk factors to predict future dementia? Currently, about 50 million people across the world have dementia, and without adequate preventive measures, that number is expected to triple by 2050. If we control risk factors in middle age, we may avert or delay the future development of dementia or reduce dementia severity. Preventive tools help select people who are best suited to lifestyle modification programmes aimed at regulating risk factors.  What did we want to find out? We wanted to find out what tools are available for middle‐aged adults (aged 45 to 65 years), and how well they predict dementia later in life (at least five years after the initial assessment). We looked for tools that included risk factors widely accepted to be linked to dementia onset. What did we do?  We searched for studies that evaluated tools used in middle‐aged adults to identify those at high risk of dementia later in life. We investigated how well these tools predicted future dementia based on an accuracy value. If the accuracy value is more than a recommended standard of 0.75, we can say that the tool is accurate at predicting future dementia. It is also important to establish that a tool developed in one group of people (in the original development study) can accurately predict dementia in another group of people (in validation studies); only then can it be applied in routine healthcare practice. We compared and summarised the results of the studies. What did we find? We found 20 studies that described 14 different tools for dementia prediction. The tools included between two and 11 modifiable risk factors for dementia. Seven of the tools featured in two or more studies and were considered validated. Seven studies used a tool called Cardiovascular Risk Factors, Ageing, and Dementia (CAIDE). The CAIDE tool included current measures of a person's blood pressure, weight and height, cholesterol level, and frequency of exercise to predict future dementia. The combined accuracy value across the studies was 0.71, not high enough for us to consider CAIDE a reliable tool for predicting future dementia.  What are the limitations of the evidence? Half (seven) of the tools were used in a single study, so we were unable to measure how well they predicted future dementia. Most studies provided too little information for us to assess accuracy values. How up to date is this evidence? The evidence is up‐to‐date to June 2022.","6","John Wiley & Sons, Ltd","1465-1858","*Cognitive Dysfunction [complications, diagnosis]; *Dementia [complications, etiology]; *Hypertension [complications]; Humans; Middle Aged; Obesity [complications]; Prognosis","10.1002/14651858.CD014885.pub2","http://dx.doi.org/10.1002/14651858.CD014885.pub2","Dementia and Cognitive Improvement"
"CD015201","Tadayon Najafabadi, B; Rayner, DG; Shokraee, K; Shokraie, K; Panahi, P; Rastgou, P; Seirafianpour, F; Momeni Landi, F; Alinia, P; Parnianfard, N; Hemmati, N; Banivaheb, B; Radmanesh, R; Alvand, S; Shahbazi, P; Dehghanbanadaki, H; Shaker, E; Same, K; Mohammadi, E; Malik, A; Srivastava, A; Nejat, P; Tamara, A; Chi, Y; Yuan, Y; Hajizadeh, N; Chan, C; Zhen, J; Tahapary, D; Anderson, L; Apatu, E; Schoonees, A; Naude, CE; Thabane, L; Foroutan, F","Obesity as an independent risk factor for COVID‐19 severity and mortality","Cochrane Database of Systematic Reviews","2023","Abstract - Background Since December 2019, the world has struggled with the COVID‐19 pandemic. Even after the introduction of various vaccines, this disease still takes a considerable toll. In order to improve the optimal allocation of resources and communication of prognosis, healthcare providers and patients need an accurate understanding of factors (such as obesity) that are associated with a higher risk of adverse outcomes from the COVID‐19 infection. Objectives To evaluate obesity as an independent prognostic factor for COVID‐19 severity and mortality among adult patients in whom infection with the COVID‐19 virus is confirmed. Search methods MEDLINE, Embase, two COVID‐19 reference collections, and four Chinese biomedical databases were searched up to April 2021. Selection criteria We included case‐control, case‐series, prospective and retrospective cohort studies, and secondary analyses of randomised controlled trials if they evaluated associations between obesity and COVID‐19 adverse outcomes including mortality, mechanical ventilation, intensive care unit (ICU) admission, hospitalisation, severe COVID, and COVID pneumonia. Given our interest in ascertaining the independent association between obesity and these outcomes, we selected studies that adjusted for at least one factor other than obesity. Studies were evaluated for inclusion by two independent reviewers working in duplicate.  Data collection and analysis Using standardised data extraction forms, we extracted relevant information from the included studies. When appropriate, we pooled the estimates of association across studies with the use of random‐effects meta‐analyses. The Quality in Prognostic Studies (QUIPS) tool provided the platform for assessing the risk of bias across each included study. In our main comparison, we conducted meta‐analyses for each obesity class separately. We also meta‐analysed unclassified obesity and obesity as a continuous variable (5 kg/m 2  increase in BMI (body mass index)). We used the GRADE framework to rate our certainty in the importance of the association observed between obesity and each outcome. As obesity is closely associated with other comorbidities, we decided to prespecify the minimum adjustment set of variables including age, sex, diabetes, hypertension, and cardiovascular disease for subgroup analysis.  Main results We identified 171 studies, 149 of which were included in meta‐analyses.  As compared to 'normal' BMI (18.5 to 24.9 kg/m 2 ) or patients without obesity, those with obesity classes I (BMI 30 to 35 kg/m 2 ), and II (BMI 35 to 40 kg/m 2 ) were not at increased odds for mortality (Class I: odds ratio [OR] 1.04, 95% confidence interval [CI] 0.94 to 1.16, high certainty (15 studies, 335,209 participants); Class II: OR 1.16, 95% CI 0.99 to 1.36, high certainty (11 studies, 317,925 participants)). However, those with class III obesity (BMI 40 kg/m 2  and above) may be at increased odds for mortality (Class III: OR 1.67, 95% CI 1.39 to 2.00, low certainty, (19 studies, 354,967 participants)) compared to normal BMI or patients without obesity. For mechanical ventilation, we observed increasing odds with higher classes of obesity in comparison to normal BMI or patients without obesity (class I: OR 1.38, 95% CI 1.20 to 1.59, 10 studies, 187,895 participants, moderate certainty; class II: OR 1.67, 95% CI 1.42 to 1.96, 6 studies, 171,149 participants, high certainty; class III: OR 2.17, 95% CI 1.59 to 2.97, 12 studies, 174,520 participants, high certainty). However, we did not observe a dose‐response relationship across increasing obesity classifications for ICU admission and hospitalisation. Authors' conclusions Our findings suggest that obesity is an important independent prognostic factor in the setting of COVID‐19. Consideration of obesity may inform the optimal management and allocation of limited resources in the care of COVID‐19 patients. Plain language summary Obesity and adverse COVID‐19 outcomes What are the effects of obesity on COVID‐19 outcomes? Key messages • There is enough evidence to support the finding that extreme obesity (BMI > 40 kg/m 2 ) increases the chance of a person dying, requiring a breathing tube, being hospitalised, and being admitted to the ICU due to COVID‐19.  • Obesity in general will result in a person requiring a breathing tube. • The higher one's BMI gets, the higher the chance that a person will suffer from severe COVID‐19 disease.   What is obesity? Obesity is defined as abnormal or excessive fat accumulation in different parts of the human body and it presents a risk to health. To assess obesity, different indices such as body mass index (BMI) can be used, which is one's weight in kilograms divided by the square of height in metres. The WHO has classified obesity into three classes. According to this classification, class I obesity includes a BMI ranging from 30 to 35 kg/m 2 , class II from 35 to 40 kg/m 2 , and class III from 40 kg/m 2  and more. What did we want to find out? We wanted to find out whether obesity has any effects on mortality, requiring a breathing tube, hospitalisation, ICU admission, severe disease or pneumonia due to COVID‐19 disease.   What did we do? We conducted a systematic search in medical databases for evidence looking at the association of obesity and mortality and other outcomes from December 2019 to April 2021. We then categorised and rated these findings based on our confidence in the evidence, study size, and quality. What did we find? We identified 171 eligible studies, with 149 studies (12,045,976 participants) providing quantitative data for at least one of our meta‐analyses. In terms of the outcomes, 111 studies reported on mortality, 48 on requiring a breathing tube, 47 on ICU admission, 34 on hospitalisation, 32 on severe COVID‐19, six on pneumonia, five on length of hospitalisation, two on length of ICU admission, and one on the duration of the requirement of a breathing tube.  Main results Our findings indicate that there is a high certainty of evidence that class III obesity is associated with an increased risk of mortality among COVID‐19 patients. However, we found that, in mild cases of obesity (classes I and II), this factor might not be independently associated with increased risk of mortality in COVID‐19 patients. Similarly, we are very certain that obesity is an independent important factor associated with the risk of requiring a breathing tube in COVID‐19 patients. However, the effect estimate sizes were not consistent with a dose‐response relationship across increasing obesity classes for ICU admission, hospitalisation, severe COVID‐19 disease and pneumonia. To conclude, this review investigated the potential association between obesity and adverse COVID‐19 outcomes. We were able to gather evidence from multiple studies and concluded that the association of obesity with mortality and requiring a breathing tube is of high certainty. What are the limitations of the evidence? Although BMI is a widely used measurement, the relationship between BMI and body fat is non‐linear. Moreover, our review did not discriminate against self‐reported and measured BMI. Finally, we were unable to keep up with the rapid pace of publications on COVID‐19 despite our best efforts.  How up‐to‐date is the evidence?  The evidence is up‐to‐date to April 2021.","5","John Wiley & Sons, Ltd","1465-1858","*COVID-19; *Pandemics; Adult; Humans; Obesity; Prospective Studies; Retrospective Studies; Risk Factors","10.1002/14651858.CD015201","http://dx.doi.org/10.1002/14651858.CD015201","Metabolic and Endocrine Disorders"
"CD013775.PUB2","Perais, J; Agarwal, R; Evans, JR; Loveman, E; Colquitt, JL; Owens, D; Hogg, RE; Lawrenson, JG; Takwoingi, Y; Lois, N","Prognostic factors for the development and progression of proliferative diabetic retinopathy in people with diabetic retinopathy","Cochrane Database of Systematic Reviews","2023","Abstract - Background Diabetic retinopathy (DR) is characterised by neurovascular degeneration as a result of chronic hyperglycaemia. Proliferative diabetic retinopathy (PDR) is the most serious complication of DR and can lead to total (central and peripheral) visual loss. PDR is characterised by the presence of abnormal new blood vessels, so‐called “new vessels,” at the optic disc (NVD) or elsewhere in the retina (NVE). PDR can progress to high‐risk characteristics (HRC) PDR (HRC‐PDR), which is defined by the presence of NVD more than one‐fourth to one‐third disc area in size plus vitreous haemorrhage or pre‐retinal haemorrhage, or vitreous haemorrhage or pre‐retinal haemorrhage obscuring more than one disc area. In severe cases, fibrovascular membranes grow over the retinal surface and tractional retinal detachment with sight loss can occur, despite treatment. Although most, if not all, individuals with diabetes will develop DR if they live long enough, only some progress to the sight‐threatening PDR stage.  Objectives To determine risk factors for the development of PDR and HRC‐PDR in people with diabetes and DR. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL; which contains the Cochrane Eyes and Vision Trials Register; 2022, Issue 5), Ovid MEDLINE, and Ovid Embase. The date of the search was 27 May 2022. Additionally, the search was supplemented by screening reference lists of eligible articles. There were no restrictions to language or year of publication.  Selection criteria We included prospective or retrospective cohort studies and case‐control longitudinal studies evaluating prognostic factors for the development and progression of PDR, in people who have not had previous treatment for DR. The target population consisted of adults (≥18 years of age) of any gender, sexual orientation, ethnicity, socioeconomic status, and geographical location, with non‐proliferative diabetic retinopathy (NPDR) or PDR with less than HRC‐PDR, diagnosed as per standard clinical practice. Two review authors independently screened titles and abstracts, and full‐text articles, to determine eligibility; discrepancies were resolved through discussion. We considered prognostic factors measured at baseline and any other time points during the study and in any clinical setting. Outcomes were evaluated at three and eight years (± two years) or lifelong.  Data collection and analysis Two review authors independently extracted data from included studies using a data extraction form that we developed and piloted prior to the data collection stage. We resolved any discrepancies through discussion. We used the Quality in Prognosis Studies (QUIPS) tool to assess risk of bias. We conducted meta‐analyses in clinically relevant groups using a random‐effects approach. We reported hazard ratios (HR), odds ratios (OR), and risk ratios (RR) separately for each available prognostic factor and outcome, stratified by different time points. Where possible, we meta‐analysed adjusted prognostic factors. We evaluated the certainty of the evidence with an adapted version of the GRADE framework.   Main results We screened 6391 records. From these, we identified 59 studies (87 articles) as eligible for inclusion. Thirty‐five were prospective cohort studies, 22 were retrospective studies, 18 of which were cohort and six were based on data from electronic registers, and two were retrospective case‐control studies. Twenty‐three studies evaluated participants with type 1 diabetes (T1D), 19 with type 2 diabetes (T2D), and 17 included mixed populations (T1D and T2D). Studies on T1D included between 39 and 3250 participants at baseline, followed up for one to 45 years. Studies on T2D included between 100 and 71,817 participants at baseline, followed up for one to 20 years. The studies on mixed populations of T1D and T2D ranged from 76 to 32,553 participants at baseline, followed up for four to 25 years.  We found evidence indicating that higher glycated haemoglobin (haemoglobin A1c (HbA1c)) levels (adjusted OR ranged from 1.11 (95% confidence interval (CI) 0.93 to 1.32) to 2.10 (95% CI 1.64 to 2.69) and more advanced stages of retinopathy (adjusted OR ranged from 1.38 (95% CI 1.29 to 1.48) to 12.40 (95% CI 5.31 to 28.98) are independent risk factors for the development of PDR in people with T1D and T2D. We rated the evidence for these factors as of moderate certainty because of moderate to high risk of bias in the studies.  There was also some evidence suggesting several markers for renal disease (for example, nephropathy (adjusted OR ranged from 1.58 (95% CI not reported) to 2.68 (2.09 to 3.42), and creatinine (adjusted meta‐analysis HR 1.61 (95% CI 0.77 to 3.36)), and, in people with T1D, age at diagnosis of diabetes (< 12 years of age) (standardised regression estimate 1.62, 95% CI 1.06 to 2.48), increased triglyceride levels (adjusted RR 1.55, 95% CI 1.06 to 1.95), and larger retinal venular diameters (RR 4.28, 95% CI 1.50 to 12.19) may increase the risk of progression to PDR. The certainty of evidence for these factors, however, was low to very low, due to risk of bias in the included studies, inconsistency (lack of studies preventing the grading of consistency or variable outcomes), and imprecision (wide CIs). There was no substantial and consistent evidence to support duration of diabetes, systolic or diastolic blood pressure, total cholesterol, low‐ (LDL) and high‐ (HDL) density lipoproteins, gender, ethnicity, body mass index (BMI), socioeconomic status, or tobacco and alcohol consumption as being associated with incidence of PDR. There was insufficient evidence to evaluate prognostic factors associated with progression of PDR to HRC‐PDR.  Authors' conclusions Increased HbA1c is likely to be associated with progression to PDR; therefore, maintaining adequate glucose control throughout life, irrespective of stage of DR severity, may help to prevent progression to PDR and risk of its sight‐threatening complications. Renal impairment in people with T1D or T2D, as well as younger age at diagnosis of diabetes mellitus (DM), increased triglyceride levels, and increased retinal venular diameters in people with T1D may also be associated with increased risk of progression to PDR. Given that more advanced DR severity is associated with higher risk of progression to PDR, the earlier the disease is identified, and the above systemic risk factors are controlled, the greater the chance of reducing the risk of PDR and saving sight. Plain language summary Risk factors for the development and progression of proliferative diabetic retinopathy (a diabetes complication affecting eyes) Review question We wanted to find out which factors may increase or reduce the chance that people with diabetes develop proliferative diabetic retinopathy and high‐risk proliferative diabetic retinopathy, both sight‐threatening complications of diabetes. Background In diabetes, over time, raised blood sugar levels damage fine blood vessels in the retina, the layer at the back of the eye that gives people sight. This is called ‘diabetic retinopathy’. In some people with diabetes and diabetic retinopathy, abnormal and fragile blood vessels grow in the retina: so‐called 'new vessels'. When new vessels are present, we say there is 'proliferative diabetic retinopathy', also called 'PDR'. These new vessels are weak and can bleed inside the eye, causing what is known as a 'vitreous haemorrhage'. The blood inside the eye takes away the vision, although, if it clears on its own (which sometimes happens) or with surgery, vision most often recovers. Scar tissue can also grow over the new vessels. Scarring can pull on the retina and cause what is known as a tractional retinal detachment, the most severe sight‐threatening complication of diabetic retinopathy. Tractional retinal detachment can cause total blindness if not treated with surgery promptly. While most people with diabetes develop diabetic retinopathy, only a few progress to these severe complications. It is unclear why this is the case. Sight loss is usually preventable if treatment is done early. Therefore, it is essential to know who is at risk of progressing to PDR, so that these people can be followed closely and treated in a timely way. We did this review to find out the risk factors which may determine why some people develop PDR.  Study characteristics We included studies in which people with diabetes, who had never been treated for diabetic retinopathy, were followed up over time to determine who developed PDR and who progressed to severe stages of PDR (called 'high‐risk characteristics PDR' (HRC‐PDR)). To be included in our review, these studies had to investigate risk factors for PDR and HRC‐PDR: for example, blood sugar, blood pressure, cholesterol, and kidney disease, amongst others. We included studies looking at adults (18 years of age and older) of any gender, ethnicity, sexual orientation, socioeconomic status, and nation, written in any language, in this review. Key results Of the 6391 articles we found, 59 studies (87 articles) were eligible, and we included them in our review. We found that higher blood sugar (which means poorer diabetes control) and more advanced diabetic retinopathy (more changes from diabetes in the retina) put people at higher risk of having PDR. People with kidney disease seemed also to be at higher risk of progressing to PDR. It is also possible that people with type 1 diabetes who were diagnosed at a young age, and those with higher triglyceride levels (triglycerides are a type of fat in the blood, like cholesterol) or who have retinal veins with larger diameters, are more at risk of developing PDR. Other risk factors studied ‐ for example, duration of diabetes, blood pressure, and cholesterol ‐ did not seem to be risk factors for PDR. There was not enough information from the included studies for us to analyse risk factors for HRC‐PDR.  Authors' conclusions People living with type 1 or type 2 diabetes who have poor blood sugar control are likely to be at increased risk of developing PDR. Evidence suggests that better blood sugar control, even in people who already have the earlier stages of diabetic retinopathy, may help to prevent it from progressing to PDR. Those with kidney disease may also be at increased risk of progressing to PDR. Additionally, people with type 1 diabetes, who were diagnosed at a younger age, or who have higher triglyceride levels or larger retinal veins, may be more susceptible to developing PDR. How up to date is this evidence? The evidence is up to date to 27 May 2022. ","2","John Wiley & Sons, Ltd","1465-1858","*Diabetes Mellitus, Type 1 [complications]; *Diabetes Mellitus, Type 2 [complications]; *Diabetic Retinopathy [complications]; Adult; Female; Glycated Hemoglobin; Humans; Male; Prognosis; Prospective Studies; Retinal Hemorrhage; Retrospective Studies; Triglycerides; Vitreous Hemorrhage [complications]","10.1002/14651858.CD013775.pub2","http://dx.doi.org/10.1002/14651858.CD013775.pub2","Eyes and Vision"
"CD013847.PUB2","Neligan, A; Adan, G; Nevitt, SJ; Pullen, A; Sander, JW; Bonnett, L; Marson, AG","Prognosis of adults and children following a first unprovoked seizure","Cochrane Database of Systematic Reviews","2023","Abstract - Background Epilepsy is clinically defined as two or more unprovoked epileptic seizures more than 24 hours apart. Given that, a diagnosis of epilepsy can be associated with significant morbidity and mortality, it is imperative that clinicians (and people with seizures and their relatives) have access to accurate and reliable prognostic estimates, to guide clinical practice on the risks of developing further unprovoked seizures (and by definition, a diagnosis of epilepsy) following single unprovoked epileptic seizure. Objectives 1. To provide an accurate estimate of the proportion of individuals going on to have further unprovoked seizures at subsequent time points following a single unprovoked epileptic seizure (or cluster of epileptic seizures within a 24‐hour period, or a first episode of status epilepticus), of any seizure type (overall prognosis). 2. To evaluate the mortality rate following a first unprovoked epileptic seizure. Search methods We searched the following databases on 19 September 2019 and again on 30 March 2021, with no language restrictions. The Cochrane Register of Studies (CRS Web), MEDLINE Ovid (1946 to March 29, 2021), SCOPUS (1823 onwards),  ClinicalTrials.gov , the World Health Organization (WHO) International Clinical Trials Registry Platform ( ICTRP ). CRS Web includes randomized or quasi‐randomized, controlled trials from PubMed, Embase, ClinicalTrials.gov, the World Health Organization International Clinical Trials Registry Platform (ICTRP), the Cochrane Central Register of Controlled Trials (CENTRAL), and the Specialized Registers of Cochrane Review Groups including Epilepsy. In MEDLINE (Ovid) the coverage end date always lags a few days behind the search date. Selection criteria We included studies, both retrospective and prospective, of all age groups (except those in the neonatal period (< 1 month of age)), of people with a single unprovoked seizure, followed up for a minimum of six months, with no upper limit of follow‐up, with the study end point being seizure recurrence, death, or loss to follow‐up. To be included, studies must have included at least 30 participants. We excluded studies that involved people with seizures that occur as a result of an acute precipitant or provoking factor, or in close temporal proximity to an acute neurological insult, since these are not considered epileptic in aetiology (acute symptomatic seizures). We also excluded people with situational seizures, such as febrile convulsions. Data collection and analysis Two review authors conducted the initial screening of titles and abstracts identified through the electronic searches, and removed non‐relevant articles. We obtained the full‐text articles of all remaining potentially relevant studies, or those whose relevance could not be determined from the abstract alone and two authors independently assessed for eligibility. All disagreements were resolved through discussion with no need to defer to a third review author. We extracted data from included studies using a data extraction form based on the  ch ecklist for critical  a ppraisal and data extraction for systematic  r eviews of prediction  m odelling  s tudies (CHARMS). Two review authors then appraised the included studies, using a standardised approach based on the  qu ality  i n  p rognostic  s tudies (QUIPS) tool, which was adapted for overall prognosis (seizure recurrence). We conducted a meta‐analysis using Review Manager 2014, with a random‐effects generic inverse variance meta‐analysis model, which accounted for any between‐study heterogeneity in the prognostic effect. We then summarised the meta‐analysis by the pooled estimate (the average prognostic factor effect), its 95% confidence interval (CI), the estimates of I² and Tau² (heterogeneity), and a 95% prediction interval for the prognostic effect in a single population at three various time points, 6 months, 12 months and 24 months. Subgroup analysis was performed according to the ages of the cohorts included; studies involving all ages, studies that recruited adult only and those that were purely paediatric. Main results Fifty‐eight studies (involving 54 cohorts), with a total of 12,160 participants (median 147, range 31 to 1443), met the inclusion criteria for the review. Of the 58 studies, 26 studies were paediatric studies, 16 were adult and the remaining 16 studies were a combination of paediatric and adult populations. Most included studies had a cohort study design with two case‐control studies and one nested case‐control study. Thirty‐two studies (29 cohorts) reported a prospective longitudinal design whilst 15 studies had a retrospective design whilst the remaining studies were randomised controlled trials. Nine of the studies included presented mortality data following a first unprovoked seizure. For a mortality study to be included, a proportional mortality ratio (PMR) or a standardised mortality ratio (SMR) had to be given at a specific time point following a first unprovoked seizure. To be included in the meta‐analysis a study had to present clear seizure recurrence data at 6 months, 12 months or 24 months. Forty‐six studies were included in the meta‐analysis, of which 23 were paediatric, 13 were adult, and 10 were a combination of paediatric and adult populations. A meta‐analysis was performed at three time points; six months, one year and two years for all ages combined, paediatric and adult studies, respectively. We found an estimated overall seizure recurrence of all included studies at six months of 27% (95% CI 24% to 31%), 36% (95% CI 33% to 40%) at one year and 43% (95% CI 37% to 44%) at two years, with slightly lower estimates for adult subgroup analysis and slightly higher estimates for paediatric subgroup analysis. It was not possible to provide a summary estimate of the risk of seizure recurrence beyond these time points as most of the included studies were of short follow‐up and too few studies presented recurrence rates at a single time point beyond two years. The evidence presented was found to be of moderate certainty. Authors' conclusions Despite the limitations of the data (moderate‐certainty of evidence), mainly relating to clinical and methodological heterogeneity we have provided summary estimates for the likely risk of seizure recurrence at six months, one year and two years for both children and adults. This provides information that is likely to be useful for the clinician counselling patients (or their parents) on the probable risk of further seizures in the short‐term whilst acknowledging the paucity of long‐term recurrence data, particularly beyond 10 years. Plain language summary Predicting a second seizure after a single unprovoked seizure Why was this review performed? A single unprovoked seizure is fairly common, with estimates that up to 3% to 4% of the population will have one by age 85. This translates to approximately one in 25 people having an epileptic seizure during their lifetime. It is therefore of the utmost importance that accurate prognostic data are available so that clinicians can reliably counsel people on the risk of further seizures, and factors that predict the recurrence of seizures and therefore the development of epilepsy. What is the aim of the review? The main objective of this review is to provide people presenting with a single seizure, their families, and the clinicians looking after them, with more accurate information relating to the risk of further unprovoked seizures and the development of epilepsy. The additional objective of this review is to provide people presenting with a single seizure, their families, and the clinicians looking after them, with more accurate information relating to the risk of premature death following an unprovoked seizure. Key messages Despite some quite big differences in the design of the studies included in this review, we were able to provide information on the risk of having another seizure at 6 months, 12 months and 24 months. What was studied in the review? We searched for relevant studies that had a reliable design and that reported the number of people who had a second seizure after a first unprovoked seizure. We found 58 studies involving 12,160 people. Twenty‐six studies involved children only, 16 were adult only and the remaining 16 studies were a combination of children and adults. People had to have been followed up for a minimum of six months and include a minimum number of 30 people. What were the main results of the review? We collected the reported second seizure rates at 6 months, 12 months and 24 months. We then combined the data at these three set time points and were able to compare the chances of having a second seizure according to how much time had passed after the first seizure. At six months the chances of having a second event was 27%, whilst it was 36% at one year and finally at two years it was 43%. The chances of having a second seizure are slightly higher in children compared to adults. How up to date is this review? The evidence is current to March 2021.","1","John Wiley & Sons, Ltd","1465-1858","*Epilepsies, Partial [drug therapy]; *Epilepsy [drug therapy]; Adult; Anticonvulsants [therapeutic use]; Case-Control Studies; Child; Cohort Studies; Humans; Prognosis; Prospective Studies; Retrospective Studies; Seizures [diagnosis, drug therapy, etiology]","10.1002/14651858.CD013847.pub2","http://dx.doi.org/10.1002/14651858.CD013847.pub2","Epilepsy"
"CD012749.PUB2","Brignell, A; Harwood, RC; May, T; Woolfenden, S; Montgomery, A; Iorio, A; Williams, K","Overall prognosis of preschool autism spectrum disorder diagnoses","Cochrane Database of Systematic Reviews","2022","Abstract - Background Autism spectrum disorder is a neurodevelopmental disorder characterised by social communication difficulties, restricted interests and repetitive behaviours. The clinical pathway for children with a diagnosis of autism spectrum disorder is varied, and current research suggests some children may not continue to meet diagnostic criteria over time. Objectives The primary objective of this review was to synthesise the available evidence on the proportion of preschool children who have a diagnosis of autism spectrum disorder at baseline (diagnosed before six years of age) who continue to meet diagnostic criteria at follow‐up one or more years later (up to 19 years of age). Search methods We searched MEDLINE, Embase, PsycINFO, and eight other databases in October 2017 and ran top‐up searches up to July 2021. We also searched reference lists of relevant systematic reviews. Selection criteria Two review authors independently assessed prospective and retrospective follow‐up studies that used the same measure and process within studies to diagnose autism spectrum disorder at baseline and follow‐up. Studies were required to have at least one year of follow‐up and contain at least 10 participants. Participants were all aged less than six years at baseline assessment and followed up before 19 years of age. Data collection and analysis We extracted data on study characteristics and the proportion of children diagnosed with autism spectrum disorder at baseline and follow‐up. We also collected information on change in scores on measures that assess the dimensions of autism spectrum disorder (i.e. social communication and restricted interests and repetitive behaviours). Two review authors independently extracted data on study characteristics and assessed risk of bias using a modified quality in prognosis studies (QUIPS) tool. We conducted a random‐effects meta‐analysis or narrative synthesis, depending on the type of data available. We also conducted prognostic factor analyses to explore factors that may predict diagnostic outcome. Main results In total, 49 studies met our inclusion criteria and 42 of these (11,740 participants) had data that could be extracted. Of the 42 studies, 25 (60%) were conducted in North America, 13 (31%) were conducted in Europe and the UK, and four (10%) in Asia. Most (52%) studies were published before 2014. The mean age of the participants was 3.19 years (range 1.13 to 5.0 years) at baseline and 6.12 years (range 3.0 to 12.14 years) at follow‐up. The mean length of follow‐up was 2.86 years (range 1.0 to 12.41 years). The majority of the children were boys (81%), and just over half (60%) of the studies primarily included participants with intellectual disability (intelligence quotient < 70). The mean sample size was 272 (range 10 to 8564). Sixty‐nine per cent of studies used one diagnostic assessment tool, 24% used two tools and 7% used three or more tools. Diagnosis was decided by a multidisciplinary team in 41% of studies. No data were available for the outcomes of social communication and restricted and repetitive behaviours and interests. Of the 42 studies with available data, we were able to synthesise data from 34 studies (69% of all included studies; n = 11,129) in a meta‐analysis. In summary, 92% (95% confidence interval 89% to 95%) of participants continued to meet diagnostic criteria for autism spectrum disorder from baseline to follow‐up one or more years later; however, the quality of the evidence was judged as low due to study limitations and inconsistency. The majority of the included studies (95%) were rated at high risk of bias. We were unable to explore the outcomes of change in social communication and restricted and repetitive behaviour and interests between baseline and follow‐up as none of the included studies provided separate domain scores at baseline and follow‐up. Details on conflict of interest were reported in 24 studies. Funding support was reported by 30 studies, 12 studies omitted details on funding sources and two studies reported no funding support. Declared funding sources were categorised as government, university or non‐government organisation or charity groups. We considered it unlikely funding sources would have significantly influenced the outcomes, given the nature of prognosis studies. Authors' conclusions Overall, we found that nine out of 10 children who were diagnosed with autism spectrum disorder before six years of age continued to meet diagnostic criteria for autism spectrum disorder a year or more later, however the evidence was uncertain. Confidence in the evidence was rated low using GRADE, due to heterogeneity and risk of bias, and there were few studies that included children diagnosed using a current classification system, such as the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM‐5) or the eleventh revision of the International Classification of Diseases (ICD‐11). Future studies that are well‐designed, prospective and specifically assess prognosis of autism spectrum disorder diagnoses are needed. These studies should also include contemporary diagnostic assessment methods across a broad range of participants and investigate a range of relevant prognostic factors. Plain language summary What proportion of preschool aged children diagnosed with autism spectrum disorder retain their diagnosis one or more years later? Key messages ‐ Nine out of 10 preschool aged children diagnosed with autism in a research setting may continue to meet diagnostic criteria one or more years later. ‐ Due to lack of robust evidence, this finding may not be able to be generalised to children outside a research setting, and we were not able to identify any child or research study factors that influenced if a child retained their diagnosis. ‐ Future research should focus on designing a robust study exploring whether a child retains their autism diagnosis over time in clinical practice and what other factors, if any, may change how likely a child is to retain their diagnosis. What is autism? Autism (autism spectrum disorder) is a common neurodevelopmental condition that is generally considered to be lifelong. It is characterised by difficulties in social communication, and restricted interests and repetitive behaviours. How much of a challenge these areas present for each individual is highly variable. How is autism diagnosed? Autism is diagnosed by assessing whether an individual meets a set of standardised diagnostic criteria. In children, an autism diagnostic assessment may involve a paediatrician, child psychiatrist, speech pathologist, occupational therapist and psychologist. One or more of these health professionals may observe and ask questions about a child’s social and communication skills, any difficulties in restricted interests and repetitive behaviours, and how they process and respond to sensory information from the world around them. There are diagnostic assessment tools that these professionals can use, alone or in combination, to help make the diagnosis. What is diagnostic stability, and why is it important? Diagnostic stability refers to whether an individual retains their diagnosis over time. The diagnostic stability of autism is important to help health professionals, autistic individuals and their families understand how likely it is for a diagnosis of autism spectrum disorder to be lifelong. Additionally, it helps government and community groups to plan what health, education and employment resources are required to support autistic children and their families. Diagnostic stability also helps us to understand whether the characteristics of autistic children and the way that autism spectrum disorder is currently diagnosed influences whether a child continues to meet the criteria for an autism diagnosis over time. What did we want to find out? We wanted to find out whether a preschool child who was given a diagnosis of autism spectrum disorder before the age of six years retained their diagnosis at repeat diagnostic assessment one or more years later. We also wanted to learn more about whether any factors relating to the individual child, the way the child was diagnosed with autism, or the research methods used in the studies, made it more or less likely for the child to continue to meet diagnostic criteria for autism spectrum disorder over time. The factors relating to the individual child included the children's age at the initial and follow‐up diagnostic assessments, their intelligence quotient (IQ) score, their ability to complete daily living tasks for a child of their age (adaptive behaviour score), and their ability to communicate with those around them (language score). Factors relating to the way children were diagnosed included the type of tool or criteria used to make the diagnosis, the length of time between diagnostic assessments, and whether the diagnosis was made by a multidisciplinary team. The factors related to the research methods included the year the study was published and the robustness of the evidence. What did we do? We searched for studies looking at preschool aged children diagnosed with autism. We then summarised the results, evaluated the evidence and rated our confidence in the evidence based on factors such as study methods and participation. What did we find? In total, 49 studies met our inclusion criteria and 42 of these (11,740 children) had data that could be used. The biggest study had 8564 children and the smallest had 11. These studies were from 13 countries, with 16 from the USA. The average age of the children was three years at their first diagnosis and six years at follow‐up. The average length of follow‐up was 2.86 years. We found that, in a research setting, nine out of 10 of preschool children diagnosed with autism spectrum disorder may keep their diagnosis one or more years later. What are the limitations of the evidence? We have little confidence in the evidence because not all the studies provided data about everything that we were interested in, and the studies were done with different types of people and diagnostic assessments. For the one in 10 children who no longer met diagnostic criteria for an autism diagnosis at follow‐up, we were not able to tell whether they had 'grown out' of their autism because they became more mature over time, or because they had received intervention, or whether the original diagnosis was inaccurate. How up to date is this evidence? The evidence is up to date to July 2021.","9","John Wiley & Sons, Ltd","1465-1858","*Autism Spectrum Disorder [diagnosis]; Adult; Child; Child, Preschool; Female; Humans; Infant; Male; Prognosis; Prospective Studies; Retrospective Studies; Schools; Young Adult","10.1002/14651858.CD012749.pub2","http://dx.doi.org/10.1002/14651858.CD012749.pub2","Developmental, Psychosocial and Learning Problems"
"CD015048.PUB2","Bryant, A; Hiu, S; Kunonga, PT; Gajjar, K; Craig, D; Vale, L; Winter-Roach, BA; Elattar, A; Naik, R","Impact of residual disease as a prognostic factor for survival in women with advanced epithelial ovarian cancer after primary surgery","Cochrane Database of Systematic Reviews","2022","Abstract - Background Ovarian cancer is the seventh most common cancer among women and a leading cause of death from gynaecological malignancies. Epithelial ovarian cancer is the most common type, accounting for around 90% of all ovarian cancers. This specific type of ovarian cancer starts in the surface layer covering the ovary or lining of the fallopian tube. Surgery is performed either before chemotherapy (upfront or primary debulking surgery (PDS)) or in the middle of a course of treatment with chemotherapy (neoadjuvant chemotherapy (NACT) and interval debulking surgery (IDS)), with the aim of removing all visible tumour and achieving no macroscopic residual disease (NMRD). The aim of this review is to investigate the prognostic impact of size of residual disease nodules (RD) in women who received upfront or interval cytoreductive surgery for advanced (stage III and IV) epithelial ovarian cancer (EOC). Objectives To assess the prognostic impact of residual disease after primary surgery on survival outcomes for advanced (stage III and IV) epithelial ovarian cancer. In separate analyses, primary surgery included both upfront primary debulking surgery (PDS) followed by adjuvant chemotherapy and neoadjuvant chemotherapy followed by interval debulking surgery (IDS). Each residual disease threshold is considered as a separate prognostic factor. Search methods We searched CENTRAL (2021, Issue 8), MEDLINE via Ovid (to 30 August 2021) and Embase via Ovid (to 30 August 2021). Selection criteria We included survival data from studies of at least 100 women with advanced EOC after primary surgery. Residual disease was assessed as a prognostic factor in multivariate prognostic models. We excluded studies that reported fewer than 100 women, women with concurrent malignancies or studies that only reported unadjusted results. Women were included into two distinct groups: those who received PDS followed by platinum‐based chemotherapy and those who received IDS, analysed separately. We included studies that reported all RD thresholds after surgery, but the main thresholds of interest were microscopic RD (labelled NMRD), RD 0.1 cm to 1 cm (small‐volume residual disease (SVRD)) and RD > 1 cm (large‐volume residual disease (LVRD)). Data collection and analysis Two review authors independently abstracted data and assessed risk of bias. Where possible, we synthesised the data in meta‐analysis. To assess the adequacy of adjustment factors used in multivariate Cox models, we used the 'adjustment for other prognostic factors' and 'statistical analysis and reporting' domains of the quality in prognosis studies (QUIPS) tool. We also made judgements about the certainty of the evidence for each outcome in the main comparisons, using GRADE. We examined differences between FIGO stages III and IV for different thresholds of RD after primary surgery. We considered factors such as age, grade, length of follow‐up, type and experience of surgeon, and type of surgery in the interpretation of any heterogeneity. We also performed sensitivity analyses that distinguished between studies that included NMRD in RD categories of < 1 cm and those that did not. This was applicable to comparisons involving RD < 1 cm with the exception of RD < 1 cm versus NMRD. We evaluated women undergoing PDS and IDS in separate analyses. Main results We found 46 studies reporting multivariate prognostic analyses, including RD as a prognostic factor, which met our inclusion criteria: 22,376 women who underwent PDS and 3697 who underwent IDS, all with varying levels of RD. While we identified a range of different RD thresholds, we mainly report on comparisons that are the focus of a key area of clinical uncertainty (involving NMRD, SVRD and LVRD). The comparison involving any visible disease (RD > 0 cm) and NMRD was also important. SVRD versus NMRD in a PDS setting In PDS studies, most showed an increased risk of death in all RD groups when those with macroscopic RD (MRD) were compared to NMRD. Women who had SVRD after PDS had more than twice the risk of death compared to women with NMRD (hazard ratio (HR) 2.03, 95% confidence interval (CI) 1.80 to 2.29; I 2  = 50%; 17 studies; 9404 participants; moderate‐certainty). The analysis of progression‐free survival found that women who had SVRD after PDS had nearly twice the risk of death compared to women with NMRD (HR 1.88, 95% CI 1.63 to 2.16; I 2  = 63%; 10 studies; 6596 participants; moderate‐certainty). LVRD versus SVRD in a PDS setting When we compared LVRD versus SVRD following surgery, the estimates were attenuated compared to NMRD comparisons. All analyses showed an overall survival benefit in women who had RD < 1 cm after surgery (HR 1.22, 95% CI 1.13 to 1.32; I 2  = 0%; 5 studies; 6000 participants; moderate‐certainty). The results were robust to analyses of progression‐free survival. SVRD and LVRD versus NMRD in an IDS setting The one study that defined the categories as NMRD, SVRD and LVRD showed that women who had SVRD and LVRD after IDS had more than twice the risk of death compared to women who had NMRD (HR 2.09, 95% CI 1.20 to 3.66; 310 participants; I 2  = 56%, and HR 2.23, 95% CI 1.49 to 3.34; 343 participants; I 2  = 35%; very low‐certainty, for SVRD versus NMRD and LVRD versus NMRD, respectively). LVRD versus SVRD + NMRD in an IDS setting Meta‐analysis found that women who had LVRD had a greater risk of death and disease progression compared to women who had either SVRD or NMRD (HR 1.60, 95% CI 1.21 to 2.11; 6 studies; 1572 participants; I 2  = 58% for overall survival and HR 1.76, 95% CI 1.23 to 2.52; 1145 participants; I 2  = 60% for progression‐free survival; very low‐certainty). However, this result is biased as in all but one study it was not possible to distinguish NMRD within the < 1 cm thresholds. Only one study separated NMRD from SVRD; all others included NMRD in the SVRD group, which may create bias when comparing with LVRD, making interpretation challenging. MRD versus NMRD in an IDS setting Women who had any amount of MRD after IDS had more than twice the risk of death compared to women with NMRD (HR 2.11, 95% CI 1.35 to 3.29, I 2  = 81%; 906 participants; very low‐certainty). Authors' conclusions In a PDS setting, there is moderate‐certainty evidence that the amount of RD after primary surgery is a prognostic factor for overall and progression‐free survival in women with advanced ovarian cancer. We separated our analysis into three distinct categories for the survival outcome including NMRD, SVRD and LVRD. After IDS, there may be only two categories required, although this is based on very low‐certainty evidence, as all but one study included NMRD in the SVRD category. The one study that separated NMRD from SVRD showed no improved survival outcome in the SVRD category, compared to LVRD. Further low‐certainty evidence also supported restricting to two categories, where women who had any amount of MRD after IDS had a significantly greater risk of death compared to women with NMRD. Therefore, the evidence presented in this review cannot conclude that using three categories applies in an IDS setting (very low‐certainty evidence), as was supported for PDS (which has convincing moderate‐certainty evidence). Plain language summary The impact of remaining (residual) disease after surgery on the survival prognosis for women with advanced epithelial ovarian cancer Review question We aimed to assess the effect on survival (the 'prognostic impact') of the amount of disease remaining after surgery (residual disease) during the initial treatment stage for women with advanced ovarian cancer. We looked at both surgery before chemotherapy ('primary debulking surgery') followed by adjuvant (additional) chemotherapy and chemotherapy first ('neoadjuvant chemotherapy') followed by surgery ('interval debulking surgery'). This review should help to determine the prognostic impact of residual disease after surgery on survival and work out acceptable definitions of residual disease thresholds. Background Ovarian cancer is the seventh most common cancer among women and a leading cause of death in women with gynaecological cancers. Ovarian cancers can develop from different cell types within the ovary/fallopian tubes. Most ovarian cancers are 'epithelial', arising from either the surface layer of the ovary or the lining of the fallopian tube. Newly diagnosed ovarian cancer is treated with a combination of surgery and chemotherapy, with surgery performed either before (called upfront or primary debulking surgery) or around the mid‐point of chemotherapy (called interval debulking surgery). Ovarian cancer has normally spread throughout the abdominal cavity by the time of diagnosis, so, unlike many other cancers, surgery is still performed, even though it may not remove the cancer in its entirety. The aim of surgery is to remove as much of the visible (macroscopic) cancer tissue as possible, which is called debulking or cytoreductive surgery. Studies have shown that the amount of the visible cancer that can be removed is likely to be an important prognostic factor for survival of women with advanced epithelial ovarian cancer. The aim of this review was to investigate how well the amount of remaining (residual) disease after surgery for newly diagnosed ovarian cancer predicts how long women will survive following a diagnosis of epithelial ovarian cancer (prognosis). Review methods We searched electronic databases up to the end of August 2021 and we also searched for unpublished studies. We included studies that reported residual disease as a prognostic factor, which also examined other prognostic factors at the same time. Key results We found 46 studies (including 22,376 women in 31 primary debulking surgery studies and 3697 women in 15 interval debulking surgery studies). Each study included more than 100 women, used statistical adjustment for important prognostic factors (multivariate analysis) and met our inclusion criteria. Our analyses showed the prognostic importance of surgery leaving no visible tumour deposits ('no macroscopic residual disease') both when women had upfront debulking surgery or interval debulking surgery. Both overall survival and progression‐free survival (survival without disease worsening, which was reported for upfront debulking surgery) were prolonged if this was achieved. Primary debulking surgery for newly diagnosed ovarian cancer Complete surgical removal of all visible tumour after upfront or primary debulking surgery improved survival, and this was also the case for those with a small amount of residual disease (0.1 cm to 1 cm). There was evidence to suggest that three categories of residual disease should be used (no macroscopic residual disease, small‐volume and large‐volume residual disease (more than 1 cm). Interval debulking surgery for newly diagnosed ovarian cancer When chemotherapy was given before surgery (interval debulking surgery), there was an association with improved survival if the remaining tumour was reduced to 'no macroscopic residual disease' (removal of all visible tumour). Women with small‐volume residual disease had no survival advantage compared to those with large‐volume residual disease, with both groups having a poorer prognosis compared to those with no visible tumour deposits; however, this evidence was of very low certainty. Any visible residual disease after interval debulking surgery was associated with poorer survival compared to women with none. Most interval debulking surgery studies included no visible tumour deposits in the small‐volume residual disease category, which limits our interpretation of these findings. Certainty of the evidence We judged our certainty of the evidence as 'moderate' for overall survival and progression‐free survival in the analyses involving primary debulking surgery studies. For the interval debulking surgery studies, the certainty of evidence was very low for overall survival in all comparisons and those that involved progression‐free survival. This was largely due to all but one study including 'no macroscopic residual disease' in the small‐volume residual disease category. Main conclusions The evidence in the review suggests that following primary debulking surgery three categories for the amount of residual disease should be used: no macroscopic residual disease, small‐volume and large‐volume residual disease. The evidence is more limited for interval debulking surgery and further studies are needed, but there may not be a survival difference between those with small‐ and large‐volume residual disease. Until there is evidence for a survival benefit for those with small‐volume compared to large‐volume residual disease, it may only be important to use two residual disease categories when classifying surgical outcomes: 'no macroscopic residual disease' and 'macroscopic residual disease' (remaining visible disease of more than 0 cm). However, this is based on very low‐certainty evidence and more information may change this finding.","9","John Wiley & Sons, Ltd","1465-1858","*Clinical Decision-Making; *Ovarian Neoplasms [drug therapy, pathology, surgery]; Carcinoma, Ovarian Epithelial [drug therapy, surgery]; Chemotherapy, Adjuvant [methods]; Female; Humans; Neoadjuvant Therapy [methods]; Neoplasm, Residual; Prognosis; Uncertainty","10.1002/14651858.CD015048.pub2","http://dx.doi.org/10.1002/14651858.CD015048.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD015196.PUB2","Taylor-Rowan, M; Kraia, O; Kolliopoulou, C; Noel-Storr, AH; Alharthi, AA.; Cross, AJ; Stewart, C; Myint, PK; McCleery, J; Quinn, TJ","Anticholinergic burden for prediction of cognitive decline or neuropsychiatric symptoms in older adults with mild cognitive impairment or dementia","Cochrane Database of Systematic Reviews","2022","Abstract - Background Medications with anticholinergic properties are commonly prescribed to older adults with a pre‐existing diagnosis of dementia or cognitive impairment. The cumulative anticholinergic effect of all the medications a person takes is referred to as the anticholinergic burden because of its potential to cause adverse effects. It is possible that a high anticholinergic burden may be a risk factor for further cognitive decline or neuropsychiatric disturbances in people with dementia. Neuropsychiatric disturbances are the most frequent complication of dementia that require hospitalisation, accounting for almost half of admissions; hence, identification of modifiable prognostic factors for these outcomes is crucial. There are various scales available to measure anticholinergic burden but agreement between them is often poor. Objectives Our primary objective was to assess whether anticholinergic burden, as defined at the level of each individual scale, was a prognostic factor for further cognitive decline or neuropsychiatric disturbances in older adults with pre‐existing diagnoses of dementia or cognitive impairment. Our secondary objective was to investigate whether anticholinergic burden was a prognostic factor for other adverse clinical outcomes, including mortality, impaired physical function, and institutionalisation. Search methods We searched these databases from inception to 29 November 2021: MEDLINE OvidSP, Embase OvidSP, PsycINFO OvidSP, CINAHL EBSCOhost, and ISI Web of Science Core Collection on ISI Web of Science. Selection criteria We included prospective and retrospective longitudinal cohort and case‐control observational studies, with a minimum of one‐month follow‐up, which examined the association between an anticholinergic burden measurement scale and the above stated adverse clinical outcomes, in older adults with pre‐existing diagnoses of dementia or cognitive impairment.   Data collection and analysis Two review authors independently assessed studies for inclusion, and undertook data extraction, risk of bias assessment, and GRADE assessment. We summarised risk associations between anticholinergic burden and all clinical outcomes in a narrative fashion. We also evaluated the risk association between anticholinergic burden and mortality using a random‐effects meta‐analysis.  We established adjusted pooled rates for the anticholinergic cognitive burden (ACB) scale; then, as an exploratory analysis, established pooled rates on the prespecified association across scales.  Main results We identified 18 studies that met our inclusion criteria (102,684 older adults). Anticholinergic burden was measured using five distinct measurement scales: 12 studies used the ACB scale; 3 studies used the Anticholinergic Risk Scale (ARS); 1 study used the Anticholinergic Drug Scale (ADS); 1 study used the Anticholinergic Effect on Cognition (AEC) Scale; and 2 studies used a list developed by Tune and Egeli.  Risk associations between anticholinergic burden and adverse clinical outcomes were highly heterogenous. Four out of 10 (40%) studies reported a significantly increased risk of greater long‐term cognitive decline for participants with an anticholinergic burden compared to participants with no or minimal anticholinergic burden. No studies investigated neuropsychiatric disturbance outcomes. One out of four studies (25%) reported a significant association with reduced physical function for participants with an anticholinergic burden versus participants with no or minimal anticholinergic burden. No study (out of one investigating study) reported a significant association between anticholinergic burden and risk of institutionalisation. Six out of 10 studies (60%) found a significantly increased risk of mortality for those with an anticholinergic burden compared to those with no or minimal anticholinergic burden. Pooled analysis of adjusted mortality hazard ratios (HR) measured anticholinergic burden with the ACB scale, and suggested a significantly increased risk of death for those with a high ACB score relative to those with no or minimal ACB scores (HR 1.153, 95% confidence interval (CI) 1.030 to 1.292; 4 studies, 48,663 participants). An exploratory pooled analysis of adjusted mortality HRs across anticholinergic burden scales also suggested a significantly increased risk of death for those with a high anticholinergic burden (HR 1.102, 95% CI 1.044 to 1.163; 6 studies, 68,381 participants).   Overall GRADE evaluation of results found low‐ or very low‐certainty evidence for all outcomes.  Authors' conclusions There is low‐certainty evidence that older adults with dementia or cognitive impairment who have a significant anticholinergic burden may be at increased risk of death. No firm conclusions can be drawn for risk of accelerated cognitive decline, neuropsychiatric disturbances, decline in physical function, or institutionalisation.  Plain language summary The impact of cumulative medications with anticholinergic effects on future adverse clinical outcomes in people with dementia Key messages Anticholinergic medicines may increase the risk of death in older adults who have dementia.  However, the evidence is low certainty, and we cannot say for certain if the anticholinergic medicines cause death, or if they are simply more likely to be used by people who are already at an increased risk of dying due to ongoing health problems.  We cannot draw firm conclusions for the risk that anticholinergic medicines pose to the development of other undesirable clinical outcomes, such as further deterioration of memory and thinking, or behavioural and psychological issues.  More research is needed to establish whether anticholinergic medicines cause unintended problems for older adults who have dementia. What are anticholinergic medicines? Medicines can be classified by their ability to block the action of a chemical signalling system in the body, called the cholinergic system. Medicines that do this are said to have anticholinergic effects, and therefore, are referred to as anticholinergic medicines. What did we want to find out?  Anticholinergic medicines are commonly used to treat a number of medical conditions that people with dementia frequently experience. Typical examples are medicines used to treat urinary tract infections or episodes of agitation. However, because the cholinergic system in the brain plays an important role in learning, memory, and emotional regulation, there are theoretical reasons to believe that the use of anticholinergic medicines may unintentionally exacerbate psychological problems in this population. In this review, we investigated the link between anticholinergic medicines and future occurrence of undesirable clinical outcomes in people with dementia.  What did we do? We searched for studies that looked at the link between anticholinergic medicines and a range of clinical outcomes in people with dementia. We compared and summarised the results of identified studies and rated our confidence in the evidence, based on factors, such as study methods and sizes.  What did we find? We found a total of 18 studies, involving 102,684 adults aged 50 years or more, who had issues with memory and thinking. We found that the evidence was highly inconsistent regarding the link between anticholinergic medicines and increased issues with memory and thinking in people with dementia. There were no studies that investigated the link between anticholinergic medicines and frequency of behavioural disturbances. Therefore, we could not draw any conclusions about whether anticholinergic medicines cause issues with memory and thinking, or behavioural disturbances in this population. However, we did find there was a more consistent link between anticholinergic medicines and the risk of death. Those who were taking anticholinergic medicines had a 15% higher risk of dying than those who were not taking anticholinergic medicines.   What are the limitations of the evidence? The available evidence is very low certainty because of the inconsistency of study results, and the lack of control for health conditions that could be linked with both the clinical outcomes and the prescribing of anticholinergic medicines themselves. It is possible that anticholinergic medicines may not actually cause death, but are simply more likely to be given to people who are already at an increased risk of dying due to ongoing health problems.   How up to date is this evidence? We searched for studies published up to 29 November 2021.","8","John Wiley & Sons, Ltd","1465-1858","*Cognitive Dysfunction [chemically induced]; *Dementia [chemically induced]; Aged; Cholinergic Antagonists [adverse effects]; Humans; Prospective Studies; Retrospective Studies","10.1002/14651858.CD015196.pub2","http://dx.doi.org/10.1002/14651858.CD015196.pub2","Dementia and Cognitive Improvement"
"CD013139.PUB2","Vernooij, LM; van Klei, WA; Moons, KG; Takada, T; van Waes, J; Damen, JAAG","The comparative and added prognostic value of biomarkers to the Revised Cardiac Risk Index for preoperative prediction of major adverse cardiac events and all‐cause mortality in patients who undergo noncardiac surgery","Cochrane Database of Systematic Reviews","2021","Abstract - Background The Revised Cardiac Risk Index (RCRI) is a widely acknowledged prognostic model to estimate preoperatively the probability of developing in‐hospital major adverse cardiac events (MACE) in patients undergoing noncardiac surgery. However, the RCRI does not always make accurate predictions, so various studies have investigated whether biomarkers added to or compared with the RCRI could improve this. Objectives Primary: To investigate the added predictive value of biomarkers to the RCRI to preoperatively predict in‐hospital MACE and other adverse outcomes in patients undergoing noncardiac surgery. Secondary: To investigate the prognostic value of biomarkers compared to the RCRI to preoperatively predict in‐hospital MACE and other adverse outcomes in patients undergoing noncardiac surgery. Tertiary: To investigate the prognostic value of other prediction models compared to the RCRI to preoperatively predict in‐hospital MACE and other adverse outcomes in patients undergoing noncardiac surgery. Search methods We searched MEDLINE and Embase from 1 January 1999 (the year that the RCRI was published) until 25 June 2020. We also searched ISI Web of Science and SCOPUS for articles referring to the original RCRI development study in that period. Selection criteria We included studies among adults who underwent noncardiac surgery, reporting on (external) validation of the RCRI and: ‐ the addition of biomarker(s) to the RCRI; or ‐ the comparison of the predictive accuracy of biomarker(s) to the RCRI; or ‐ the comparison of the predictive accuracy of the RCRI to other models. Besides MACE, all other adverse outcomes were considered for inclusion. Data collection and analysis We developed a data extraction form based on the CHARMS checklist. Independent pairs of authors screened references, extracted data and assessed risk of bias and concerns regarding applicability according to PROBAST. For biomarkers and prediction models that were added or compared to the RCRI in ≥ 3 different articles, we described study characteristics and findings in further detail. We did not apply GRADE as no guidance is available for prognostic model reviews. Main results We screened 3960 records and included 107 articles.   Over all objectives we rated risk of bias as high in ≥ 1 domain in 90% of included studies, particularly in the analysis domain. Statistical pooling or meta‐analysis of reported results was impossible due to heterogeneity in various aspects: outcomes used, scale by which the biomarker was added/compared to the RCRI, prediction horizons and studied populations.  Added predictive value of biomarkers to the RCRI Fifty‐one studies reported on the added value of biomarkers to the RCRI. Sixty‐nine different predictors were identified derived from blood (29%), imaging (33%) or other sources (38%). Addition of NT‐proBNP, troponin or their combination improved the RCRI for predicting MACE (median delta c‐statistics: 0.08, 0.14 and 0.12 for NT‐proBNP, troponin and their combination, respectively). The median total net reclassification index (NRI) was 0.16 and 0.74 after addition of troponin and NT‐proBNP to the RCRI, respectively. Calibration was not reported. To predict myocardial infarction, the median delta c‐statistic when NT‐proBNP was added to the RCRI was 0.09, and 0.06 for prediction of all‐cause mortality and MACE combined. For BNP and copeptin, data were not sufficient to provide results on their added predictive performance, for any of the outcomes. Comparison of the predictive value of biomarkers to the RCRI  Fifty‐one studies assessed the predictive performance of biomarkers alone compared to the RCRI. We identified 60 unique predictors derived from blood (38%), imaging (30%) or other sources, such as the American Society of Anesthesiologists (ASA) classification (32%). Predictions were similar between the ASA classification and the RCRI for all studied outcomes. In studies different from those identified in objective 1, the median delta c‐statistic was 0.15 and 0.12 in favour of  BNP and NT‐proBNP alone, respectively, when compared to the RCRI, for the prediction of MACE. For C‐reactive protein, the predictive performance was similar to the RCRI. For other biomarkers and outcomes, data were insufficient to provide summary results. One study reported on calibration and none on reclassification. Comparison of the predictive value of other prognostic models to the RCRI    Fifty‐two articles compared the predictive ability of the RCRI to other prognostic models. Of these, 42% developed a new prediction model, 22% updated the RCRI, or another prediction model, and 37% validated an existing prediction model. None of the other prediction models showed better performance in predicting MACE than the RCRI. To predict myocardial infarction and cardiac arrest, ACS‐NSQIP‐MICA had a higher median delta c‐statistic of 0.11 compared to the RCRI. To predict all‐cause mortality, the median delta c‐statistic was 0.15 higher in favour of ACS‐NSQIP‐SRS compared to the RCRI. Predictive performance was not better for CHADS 2 , CHA 2 DS 2 ‐VASc, R 2 CHADS 2 , Goldman index, Detsky index or VSG‐CRI compared to the RCRI for any of the outcomes. Calibration and reclassification were reported in only one and three studies, respectively. Authors' conclusions Studies included in this review suggest that the predictive performance of the RCRI in predicting MACE is improved when NT‐proBNP, troponin or their combination are added. Other studies indicate that BNP and NT‐proBNP, when used in isolation, may even have a higher discriminative performance than the RCRI. There was insufficient evidence of a difference between the predictive accuracy of the RCRI and other prediction models in predicting MACE. However, ACS‐NSQIP‐MICA and ACS‐NSQIP‐SRS outperformed the RCRI in predicting myocardial infarction and cardiac arrest combined, and all‐cause mortality, respectively. Nevertheless, the results cannot be interpreted as conclusive due to high risks of bias in a majority of papers, and pooling was impossible due to heterogeneity in outcomes, prediction horizons, biomarkers and studied populations. Future research on the added prognostic value of biomarkers to existing prediction models should focus on biomarkers with good predictive accuracy in other settings (e.g. diagnosis of myocardial infarction) and identification of biomarkers from omics data. They should be compared to novel biomarkers with so far insufficient evidence compared to established ones, including NT‐proBNP or troponins. Adherence to recent guidance for prediction model studies (e.g. TRIPOD; PROBAST) and use of standardised outcome definitions in primary studies is highly recommended to facilitate systematic review and meta‐analyses in the future.  Plain language summary Can biomarkers improve predictions of the RCRI tool to predict heart‐related complications in patients undergoing surgery other than heart surgery? Background and review question Although patients undergo surgery to maintain or increase life expectancy or to improve quality of life, surgery is not without risks. Some patients will develop a heart‐related complication after surgery other than heart surgery, such as a heart infarction. Several tools try to predict someone's chance of developing a heart complication after surgery using information collected in the period before surgery. The Revised Cardiac Risk Index (RCRI) is such a tool that tries to estimate the chance of developing a heart complication during hospital admission in patients undergoing surgery other than heart surgery. It uses information on whether the patient has in the past experienced a heart infarction, heart failure and/or a stroke during his/her life, their use of insulin for the treatment of diabetes mellitus, their current renal (kidney) function and whether he/she will undergo high or non‐high risk surgery. The RCRI is commonly used by physicians, but the predictions are not always very accurate. Therefore, several researchers have attempted to improve these predictions by adding extra information to this tool. This information can be derived from so‐called biomarkers, which are, for example, measurements from blood, imaging techniques or other characteristics, such as age, smoking status or physical condition of the patient. The aim of this systematic review was to investigate whether the addition of such biomarkers to the RCRI improves predictions of heart‐related complications during hospitalisation in patients undergoing surgery other than heart surgery. In addition, we investigated whether biomarkers and other prediction tools resulted in better predictions of heart‐related complications during hospitalisation compared to the predictions of the RCRI in patients undergoing surgery other than heart surgery. Key results We identified 69 different predictors that were added to the RCRI tool to improve predictions of these heart‐related complications. The evidence is current to 25 June 2020. Predictions seem to improve with the addition of some biomarkers derived from blood. These are troponin (which measures muscular damage of the heart), brain natriuretic peptide (BNP) and (NT‐pro)brain natriuretic peptide (NT‐proBNP) (which both measure severity of heart failure). In addition, there were 60 biomarkers that were studied to compare their predictions to the RCRI. Other studies included in this review suggest that BNP and NT‐proBNP alone may predict heart‐related complications even better than the RCRI. Sixty‐five prediction tools other than the RCRI tried to improve its predictions. The American College of Surgeons National Surgical Quality Improvement (ACS‐NSQIP) and ACS‐NSQIP‐MICA (myocardial infarction or cardiac arrest) surgical risk score tools could make better predictions than the RCRI, but this was only true for certain outcomes, and not for heart‐related complications. However, for all of these research questions, we are not confident in the results due to large variation in the research methods applied and signs of less accurate research approaches having been used. Authors' conclusions Troponin, BNP and NT‐proBNP may improve the ability of the RCRI to predict heart‐related complications. The ACS‐NSQOP‐MICA and ACS‐NSQIP surgical risk score tools seem to be better at predicting postoperative complications than the RCRI tool, but not heart‐related complications. However, due to deficiencies in how the studies were conducted, we are uncertain whether the results we found apply to all patients undergoing surgery other than heart surgery. We need more and better research on biomarkers with promising predictive performance in other settings. ","12","John Wiley & Sons, Ltd","1465-1858","*Heart Arrest; *Myocardial Infarction; Adult; Bias; Biomarkers; Humans; Peptide Fragments; Predictive Value of Tests; Prognosis; Risk Assessment","10.1002/14651858.CD013139.pub2","http://dx.doi.org/10.1002/14651858.CD013139.pub2","Heart"
"CD013091.PUB2","Atakpa, EC; Thorat, MA; Cuzick, J; Brentnall, AR","Mammographic density, endocrine therapy and breast cancer risk: a prognostic and predictive biomarker review","Cochrane Database of Systematic Reviews","2021","Abstract - Background Endocrine therapy is effective at preventing or treating breast cancer. Some forms of endocrine therapy have been shown to reduce mammographic density. Reduced mammographic density for women receiving endocrine therapy could be used to estimate the chance of breast cancer returning or developing breast cancer in the first instance (a prognostic biomarker). In addition, changes in mammographic density might be able to predict how well a woman responds to endocrine therapy (a predictive biomarker). The role of breast density as a prognostic or predictive biomarker could help improve the management of breast cancer. Objectives To assess the evidence that a reduction in mammographic density following endocrine therapy for breast cancer prevention in women without previous breast cancer, or for treatment in women with early‐stage hormone receptor‐positive breast cancer, is a prognostic or predictive biomarker. Search methods We searched the Cochrane Breast Cancer Group Specialised Register, CENTRAL, MEDLINE, Embase, and two trials registers on 3 August 2020 along with reference checking, bibliographic searching, and contact with study authors to obtain further data. Selection criteria We included randomised, cohort and case‐control studies of adult women with or without breast cancer receiving endocrine therapy. Endocrine therapy agents included were selective oestrogen receptor modulators and aromatase inhibitors. We required breast density before start of endocrine therapy and at follow‐up. We included studies published in English. Data collection and analysis We used standard methodological procedures expected by Cochrane. Two review authors independently extracted data and assessed risk of bias using adapted Quality in Prognostic Studies (QUIPS) and Risk Of Bias In Non‐randomised Studies ‐ of Interventions (ROBINS‐I) tools. We used the GRADE approach to evaluate the certainty of the evidence. We did not perform a quantitative meta‐analysis due to substantial heterogeneity across studies. Main results Eight studies met our inclusion criteria, of which seven provided data on outcomes listed in the protocol (5786 women). There was substantial heterogeneity across studies in design, sample size (349 to 1066 women), participant characteristics, follow‐up (5 to 14 years), and endocrine therapy agent. There were five breast density measures and six density change definitions. All studies had at least one domain as at moderate or high risk of bias. Common concerns were whether the study sample reflected the review target population, and likely post hoc definitions of breast density change. Most studies on prognosis for women receiving endocrine therapy reported a reduced risk associated with breast density reduction. Across endpoints, settings, and agents, risk ratio point estimates (most likely value) were between 0.1 and 1.5, but with substantial uncertainty. There was greatest consistency in the direction and magnitude of the effect for tamoxifen (across endpoints and settings, risk ratio point estimates were between 0.3 and 0.7). The findings are summarised as follows. Prognostic biomarker findings: Treatment Breast cancer mortality Two studies of 823 women on tamoxifen (172 breast cancer deaths) reported risk ratio point estimates of ~0.4 and ~0.5 associated with a density reduction. The certainty of the evidence was low. Recurrence Two studies of 1956 women on tamoxifen reported risk ratio point estimates of ~0.4 and ~0.7 associated with a density reduction. There was risk of bias in methodology for design and analysis of the studies and considerable uncertainty over the size of the effect. One study of 175 women receiving an aromatase inhibitor reported a risk ratio point estimate of ~0.1 associated with a density reduction. There was considerable uncertainty about the effect size and a moderate or high risk of bias in all domains. One study of 284 women receiving exemestane or tamoxifen as part of a randomised controlled trial reported risk ratio point estimates of ~1.5 (loco‐regional recurrence) and ~1.3 (distance recurrence) associated with a density reduction. There was risk of bias in reporting and study confounding, and uncertainty over the size of the effects. The certainty of the evidence for all recurrence endpoints was very low. Incidence of a secondary primary breast cancer Two studies of 451 women on exemestane, tamoxifen, or unknown endocrine therapy reported risk ratio point estimates of ~0.5 and ~0.6 associated with a density reduction. There was risk of bias in reporting and study confounding, and uncertainty over the effect size. The certainty of the evidence was very low. We were unable to find data regarding the remaining nine outcomes prespecified in the review protocol. Prevention Incidence of invasive breast cancer and ductal carcinoma in situ (DCIS) One study of 507 women without breast cancer who were receiving preventive tamoxifen as part of a randomised controlled trial (51 subsequent breast cancers) reported a risk ratio point estimate of ~0.3 associated with a density reduction. The certainty of the evidence was low. Predictive biomarker findings: One study of a subset of 1065 women from a randomised controlled trial assessed how much the effect of endocrine therapy could be explained by breast density declines in those receiving endocrine therapy. This study evaluated the prevention of invasive breast cancer and DCIS. We found some evidence to support the hypothesis, with a risk ratio interaction point estimate ~0.5. However, the 95% confidence interval included unity, and data were based on 51 women with subsequent breast cancer in the tamoxifen group. The certainty of the evidence was low. Authors' conclusions There is low‐/very low‐certainty evidence to support the hypothesis that breast density change following endocrine therapy is a prognostic biomarker for treatment or prevention. Studies suggested a potentially large effect size with tamoxifen, but the evidence was limited. There was less evidence that breast density change following tamoxifen preventive therapy is a predictive biomarker than prognostic biomarker. Evidence for breast density change as a prognostic treatment biomarker was stronger for tamoxifen than aromatase inhibitors. There were no studies reporting mammographic density change following endocrine therapy as a predictive biomarker in the treatment setting, nor aromatase inhibitor therapy as a prognostic or predictive biomarker in the preventive setting. Further research is warranted to assess mammographic density as a biomarker for all classes of endocrine therapy and review endpoints. Plain language summary Reduced breast density following endocrine therapy as an indicator of breast cancer risk What is the issue? Breast cancer is a common cancer and cause of death in women worldwide. Treatment options for breast cancer include endocrine therapy. Endocrine therapy can also be used to prevent breast cancer for women who have not been diagnosed with breast cancer. It would help doctors and their patients to understand whether some patients are likely to have greater benefit from endocrine therapy than others. The structure of the breast is likely to change following endocrine therapy. These structural changes are seen when women have a mammogram (breast x‐ray). They appear as a decrease in the area of white tissue (breast density) on the mammogram. We wanted to find out whether reductions in breast density after endocrine therapy can help to determine how well endocrine therapy works. Review question We searched for previously published studies. We assessed whether a reduction in breast density after receiving endocrine therapy was associated with better outcomes. For women without breast cancer, this focused on whether those with decreased breast density were less likely to develop breast cancer. For women with breast cancer, this included whether those with greater decreases in breast density were less likely to die from breast cancer. Study characteristics We performed the search on 3 August 2020. We included studies of adult women with breast cancer if the women's breast cancer had been diagnosed at an early stage and could be treated with endocrine therapy (hormone receptor‐positive breast cancer). We included drugs often used in practice (tamoxifen and aromatase inhibitors). We found a wide variety of studies. The studies varied in terms of how they had been planned and the characteristics of the women included in the studies, as well as in how breast density change was measured. Key results Most studies reported a reduced risk of breast cancer after endocrine therapy for women who had a breast density reduction compared with women who did not have a reduction. There was slightly stronger evidence for the drug tamoxifen. • Two studies reported on breast density reduction following tamoxifen and risk of breast cancer death. The findings were based on 172 women who died from breast cancer. Overall, the certainty of the evidence was low. • Two studies considered if breast cancer returned after treatment with tamoxifen. There were concerns about the study methods and certainty of findings in these two studies. Overall, the certainty of the evidence was very low. • One study considered treatment with an aromatase inhibitor and the chance of breast cancer returning. There was considerable uncertainty about the effect size because there were only 175 women in the study. The certainty of the evidence was very low due to potential risk of bias in the study. • One study considered if breast cancer returned locally or at a distance from the original tumour. There was risk of bias in reporting and uncertainty about the sizes of the effect. The certainty of the evidence for both outcomes was very low. • Two studies looked at the chance of women with breast cancer being diagnosed later with a new breast cancer, such as in the opposite breast. There was risk of bias in reporting and uncertainty about the size of the effect. The certainty of the evidence was very low. • One study considered women who had not previously had breast cancer and who received tamoxifen. Results were based on 51 women who developed breast cancer. Overall, the certainty of the evidence was low. • One study considered whether the beneficial effect of tamoxifen could be explained by a decrease in breast density. There was some evidence to support this, but there was uncertainty about the strength of the effect. The results were based on 51 women who developed breast cancer after receiving tamoxifen. The certainty of the evidence was low. Overall, we found some evidence that breast density change following tamoxifen therapy is useful information to help determine how well the drug will work in future. However, there is much uncertainty about the strength of this effect. This was due to small numbers of women in the studies, relatively few studies for each outcome, and limitations in many of the studies such as how breast density change was measured. More research is needed to help assess these issues. Quality of the evidence Overall, we assessed the certainty of the available evidence as low or very low.","10","John Wiley & Sons, Ltd","1465-1858","*Breast Density; *Breast Neoplasms [diagnostic imaging, drug therapy]; Biomarkers; Female; Humans; Prognosis; Randomized Controlled Trials as Topic; Tamoxifen","10.1002/14651858.CD013091.pub2","http://dx.doi.org/10.1002/14651858.CD013091.pub2","Breast Cancer"
"CD013540.PUB2","Taylor-Rowan, M; Edwards, S; Noel-Storr, AH; McCleery, J; Myint, PK; Soiza, R; Stewart, C; Loke, YK; Quinn, TJ","Anticholinergic burden (prognostic factor) for prediction of dementia or cognitive decline in older adults with no known cognitive syndrome","Cochrane Database of Systematic Reviews","2021","Abstract - Background Medications with anticholinergic properties are commonly prescribed to older adults. The cumulative anticholinergic effect of all the medications a person takes is referred to as the 'anticholinergic burden' because of its potential to cause adverse effects. It is possible that high anticholinergic burden may be a risk factor for development of cognitive decline or dementia. There are various scales available to measure anticholinergic burden but agreement between them is often poor. Objectives To assess whether anticholinergic burden, as defined at the level of each individual scale, is a prognostic factor for future cognitive decline or dementia in cognitively unimpaired older adults. Search methods We searched the following databases from inception to 24 March 2021: MEDLINE (OvidSP), Embase (OvidSP), PsycINFO (OvidSP), CINAHL (EBSCOhost), and ISI Web of Science Core Collection (ISI Web of Science). Selection criteria We included prospective and retrospective longitudinal cohort and case‐control observational studies with a minimum of one year' follow‐up that examined the association between an anticholinergic burden measurement scale and future cognitive decline or dementia in cognitively unimpaired older adults. Data collection and analysis Two review authors independently assessed studies for inclusion, and undertook data extraction, assessment of risk of bias, and GRADE assessment. We extracted odds ratios (OR) and hazard ratios, with 95% confidence intervals (CI), and linear data on the association between anticholinergic burden and cognitive decline or dementia. We intended to pool each metric separately; however, only OR‐based data were suitable for pooling via a random‐effects meta‐analysis. We initially established adjusted and unadjusted pooled rates for each available anticholinergic scale; then, as an exploratory analysis, established pooled rates on the prespecified association across scales. We examined variability based on severity of anticholinergic burden. Main results We identified 25 studies that met our inclusion criteria (968,428 older adults). Twenty studies were conducted in the community care setting, two in primary care clinics, and three in secondary care settings. Eight studies (320,906 participants) provided suitable data for meta‐analysis. The Anticholinergic Cognitive Burden scale (ACB scale) was the only scale with sufficient data for 'scale‐based' meta‐analysis. Unadjusted ORs suggested an increased risk for cognitive decline or dementia in older adults with an anticholinergic burden (OR 1.47, 95% CI 1.09 to 1.96) and adjusted ORs similarly suggested an increased risk for anticholinergic burden, defined according to the ACB scale (OR 2.63, 95% CI 1.09 to 6.29). Exploratory analysis combining adjusted ORs across available scales supported these results (OR 2.16, 95% CI 1.38 to 3.38), and there was evidence of variability in risk based on severity of anticholinergic burden (ACB scale 1: OR 2.18, 95% CI 1.11 to 4.29; ACB scale 2: OR 2.71, 95% CI 2.01 to 3.56; ACB scale 3: OR 3.27, 95% CI 1.41 to 7.61); however, overall GRADE evaluation of certainty of the evidence was low. Authors' conclusions There is low‐certainty evidence that older adults without cognitive impairment who take medications with anticholinergic effects may be at increased risk of cognitive decline or dementia. Plain language summary The impact of medications with anticholinergic effects on future problems with memory and thinking What was the aim of this review? Medicines can be classified by their ability to block the action of a chemical signalling system in the body called the cholinergic system. Medicines that do this are said to have anticholinergic effects. There are various measurement scales to quantify the effects of anticholinergic medicines. The overall anticholinergic effect caused by all the anticholinergic medications a person is taking is referred to as 'anticholinergic burden.' We aimed to investigate if older people who have no problems with memory or thinking are more likely to develop dementia when prescribed anticholinergic medicines than people who are not prescribed these medicines. Anticholinergic burden ratings can vary with the scale used because different scales score medicines in different ways. Therefore, we also wanted to know if any particular anticholinergic burden measurement scale was more strongly associated with increased risk of dementia than other scales. Key messages There may be a link between anticholinergic medicine use and future risk of dementia. However, there are limitations in the published evidence, and we cannot say definitively if dementia is caused by the anticholinergic medicines themselves or by other factors. There were too few studies to allow us to compare the various anticholinergic measurement tools. What was studied in the review? There are more than 40 million older people worldwide living with dementia. These numbers are expected to rise to over 100 million by 2050 and at present there are very limited treatment options available. Therefore, it is important to identify factors that may increase the risk of dementia. Because the cholinergic system in the brain plays an important role in learning and memory, there are theoretical reasons to believe that medications with anticholinergic effects could cause future dementia. Research has suggested that these medications may have unintended effects on memory and thinking, potentially resulting in dementia. If this is the case, one way to reduce the numbers of older people who develop dementia may be to avoid prescribing these medicines. Many commonly used medications have anticholinergic effects, for example medications for hay fever, insomnia (difficulty getting to sleep or staying asleep for long enough to feel refreshed), and depression. In this review, we investigated the link between anticholinergic medicines, as measured by various measurement scales, and future dementia. What were the main results of the review? We found 25 studies, including 968,428 people aged 50 years or more. Despite the relatively large number of studies, differences in design and methods only allowed us to combine a few of them in analyses. We found that there is a consistent link between use of anticholinergic medicines and risk of future dementia. We cannot say if these medicines play a causal role; however, if they do, taking these medicines could potentially double a person's risk of dementia. Of the anticholinergic measurement scales available, we could assess one commonly used tool – the 'Anticholinergic Cognitive Burden scale.' If this scale identified someone as having high anticholinergic burden, the risk of future dementia was more than two times higher than for someone with no anticholinergic burden. The evidence included in this review was of a low quality overall and may have exaggerated the strength of the association between anticholinergic medicines and dementia. For example, anticholinergic medicines may be prescribed for the early symptoms of dementia. This would give a strong link but would not imply that the medicine caused the memory problems. Similarly, there is a risk that studies are only published when they show an association between anticholinergic medicines and future dementia. It may be that the only way to truly establish if anticholinergic medications are associated with future dementia would be to conduct a study where some people have their anticholinergic medications stopped or changed to an alternative and others continue their usual medications. How up to date was this review? We searched for studies published up to 24 March 2021.","5","John Wiley & Sons, Ltd","1465-1858","Aged; Aged, 80 and over; Analysis of Variance; Bias; Cholinergic Antagonists [*adverse effects, pharmacology]; Cognitive Dysfunction [*chemically induced]; Confidence Intervals; Dementia [*chemically induced]; Female; Humans; Male; Middle Aged; Observational Studies as Topic; Odds Ratio; Prognosis; Syndrome; Treatment Outcome","10.1002/14651858.CD013540.pub2","http://dx.doi.org/10.1002/14651858.CD013540.pub2","Dementia and Cognitive Improvement"
"CD013491.PUB2","Moriarty, AS; Meader, N; Snell, KIE; Riley, RD; Paton, LW; Chew-Graham, CA; Gilbody, S; Churchill, R; Phillips, RS; Ali, S; McMillan, D","Prognostic models for predicting relapse or recurrence of major depressive disorder in adults","Cochrane Database of Systematic Reviews","2021","Abstract - Background Relapse (the re‐emergence of depressive symptoms after some level of improvement but preceding recovery) and recurrence (onset of a new depressive episode after recovery) are common in depression, lead to worse outcomes and quality of life for patients and exert a high economic cost on society. Outcomes can be predicted by using multivariable prognostic models, which use information about several predictors to produce an individualised risk estimate. The ability to accurately predict relapse or recurrence while patients are well (in remission) would allow the identification of high‐risk individuals and may improve overall treatment outcomes for patients by enabling more efficient allocation of interventions to prevent relapse and recurrence. Objectives To summarise the predictive performance of prognostic models developed to predict the risk of relapse, recurrence, sustained remission or recovery in adults with major depressive disorder who meet criteria for remission or recovery. Search methods We searched the Cochrane Library (current issue); Ovid MEDLINE (1946 onwards); Ovid Embase (1980 onwards); Ovid PsycINFO (1806 onwards); and Web of Science (1900 onwards) up to May 2020. We also searched sources of grey literature, screened the reference lists of included studies and performed a forward citation search. There were no restrictions applied to the searches by date, language or publication status . Selection criteria We included development and external validation (testing model performance in data separate from the development data) studies of any multivariable prognostic models (including two or more predictors) to predict relapse, recurrence, sustained remission, or recovery in adults (aged 18 years and over) with remitted depression, in any clinical setting. We included all study designs and accepted all definitions of relapse, recurrence and other related outcomes. We did not specify a comparator prognostic model. Data collection and analysis Two review authors independently screened references; extracted data (using a template based on the CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS)); and assessed risks of bias of included studies (using the Prediction model Risk Of Bias ASsessment Tool (PROBAST)). We referred any disagreements to a third independent review author. Where we found sufficient (10 or more) external validation studies of an individual model, we planned to perform a meta‐analysis of its predictive performance, specifically with respect to its calibration (how well the predicted probabilities match the observed proportions of individuals that experience the outcome) and discrimination (the ability of the model to differentiate between those with and without the outcome). Recommendations could not be qualified using the GRADE system, as guidance is not yet available for prognostic model reviews. Main results We identified 11 eligible prognostic model studies (10 unique prognostic models). Seven were model development studies; three were model development and external validation studies; and one was an external validation‐only study. Multiple estimates of performance measures were not available for any of the models and, meta‐analysis was therefore not possible. Ten out of the 11 included studies were assessed as being at high overall risk of bias. Common weaknesses included insufficient sample size, inappropriate handling of missing data and lack of information about discrimination and calibration. One paper (Klein 2018) was at low overall risk of bias and presented a prognostic model including the following predictors: number of previous depressive episodes, residual depressive symptoms and severity of the last depressive episode. The external predictive performance of this model was poor (C‐statistic 0.59; calibration slope 0.56; confidence intervals not reported). None of the identified studies examined the clinical utility (net benefit) of the developed model. Authors' conclusions Of the 10 prognostic models identified (across 11 studies), only four underwent external validation. Most of the studies (n = 10) were assessed as being at high overall risk of bias, and the one study that was at low risk of bias presented a model with poor predictive performance. There is a need for improved prognostic research in this clinical area, with future studies conforming to current best practice recommendations for prognostic model development/validation and reporting findings in line with the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement. Plain language summary Predicting relapse or recurrence of depression What is the aim of this review? Relapse and recurrence (becoming unwell again after making an improvement) are common in depression and lead to increased disability and decreased quality of life for patients. Relapse is a re‐emergence of the initial episode of depression after some initial improvement, whereas recurrence is the onset of a new episode of depression after recovery. Outcomes, such as relapse and recurrence, can sometimes be predicted while people are well, using information available at the time. A mathematical calculation can be performed to assess an individual person's risk; this calculation is known as a 'prognostic model' or a prediction tool. In most health services, including the National Health Service (NHS) in the UK, resources such as doctors and therapists need to be used in the best way possible, for the people who will gain the most benefit from them. If accurate prediction tools are available, the information can be used to identify the most 'high risk' patients and make sure they receive additional support to try to prevent a relapse or a recurrence. The aim of this review was to identify studies that have attempted to develop a prediction tool for relapse or recurrence of depression in adults. We were interested in studies that had attempted to make this prediction while patients were well. We also included tools that predicted the chance of patients staying well. If we had found multiple studies that tested the same prediction tool, we planned to combine these to work out a better summary of how well that tool worked. Key messages We identified 10 prediction tools (over 11 studies) for relapse or recurrence. These were either not proven to be good at predicting relapse/recurrence, or the studies had problems with how they were carried out, meaning that none of the prediction tools were at a stage where they could be used in the real world. Further work is needed to improve prediction of relapse or recurrence of depression. What was studied in the review? We collected and analysed the results of 11 relevant studies. We were interested in several things: how researchers had defined relapse and recurrence (for example, whether they had used clinical interviews or self‐report questionnaires to diagnose depressive symptoms); what information was gathered to help make predictions; the techniques used by the researchers to help develop the tools; and how well the tools predicted. We were also interested in whether the tools were tested in a separate group of participants, which is essential to ensure that the model can predict accurately in patients in the real world. Finally, we assessed the studies to determine how confident we could be in the results, given the approaches taken by researchers (this is called 'risk of bias') and how relevant the studies were to our review (this is called 'applicability'). What are the main results of the review? We found 11 studies. Ten of these developed different models and one study tested one of the models developed in a previous study. It was not possible to combine results for any particular tool. Ten of the 11 studies were rated at high risk of bias. This means that we cannot be confident in the results that were presented, due to some issues with the way the studies were conducted. The most common issue was that there were not enough participants included in the studies. Other common problems involved the statistical approaches used by the researchers. One study was at low overall risk of bias, which means that we can be more confident in trusting the results. However, this tool did not make accurate predictions about relapse or recurrence. We found no studies that could be used in clinical practice; further work is needed to develop tools for predicting relapse or recurrence of depression. How up‐to‐date is the review? The literature search for this review was completed in May 2020.","5","John Wiley & Sons, Ltd","1465-1858","*Depressive Disorder, Major; *Multivariate Analysis; Bias; Humans; Models, Theoretical; Prognosis; Recurrence; Reproducibility of Results","10.1002/14651858.CD013491.pub2","http://dx.doi.org/10.1002/14651858.CD013491.pub2","Common Mental Disorders"
"CD013316.PUB2","McAleenan, A; Kelly, C; Spiga, F; Kernohan, A; Cheng, H-Y; Dawson, S; Schmidt, L; Robinson, T; Brandner, S; Faulkner, CL; Wragg, C; Jefferies, S; Howell, A; Vale, L; Higgins, JP T; Kurian, KM","Prognostic value of test(s) for O6‐methylguanine–DNA methyltransferase (MGMT) promoter methylation for predicting overall survival in people with glioblastoma treated with temozolomide","Cochrane Database of Systematic Reviews","2021","Abstract - Background Glioblastoma is an aggressive form of brain cancer. Approximately five in 100 people with glioblastoma survive for five years past diagnosis. Glioblastomas that have a particular modification to their DNA (called methylation) in a particular region (the O 6 ‐methylguanine–DNA methyltransferase (MGMT) promoter) respond better to treatment with chemotherapy using a drug called temozolomide. Objectives To determine which method for assessing MGMT methylation status best predicts overall survival in people diagnosed with glioblastoma who are treated with temozolomide. Search methods We searched MEDLINE, Embase, BIOSIS, Web of Science Conference Proceedings Citation Index to December 2018, and examined reference lists. For economic evaluation studies, we additionally searched NHS Economic Evaluation Database (EED) up to December 2014. Selection criteria Eligible studies were longitudinal (cohort) studies of adults with diagnosed glioblastoma treated with temozolomide with/without radiotherapy/surgery. Studies had to have related MGMT status in tumour tissue (assessed by one or more method) with overall survival and presented results as hazard ratios or with sufficient information (e.g. Kaplan‐Meier curves) for us to estimate hazard ratios. We focused mainly on studies comparing two or more methods, and listed brief details of articles that examined a single method of measuring MGMT promoter methylation. We also sought economic evaluations conducted alongside trials, modelling studies and cost analysis. Data collection and analysis Two review authors independently undertook all steps of the identification and data extraction process for multiple‐method studies. We assessed risk of bias and applicability using our own modified and extended version of the QUality In Prognosis Studies (QUIPS) tool. We compared different techniques, exact promoter regions (5'‐cytosine‐phosphate‐guanine‐3' (CpG) sites) and thresholds for interpretation within studies by examining hazard ratios. We performed meta‐analyses for comparisons of the three most commonly examined methods (immunohistochemistry (IHC), methylation‐specific polymerase chain reaction (MSP) and pyrosequencing (PSQ)), with ratios of hazard ratios (RHR), using an imputed value of the correlation between results based on the same individuals. Main results We included 32 independent cohorts involving 3474 people that compared two or more methods. We found evidence that MSP (CpG sites 76 to 80 and 84 to 87) is more prognostic than IHC for MGMT protein at varying thresholds (RHR 1.31, 95% confidence interval (CI) 1.01 to 1.71). We also found evidence that PSQ is more prognostic than IHC for MGMT protein at various thresholds (RHR 1.36, 95% CI 1.01 to 1.84). The data suggest that PSQ (mainly at CpG sites 74 to 78, using various thresholds) is slightly more prognostic than MSP at sites 76 to 80 and 84 to 87 (RHR 1.14, 95% CI 0.87 to 1.48). Many variants of PSQ have been compared, although we did not see any strong and consistent messages from the results. Targeting multiple CpG sites is likely to be more prognostic than targeting just one. In addition, we identified and summarised 190 articles describing a single method for measuring MGMT promoter methylation status. Authors' conclusions PSQ and MSP appear more prognostic for overall survival than IHC. Strong evidence is not available to draw conclusions with confidence about the best CpG sites or thresholds for quantitative methods. MSP has been studied mainly for CpG sites 76 to 80 and 84 to 87 and PSQ at CpG sites ranging from 72 to 95. A threshold of 9% for CpG sites 74 to 78 performed better than higher thresholds of 28% or 29% in two of three good‐quality studies making such comparisons. Plain language summary Which method of determining MGMT promoter methylation best predicts survival in people with glioblastoma treated with temozolomide? What was the aim of this review? Glioblastoma is a very aggressive type of brain cancer. People with glioblastoma are usually treated with surgical removal of the tumour followed by radiotherapy, chemotherapy, or both. The standard chemotherapy is a medicine called temozolomide. Some glioblastoma tumours have a particular modification in their DNA (which contains the genetic code of organisms), and knowing whether a person has this modification is useful to predict how long the person may live after their diagnosis with cancer and how they may respond to temozolomide. The modification is known as 'methylation of the MGMT promoter region' and it can also affect MGMT protein expression (the way MGMT is made and modified). There are many ways to work out whether a tumour has this modification. In this review, we attempted to work out which method is best. What we found We identified 32 studies comparing different ways to measure whether the MGMT promoter region is methylated. The main three methods were called 'methylation‐specific polymerase chain reaction (PCR),' 'pyrosequencing' (both of which look directly at the MGMT promoter region) and 'immunohistochemistry' (which looks at MGMT protein expression). We found that methylation‐specific PCR and pyrosequencing are better at predicting overall survival than immunohistochemistry. Methylation‐specific PCR and pyrosequencing can be carried out by targeting different parts of the tumour DNA. Pyrosequencing can be performed using different cut‐off thresholds to determine whether a tumour is methylated or unmethylated. We did not identify very clear signals in terms of the best parts of the DNA to target or which are the best cut‐off thresholds. How reliable are results of the studies in this review? We rated our confidence in the evidence as 'moderate' for our conclusions about methylation‐specific PCR, but as 'low' for pyrosequencing. Although there were many studies, they all looked at different variants of the methods, so it is difficult to work out exactly which variant is best. What are the implications of this review? Our review indicates both methylation‐specific PCR and pyrosequencing provide better predictions of survival than immunohistochemistry. There is some evidence that pyrosequencing may be better than methylation‐specific PCR at predicting overall survival, depending on the DNA targets and cut‐off thresholds used. We documented the most frequent DNA targets used in methylation‐specific PCR and pyrosequencing. We described cut‐off thresholds used in pyrosequencing, although it is unclear which of these is best.","3","John Wiley & Sons, Ltd","1465-1858","*DNA Methylation; Adult; Antineoplastic Agents, Alkylating [therapeutic use]; Bias; Brain Neoplasms [drug therapy, enzymology, *mortality]; Cohort Studies; CpG Islands [genetics]; DNA Modification Methylases [*metabolism]; DNA Repair Enzymes [*metabolism]; Glioblastoma [drug therapy, enzymology, *mortality]; High-Throughput Nucleotide Sequencing; Humans; Immunohistochemistry; Polymerase Chain Reaction [methods]; Predictive Value of Tests; Prognosis; Promoter Regions, Genetic [*genetics]; Temozolomide [therapeutic use]; Tumor Suppressor Proteins [*metabolism]","10.1002/14651858.CD013316.pub2","http://dx.doi.org/10.1002/14651858.CD013316.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD012022.PUB2","Kreuzberger, N; Damen, JAAG; Trivella, M; Estcourt, LJ; Aldin, A; Umlauff, L; Vazquez-Montes, MDLA; Wolff, R; Moons, KG; Monsef, I; Foroutan, F; Kreuzer, K-A; Skoetz, N","Prognostic models for newly‐diagnosed chronic lymphocytic leukaemia in adults: a systematic review and meta‐analysis","Cochrane Database of Systematic Reviews","2020","Abstract - Background Chronic lymphocytic leukaemia (CLL) is the most common cancer of the lymphatic system in Western countries. Several clinical and biological factors for CLL have been identified. However, it remains unclear which of the available prognostic models combining those factors can be used in clinical practice to predict long‐term outcome in people newly‐diagnosed with CLL. Objectives To identify, describe and appraise all prognostic models developed to predict overall survival (OS), progression‐free survival (PFS) or treatment‐free survival (TFS) in newly‐diagnosed (previously untreated) adults with CLL, and meta‐analyse their predictive performances. Search methods We searched MEDLINE (from January 1950 to June 2019 via Ovid), Embase (from 1974 to June 2019) and registries of ongoing trials (to 5 March 2020) for development and validation studies of prognostic models for untreated adults with CLL. In addition, we screened the reference lists and citation indices of included studies. Selection criteria We included all prognostic models developed for CLL which predict OS, PFS, or TFS, provided they combined prognostic factors known before treatment initiation, and any studies that tested the performance of these models in individuals other than the ones included in model development (i.e. 'external model validation studies'). We included studies of adults with confirmed B‐cell CLL who had not received treatment prior to the start of the study. We did not restrict the search based on study design. Data collection and analysis We developed a data extraction form to collect information based on the Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies (CHARMS). Independent pairs of review authors screened references, extracted data and assessed risk of bias according to the Prediction model Risk Of Bias ASsessment Tool (PROBAST). For models that were externally validated at least three times, we aimed to perform a quantitative meta‐analysis of their predictive performance, notably their calibration (proportion of people predicted to experience the outcome who do so) and discrimination (ability to differentiate between people with and without the event) using a random‐effects model. When a model categorised individuals into risk categories, we pooled outcome frequencies per risk group (low, intermediate, high and very high). We did not apply GRADE as guidance is not yet available for reviews of prognostic models. Main results From 52 eligible studies, we identified 12 externally validated models: six were developed for OS, one for PFS and five for TFS. In general, reporting of the studies was poor, especially predictive performance measures for calibration and discrimination; but also basic information, such as eligibility criteria and the recruitment period of participants was often missing. We rated almost all studies at high or unclear risk of bias according to PROBAST. Overall, the applicability of the models and their validation studies was low or unclear; the most common reasons were inappropriate handling of missing data and serious reporting deficiencies concerning eligibility criteria, recruitment period, observation time and prediction performance measures. We report the results for three models predicting OS, which had available data from more than three external validation studies: CLL International Prognostic Index (CLL‐IPI) This score includes five prognostic factors: age, clinical stage, IgHV mutational status, B2‐microglobulin and TP53 status.  Calibration : for the low‐, intermediate‐ and high‐risk groups, the pooled five‐year survival per risk group from validation studies corresponded to the frequencies observed in the model development study. In the very high‐risk group, predicted survival from CLL‐IPI was lower than observed from external validation studies.  Discrimination:  the pooled c‐statistic of seven external validation studies (3307 participants, 917 events) was 0.72 (95% confidence interval (CI) 0.67 to 0.77). The 95% prediction interval (PI) of this model for the c‐statistic, which describes the expected interval for the model's discriminative ability in a new external validation study, ranged from 0.59 to 0.83. Barcelona‐Brno score Aimed at simplifying the CLL‐IPI, this score includes three prognostic factors: IgHV mutational status, del(17p) and del(11q).  Calibration : for the low‐ and intermediate‐risk group, the pooled survival per risk group corresponded to the frequencies observed in the model development study, although the score seems to overestimate survival for the high‐risk group.  Discrimination : the pooled c‐statistic of four external validation studies (1755 participants, 416 events) was 0.64 (95% CI 0.60 to 0.67); 95% PI 0.59 to 0.68. MDACC 2007 index score The authors presented two versions of this model including six prognostic factors to predict OS: age, B2‐microglobulin, absolute lymphocyte count, gender, clinical stage and number of nodal groups. Only one validation study was available for the more comprehensive version of the model, a formula with a nomogram, while seven studies (5127 participants, 994 events) validated the simplified version of the model, the index score.  Calibration : for the low‐ and intermediate‐risk groups, the pooled survival per risk group corresponded to the frequencies observed in the model development study, although the score seems to overestimate survival for the high‐risk group.  Discrimination : the pooled c‐statistic of the seven external validation studies for the index score was 0.65 (95% CI 0.60 to 0.70); 95% PI 0.51 to 0.77. Authors' conclusions Despite the large number of published studies of prognostic models for OS, PFS or TFS for newly‐diagnosed, untreated adults with CLL, only a minority of these (N = 12) have been externally validated for their respective primary outcome. Three models have undergone sufficient external validation to enable meta‐analysis of the model's ability to predict survival outcomes. Lack of reporting prevented us from summarising calibration as recommended. Of the three models, the CLL‐IPI shows the best discrimination, despite overestimation. However, performance of the models may change for individuals with CLL who receive improved treatment options, as the models included in this review were tested mostly on retrospective cohorts receiving a traditional treatment regimen. In conclusion, this review shows a clear need to improve the conducting and reporting of both prognostic model development and external validation studies. For prognostic models to be used as tools in clinical practice, the development of the models (and their subsequent validation studies) should adapt to include the latest therapy options to accurately predict performance. Adaptations should be timely. Plain language summary How well do tools predict what happens with adults with newly‐diagnosed chronic lymphocytic leukaemia (CLL) over time? What was the aim of this review? There are many types of blood cancers called leukaemia. Chronic lymphocytic leukaemia (CLL) is the most common type. Twenty‐five per cent of people who have leukaemia have CLL. It is natural for people with newly‐diagnosed CLL and their families to want to know what will happen with their health in the future. They may be wondering if or when they will need treatment, if or when their disease will get worse or how long people live with CLL. Researchers identified several characteristics that are associated with these outcomes. From these characteristics, they have tried to design tools that help predict what may happen to groups of people with newly‐diagnosed CLL. The aim of this Cochrane Review is to evaluate and summarise those tools and studies that test the tools with other patient data. What are the key messages from this review? Reviewers found that there is no reliable way to predict what might happen over time to people who have (untreated) CLL. One reason is because the prediction tools have not been tested enough times with enough different people to know how well they really work. Another reason is because researchers continue to develop more effective CLL treatment options that have better results, and the prediction tools have not kept up with advances in treatment. What are the main results of the review?   We identified 52 tools that were designed to predict what may happen to people newly‐diagnosed with CLL. To find the best tools, we had to select the studies carefully. To apply these tools in clinical practice: ‐ a tool has to be tested by different researchers to predict what may happen with individuals with CLL in different geographic locations using different groups of people (i.e. age, gender, stage) with CLL. In other words, we would not include a tool if it was only tested on the people who provided their data to create it; ‐ the results of the tool should be consistent to prove that it works; ‐ the tests of the tool have to provide enough information to show how well the tool works. For example, the tests have to include large groups of people and enough information about the type of CLL they have. We found three tools that met these requirements: the CLL International Prognostic Index (CLL‐IPI), the Barcelona‐Brno score, and the MDACC 2007 index score. The CLL‐IPI did the best job at identifying people who would survive longer with CLL and people who would survive less long. However, we rated the quality of the CLL‐IPI studies as low because they did not provide all the information necessary to know how accurate the tool was. The Barcelona‐Brno score and the MDACC 2007 index score, tested on a smaller overall number of patients, showed lower discrimination between persons with a good as compared to a worse prognosis, and showed a similarly low quality of the studies. Conclusion More and better research is needed to develop and test the tools to help predict how CLL will behave for different groups of people over time. The tools must also adapt to accurately predict the performance of new treatments.","7","John Wiley & Sons, Ltd","1465-1858","*Models, Theoretical; Adult; Age Factors; Bias; Biomarkers, Tumor; Calibration; Confidence Intervals; Discriminant Analysis; Disease-Free Survival; Female; Genes, p53 [genetics]; Humans; Immunoglobulin Heavy Chains [genetics]; Immunoglobulin Variable Region [genetics]; Leukemia, Lymphocytic, Chronic, B-Cell [*mortality, pathology]; Male; Neoplasm Staging; Prognosis; Progression-Free Survival; Receptors, Antigen, B-Cell [genetics]; Reproducibility of Results; Tumor Suppressor Protein p53 [genetics]","10.1002/14651858.CD012022.pub2","http://dx.doi.org/10.1002/14651858.CD012022.pub2","Haematology"
"CD012643.PUB3","Aldin, A; Umlauff, L; Estcourt, LJ; Collins, G; Moons, KG; Engert, A; Kobe, C; von Tresckow, B; Haque, M; Foroutan, F; Kreuzberger, N; Trivella, M; Skoetz, N","Interim PET‐results for prognosis in adults with Hodgkin lymphoma: a systematic review and meta‐analysis of prognostic factor studies","Cochrane Database of Systematic Reviews","2020","Abstract - Background Hodgkin lymphoma (HL) is one of the most common haematological malignancies in young adults and, with cure rates of 90%, has become curable for the majority of individuals. Positron emission tomography (PET) is an imaging tool used to monitor a tumour’s metabolic activity, stage and progression. Interim PET during chemotherapy has been posited as a prognostic factor in individuals with HL to distinguish between those with a poor prognosis and those with a better prognosis. This distinction is important to inform decision‐making on the clinical pathway of individuals with HL. Objectives To determine whether in previously untreated adults with HL receiving first‐line therapy, interim PET scan results can distinguish between those with a poor prognosis and those with a better prognosis, and thereby predict survival outcomes in each group. Search methods We searched MEDLINE, Embase, CENTRAL and conference proceedings up until April 2019. We also searched one trial registry ( ClinicalTrials.gov ). Selection criteria We included retrospective and prospective studies evaluating interim PET scans in a minimum of 10 individuals with HL (all stages) undergoing first‐line therapy. Interim PET was defined as conducted during therapy (after one, two, three or four treatment cycles). The minimum follow‐up period was at least 12 months. We excluded studies if the trial design allowed treatment modification based on the interim PET scan results. Data collection and analysis We developed a data extraction form according to the Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies (CHARMS). Two teams of two review authors independently screened the studies, extracted data on overall survival (OS), progression‐free survival (PFS) and PET‐associated adverse events (AEs), assessed risk of bias (per outcome) according to the Quality in Prognosis Studies (QUIPS) tool, and assessed the certainty of the evidence (GRADE). We contacted investigators to obtain missing information and data. Main results Our literature search yielded 11,277 results. In total, we included 23 studies (99 references) with 7335 newly‐diagnosed individuals with classic HL (all stages). Participants in 16 studies underwent (interim) PET combined with computed tomography (PET‐CT), compared to PET only in the remaining seven studies. The standard chemotherapy regimen included ABVD (16) studies, compared to BEACOPP or other regimens (seven studies). Most studies (N = 21) conducted interim PET scans after two cycles (PET2) of chemotherapy, although PET1, PET3 and PET4 were also reported in some studies. In the meta‐analyses, we used PET2 data if available as we wanted to ensure homogeneity between studies. In most studies interim PET scan results were evaluated according to the Deauville 5‐point scale (N = 12). Eight studies were not included in meta‐analyses due to missing information and/or data; results were reported narratively. For the remaining studies, we pooled the unadjusted hazard ratio (HR). The timing of the outcome measurement was after two or three years (the median follow‐up time ranged from 22 to 65 months) in the pooled studies. Eight studies explored the independent prognostic ability of interim PET by adjusting for other established prognostic factors (e.g. disease stage, B symptoms). We did not pool the results because the multivariable analyses adjusted for a different set of factors in each study. Overall survival Twelve (out of 23) studies reported OS. Six of these were assessed as low risk of bias in all of the first four domains of QUIPS (study participation, study attrition, prognostic factor measurement and outcome measurement). The other six studies were assessed as unclear, moderate or high risk of bias in at least one of these four domains. Four studies were assessed as low risk, and eight studies as high risk of bias for the domain other prognostic factors (covariates). Nine studies were assessed as low risk, and three studies as high risk of bias for the domain 'statistical analysis and reporting'. We pooled nine studies with 1802 participants. Participants with HL who have a negative interim PET scan result probably have a large advantage in OS compared to those with a positive interim PET scan result (unadjusted HR 5.09, 95% confidence interval (CI) 2.64 to 9.81, I² = 44%, moderate‐certainty evidence). In absolute values, this means that 900 out of 1000 participants with a negative interim PET scan result will probably survive longer than three years compared to 585 (95% CI 356 to 757) out of 1000 participants with a positive result. Adjusted results from two studies also indicate an independent prognostic value of interim PET scan results (moderate‐certainty evidence). Progression‐free survival Twenty‐one studies reported PFS. Eleven out of 21 were assessed as low risk of bias in the first four domains. The remaining were assessed as unclear, moderate or high risk of bias in at least one of the four domains. Eleven studies were assessed as low risk, and ten studies as high risk of bias for the domain other prognostic factors (covariates). Eight studies were assessed as high risk, thirteen as low risk of bias for statistical analysis and reporting. We pooled 14 studies with 2079 participants. Participants who have a negative interim PET scan result may have an advantage in PFS compared to those with a positive interim PET scan result, but the evidence is very uncertain (unadjusted HR 4.90, 95% CI 3.47 to 6.90, I² = 45%, very low‐certainty evidence). This means that 850 out of 1000 participants with a negative interim PET scan result may be progression‐free longer than three years compared to 451 (95% CI 326 to 569) out of 1000 participants with a positive result. Adjusted results (not pooled) from eight studies also indicate that there may be an independent prognostic value of interim PET scan results (low‐certainty evidence). PET‐associated adverse events No study measured PET‐associated AEs. Authors' conclusions This review provides moderate‐certainty evidence that interim PET scan results predict OS, and very low‐certainty evidence that interim PET scan results predict progression‐free survival in treated individuals with HL. This evidence is primarily based on unadjusted data. More studies are needed to test the adjusted prognostic ability of interim PET against established prognostic factors. Plain language summary Imaging with positron emission tomography (PET) during chemotherapy to predict outcome in adults with Hodgkin lymphoma Review question This Cochrane Review aimed to find out whether the results of a positron emission tomography (PET) during therapy in people with Hodgkin lymphoma (HL) can help to distinguish between those with a poor prognosis and those with a better prognosis, and predict survival outcomes in each group. Background Hodgkin lymphoma is a cancer which affects the lymphoid system of the body. It is considered a relatively rare disease (two to three cases per 100,000 people every year in Western countries), that is most common in young adults in their twenties, but it can also occur in children and elderly people. As treatment options have improved, most people with HL can now be cured. It is important that individuals receive the treatment with the greatest efficacy and least toxicity possible. PET is an imaging tool for assessing the disease stage of an individual, and monitoring tumour activity. It has been suggested that PET performed during therapy (so‐called interim PET, e.g. after two cycles of chemotherapy) can distinguish between people who respond well to therapy and those who do not respond well. The aim of this review was to demonstrate the prognostic ability to distinguish between these groups, and predict survival outcomes in each group, to help clinicians make an informed decision on the treatment pathway to improve long‐term outcomes and safety for people with HL. Study characteristics We included 23 studies to explore the association between interim PET scan results after one to four cycles of chemotherapy and survival outcomes in adults with HL (all stages). We contacted 10 authors, and six provided us with relevant information and/or data. Key results In 16 included studies, participants received either ABVD chemotherapy or BEACOPP chemotherapy (four studies) only, with or without radiotherapy. In 16 studies, participants underwent an interim PET scan in combination with a computed tomography (CT) (PET‐CT), which have higher accuracy in detecting primary and secondary cancers than a PET scan alone. In the remaining seven studies, PET‐only was conducted. Twenty‐one studies conducted interim PET scans after two cycles (PET2) of chemotherapy. Eight studies did not report enough data on our outcomes or population of interest, so we reported the results from these studies narratively. We combined individual study results in meta‐analyses to provide robust evidence for our outcomes of interest overall survival and progression‐free survival. No study measured PET‐associated adverse events (harms). For overall survival, combined results from nine studies (1802 participants) show that there is probably a large advantage in overall survival for people with a negative interim PET scan compared to people with a positive interim PET scan. For progression‐free survival, combined results from 14 studies (2079 participants) show that interim PET‐negative people may have an advantage for progression‐free survival, compared to interim PET‐positive people, but we are uncertain about this result. These are unadjusted results, where interim PET was tested as the only prognostic factor. Eight studies reported adjusted results, where the independent prognostic ability of interim PET was assessed against other established prognostic factors (e.g. disease stage, B symptoms). We could not combine individual study results because the studies did not include identical sets of covariates. Nevertheless, their results indicate a probable independent prognostic ability of interim PET to predict both outcomes. Certainty of the evidence Regarding the unadjusted results, we rated our certainty of the evidence as 'moderate' for overall survival. This means that the true effect is likely to be close to the estimated effect, but there is a possibility that it is substantially different. For progression‐free survival, we rated our certainty of the evidence as 'very low', meaning that we have little confidence in the effect estimate, and that the true effect is likely to be substantially different from the estimated effect. Regarding the adjusted results, we rated our certainty of the evidence as 'moderate' for overall survival, and 'low' for progression‐free survival. How up‐to‐date is this review? We searched data bases up until 2 April 2019, and one trial registry on 25 January 2019.","1","John Wiley & Sons, Ltd","1465-1858","Antineoplastic Combined Chemotherapy Protocols [*therapeutic use]; Chemoradiotherapy; Decision Making; Disease Progression; Disease-Free Survival; Hodgkin Disease [*drug therapy]; Humans; Positron Emission Tomography Computed Tomography [*methods]; Prognosis; Young Adult","10.1002/14651858.CD012643.pub3","http://dx.doi.org/10.1002/14651858.CD012643.pub3","Haematology"
"CD011284.PUB2","Hayden, JA; Wilson, MN; Riley, RD; Iles, R; Pincus, T; Ogilvie, R","Individual recovery expectations and prognosis of outcomes in non‐specific low back pain: prognostic factor review","Cochrane Database of Systematic Reviews","2019","Abstract - Background Low back pain is costly and disabling. Prognostic factor evidence can help healthcare providers and patients understand likely prognosis, inform the development of prediction models to identify subgroups, and may inform new treatment strategies. Recent studies have suggested that people who have poor expectations for recovery experience more back pain disability, but study results have differed. Objectives To synthesise evidence on the association between recovery expectations and disability outcomes in adults with low back pain, and explore sources of heterogeneity. Search methods The search strategy included broad and focused electronic searches of MEDLINE, Embase, CINAHL, and PsycINFO to 12 March 2019, reference list searches of relevant reviews and included studies, and citation searches of relevant expectation measurement tools. Selection criteria We included low back pain prognosis studies from any setting assessing general, self‐efficacy, and treatment expectations (measured dichotomously and continuously on a 0 ‐ 10 scale), and their association with work participation, clinically important recovery, functional limitations, or pain intensity outcomes at short (3 months), medium (6 months), long (12 months), and very long (> 16 months) follow‐up. Data collection and analysis We extracted study characteristics and all reported estimates of unadjusted and adjusted associations between expectations and related outcomes. Two review authors independently assessed risks of bias using the Quality in Prognosis Studies (QUIPS) tool. We conducted narrative syntheses and meta‐analyses when appropriate unadjusted or adjusted estimates were available. Two review authors independently graded and reported the overall quality of evidence. Main results We screened 4635 unique citations to include 60 studies (30,530 participants). Thirty‐five studies were conducted in Europe, 21 in North America, and four in Australia. Study populations were mostly chronic (37%), from healthcare (62%) or occupational settings (26%). General expectation was the most common type of recovery expectation measured (70%); 16 studies measured more than one type of expectation. Usable data for syntheses were available for 52 studies (87% of studies; 28,885 participants). We found moderate‐quality evidence that positive recovery expectations are strongly associated with better work participation (narrative synthesis: 21 studies; meta‐analysis: 12 studies, 4777 participants: odds ratio (OR) 2.43, 95% confidence interval (CI) 1.64 to 3.62), and low‐quality evidence for clinically important recovery outcomes (narrative synthesis: 12 studies; meta‐analysis: 5 studies, 1820 participants: OR 1.89, 95% CI 1.49 to 2.41), both at follow‐up times closest to 12 months, using adjusted data. The association of recovery expectations with other outcomes of interest, including functional limitations (narrative synthesis: 10 studies; meta‐analysis: 3 studies, 1435 participants: OR 1.40, 95% CI 0.85 to 2.31) and pain intensity (narrative synthesis: 9 studies; meta‐analysis: 3 studies, 1555 participants: OR 1.15, 95% CI 1.08 to 1.23) outcomes at follow‐up times closest to 12 months using adjusted data, is less certain, achieving very low‐ and low‐quality evidence, respectively. No studies reported statistically significant or clinically important negative associations between recovery expectations and any low back pain outcome. Authors' conclusions We found that individual recovery expectations are probably strongly associated with future work participation (moderate‐quality evidence) and may be associated with clinically important recovery outcomes (low‐quality evidence). The association of recovery expectations with other outcomes of interest is less certain. Our findings suggest that recovery expectations should be considered in future studies, to improve prognosis and management of low back pain. Plain language summary The impact of individual recovery expectations on pain, limitations in activities and return to work in low back pain What is the aim of this review? The aim of this Cochrane Review is to find out if positive recovery expectations of people with low back pain are related to their future pain, activities they are able to do and return to work. Are people who think they will recover from their low back pain more likely to get better? Key messages People with low back pain who have positive expectations of their own recovery are more likely to return to work and to recover from pain and increase the activities they are able to do. What was studied in this review? Low back pain is costly and causes a lot of disability. It is important to understand what characteristics of a person with low back pain are connected with how well they will recover (also known as their ‘prognosis’). People’s characteristics are often not changeable, including a characteristic like age. However, there is evidence that someone’s expectations of recovery may be changeable. If positive expectations are indeed connected to improved back pain outcomes then helping a person to have positive expectations of their own recovery may help them to recover. For this review, we examined three types of recovery expectations and their relation to back pain outcomes: general expectations of recovery (e.g. will your back pain last only a short time?), self‐efficacy expectations (e.g. do you believe you will be able to return to your normal activities?) and treatment expectations (e.g. will physiotherapy improve your back pain?). What are the main results of this review? We reviewed 4635 references and included 60 relevant studies. These studies included information about 30,530 people with low back pain. They looked at people's expectations of their own recovery and how that was related to their pain, limitations in activities and return to work one year after their back pain episode. Overall, we found good evidence that positive expectations of recovery are related to a higher likelihood of returning to work. The evidence about positive recovery expectations with other recovery, limitations in activities and pain intensity outcomes is not as strong. We did not find any studies that showed that positive expectations of recovery were related to worse low back pain outcomes. How up‐to‐date is this review? The review authors searched for studies that had been published up to 12 March 2019.","11","John Wiley & Sons, Ltd","1465-1858","*Motivation; Adult; Chronic Pain [psychology, therapy]; Humans; Low Back Pain [*psychology, *therapy]; Pain Measurement; Prognosis; Randomized Controlled Trials as Topic; Treatment Outcome","10.1002/14651858.CD011284.pub2","http://dx.doi.org/10.1002/14651858.CD011284.pub2","Back and Neck"
"CD012661.PUB2","Richter, B; Hemmingsen, B; Metzendorf, MI; Takwoingi, Y","Development of type 2 diabetes mellitus in people with intermediate hyperglycaemia","Cochrane Database of Systematic Reviews","2018","Abstract - Background Intermediate hyperglycaemia (IH) is characterised by one or more measurements of elevated blood glucose concentrations, such as impaired fasting glucose (IFG), impaired glucose tolerance (IGT) and elevated glycosylated haemoglobin A1c (HbA1c). These levels are higher than normal but below the diagnostic threshold for type 2 diabetes mellitus (T2DM). The reduced threshold of 5.6 mmol/L (100 mg/dL) fasting plasma glucose (FPG) for defining IFG, introduced by the American Diabetes Association (ADA) in 2003, substantially increased the prevalence of IFG. Likewise, the lowering of the HbA1c threshold from 6.0% to 5.7% by the ADA in 2010 could potentially have significant medical, public health and socioeconomic impacts. Objectives To assess the overall prognosis of people with IH for developing T2DM, regression from IH to normoglycaemia and the difference in T2DM incidence in people with IH versus people with normoglycaemia. Search methods We searched MEDLINE, Embase, ClincialTrials.gov and the International Clinical Trials Registry Platform (ICTRP) Search Portal up to December 2016 and updated the MEDLINE search in February 2018. We used several complementary search methods in addition to a Boolean search based on analytical text mining. Selection criteria We included prospective cohort studies investigating the development of T2DM in people with IH. We used standard definitions of IH as described by the ADA or World Health Organization (WHO). We excluded intervention trials and studies on cohorts with additional comorbidities at baseline, studies with missing data on the transition from IH to T2DM, and studies where T2DM incidence was evaluated by documents or self‐report only. Data collection and analysis One review author extracted study characteristics, and a second author checked the extracted data. We used a tailored version of the Quality In Prognosis Studies (QUIPS) tool for assessing risk of bias. We pooled incidence and incidence rate ratios (IRR) using a random‐effects model to account for between‐study heterogeneity. To meta‐analyse incidence data, we used a method for pooling proportions. For hazard ratios (HR) and odds ratios (OR) of IH versus normoglycaemia, reported with 95% confidence intervals (CI), we obtained standard errors from these CIs and performed random‐effects meta‐analyses using the generic inverse‐variance method. We used multivariable HRs and the model with the greatest number of covariates. We evaluated the certainty of the evidence with an adapted version of the GRADE framework. Main results We included 103 prospective cohort studies. The studies mainly defined IH by IFG 5.6  (FPG mmol/L 5.6 to 6.9 mmol/L or 100 mg/dL to 125 mg/dL), IFG 6.1  (FPG 6.1 mmol/L to 6.9 mmol/L or 110 mg/dL to 125 mg/dL), IGT (plasma glucose 7.8 mmol/L to 11.1 mmol/L or 140 mg/dL to 199 mg/dL two hours after a 75 g glucose load on the oral glucose tolerance test, combined IFG and IGT (IFG/IGT), and elevated HbA1c (HbA1c 5.7 : HbA1c 5.7% to 6.4% or 39 mmol/mol to 46 mmol/mol; HbA1c 6.0 : HbA1c 6.0% to 6.4% or 42 mmol/mol to 46 mmol/mol). The follow‐up period ranged from 1 to 24 years. Ninety‐three studies evaluated the overall prognosis of people with IH measured by cumulative T2DM incidence, and 52 studies evaluated glycaemic status as a prognostic factor for T2DM by comparing a cohort with IH to a cohort with normoglycaemia. Participants were of Australian, European or North American origin in 41 studies; Latin American in 7; Asian or Middle Eastern in 50; and Islanders or American Indians in 5. Six studies included children and/or adolescents. Cumulative incidence of T2DM associated with IFG 5.6 , IFG 6.1 , IGT and the combination of IFG/IGT increased with length of follow‐up. Cumulative incidence was highest with IFG/IGT, followed by IGT, IFG 6.1  and IFG 5.6 . Limited data showed a higher T2DM incidence associated with HbA1c 6.0  compared to HbA1c 5.7 . We rated the evidence for overall prognosis as of moderate certainty because of imprecision (wide CIs in most studies). In the 47 studies reporting restitution of normoglycaemia, regression ranged from 33% to 59% within one to five years follow‐up, and from 17% to 42% for 6 to 11 years of follow‐up (moderate‐certainty evidence). Studies evaluating the prognostic effect of IH versus normoglycaemia reported different effect measures (HRs, IRRs and ORs). Overall, the effect measures all indicated an elevated risk of T2DM at 1 to 24 years of follow‐up. Taking into account the long‐term follow‐up of cohort studies, estimation of HRs for time‐dependent events like T2DM incidence appeared most reliable. The pooled HR and the number of studies and participants for different IH definitions as compared to normoglycaemia were: IFG 5.6 : HR 4.32 (95% CI 2.61 to 7.12), 8 studies, 9017 participants; IFG 6.1 : HR 5.47 (95% CI 3.50 to 8.54), 9 studies, 2818 participants; IGT: HR 3.61 (95% CI 2.31 to 5.64), 5 studies, 4010 participants; IFG and IGT: HR 6.90 (95% CI 4.15 to 11.45), 5 studies, 1038 participants; HbA1c 5.7 : HR 5.55 (95% CI 2.77 to 11.12), 4 studies, 5223 participants; HbA1c 6.0 : HR 10.10 (95% CI 3.59 to 28.43), 6 studies, 4532 participants. In subgroup analyses, there was no clear pattern of differences between geographic regions. We downgraded the evidence for the prognostic effect of IH versus normoglycaemia to low‐certainty evidence due to study limitations because many studies did not adequately adjust for confounders. Imprecision and inconsistency required further downgrading due to wide 95% CIs and wide 95% prediction intervals (sometimes ranging from negative to positive prognostic factor to outcome associations), respectively. This evidence is up to date as of 26 February 2018. Authors' conclusions Overall prognosis of people with IH worsened over time. T2DM cumulative incidence generally increased over the course of follow‐up but varied with IH definition. Regression from IH to normoglycaemia decreased over time but was observed even after 11 years of follow‐up. The risk of developing T2DM when comparing IH with normoglycaemia at baseline varied by IH definition. Taking into consideration the uncertainty of the available evidence, as well as the fluctuating stages of normoglycaemia, IH and T2DM, which may transition from one stage to another in both directions even after years of follow‐up, practitioners should be careful about the potential implications of any active intervention for people 'diagnosed' with IH. Plain language summary Development of type 2 diabetes mellitus in people with intermediate hyperglycaemia ('prediabetes') Review question We wanted to find out whether raised blood sugar ('prediabetes') increases the risk of developing type 2 diabetes and how many of these people return to having normal blood sugar levels (normoglycaemia). We also investigated the difference in type 2 diabetes development in people with prediabetes compared to people with normoglycaemia. Background Type 2 diabetes is often diagnosed by blood sugar measurements like fasting blood glucose or glucose measurements after an oral glucose tolerance test (drinking 75 g of glucose on an empty stomach) or by measuring glycosylated haemoglobin A1c (HbA1c), a long‐term marker of blood glucose levels. Type 2 diabetes can have bad effects on health in the long term (diabetic complications), like severe eye or kidney disease or diabetic feet, eventually resulting in foot ulcers. Raised blood glucose levels (hyperglycaemia), which are above normal ranges but below the limit of diagnosing type 2 diabetes, indicate prediabetes, or intermediate hyperglycaemia. The way prediabetes is defined has important effects on public health because some physicians treat people with prediabetes with medications that can be harmful. For example, reducing the threshold for defining impaired fasting glucose (after an overnight fast) from 6.1 mmol/L or 110 mg/dL to 5.6 mmol/L or 100 mg/dL, as done by the American Diabetes Association (ADA), dramatically increased the number of people diagnosed with prediabetes worldwide. Study characteristics We searched for observational studies (studies where no intervention takes place but people are observed over prolonged periods of time) that investigated how many people with prediabetes at the beginning of the study developed type 2 diabetes. We also evaluated studies comparing people with prediabetes to people with normoglycaemia. Prediabetes was defined by different blood glucose measurements. We found 103 studies, monitoring people over 1 to 24 years. More than 250,000 participants began the studies. In 41 studies the participants were of Australian, European or North American origin, in 7 studies participants were primarily of Latin American origin and in 50 studies participants were of Asian or Middle Eastern origin. Three studies had American Indians as participants, and one study each invited people from Mauritius and Nauru. Six studies included children, adolescents or both as participants. This evidence is up to date as of 26 February 2018. Key results Generally, the development of new type 2 diabetes (diabetes incidence) in people with prediabetes increased over time. However, many participants also reverted from prediabetes back to normal blood glucose levels. Compared to people with normoglycaemia, those with prediabetes (any definition) showed an increased risk of developing type 2 diabetes, but results showed wide differences and depended on how prediabetes was measured. There were no clear differences with regard to several regions in the world or different populations. Because people with prediabetes may develop diabetes but may also change back to normoglycaemia almost any time, doctors should be careful about treating prediabetes because we are not sure whether this will result in more benefit than harm, especially when done on a global scale affecting many people worldwide. Certainty of the evidence The certainty of the evidence for overall prognosis was moderate because results varied widely. The certainty of evidence for studies comparing prediabetic with normoglycaemic people was low because the results were not precise and varied widely. In our included observational studies the researchers often did not investigate well enough whether factors like physical inactivity, age or increased body weight also influenced the development of type 2 diabetes, thus making the relationship between prediabetes and the development of type 2 diabetes less clear.","10","John Wiley & Sons, Ltd","1465-1858","Blood Glucose [analysis]; Diabetes Mellitus, Type 2 [epidemiology, *etiology]; Disease Progression; Humans; Hyperglycemia [blood, *complications]; Incidence; Prediabetic State [blood]; Prognosis; Prospective Studies","10.1002/14651858.CD012661.pub2","http://dx.doi.org/10.1002/14651858.CD012661.pub2","Metabolic and Endocrine Disorders"
"CD012841.PUB2","Westby, MJ; Dumville, JC; Stubbs, N; Norman, G; Wong, JKF; Cullum, N; Riley, RD","Protease activity as a prognostic factor for wound healing in venous leg ulcers","Cochrane Database of Systematic Reviews","2018","Abstract - Background Venous leg ulcers (VLUs) are a common type of complex wound that have a negative impact on people's lives and incur high costs for health services and society. It has been suggested that prolonged high levels of protease activity in the later stages of the healing of chronic wounds may be associated with delayed healing. Protease modulating treatments have been developed which seek to modulate protease activity and thereby promote healing in chronic wounds. Objectives To determine whether protease activity is an independent prognostic factor for the healing of venous leg ulcers. Search methods In February 2018, we searched the following databases: Cochrane Central Register of Controlled Trials (CENTRAL), Ovid MEDLINE, Ovid Embase and CINAHL. Selection criteria We included prospective and retrospective longitudinal studies with any follow‐up period that recruited people with VLUs and investigated whether protease activity in wound fluid was associated with future healing of VLUs. We included randomised controlled trials (RCTs) analysed as cohort studies, provided interventions were taken into account in the analysis, and case‐control studies if there were no available cohort studies. We also included prediction model studies provided they reported separately associations of individual prognostic factors (protease activity) with healing. Studies of any type of protease or combination of proteases were eligible, including proteases from bacteria, and the prognostic factor could be examined as a continuous or categorical variable; any cut‐off point was permitted. The primary outcomes were time to healing (survival analysis) and the proportion of people with ulcers completely healed; the secondary outcome was change in ulcer size/rate of wound closure. We extracted unadjusted (simple) and adjusted (multivariable) associations between the prognostic factor and healing. Data collection and analysis Two review authors independently assessed studies for inclusion at each stage, and undertook data extraction, assessment of risk of bias and GRADE assessment. We collected association statistics where available. No study reported adjusted analyses: instead we collected unadjusted results or calculated association measures from raw data. We calculated risk ratios when both outcome and prognostic factor were dichotomous variables. When the prognostic factor was reported as continuous data and healing outcomes were dichotomous, we either performed regression analysis or analysed the impact of healing on protease levels, analysing as the standardised mean difference. When both prognostic factor and outcome were continuous data, we reported correlation coefficients or calculated them from individual participant data. We displayed all results on forest plots to give an overall visual representation. We planned to conduct meta‐analyses where this was appropriate, otherwise we summarised narratively. Main results We included 19 studies comprising 21 cohorts involving 646 participants. Only 11 studies (13 cohorts, 522 participants) had data available for analysis. Of these, five were prospective cohort studies, four were RCTs and two had a type of case‐control design. Follow‐up time ranged from four to 36 weeks. Studies covered 10 different matrix metalloproteases (MMPs) and two serine proteases (human neutrophil elastase and urokinase‐type plasminogen activators). Two studies recorded complete healing as an outcome; other studies recorded partial healing measures. There was clinical and methodological heterogeneity across studies; for example, in the definition of healing, the type of protease and its measurement, the distribution of active and bound protease species, the types of treatment and the reporting of results. Therefore, meta‐analysis was not performed. No study had conducted multivariable analyses and all included evidence was of very low certainty because of the lack of adjustment for confounders, the high risk of bias for all studies except one, imprecision around the measures of association and inconsistency in the direction of association. Collectively the research indicated complete uncertainty as to the association between protease activity and VLU healing. Authors' conclusions This review identified very low validity evidence regarding any association between protease activity and VLU healing and there is complete uncertainty regarding the relationship. The review offers information for both future research and systematic review methodology. Plain language summary Protease activity and its association with future healing of venous leg ulcers What is the aim of this review? The aim of this Cochrane Review was to find out if there is a link between different levels of protease in venous leg ulcers (open skin wounds on the lower leg caused by problems with the way blood flows through the veins) now and the healing of wounds at some time in the future. Protease is an enzyme, a chemical naturally produced by the body that breaks down proteins and which may affect wound healing. We wanted to know whether having higher protease levels meant that wounds were less likely to heal or to heal more slowly. If so, this could help find the most useful treatments for each person with a leg ulcer. Review authors from Cochrane collected and analysed all relevant studies to answer this question and found 19 studies. Key messages At the moment, there is complete uncertainty about any association between protease activity and venous leg ulcer healing, but this review did give pointers on what may be important for future research on natural chemicals present in wounds and their effect on healing. What was studied in the review? Venous leg ulcers can last weeks, months or years. Leg ulcers can be painful, may become infected, and may affect mobility and quality of life. The usual treatment for venous leg ulcers is compression therapy (e.g. compression (elastic) bandages), but even this does not work for everyone (about a third of people still have wounds that have not healed after six months). We wanted to find out why these wounds often do not heal, and whether there are factors in the wound (called biomarkers) that can indicate which wounds are unlikely to heal. It has been suggested that wounds are slow to heal when there are high levels of protease. In this review, we investigated whether there was any evidence that higher protease levels at the start of a study were associated with slower healing leg ulcers or less healing at a future time point (such as six months). In February 2018, we searched for relevant studies that had a reliable design and that investigated links between protease levels and future healing of venous leg ulcers. We found 19 studies involving 646 people. Not all studies reported the age and sex of participants. In those that did, the average age of the participants varied from 51 to 75 years. Eleven studies gave results we could use, involving 13 groups of people. Most people had wounds that had been there for at least three months. What were the main results of the review? There were many differences among the included studies: for example, how they defined healing, the type of proteases and how they measured them, the types of treatment and how they reported results. This lack of consistency meant we could not combine and compare the results, so we summarised the findings in a general way. A bigger problem was that none of the studies had analysed the data appropriately as they did not take into account the impact of age or infection or treatments, and so we could not be sure that it was the protease levels that were important for healing, rather than age or other factors. Most studies were small and could have been better conducted, so it was difficult to be sure how meaningful the results were. Overall, the certainty of the evidence was very low. Further studies are needed to explore the importance of biomarkers for wound healing. How up to date is this review? We searched for studies that had been published up to February 2018.","9","John Wiley & Sons, Ltd","1465-1858","*Wound Healing; Case‐Control Studies; Humans; Peptide Hydrolases [*metabolism]; Prognosis; Prospective Studies; Randomized Controlled Trials as Topic; Regression Analysis; Retrospective Studies; Survival Analysis; Varicose Ulcer [*enzymology]","10.1002/14651858.CD012841.pub2","http://dx.doi.org/10.1002/14651858.CD012841.pub2","Wounds"
