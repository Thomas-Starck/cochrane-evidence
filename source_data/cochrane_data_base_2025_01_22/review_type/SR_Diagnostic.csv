Cochrane Review ID,Author(s),Title,Source,Year,Abstract,Issue,Publisher,ISSN,Keywords,DOI,URL,Cochrane Review Group Code
"CD016134","Yunas, I; Gallos, ID; Devall, AJ; Podesek, M; Allotey, J; Takwoingi, Y; Coomarasamy, A","Tests for diagnosis of postpartum haemorrhage at vaginal birth","Cochrane Database of Systematic Reviews","2025","Abstract - Background Postpartum haemorrhage (PPH) is the leading cause of maternal mortality worldwide. Accurate diagnosis of PPH can prevent adverse outcomes by enabling early treatment. Objectives What is the accuracy of methods (index tests) for diagnosing primary PPH (blood loss ≥ 500 mL in the first 24 hours after birth) and severe primary PPH (blood loss ≥ 1000 mL in the first 24 hours after birth) (target conditions) in women giving birth vaginally (participants) compared to weighed blood loss measurement or other objective measurements of blood loss (reference standards)? Search methods We searched CENTRAL, MEDLINE, Embase, Web of Science Core Collection, ClinicalTrials.gov, and the World Health Organization International Clinical Trials Registry Platform to 24 May 2024. Selection criteria We included women who gave birth vaginally in any setting. Study types included diagnostic cohort studies and cross‐sectional studies that reported 2 x 2 data (number of true positive, false positive, false negative, and true negative results) or where the 2 x 2 data could be derived from test accuracy estimates. Eligible index tests included: visual estimation; calibrated blood collection devices; approach with calibrated drape and observations; blood loss estimation using the SAPHE (Signalling a Postpartum Hemorrhage Emergency) Mat; blood loss field image analysis and other technologies; uterine atony assessment; clinical variables (e.g. heart rate, blood pressure, shock index); early warning charts; haemoglobin levels; and predelivery fibrinogen levels. Eligible reference standards included objective methods such as: gravimetric blood loss measurement, which involves weighing collected blood, as well as weighing blood‐soaked pads, gauze and sheets, and subtracting their dry weight; calibrated devices to measure blood volume (volumetric blood loss measurement); the alkaline‐haematin method of blood loss estimation; and blood extracted using machine‐extraction and measured spectrometrically as oxyhaemoglobin. Data collection and analysis At least two review authors, working independently, undertook study screening, selection, data extraction, assessment of risk of bias, and assessment of the certainty of the evidence. We resolved any differences through consensus or with input from another author. We generated 2 x 2 tables of the true positives, true negatives, false positives, and false negatives to calculate the sensitivity, specificity, and 95% confidence intervals for each index test. We presented sensitivity and specificity estimates from studies in forest plots. Where possible, we conducted meta‐analyses for each index test and reference standard combination for each target condition. We examined heterogeneity by visual inspection of the forest plots. Main results Our review included 18 studies with a total of 291,040 participants. Fourteen studies evaluated PPH and seven studies evaluated severe PPH. Most studies were conducted in a hospital setting (16 of 18). There were four studies at high risk of bias for the patient selection domain and 14 studies at low risk. For the index test domain, 10 studies were at low risk of bias, seven studies at high risk, and one study at uncertain risk. For the reference standard domain, one study was at high risk of bias and 17 studies at low risk. For the flow and timing domain, three studies were at high risk of bias and 15 studies at low risk. The applicability concerns were low for all studies across the domains. In the abstract, we have prioritised reporting results for common, important thresholds for index tests or where the certainty of the evidence for the sensitivity estimate was at least moderate. Full results are in the main body of the review. PPH (blood loss ≥ 500 mL) For PPH, visual estimation with gravimetric blood loss measurement as the reference standard had 48% sensitivity (95% confidence interval (CI) 44% to 53%; moderate certainty) and 97% specificity (95% CI 95% to 99%; high certainty) (4 studies, 196,305 participants). Visual estimation with volumetric blood loss measurement as the reference standard showed 22% sensitivity (95% CI 12% to 37%; moderate certainty) and 99% specificity (95% CI 97% to 100%; moderate certainty) (2 studies, 514 participants). The diagnostic approach with calibrated drape plus observations, with gravimetric blood loss measurement as the reference standard for PPH, showed 93% sensitivity (95% CI 92% to 94%; high certainty) and 95% specificity (95% CI 95% to 96%; high certainty) (2 studies, 53,762 participants). A haemoglobin level of less than 10 g/dL with gravimetric blood loss measurement as the reference standard showed 37% sensitivity (95% CI 30% to 44%; high certainty) and 79% specificity (95% CI 76% to 82%; high certainty) (1 study, 1058 participants). Severe PPH (blood loss ≥ 1000 mL) For severe PPH, visual estimation, with volumetric plus gravimetric blood loss measurement as the reference standard, showed 9% sensitivity (95% CI 0% to 41%; low certainty) and 100% specificity (95% CI 99% to 100%; moderate certainty) (1 study, 274 participants). A shock index level of 1.0 or higher (commonly used as a threshold for severe PPH) up to two hours after birth, with gravimetric blood loss measurement as the reference standard, showed 30% sensitivity (95% CI 27% to 33%; moderate certainty) and 93% specificity (95% CI 92% to 93%; moderate certainty) (1 study, 30,820 participants). A haemoglobin level of less than 10 g/dL, with gravimetric blood loss measurement as the reference standard, showed 71% sensitivity (95% CI 51% to 87%; moderate certainty) and 77% specificity (95% CI 75% to 80%; high certainty) (1 study, 1058 participants). Authors' conclusions Visual estimation of blood loss to diagnose PPH showed low sensitivity and is likely to miss the diagnosis in half of women giving birth vaginally. A diagnostic approach using a calibrated drape to objectively measure blood loss plus clinical observations showed high sensitivity and specificity for diagnosing PPH. Other index tests showed low to moderate sensitivities in diagnosing PPH and severe PPH. Future research should determine the accuracy of diagnostic tests in non‐hospital settings and consider combining index tests to increase the sensitivity of PPH diagnosis. Funding Bill and Melinda Gates Foundation Registration PROSPERO (CRD42024541874) Plain language summary How accurate are tests that diagnose excessive bleeding (postpartum haemorrhage) after women give birth vaginally? Key messages • Our study found that using visual estimation to diagnose postpartum haemorrhage (PPH) was inaccurate. Using a plastic drape to collect and measure the volume of blood loss – alongside observations including heart rate, blood pressure, tone of the womb, and flow of blood – showed high accuracy. • Other tests, including blood tests and measurements such as heart rate and blood pressure alone, showed varying levels of accuracy. What is postpartum haemorrhage (PPH)? The World Health Organization defines PPH as blood loss of 500 mL or more in the first 24 hours after delivery in women who gave birth vaginally. Severe PPH is when 1000 mL or more of blood is lost in the same time period. Why is an accurate diagnosis of PPH important? PPH is the commonest cause of mothers dying worldwide. Accurately diagnosing it can help in early treatment. What are the tests used to diagnose PPH? The commonest test used to diagnose PPH and severe PPH is visual estimation, where a birth attendant estimates the volume of blood lost by looking at the extent to which bedsheets and pads are soaked with blood. Other tests include measuring the volume of blood lost using a plastic drape, tray or bowl with markings indicating the volume. Blood can also be collected and weighed. This weight is converted to a volume using a formula. Other tests include measuring changes in (1) the levels of certain chemicals in the birthing woman's blood, or (2) variables such as her heart rate and blood pressure. What did we want to find out? We wanted to identify different tests and methods used to diagnose PPH and severe PPH and find out how accurate they are. What did we do? We looked for studies that assessed the accuracy of tests used to diagnose PPH and severe PPH when compared with a reliable standard such as weighed blood loss or the measured volume of blood loss. Women of any age who gave birth vaginally in any setting (hospitals, delivery units in the community, home births) were included. We included studies which provided data that we could use to determine two measures of test accuracy: (1) sensitivity (percentage of women with the condition who are correctly identified by the test); and (2) specificity (percentage of women without the condition who are correctly excluded by the test). Where appropriate, we combined the results across these studies. We excluded studies that did not provide this type of information. Examples of tests we looked for included the assessor: looking at the blood loss and estimating the blood volume (visual estimation); measuring the volume of blood loss in a drape or other collecting device with markings indicating the volume (volumetric method); weighing blood loss using scales (gravimetric method); measuring changes in variables such as heart rate and blood pressure, or changes in blood levels of chemicals such as haemoglobin and fibrinogen. We also tried to find studies which combined the tests mentioned above and tests using new technologies such as camera systems and artificial intelligence. What did we find? We found 18 studies with a total of 291,040 participants. Fourteen studies assessed diagnostic tests for PPH and seven studies assessed tests for severe PPH. Most of the studies were performed in hospitals. Visual estimation had poor sensitivity. In one of our analyses, we found that visual estimation will only diagnose 48 out of 100 women who have PPH, but 52 women with PPH will be missed (i.e. will be false negatives). These missed cases may not receive treatment and so may suffer avoidable harm and may even die. Of every 100 women without PPH, three will be wrongly diagnosed as having it (i.e. will be false positives). These incorrectly diagnosed women may receive treatment which they don't require and may suffer harm as a result. A diagnostic approach which used a calibrated drape to measure the volume of blood loss, along with observations such as heart rate, blood pressure, the tone of the womb, and flow of blood to diagnose PPH had very good sensitivity and specificity. Using this approach will diagnose 93 out of 100 women who have PPH, and only seven women with PPH will be missed (i.e. will be false negatives). Of every 100 women without PPH, five will be wrongly diagnosed as having it (i.e. will be false positives). Other tests showed varying levels of accuracy for diagnosing PPH and severe PPH. What are the limitations of the evidence? The studies we found were mainly performed in hospitals. We would have liked more information on how accurate the tests are in other settings, such as in the community and at home. We would also have liked to know how accurate the other tests are when used in combination, particularly the combination of measured blood loss, changes in factors such as heart rate and blood pressure, and changes in blood chemical levels. How current is this evidence? This evidence is current to 24 May 2024.","1","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.CD016134","http://dx.doi.org/10.1002/14651858.CD016134","Central Editorial Service"
"CD013725.PUB2","Naeem, F; McCleery, J; Hietamies, TM; Abakar Ismail, F; Clinton, S; O'Mahony, A; Ponce, OJ; Quinn, TJ","Diagnostic test accuracy of self‐administered cognitive assessment tools for dementia","Cochrane Database of Systematic Reviews","2024","Abstract - Background Dementia is a chronic and progressive clinical syndrome that can present with a range of cognitive and behavioural symptoms. Global prevalence is projected to increase due to ageing populations, particularly in resource‐limited settings, with significant associated health and social care costs. There is a critical need for accurate cognitive assessment as part of the diagnostic workup for dementia. Although self‐administered cognitive assessment tools are not diagnostic, they can be used to assess cognition. The role of these tests is uncertain, and their diagnostic test accuracy remains unclear, but they may be useful tools in circumstances where face‐to‐face assessment may be difficult. Objectives Primary objective To assess the test accuracy of any self‐administered cognitive assessment tool for the diagnosis of any form of dementia in any setting, including community and secondary health care. Secondary objectives To identify: the quality and quantity of the research evidence describing test accuracy of self‐administered testing; sources of heterogeneity in the test accuracy described; and gaps in the evidence where further research is required. Search methods We searched MEDLINE (Ovid SP), Embase (Ovid), Web of Science ‐ BIOSIS Citation Index, PsycINFO (Ovid), LILACS (BIREME), and CINAHL (EBSCO). The most recent searches were run on 2 November 2022. Selection criteria We included cross‐sectional studies investigating the accuracy of a self‐administered cognitive assessment tool. We included all settings, including community and secondary health care. The target condition of interest was a clinical diagnosis of dementia, therefore, we included only studies where the index test was administered alongside a reference standard clinical assessment. Our population of interest was any adult (over 18 years of age). Data collection and analysis Two review authors independently reviewed included studies, assessed risk of bias using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool, and extracted data. We obtained information on study design and participant characteristics, setting of index test, details of index text, reference standard used, and results reported as sensitivity and specificity. We performed a meta‐analysis on three studies that used the same threshold score. Main results The review included 11 eligible studies, with a total of 2303 participants, which evaluated the diagnostic test accuracy of six different self‐administered cognitive‐assessment tools. The studies were conducted in Europe, North America, and South Korea within a variety of community and healthcare settings. Our quality assessment found that four studies had a low risk of bias across all domains. Six studies had a high or unclear risk of bias due to patient selection, with concerns around lack of a clear sampling strategy or exclusion criteria, or both. Six studies had a high or unclear risk of bias with regard to the index test due to lack of information about how the test was conducted and evaluated. For the diagnosis of dementia, sensitivity of self‐administered cognitive assessment tools ranged from 55% to 100% and specificity ranged from 45% to 100%. Three studies described the diagnostic test accuracy of Test Your Memory at a threshold of 42/50. Quantitative meta‐analysis estimated a summary point with 94% sensitivity (95% confidence interval (CI) 90% to 96%) and 66% specificity (95% CI 45% to 82%) at this threshold. Authors' conclusions There is insufficient evidence to recommend the use of any single self‐administered cognitive assessment tool. The tools had test accuracy scores that are similar to the range seen with standard pencil and paper cognitive screening tests conducted by clinicians. Further research on the optimal test and threshold score, and how that may be impacted by setting, language, and educational level is needed. Plain language summary How accurate are cognitive assessment tools, that are completed by people themselves, at detecting dementia? Key messages ‐ The evidence suggests that cognitive assessment tools that are completed by people themselves could be used in the detection and diagnosis of dementia. ‐ There is not enough evidence to recommend one type of assessment tool over another. ‐ Further research on the use of these assessment tools in different settings, such as clinics or people's homes, and the scores that indicate dementia is needed. Why is improving the detection of dementia important? The number of people diagnosed with dementia is expected to increase significantly in the future because global populations are living longer. This will have significant healthcare and social costs. Dementia is a permanent condition where memory deteriorates until people need assistance with their day‐to‐day activities. Accurate and quick diagnosis allows people with dementia and their families to access treatment and support. Failure to recognise the condition when it is present (a false‐negative test result) may lead to a delay in accessing treatments to help with memory, thinking, and behaviours, and accessing support from social services, such as social work and occupational therapy. An incorrect diagnosis of dementia (a false‐positive test) may be emotionally and psychologically distressing for people and their families. Currently, diagnosis of dementia involves a face‐to‐face assessment at a specialised clinic, where the medical history, a physical examination, blood tests, and brain scans are considered. What is a self‐administered cognitive assessment tool? Self‐administered cognitive assessment tools are tests completed by people themselves. They are designed to assess different aspects of mental capacity (i.e. cognition) including memory, language, and perception. The tools may involve questions to answer or tasks to complete. These self‐administered tools would not replace the detailed clinical assessment required for a diagnosis of dementia, but they could be useful in situations where face‐to‐face assessment might be difficult, such as during global pandemics, or in areas with few medical services. There are a number of different cognitive assessment tools; these vary in their content, scoring systems, and format (electronic or written). What did we want to find out? We wanted to find out how accurate cognitive assessment tools that can be completed by people themselves are at detecting dementia. What did we do? We searched for studies that investigated the accuracy of self‐administered cognitive assessment tools for dementia in adults (over the age of 18 years) in any healthcare setting. What did we find? We found 11 studies with a total of 2303 participants that tested six different tools. Five studies looked at one test called 'Test Your Memory'; this was tested in five different languages. Two studies looked at the 'Self‐Administered Gerocognitive Examination'; one study tested a written version of the test and the other tested an electronic version. One study looked at each of the following cognitive assessment tools individually: the 'Clock Completion Test', 'Korean Dementia Screening Questionnaire‐Cognition', 'BrainCheck Memory', and 'MyMemCheck'. These tools all have different scoring systems and some studies looked at multiple cut‐off scores ‐ to indicate whether or not the person had dementia ‐ within each tool. Studies took place across Europe, the USA, and South Korea. The healthcare settings included community centres, nursing homes, and hospital clinics, and some studies included multiple settings. The average age of the people taking the tests varied amongst studies from 50 to 60 years to 80 to 90 years. We found that these cognitive assessment tools could correctly identify people with dementia between 55% and 100% of the time, and could correctly rule out dementia between 45% and 100% of the time. These ranges are due to differences in study populations, settings, type of assessment tool, and diagnostic cut‐off scores (i.e. score at which dementia was diagnosed). We summarised results from three studies that used 'Test Your Memory' and found that using a cut‐off score of 42/50, the assessment tool could correctly identify people with dementia around 94% of the time and correctly rule out dementia around 66% of the time. How reliable are the results of this review? In the studies we found, the tools' diagnosis of whether a person had dementia was confirmed by a traditional, face‐to‐face clinical assessment for dementia. There were some problems with the methods used in some studies, including the way participants were selected for the studies, and the timings and details of the clinical assessments were not always clear. Some studies had small numbers of participants. The test accuracy results are a summary based on evidence from the studies we examined. The results produced from individual studies varied considerably, so we cannot be sure these cognitive assessment tools will always produce the same results in clinical practice. How up to date is this review? The evidence is up to date to November 2022.","12","John Wiley & Sons, Ltd","1465-1858","*Dementia [diagnosis]; *Sensitivity and Specificity; Aged; Cognition; Humans; Mental Status and Dementia Tests [standards]; Neuropsychological Tests [standards]","10.1002/14651858.CD013725.pub2","http://dx.doi.org/10.1002/14651858.CD013725.pub2","Central Editorial Service"
"CD014780","Davenport, C; Arevalo-Rodriguez, I; Mateos-Haro, M; Berhane, S; Dinnes, J; Spijker, R; Buitrago-Garcia, D; Ciapponi, A; Takwoingi, Y; Deeks, JJ; Emperador, D; Leeflang, MM. G.; Van den Bruel, A","The effect of sample site and collection procedure on identification of SARS‐CoV‐2 infection","Cochrane Database of Systematic Reviews","2024","Abstract - Background Sample collection is a key driver of accuracy in the diagnosis of SARS‐CoV‐2 infection. Viral load may vary at different anatomical sampling sites and accuracy may be compromised by difficulties obtaining specimens and the expertise of the person taking the sample. It is important to optimise sampling accuracy within cost, safety and accessibility constraints. Objectives To compare the sensitivity of different sampling collection sites and methods for the detection of current SARS‐CoV‐2 infection with any molecular or antigen‐based test. Search methods Electronic searches of the Cochrane COVID‐19 Study Register and the COVID‐19 Living Evidence Database from the University of Bern (which includes daily updates from PubMed and Embase and preprints from medRxiv and bioRxiv) were undertaken on 22 February 2022. We included independent evaluations from national reference laboratories, FIND and the Diagnostics Global Health website. We did not apply language restrictions. Selection criteria We included studies of symptomatic or asymptomatic people with suspected SARS‐CoV‐2 infection undergoing testing. We included studies of any design that compared results from different sample types (anatomical location, operator, collection device) collected from the same participant within a 24‐hour period. Data collection and analysis Within a sample pair, we defined a reference sample and an index sample collected from the same participant within the same clinical encounter (within 24 hours). Where the sample comparison was different anatomical sites, the reference standard was defined as a nasopharyngeal or combined naso/oropharyngeal sample collected into the same sample container and the index sample as the alternative anatomical site. Where the sample comparison was concerned with differences in the sample collection method from the same site, we defined the reference sample as that closest to standard practice for that sample type. Where the sample pair comparison was concerned with differences in personnel collecting the sample, the more skilled or experienced operator was considered the reference sample. Two review authors independently assessed the risk of bias and applicability concerns using the QUADAS‐2 and QUADAS‐C checklists, tailored to this review. We present estimates of the difference in the sensitivity (reference sample (%) minus index sample sensitivity (%)) in a pair and as an average across studies for each index sampling method using forest plots and tables. We examined heterogeneity between studies according to population (age, symptom status) and index sample (time post‐symptom onset, operator expertise, use of transport medium) characteristics. Main results This review includes 106 studies reporting 154 evaluations and 60,523 sample pair comparisons, of which 11,045 had SARS‐CoV‐2 infection. Ninety evaluations were of saliva samples, 37 nasal, seven oropharyngeal, six gargle, six oral and four combined nasal/oropharyngeal samples. Four evaluations were of the effect of operator expertise on the accuracy of three different sample types. The majority of included evaluations (146) used molecular tests, of which 140 used RT‐PCR (reverse transcription polymerase chain reaction). Eight evaluations were of nasal samples used with Ag‐RDTs (rapid antigen tests). The majority of studies were conducted in Europe (35/106, 33%) or the USA (27%) and conducted in dedicated COVID‐19 testing clinics or in ambulatory hospital settings (53%). Targeted screening or contact tracing accounted for only 4% of evaluations. Where reported, the majority of evaluations were of adults (91/154, 59%), 28 (18%) were in mixed populations with only seven (4%) in children. The median prevalence of confirmed SARS‐CoV‐2 was 23% (interquartile (IQR) 13%‐40%). Risk of bias and applicability assessment were hampered by poor reporting in 77% and 65% of included studies, respectively. Risk of bias was low across all domains in only 3% of evaluations due to inappropriate inclusion or exclusion criteria, unclear recruitment, lack of blinding, nonrandomised sampling order or differences in testing kit within a sample pair. Sixty‐eight percent of evaluation cohorts were judged as being at high or unclear applicability concern either due to inflation of the prevalence of SARS‐CoV‐2 infection in study populations by selectively including individuals with confirmed PCR‐positive samples or because there was insufficient detail to allow replication of sample collection. When used with RT‐PCR • There was no evidence of a difference in sensitivity between gargle and nasopharyngeal samples (on average ‐1 percentage points, 95% CI ‐5 to +2, based on 6 evaluations, 2138 sample pairs, of which 389 had SARS‐CoV‐2). • There was no evidence of a difference in sensitivity between saliva collection from the deep throat and nasopharyngeal samples (on average +10 percentage points, 95% CI ‐1 to +21, based on 2192 sample pairs, of which 730 had SARS‐CoV‐2). • There was evidence that saliva collection using spitting, drooling or salivating was on average ‐12 percentage points less sensitive (95% CI ‐16 to ‐8, based on 27,253 sample pairs, of which 4636 had SARS‐CoV‐2) compared to nasopharyngeal samples. We did not find any evidence of a difference in the sensitivity of saliva collected using spitting, drooling or salivating (sensitivity difference: range from ‐13 percentage points (spit) to –21 percentage points (salivate)). • Nasal samples (anterior and mid‐turbinate collection combined) were, on average, 12 percentage points less sensitive compared to nasopharyngeal samples (95% CI ‐17 to ‐7), based on 9291 sample pairs, of which 1485 had SARS‐CoV‐2. We did not find any evidence of a difference in sensitivity between nasal samples collected from the mid‐turbinates (3942 sample pairs) or from the anterior nares (8272 sample pairs). • There was evidence that oropharyngeal samples were, on average, 17 percentage points less sensitive than nasopharyngeal samples (95% CI ‐29 to ‐5), based on seven evaluations, 2522 sample pairs, of which 511 had SARS‐CoV‐2. A much smaller volume of evidence was available for combined nasal/oropharyngeal samples and oral samples. Age, symptom status and use of transport media do not appear to affect the sensitivity of saliva samples and nasal samples. When used with Ag‐RDTs • There was no evidence of a difference in sensitivity between nasal samples compared to nasopharyngeal samples (sensitivity, on average, 0 percentage points ‐0.2 to +0.2, based on 3688 sample pairs, of which 535 had SARS‐CoV‐2). Authors' conclusions When used with RT‐PCR, there is no evidence for a difference in sensitivity of self‐collected gargle or deep‐throat saliva samples compared to nasopharyngeal samples collected by healthcare workers when used with RT‐PCR. Use of these alternative, self‐collected sample types has the potential to reduce cost and discomfort and improve the safety of sampling by reducing risk of transmission from aerosol spread which occurs as a result of coughing and gagging during the nasopharyngeal or oropharyngeal sample collection procedure. This may, in turn, improve access to and uptake of testing. Other types of saliva, nasal, oral and oropharyngeal samples are, on average, less sensitive compared to healthcare worker‐collected nasopharyngeal samples, and it is unlikely that sensitivities of this magnitude would be acceptable for confirmation of SARS‐CoV‐2 infection with RT‐PCR. When used with Ag‐RDTs, there is no evidence of a difference in sensitivity between nasal samples and healthcare worker‐collected nasopharyngeal samples for detecting SARS‐CoV‐2. The implications of this for self‐testing are unclear as evaluations did not report whether nasal samples were self‐collected or collected by healthcare workers. Further research is needed in asymptomatic individuals, children and in Ag‐RDTs, and to investigate the effect of operator expertise on accuracy. Quality assessment of the evidence base underpinning these conclusions was restricted by poor reporting. There is a need for further high‐quality studies, adhering to reporting standards for test accuracy studies. Plain language summary How accurate are different types of sample collection for diagnosing COVID‐19 infection? Key messages • When used with RT‐PCR tests (a molecular test that detects genetic material in COVID‐19 using a technique called reverse transcription polymerase chain reaction), self‐collected gargle and deep throat saliva samples have a similar sensitivity compared to trained healthcare worker‐collected nasopharyngeal samples (taken from the back of the throat through the nose) in detecting COVID‐19. • When used with RT‐PCR, samples collected from the nose, oropharynx (throat via the mouth), oral cavity, and other saliva collection methods are less sensitive for detecting COVID‐19 compared to healthcare worker‐collected nasopharyngeal samples. • When used with rapid antigen tests (Ag‐RDTs; at‐home/self‐tests), samples collected from the nose have a similar sensitivity to healthcare worker‐collected nasopharyngeal samples in detecting COVID‐19. Why is improving the diagnosis of COVID‐19 important? Coronavirus disease (COVID‐19) is caused by infection with severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2). People with suspected COVID‐19 may decide to take a test to know whether they are infected, so that they can receive treatment, and follow recommended guidance to self‐isolate and inform close contacts. Not detecting COVID‐19 when it is present (a false negative result) risks spreading infection and results in missed opportunities for treatment. Types of sample collection methods for diagnosing COVID‐19? The type and quality of sample taken for confirmation of COVID‐19 affects the reliability of diagnosis. The most accurate type of sample to diagnose COVID‐19 is that taken by a trained healthcare worker from the back of the throat through the nose (a nasopharyngeal sample). This type of test detects genetic material in the virus using a technique called reverse transcription polymerase chain reaction (RT‐PCR). However, this sample is difficult to obtain correctly, causes discomfort and risks spreading infection if individuals cough or sneeze when the sample is taken. Alternative sample types, particularly those that can be self‐collected using rapid antigen tests (Ag‐RDTs; i.e. self‐tests), may reduce cost and discomfort and improve the safety of sampling. This may, in turn, improve access to and uptake of testing. What did we want to find out? We wanted to compare the sensitivity of different sample sites and collection methods in detecting COVID‐19 with molecular tests (RT‐PCR tests) or self‐based tests (Ag‐RDT tests). What did we do? We searched for studies that had compared the accuracy of nasopharyngeal samples to any alternative that could be used in patients outside of hospital, including nose (nasal) samples, throat samples taken through the mouth (oropharyngeal), gargle samples and saliva samples. We looked at the use of samples with either RT‐PCR or Ag‐RDTs. We also searched for studies that had compared different methods for taking samples, such as samples collected by a healthcare worker compared to those collected by individuals with no or minimal instructions. What did we find? The review included 106 studies with a total of 60,523 participants, of whom 11,045 had COVID‐19 infection. Fifty‐nine per cent of studies were conducted on adults and 79% on symptomatic or mixed symptomatic and asymptomatic participants. Sixty per cent of studies took place in Europe or the USA; just over half (55%) took place in dedicated COVID‐19 testing centres or in outpatient settings. Main results With RT‐PCR, on average: ‐ 100% of positive nasopharyngeal samples collected by healthcare workers would also test positive on self‐collected gargle samples or saliva samples (collected by coughing and then spitting (deep throat saliva)); ‐ 88% of positive nasopharyngeal samples collected by healthcare workers would also test positive with self‐ or healthcare worker‐collected nose samples; ‐ 87% of positive nasopharyngeal samples collected by healthcare workers would also be detected by saliva self‐collected using spitting, 84% by saliva self‐collected using drooling and 79% by saliva self‐collected by sucking on a swab; and ‐ 83% of positive nasopharyngeal samples collected by healthcare workers would also be detected by self‐ or healthcare worker‐collected oropharyngeal samples. With Ag‐RDTs, on average: ‐ 100% of positive nasopharyngeal samples collected by healthcare workers would also be detected by self‐collected or healthcare worker‐collected nose samples. Summary results The results of these studies indicate that in a group of 1000 people, of whom 230 (23%) have COVID‐19, then: when used with PCR, compared to healthcare worker‐collected nasopharyngeal samples: ‐ no cases of COVID‐19 would be missed using self‐collected gargle samples (12 less to 5 more) or deep throat saliva samples (2 less to 48 more); ‐ 28 (16 to 39) fewer cases of COVID‐19 infection would be detected using healthcare worker‐ or self‐collected nose sample; ‐ 30 (18 to 41) fewer cases of COVID‐19 infection would be detected by saliva self‐collected using spitting, 37 (12 to 62) fewer by saliva collected by drooling and 48 (12 to 85) fewer by saliva collected by sucking on a swab; ‐ 39 (12 to 67) fewer cases of COVID‐19 infection would be detected by healthcare worker‐ or self‐collected oropharyngeal samples; and when used with Ag‐RDTs, compared to healthcare worker‐collected nasopharyngeal samples: no cases of COVID‐19 infection would be missed using healthcare worker‐ or self‐collected nose samples. What are the limitations of the evidence? It was often not clear whether included studies deliberately excluded inadequate samples or whether the results of the more accurate nasopharyngeal sample were known when alternative samples were interpreted. This may have resulted in alternative sample types appearing more accurate than they are in practice, decreasing the number of missed cases of COVID‐19 infection. More than half of studies did not give information about how long participants had had symptoms at the time of sampling. This reduces our confidence in the comparison of different sample types. Most studies evaluated self‐collected samples by adults with symptoms for use with RT‐PCR; therefore, the findings of this review may not be applicable to asymptomatic individuals or children. For studies conducted with Ag‐RDTs, it is unclear whether sensitivity estimates of nose samples are applicable to home use (self‐collected and self‐interpreted). How up to date is this review? The evidence is current to 22 February 2022.","12","John Wiley & Sons, Ltd","1465-1858","*COVID-19 [diagnosis]; *Nasopharynx [virology]; *SARS-CoV-2 [isolation & purification]; *Sensitivity and Specificity; *Specimen Handling [methods]; COVID-19 Nucleic Acid Testing [methods]; COVID-19 Testing [methods]; Humans; Nasal Cavity [virology]; Nose [virology]; Oropharynx [virology]; Pharynx [virology]; Viral Load","10.1002/14651858.CD014780","http://dx.doi.org/10.1002/14651858.CD014780","Central Editorial Service"
"CD014839.PUB2","Maung Myint, T; Chong, CH; von Huben, A; Attia, J; Webster, AC; Blosser, CD; Craig, JC; Teixeira-Pinto, A; Wong, G","Serum and urine nucleic acid screening tests for BK polyomavirus‐associated nephropathy in kidney and kidney‐pancreas transplant recipients","Cochrane Database of Systematic Reviews","2024","Abstract - Background BK polyomavirus‐associated nephropathy (BKPyVAN) occurs when BK polyomavirus (BKPyV) affects a transplanted kidney, leading to an initial injury characterised by cytopathic damage, inflammation, and fibrosis. BKPyVAN may cause permanent loss of graft function and premature graft loss. Early detection gives clinicians an opportunity to intervene by timely reduction in immunosuppression to reduce adverse graft outcomes. Quantitative nucleic acid testing (QNAT) for detection of BKPyV DNA in blood and urine is increasingly used as a screening test as diagnosis of BKPyVAN by kidney biopsy is invasive and associated with procedural risks. In this review, we assessed the sensitivity and specificity of QNAT tests in patients with BKPyVAN. Objectives We assessed the diagnostic test accuracy of blood/plasma/serum BKPyV QNAT and urine BKPyV QNAT for the diagnosis of BKPyVAN after transplantation. We also investigated the following sources of heterogeneity: types and quality of studies, era of publication, various thresholds of BKPyV‐DNAemia/BKPyV viruria and variability between assays as secondary objectives. Search methods We searched MEDLINE (OvidSP), EMBASE (OvidSP), and BIOSIS, and requested a search of the Cochrane Register of diagnostic test accuracy studies from inception to 13 June 2023. We also searched ClinicalTrials.com and the WHO International Clinical Trials Registry Platform for ongoing trials. Selection criteria We included cross‐sectional or cohort studies assessing the diagnostic accuracy of two index tests (blood/plasma/serum BKPyV QNAT or urine BKPyV QNAT) for the diagnosis of BKPyVAN, as verified by the reference standard (histopathology). Both retrospective and prospective cohort studies were included. We did not include case reports and case control studies. Data collection and analysis Two authors independently carried out data extraction from each study. We assessed the methodological quality of the included studies by using Quality Assessment of Diagnostic‐Accuracy Studies (QUADAS‐2) assessment criteria. We used the bivariate random‐effects model to obtain summary estimates of sensitivity and specificity for the QNAT test with one positivity threshold. In cases where meta‐analyses were not possible due to the small number of studies available, we detailed the descriptive evidence and used a summative approach. We explored possible sources of heterogeneity by adding covariates to meta‐regression models. Main results We included 31 relevant studies with a total of 6559 participants in this review. Twenty‐six studies included kidney transplant recipients, four studies included kidney and kidney‐pancreas transplant recipients, and one study included kidney, kidney‐pancreas and kidney‐liver transplant recipients. Studies were carried out in South Asia and the Asia‐Pacific region (12 studies), North America (9 studies), Europe (8 studies), and South America (2 studies). Index test: blood/serum/plasma BKPyV QNAT The diagnostic performance of blood BKPyV QNAT using a common viral load threshold of 10,000 copies/mL was reported in 18 studies (3434 participants). Summary estimates at 10,000 copies/mL as a cut‐off indicated that the pooled sensitivity was 0.86 (95% confidence interval (CI) 0.78 to 0.93) while the pooled specificity was 0.95 (95% CI 0.91 to 0.97). A limited number of studies were available to analyse the summary estimates for individual viral load thresholds other than 10,000 copies/mL. Indirect comparison of thresholds of the three different cut‐off values of 1000 copies/mL (9 studies), 5000 copies/mL (6 studies), and 10,000 copies/mL (18 studies), the higher cut‐off value at 10,000 copies/mL corresponded to higher specificity with lower sensitivity. The summary estimates of indirect comparison of thresholds above 10,000 copies/mL were uncertain, primarily due to a limited number of studies with wide CIs contributed to the analysis. Nonetheless, these indirect comparisons should be interpreted cautiously since differences in study design, patient populations, and methodological variations among the included studies can introduce biases. Analysis of all blood BKPyV QNAT studies, including various blood viral load thresholds (30 studies, 5658 participants, 7 thresholds), indicated that test performance remains robust, pooled sensitivity 0.90 (95% CI 0.85 to 0.94) and specificity 0.93 (95% CI 0.91 to 0.95). In the multiple cut‐off model, including the various thresholds generating a single curve, the optimal cut‐off was around 2000 copies/mL, sensitivity of 0.89 (95% CI 0.66 to 0.97) and specificity of 0.88 (95% CI 0.80 to 0.93). However, as most of the included studies were retrospective, and not all participants underwent the reference standard tests, this may result in a high risk of selection and verification bias. Index test: urine BKPyV QNAT There was insufficient data to thoroughly investigate both accuracy and thresholds of urine BKPyV QNAT resulting in an imprecise estimation of its accuracy based on the available evidence. Authors' conclusions There is insufficient evidence to suggest the use of urine BKPyV QNAT as the primary screening tool for BKPyVAN. The summary estimates of the test sensitivity and specificity of blood/serum/plasma BKPyV QNAT test at a threshold of 10,000 copies/mL for BKPyVAN were 0.86 (95% CI 0.78 to 0.93) and 0.95 (95% CI 0.91 to 0.97), respectively. The multiple cut‐off model showed that the optimal cut‐off was around 2000 copies/mL, with test sensitivity of 0.89 (95% CI 0.66 to 0.97) and specificity of 0.88 (95% CI 0.80 to 0.93). While 10,000 copies/mL is the most commonly used cut‐off, with good test performance characteristics and supports the current recommendations, it is important to interpret the results with caution because of low‐certainty evidence. Plain language summary Why is improving the diagnosis of BK polyomavirus‐associated nephropathy (BKPyVAN) important? Key messages • The studies in this review suggest that testing for BK polyomavirus in blood samples is a reliable screening tool for detecting BKPyVAN. • Using a blood BK virus level of 10,000 or more copies/mL, the chances of missing a diagnosis of BKPyVAN was 6 patients per 1000 and wrongly diagnosing BKPyVAN when a patient did not have it was 48 per 1000. • Using a blood BK virus level of 2000 or more copies/mL, the chances of missing a diagnosis of BKPyVAN was 5 patients per 1000, and wrongly diagnosing BKPyVAN when a patient did not have it was 115 per 1000. What is BKPyVAN? BK polyomavirus is a virus with symptoms that closely resemble a cold, commonly affects people during their childhood, and more than 80% of people have been exposed to the virus. This virus can persist in the kidney and usually does not cause ill effects unless the immune system is weakened, such as in kidney transplant recipients. BK polyomavirus can replicate and cause further harm to the kidney transplant, known as BKPyVAN. What is the best way to diagnose BKPyVAN? One of the ways to detect this is through a kidney biopsy (where a hollow needle is inserted into the kidney to withdraw a very small sample for testing). However, this method is invasive and not always perfect, as the needle may miss the part of the kidney affected by the virus. Other methods include testing for the presence of virus DNA in the blood and urine samples using a test called QNAT. Not recognising BKPyVAN when it is present (a false negative test result) may result in worsening kidney function and possibly needing to return to dialysis. An incorrect diagnosis when a person does not have the infection (a false positive test result) may cause an unnecessary reduction in the immunosuppression (medicines to prevent their body from trying to fight (or reject) this new kidney), thus increasing the risk of rejection of a transplanted kidney. What did we want to find out? We wanted to find out how accurate QNAT testing for the amount of BV virus in the blood or the urine was for diagnosing BKPyVAN. We also wanted to find out how much BK virus in the blood or urine would predict if a person really had BKPyVAN. What did we do? We searched for all studies that looked at QNAT testing for diagnosing BKPyVAN in people who had received a kidney or combined kidney plus pancreas transplant. We compared and summarised the results of the studies and rated our confidence in the information based on factors such as study methods and sizes. What did we find? Thirty‐one studies (6559 people) were included in this review. Studies were carried out in South Asia and the Asia‐Pacific region, North America, South America and Europe. Twenty‐six studies included people with only a kidney transplant, four studies included only kidney‐pancreas transplant recipients, and one study included kidney, kidney‐pancreas and kidney‐liver transplant recipients. All transplant recipients were at least 18 years old. The results indicate that in theory, using a blood level of 10,000 or more BK virus copies/mL to decide if a person has BKPyVAN, in 1000 kidney transplant recipients, 37 would be diagnosed as having BKPyVAN; however, 6 would be wrongly classified as not having BKPyVAN. Of the 957 classified as not having BKPyVAN, 48 positive patients would be wrongly classified as not having BKPyVAN. If the level of blood BK virus was lowered to 2000 or more virus copies/mL, 38 would be diagnosed as having BKPyVAN, and 5 positive patients would not, while 115 would be wrongly classified as having BKPyVAN. What are the limitations of the evidence? The studies in this review suggest that testing for BK polyomavirus in blood samples is a reliable screening tool for detecting BKPyVAN. It remains uncertain whether using QNAT on urine samples is a reliable method for diagnosing BKPyVAN. How up to date is the evidence? The evidence is current as of June 2023.","11","John Wiley & Sons, Ltd","1465-1858","*BK Virus [isolation & purification]; *DNA, Viral [blood, urine]; *Kidney Diseases [blood, diagnosis, urine, virology]; *Kidney Transplantation [adverse effects]; *Polyomavirus Infections [blood, diagnosis, urine]; *Tumor Virus Infections [diagnosis, urine]; Humans; Pancreas Transplantation [adverse effects]; Postoperative Complications [diagnosis, urine, virology]; Sensitivity and Specificity","10.1002/14651858.CD014839.pub2","http://dx.doi.org/10.1002/14651858.CD014839.pub2","Kidney and Transplant"
"CD013783.PUB2","Tehan, PE; Mills, J; Leask, S; Oldmeadow, C; Peterson, B; Sebastian, M; Chuter, V","Toe‐brachial index and toe systolic blood pressure for the diagnosis of peripheral arterial disease","Cochrane Database of Systematic Reviews","2024","Abstract - Background Peripheral arterial disease (PAD) of the lower limbs is caused by atherosclerotic occlusive disease in which narrowing of arteries reduces blood flow to the lower limbs. PAD is common; it is estimated to affect 236 million individuals worldwide. Advanced age, smoking, hypertension, diabetes and concomitant cardiovascular disease are common factors associated with increased risk of PAD. Complications of PAD can include claudication pain, rest pain, wounds, gangrene, amputation and increased cardiovascular morbidity and mortality. It is therefore clinically important to use diagnostic tests that accurately identify PAD. Accurate and timely detection of PAD allows clinicians to implement appropriate risk management strategies to prevent complications, slow progression or intervene when indicated. Toe‐brachial index (TBI) and toe systolic blood pressure (TSBP) are amongst a suite of non‐invasive bedside tests used to detect PAD. Both TBI and TSBP are commonly utilised by a variety of clinicians in different settings, therefore a systematic review and meta‐analysis of their diagnostic accuracy is warranted and highly relevant to inform clinical practice. Objectives To (1) estimate the accuracy of TSBP and TBI for the diagnosis of PAD in the lower extremities at different cut‐off values for test positivity in populations at risk of PAD, and (2) compare the accuracy of TBI and TSBP for the diagnosis of PAD in the lower extremities. Secondary objectives were to investigate several possible sources of heterogeneity in test accuracy, including the following: patient group tested (people with type 1 or type 2 diabetes, people with renal disease and general population), type of equipment used, positivity threshold and type of reference standard. Search methods The Cochrane Vascular Information Specialist searched the MEDLINE, Embase, CINAHL, Web of Science, LILACS, Zetoc and DARE databases and the World Health Organization International Clinical Trials Registry Platform and ClinicalTrials.gov trials registers to 27 February 2024. Selection criteria We included diagnostic case‐control, cross‐sectional, prospective and retrospective studies in which all participants had either a TSBP or TBI measurement plus a validated method of vascular diagnostic imaging for PAD. We needed to be able to cross‐tabulate (2 x 2 table) results of the index test and the reference standard to include a study. To be included, study populations had to be adults aged 18 years and over. We included studies of symptomatic and asymptomatic participants. Studies had to use TSBP and TBI (also called toe‐brachial pressure index (TBPI)), either individually, or in addition to other non‐invasive tests as index tests to diagnose PAD in individuals with suspected disease. We included data collected by photoplethysmography, laser Doppler, continuous wave Doppler, sphygmomanometers (both manual and aneroid) and manual or automated digital equipment. Data collection and analysis Two review authors independently completed data extraction using a standardised form. We extracted data to populate 2 x 2 contingency tables when available (true positives, true negatives, false positives, false negatives). Where data were not available to enable statistical analysis, we contacted study authors directly. Two review authors working independently undertook quality assessment using QUADAS‐2, with disagreements resolved by a third review author. We incorporated two additional questions into the quality appraisal to aid our understanding of the conduct of studies and make appropriate judgements about risk of bias and applicability. Main results Eighteen studies met the inclusion criteria; 13 evaluated TBI only, one evaluated TSBP only and four evaluated both TBI and TSBP. Thirteen of the studies used colour duplex ultrasound (CDU) as a reference standard, two used computed tomography angiography (CTA), one used multi‐detector row tomography (MDCT), one used angiography and one used a combination of CDU, CTA and angiography. TBI was investigated in 1927 participants and 2550 limbs. TSBP was investigated in 701 participants, of which 701 limbs had TSBP measured. Studies were generally of low methodological quality, with poor reporting of participant recruitment in regard to consecutive or random sampling, and poor reporting of blinding between index test and reference standard, as well as timing between index test and reference standard. The certainty of evidence according to GRADE for most studies was very low. Authors' conclusions Whilst a small number of diagnostic test accuracy studies have been completed for TBI and TSBP to identify PAD, the overall methodological quality was low, with most studies providing a very low certainty of evidence. The evidence base to support the use of TBI and TSBP to identify PAD is therefore limited. Whilst both TBI and TSBP are used extensively clinically, the overall diagnostic performance of these tests remains uncertain. Future research using robust methods and clear reporting is warranted to comprehensively determine the diagnostic test accuracy of the TBI and TSBP for identification of PAD with greater certainty. However, conducting such research where some of the reference tests are invasive and only clinically indicated in populations with known PAD is challenging. Plain language summary How accurate are toe‐brachial index and toe systolic blood pressures for peripheral arterial disease? Key messages ‐ There is currently a lack of quality evidence to support the use of toe‐brachial index (TBI) and toe systolic blood pressure (TSBP) to detect peripheral arterial disease, and clinicians should interpret these tests with caution. ‐ More high‐quality studies with larger numbers of participants are required to increase the certainty of evidence. Why is improving the diagnosis of peripheral arterial disease important? Peripheral arterial disease affects approximately 236 million people globally. In peripheral arterial disease, the arteries that supply blood flow to the legs and feet are affected by fatty deposits (atherosclerosis), which leads to narrowing and can cause blockages. This can result in pain, wounds and gangrene, which can lead to amputation. Not recognising peripheral arterial disease when it is present (a false‐negative test result) may result in the condition not being appropriately treated or a delay in treatment. This could lead to increased rates of wounds, amputation and premature death. An incorrect diagnosis of peripheral arterial disease (a false‐positive result) may result in unnecessary imaging or inappropriate prescription of cardiovascular risk‐reducing medications where there is no benefit to be gained and the potential for side effects. An incorrect diagnosis may also result in stress and anxiety for the patient. What is the toe‐brachial index and toe systolic blood pressure? Clinicians have a variety of tests available to detect peripheral arterial disease, which are non‐invasive and can be performed by the bedside. Two of these tests are the toe‐brachial index (TBI) and toe systolic blood pressure (TSBP). TSBP is a blood pressure taken at the great or second toe. A small blood pressure cuff is wrapped around the toe and a Doppler, laser or photoplethysmograph probe is placed on the toe to detect a blood flow signal. The cuff is then inflated until the signal disappears, and then slowly deflated until the signal re‐appears. The pressure at which the signal returns is considered the TSBP. To measure the TBI, the TSBP is simply divided by the highest blood pressure of both arms, resulting in an index. Lower values indicate that peripheral arterial disease is present. A variety of different thresholds are used in practice and research. What did we want to find out? We wanted to find out how accurate the TBI and TSBP are for detecting peripheral arterial disease in people at risk of the condition. We also wanted to compare the accuracy of the TBI with TSBP. What did we do? We searched for studies that had investigated the accuracy of either TBI or TSBP for peripheral arterial disease, and examined the quality and certainty of these studies. What did we find? We found 18 studies: 13 evaluated TBI only, one evaluated TSBP only and four evaluated both TBI and TSBP. TBI was evaluated in 17 studies including 1927 participants and TSBP was evaluated in five studies including 701 participants. Studies were conducted in the USA, Australia, the UK, Japan, Korea, the Czech Republic, France, Hungary, India and Iran. The mean reported age range of participants was between 63 and 83 years. The studies were carried out in people who were at risk of peripheral arterial disease, such as people with diabetes, kidney disease, older people or people with known risk factors. However, there were not enough studies and the studies we found were too different from each other to allow a reliable estimate of the overall accuracy of the tests for diagnosing peripheral arterial disease. What are the limitations of the evidence? Only a small number of studies were included (18) and the studies were quite different from each other. This is because our inclusion criteria were broad and inclusive. However, this made it impossible to directly compare or pool (combine) the studies together. The studies that we included were generally of low quality, and we have low or very low confidence in the results. The included studies also had some important limitations in the way that they were conducted. This may have resulted in TSBP and the TBI appearing more accurate than they really are in some of the included studies. How up‐to‐date is this evidence? This review was conducted of all literature published up to 27 February 2024.","10","John Wiley & Sons, Ltd","1465-1858","*Ankle Brachial Index; *Blood Pressure [physiology]; *Peripheral Arterial Disease [diagnosis, physiopathology]; *Toes [blood supply]; Blood Pressure Determination [methods]; Humans; Sensitivity and Specificity","10.1002/14651858.CD013783.pub2","http://dx.doi.org/10.1002/14651858.CD013783.pub2","Central Editorial Service"
"CD015522.PUB2","Kang, C; Lo, J-E; Zhang, H; Ng, SM; Lin, JC; Scott, IU; Kalpathy-Cramer, J; Liu, S-H(; Greenberg, PB","Artificial intelligence for diagnosing exudative age‐related macular degeneration","Cochrane Database of Systematic Reviews","2024","Abstract - Background Age‐related macular degeneration (AMD) is a retinal disorder characterized by central retinal (macular) damage. Approximately 10% to 20% of non‐exudative AMD cases progress to the exudative form, which may result in rapid deterioration of central vision. Individuals with exudative AMD (eAMD) need prompt consultation with retinal specialists to minimize the risk and extent of vision loss. Traditional methods of diagnosing ophthalmic disease rely on clinical evaluation and multiple imaging techniques, which can be resource‐consuming. Tests leveraging artificial intelligence (AI) hold the promise of automatically identifying and categorizing pathological features, enabling the timely diagnosis and treatment of eAMD. Objectives To determine the diagnostic accuracy of artificial intelligence (AI) as a triaging tool for exudative age‐related macular degeneration (eAMD). Search methods We searched CENTRAL, MEDLINE, Embase, three clinical trials registries, and Data Archiving and Networked Services (DANS) for gray literature. We did not restrict searches by language or publication date. The date of the last search was April 2024. Selection criteria Included studies compared the test performance of algorithms with that of human readers to detect eAMD on retinal images collected from people with AMD who were evaluated at eye clinics in community or academic medical centers, and who were not receiving treatment for eAMD when the images were taken. We included algorithms that were either internally or externally validated or both. Data collection and analysis Pairs of review authors independently extracted data and assessed study quality using the Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2) tool with revised signaling questions. For studies that reported more than one set of performance results, we extracted only one set of diagnostic accuracy data per study based on the last development stage or the optimal algorithm as indicated by the study authors. For two‐class algorithms, we collected data from the 2x2 table whenever feasible. For multi‐class algorithms, we first consolidated data from all classes other than eAMD before constructing the corresponding 2x2 tables. Assuming a common positivity threshold applied by the included studies, we chose random‐effects, bivariate logistic models to estimate summary sensitivity and specificity as the primary performance metrics. Main results We identified 36 eligible studies that reported 40 sets of algorithm performance data, encompassing over 16,000 participants and 62,000 images. We included 28 studies (78%) that reported 31 algorithms with performance data in the meta‐analysis. The remaining nine studies (25%) reported eight algorithms that lacked usable performance data; we reported them in the qualitative synthesis. Study characteristics and risk of bias Most studies were conducted in Asia, followed by Europe, the USA, and collaborative efforts spanning multiple countries. Most studies identified study participants from the hospital setting, while others used retinal images from public repositories; a few studies did not specify image sources. Based on four of the 36 studies reporting demographic information, the age of the study participants ranged from 62 to 82 years. The included algorithms used various retinal image types as model input, such as optical coherence tomography (OCT) images (N = 15), fundus images (N = 6), and multi‐modal imaging (N = 7). The predominant core method used was deep neural networks. All studies that reported externally validated algorithms were at high risk of bias mainly due to potential selection bias from either a two‐gate design or the inappropriate exclusion of potentially eligible retinal images (or participants). Findings Only three of the 40 included algorithms were externally validated (7.5%, 3/40). The summary sensitivity and specificity were 0.94 (95% confidence interval (CI) 0.90 to 0.97) and 0.99 (95% CI 0.76 to 1.00), respectively, when compared to human graders (3 studies; 27,872 images; low‐certainty evidence). The prevalence of images with eAMD ranged from 0.3% to 49%. Twenty‐eight algorithms were reportedly either internally validated (20%, 8/40) or tested on a development set (50%, 20/40); the pooled sensitivity and specificity were 0.93 (95% CI 0.89 to 0.96) and 0.96 (95% CI 0.94 to 0.98), respectively, when compared to human graders (28 studies; 33,409 images; low‐certainty evidence). We did not identify significant sources of heterogeneity among these 28 algorithms. Although algorithms using OCT images appeared more homogeneous and had the highest summary specificity (0.97, 95% CI 0.93 to 0.98), they were not superior to algorithms using fundus images alone (0.94, 95% CI 0.89 to 0.97) or multimodal imaging (0.96, 95% CI 0.88 to 0.99; P for meta‐regression = 0.239). The median prevalence of images with eAMD was 30% (interquartile range [IQR] 22% to 39%). We did not include eight studies that described nine algorithms (one study reported two sets of algorithm results) to distinguish eAMD from normal images, images of other AMD, or other non‐AMD retinal lesions in the meta‐analysis. Five of these algorithms were generally based on smaller datasets (range 21 to 218 participants per study) yet with a higher prevalence of eAMD images (range 33% to 66%). Relative to human graders, the reported sensitivity in these studies ranged from 0.95 and 0.97, while the specificity ranged from 0.94 to 0.99. Similarly, using small datasets (range 46 to 106), an additional four algorithms for detecting eAMD from other retinal lesions showed high sensitivity (range 0.96 to 1.00) and specificity (range 0.77 to 1.00). Authors' conclusions Low‐ to very low‐certainty evidence suggests that an algorithm‐based test may correctly identify most individuals with eAMD without increasing unnecessary referrals (false positives) in either the primary or the specialty care settings. There were significant concerns for applying the review findings due to variations in the eAMD prevalence in the included studies. In addition, among the included algorithm‐based tests, diagnostic accuracy estimates were at risk of bias due to study participants not reflecting real‐world characteristics, inadequate model validation, and the likelihood of selective results reporting. Limited quality and quantity of externally validated algorithms highlighted the need for high‐certainty evidence. This evidence will require a standardized definition for eAMD on different imaging modalities and external validation of the algorithm to assess generalizability. Plain language summary Is artificial intelligence (AI) better than humans for diagnosing the eye condition 'exudative age‐related macular degeneration'? Key messages • Compared to human experts, artificial intelligence (AI)‐based tests may be comparably accurate at detecting the exudative (or wet) form of age‐related macular degeneration (eAMD). • There were no significant differences in the performance regardless of the other eye conditions in the image dataset or the image types used. • More research and consistent reporting are needed to define the role of AI in the diagnosis of eAMD. What is age‐related macular degeneration?  The macula is the central part of the retina, which is located at the back of the eye. As people age, cells in the macula die or are damaged, making it difficult for them to see clearly. Age‐related macular degeneration (AMD) is a common eye condition that can worsen to exudative (or wet) AMD (eAMD), which reduces vision in the center of the eye from the growth of abnormal blood vessels. Accurate diagnosis of eAMD is important because it allows patients to receive treatment from a retinal specialist. Traditional methods of diagnosing eAMD rely on an eyecare specialist and multiple imaging techniques, which can be time‐ and resource‐consuming. Tests that use artificial intelligence (AI) hold the promise of automatically identifying eAMD. This could help more people with AMD get their eyes checked and receive timely diagnosis and treatment. How can AI help?  AI is a branch of computer science that aims to accomplish tasks that traditionally require human intelligence. AI applications have been developed to examine images of the eye and trained to select those that may show signs of eAMD. Patients can be referred for timely treatment and eye specialists are freed up from time‐consuming eye tests. What did we want to find out?  We wanted to find out how accurate AI tests are compared to human experts in diagnosing eAMD from images of eyes. What did we do?  We searched for studies anywhere in the world that compared the diagnostic performance of AI tests with those of human experts in reading eye images to diagnose eAMD. The images could be from patients seeking eye care at a community clinic or academic medical center or from a database of images. The AI‐based reading results were compared to those of human experts who reviewed the images prior to the AI tests. What did we find?  We identified 36 studies, with more than 16,000 people and 62,000 images that reported the results of 41 different AI tests. More than half of the studies were carried out in Asia, followed by Europe, the USA, and multicountry collaborations. On average, 33% of people in the studies had eAMD. For the three AI tests evaluated on new data beyond the training images, when applied to detect eAMD in 10,000 individuals (including 100 who actually had eAMD), the AI tests would incorrectly identify about 99 people as having eAMD (false positives) and miss approximately 6 cases (false negatives). For the 28 AI tests evaluated solely on training data, using the same scenario, the tests would incorrectly identify about 396 people as having eAMD (false positives) and miss approximately 7 cases (false negatives). The AI tests demonstrated similar performance to human experts, whether they were evaluated using images from their training set or from a new dataset. Performance was similar across image datasets of eAMD and various control groups or image types. What are the limitations of the evidence?  Most of the included studies had flaws in selecting, training or evaluating the AI tests. These study flaws may have made the test results appear better than they were. Consequently, our confidence in the accuracy of the test results was low. Future studies should recruit participants whose age and disease severity reflect real‐world conditions. How up‐to‐date is this evidence?  The evidence is current as of April 2024.","10","John Wiley & Sons, Ltd","1465-1858","*Artificial Intelligence; *Macular Degeneration [diagnosis]; *Sensitivity and Specificity; Bias; Humans; Tomography, Optical Coherence [methods]","10.1002/14651858.CD015522.pub2","http://dx.doi.org/10.1002/14651858.CD015522.pub2","Eyes and Vision"
"CD015618","Arevalo-Rodriguez, I; Mateos-Haro, M; Dinnes, J; Ciapponi, A; Davenport, C; Buitrago-Garcia, D; Bennouna-Dalero, T; Roqué-Figuls, M; Van den Bruel, A; von Eije, KJ; Emperador, D; Hooft, L; Spijker, R; Leeflang, MMG; Takwoingi, Y; Deeks, JJ","Laboratory‐based molecular test alternatives to RT‐PCR for the diagnosis of SARS‐CoV‐2 infection","Cochrane Database of Systematic Reviews","2024","Abstract - Background Diagnosing people with a SARS‐CoV‐2 infection played a critical role in managing the COVID‐19 pandemic and remains a priority for the transition to long‐term management of COVID‐19. Initial shortages of extraction and reverse transcription polymerase chain reaction (RT‐PCR) reagents impaired the desired upscaling of testing in many countries, which led to the search for alternatives to RNA extraction/purification and RT‐PCR testing. Reference standard methods for diagnosing the presence of SARS‐CoV‐2 infection rely primarily on real‐time reverse transcription‐polymerase chain reaction (RT‐PCR). Alternatives to RT‐PCR could, if sufficiently accurate, have a positive impact by expanding the range of diagnostic tools available for the timely identification of people infected by SARS‐CoV‐2, access to testing and the use of resources. Objectives To assess the diagnostic accuracy of alternative (to RT‐PCR assays) laboratory‐based molecular tests for diagnosing SARS‐CoV‐2 infection. Search methods We searched the COVID‐19 Open Access Project living evidence database from the University of Bern until 30 September 2020 and the WHO COVID‐19 Research Database until 31 October 2022. We did not apply language restrictions. Selection criteria We included studies of people with suspected or known SARS‐CoV‐2 infection, or where tests were used to screen for infection, and studies evaluating commercially developed laboratory‐based molecular tests for the diagnosis of SARS‐CoV‐2 infection considered as alternatives to RT‐PCR testing. We also included all reference standards to define the presence or absence of SARS‐CoV‐2, including RT‐PCR tests and established clinical diagnostic criteria. Data collection and analysis Two authors independently screened studies and resolved disagreements by discussing them with a third author. Two authors independently extracted data and assessed the risk of bias and applicability of the studies using the QUADAS‐2 tool. We presented sensitivity and specificity, with 95% confidence intervals (CIs), for each test using paired forest plots and summarised results using average sensitivity and specificity using a bivariate random‐effects meta‐analysis. We illustrated the findings per index test category and assay brand compared to the WHO's acceptable sensitivity and specificity threshold for diagnosing SARS‐CoV‐2 infection using nucleic acid tests. Main results We included data from 64 studies reporting 94 cohorts of participants and 105 index test evaluations, with 74,753 samples and 7517 confirmed SARS‐CoV‐2 cases. We did not identify any published or preprint reports of accuracy for a considerable number of commercially produced NAAT assays. Most cohorts were judged at unclear or high risk of bias in more than three QUADAS‐2 domains. Around half of the cohorts were considered at high risk of selection bias because of recruitment based on COVID status. Three quarters of 94 cohorts were at high risk of bias in the reference standard domain because of reliance on a single RT‐PCR result to determine the absence of SARS‐CoV‐2 infection or were at unclear risk of bias due to a lack of clarity about the time interval between the index test assessment and the reference standard, the number of missing results, or the absence of a participant flow diagram. For index tests categories with four or more evaluations and when summary estimations were possible, we found that: a) For RT‐PCR assays designed to omit/adapt RNA extraction/purification, the average sensitivity was 95.1% (95% CI 91.1% to 97.3%), and the average specificity was 99.7% (95% CI 98.5% to 99.9%; based on 27 evaluations, 2834 samples and 1178 SARS‐CoV‐2 cases); b) For RT‐LAMP assays, the average sensitivity was 88.4% (95% CI 83.1% to 92.2%), and the average specificity was 99.7% (95% CI 98.7% to 99.9%; 24 evaluations, 29,496 samples and 2255 SARS‐CoV‐2 cases); c) for TMA assays, the average sensitivity was 97.6% (95% CI 95.2% to 98.8%), and the average specificity was 99.4% (95% CI 94.9% to 99.9%; 14 evaluations, 2196 samples and 942 SARS‐CoV‐2 cases); d) for digital PCR assays, the average sensitivity was 98.5% (95% CI 95.2% to 99.5%), and the average specificity was 91.4% (95% CI 60.4% to 98.7%; five evaluations, 703 samples and 354 SARS‐CoV‐2 cases); e) for RT‐LAMP assays omitting/adapting RNA extraction, the average sensitivity was 73.1% (95% CI 58.4% to 84%), and the average specificity was 100% (95% CI 98% to 100%; 24 evaluations, 14,342 samples and 1502 SARS‐CoV‐2 cases). Only two index test categories fulfil the WHO‐acceptable sensitivity and specificity requirements for SARS‐CoV‐2 nucleic acid tests: RT‐PCR assays designed to omit/adapt RNA extraction/purification and TMA assays. In addition, WHO‐acceptable performance criteria were met for two assays out of 35 when tests were used according to manufacturer instructions. At 5% prevalence using a cohort of 1000 people suspected of SARS‐CoV‐2 infection, the positive predictive value of RT‐PCR assays omitting/adapting RNA extraction/purification will be 94%, with three in 51 positive results being false positives, and around two missed cases. For TMA assays, the positive predictive value of RT‐PCR assays will be 89%, with 6 in 55 positive results being false positives, and around one missed case. Authors' conclusions Alternative laboratory‐based molecular tests aim to enhance testing capacity in different ways, such as reducing the time, steps and resources needed to obtain valid results. Several index test technologies with these potential advantages have not been evaluated or have been assessed by only a few studies of limited methodological quality, so the performance of these kits was undetermined. Only two index test categories with enough evaluations for meta‐analysis fulfil the WHO set of acceptable accuracy standards for SARS‐CoV‐2 nucleic acid tests: RT‐PCR assays designed to omit/adapt RNA extraction/purification and TMA assays. These assays might prove to be suitable alternatives to RT‐PCR for identifying people infected by SARS‐CoV‐2, especially when the alternative would be not having access to testing. However, these findings need to be interpreted and used with caution because of several limitations in the evidence, including reliance on retrospective samples without information about the symptom status of participants and the timing of assessment. No extrapolation of found accuracy data for these two alternatives to any test brands using the same techniques can be made as, for both groups, one test brand with high accuracy was overrepresented with 21/26 and 12/14 included studies, respectively. Although we used a comprehensive search and had broad eligibility criteria to include a wide range of tests that could be alternatives to RT‐PCR methods, further research is needed to assess the performance of alternative COVID‐19 tests and their role in pandemic management. Plain language summary How accurate are alternative laboratory‐based molecular tests (to RT‐PCR with prior RNA extraction/purification) for identifying people infected by SARS‐CoV‐2? Key messages: * Transcription‐mediated ampliﬁcation (TMA) tests and commercially available reverse transcription polymerase chain reaction (RT‐PCR) tests specifically designed to omit/adapt RNA extraction/puriﬁcation appear accurate enough to replace the method of RT‐PCR with prior RNA (a molecule essential for most biological functions) extraction/purification for identifying people infected by severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2), especially when the alternative would be not having access to testing. However, these findings need to be interpreted and used with caution because of several limitations in the evidence. * The accuracy of other types of laboratory‐based molecular alternatives to RT‐PCR is below World Health Organisation (WHO) recommended standards or lacks a sufficient volume of evidence to draw reliable conclusions. *The extrapolation of accuracy data from the evaluated tests to any other test brand(s) using the same techniques will be challenging since main categories are overrepresented by highly accurate individual test brands. Further evaluation of these tests in real clinical practice scenarios is needed. What are alternative laboratory‐based molecular assays for identifying people infected by SARS‐CoV‐2? Laboratory‐based molecular assays (tests) aim to confirm or rule out SARS‐CoV‐2 infection in people. Alternatives to RT‐PCR assays have been developed to minimise the steps needed to process samples in the laboratory or use fewer resources to obtain the same valid results. For this review, we focus on tests commercially developed by a manufacturer, providing instructions for their professional use. Why is this question important? People with suspected SARS‐CoV‐2 infection should know if they are infected to self‐isolate, receive treatment, and inform close contacts. Failure to detect SARS‐CoV‐2 when it is present risks spreading infection and results in missed opportunities for prevention. Diagnosing SARS‐CoV‐2 infection when it is not present may lead to unnecessary self‐isolation and testing of close contacts. Currently, testing to confirm SARS‐CoV‐2 infection relies on a laboratory molecular test called RT‐PCR. This requires specialist equipment and can take 24 hours to produce a result. If assays that take less time or use readily available resources were sufficiently accurate to replace RT‐PCR testing, this could increase the number of tests that could be done, allow faster diagnosis and enable people to take appropriate action more quickly. What did we want to find out? We wanted to know whether commercially developed, alternative laboratory‐based molecular tests are accurate enough to diagnose SARS‐CoV‐2 infection compared to RT‐PCR. What did we do? We looked for studies investigating these tests in people (or samples) that were also tested for SARS‐CoV‐2 infection using RT‐PCR which modified RT‐PCR by removing or altering components of this testing process. What did we find? We included 105 evaluations evaluating alternatives to RT‐PCR in the review. The main results are based on 74,753 samples, and SARS‐CoV‐2 infection was confirmed in 7517 of these samples. Most evaluations (27/105) concerned various test types specifically designed to omit/adapt RNA extraction/purification. Twenty‐four of 105 evaluations concerned various test types operating at one temperature (isothermal tests) rather than cycling through different temperatures. Twenty‐four of 105 evaluations were concerned with various test types that combined these two alternative approaches. Main results Only tests specifically designed to omit/adapt RNA extraction/purification and TMA assays with RNA extraction (a type of isothermal test) met WHO‐acceptable standards for confirming and ruling out SARS‐CoV‐2. For illustration, the results of these studies indicate that in a group of 1000 people, 50 of whom (5%) actually have SARS‐CoV‐2: * For tests specifically designed to omit/adapt RNA extraction/purification: ‐51 people would test positive for SARS‐CoV‐2. Of these, three people (6%) would not have SARS‐CoV‐2 (false‐positive result). ‐949 people would test negative for SARS‐CoV‐2. Of these, two people (0.2%) would actually have SARS‐CoV‐2 (false‐negative result) *For TMA tests with RNA extraction: ‐55 people would test positive for SARS‐CoV‐2. Of these, six people (10%) would not have SARS‐CoV‐2 (false‐positive result). ‐945 people would test negative for SARS‐CoV‐2. Of these, one person (0.1%) would actually have SARS‐CoV‐2 (false‐negative result) What are the limitations of the evidence? Most included studies did not provide information on participants who underwent testing, such as whether they had symptoms or were close contacts of a SARS‐CoV‐2 case. We also had concerns about the method of recruiting participants for the majority of included studies, which may have resulted in an overestimation of the accuracy of these methods. In addition, the method used to determine whether individuals did not have SARS‐CoV‐2 infection was not considered reliable for most studies, which may have resulted in underestimating the accuracy of these methods. Some studies did not follow the manufacturers' instructions for using the test. Results from different test brands using the same technique varied. What does this mean? The studies included in this review suggest that two laboratory‐based molecular tests might be accurate enough to replace or supplement RT‐PCR tests with prior RNA extraction/purification for the diagnosis of SARS‐CoV‐2 infection: RT‐PCR tests developed to omit or adapt RNA extraction or puriﬁcation and TMA isothermal tests (retaining an RNA extraction step). Other alternative tests to RT‐PCR were assessed just by a few studies of limited methodological quality, and their performance should be evaluated by additional studies. Furthermore, data in these studies mostly relied on samples acquired in the past (compared to current or future samples), thus further evaluation of these tests in real clinical practice scenarios is required. Also, decisions on the optimal test for a specific setting will be driven not only by diagnostic accuracy, but also by other factors such as test complexity, time to result, acceptability to those being tested, and the setting in which the tests are to be used. How up‐to‐date is this review? This review includes evidence published up to 31 October 2022.","10","John Wiley & Sons, Ltd","1465-1858","*COVID-19 Nucleic Acid Testing [methods]; *COVID-19 [diagnosis]; *RNA, Viral [analysis]; *SARS-CoV-2 [genetics, isolation & purification]; *Sensitivity and Specificity; Bias; False Negative Reactions; False Positive Reactions; Humans; Pandemics; Real-Time Polymerase Chain Reaction [methods]; Reverse Transcriptase Polymerase Chain Reaction [methods, standards]","10.1002/14651858.CD015618","http://dx.doi.org/10.1002/14651858.CD015618","Central Editorial Service"
"CD011929.PUB2","Huttman, M; Parigi, TL; Zoncapè, M; Liguori, A; Kalafateli, M; Noel-Storr, AH; Casazza, G; Tsochatzis, E","Liver fibrosis stage based on the four factors (FIB‐4) score or Forns index in adults with chronic hepatitis C","Cochrane Database of Systematic Reviews","2024","Abstract - Background The presence and severity of liver fibrosis are important prognostic variables when evaluating people with chronic hepatitis C (CHC). Although liver biopsy remains the reference standard, non‐invasive serological markers, such as the four factors (FIB‐4) score and the Forns index, can also be used to stage liver fibrosis. Objectives To determine the diagnostic accuracy of the FIB‐4 score and Forns index in staging liver fibrosis in people with chronic hepatitis C (CHC) virus, using liver biopsy as the reference standard (primary objective). To compare the diagnostic accuracy of these tests for staging liver fibrosis in people with CHC and explore potential sources of heterogeneity (secondary objectives). Search methods We used standard Cochrane search methods for diagnostic accuracy studies (search date: 13 April 2022). Selection criteria We included diagnostic cross‐sectional or case‐control studies that evaluated the performance of the FIB‐4 score, the Forns index, or both, against liver biopsy, in the assessment of liver fibrosis in participants with CHC. We imposed no language restrictions. We excluded studies in which: participants had causes of liver disease besides CHC; participants had successfully been treated for CHC; or the interval between the index test and liver biopsy exceeded six months. Data collection and analysis Two review authors independently extracted data. We performed meta‐analyses using the bivariate model and calculated summary estimates. We evaluated the performance of both tests for three target conditions: significant fibrosis or worse (METAVIR stage ≥ F2); severe fibrosis or worse (METAVIR stage ≥ F3); and cirrhosis (METAVIR stage F4). We restricted the meta‐analysis to studies reporting cut‐offs in a specified range (+/‐0.15 for FIB‐4; +/‐0.3 for Forns index) around the original validated cut‐offs (1.45 and 3.25 for FIB‐4; 4.2 and 6.9 for Forns index). We calculated the percentage of people who would receive an indeterminate result (i.e. above the rule‐out threshold but below the rule‐in threshold) for each index test/cut‐off/target condition combination. Main results We included 84 studies (with a total of 107,583 participants) from 28 countries, published between 2002 and 2021, in the qualitative synthesis. Of the 84 studies, 82 (98%) were cross‐sectional diagnostic accuracy studies with cohort‐based sampling, and the remaining two (2%) were case‐control studies. All studies were conducted in referral centres. Our main meta‐analysis included 62 studies (100,605 participants). Overall, two studies (2%) had low risk of bias, 23 studies (27%) had unclear risk of bias, and 59 studies (73%) had high risk of bias. We judged 13 studies (15%) to have applicability concerns regarding participant selection. FIB‐4 score The FIB‐4 score's low cut‐off (1.45) is designed to rule out people with at least severe fibrosis (≥ F3). Thirty‐nine study cohorts (86,907 participants) yielded a summary sensitivity of 81.1% (95% confidence interval (CI) 75.6% to 85.6%), specificity of 62.3% (95% CI 57.4% to 66.9%), and negative likelihood ratio (LR‐) of 0.30 (95% CI 0.24 to 0.38). The FIB‐4 score's high cut‐off (3.25) is designed to rule in people with at least severe fibrosis (≥ F3). Twenty‐four study cohorts (81,350 participants) yielded a summary sensitivity of 41.4% (95% CI 33.0% to 50.4%), specificity of 92.6% (95% CI 89.5% to 94.9%), and positive likelihood ratio (LR+) of 5.6 (95% CI 4.4 to 7.1). Using the FIB‐4 score to assess severe fibrosis and applying both cut‐offs together, 30.9% of people would obtain an indeterminate result, requiring further investigations. We report the summary accuracy estimates for the FIB‐4 score when used for assessing significant fibrosis (≥ F2) and cirrhosis (F4) in the main review text. Forns index The Forns index's low cut‐off (4.2) is designed to rule out people with at least significant fibrosis (≥ F2). Seventeen study cohorts (4354 participants) yielded a summary sensitivity of 84.7% (95% CI 77.9% to 89.7%), specificity of 47.9% (95% CI 38.6% to 57.3%), and LR‐ of 0.32 (95% CI 0.25 to 0.41). The Forns index's high cut‐off (6.9) is designed to rule in people with at least significant fibrosis (≥ F2). Twelve study cohorts (3245 participants) yielded a summary sensitivity of 34.1% (95% CI 26.4% to 42.8%), specificity of 97.3% (95% CI 92.9% to 99.0%), and LR+ of 12.5 (95% CI 5.7 to 27.2). Using the Forns index to assess significant fibrosis and applying both cut‐offs together, 44.8% of people would obtain an indeterminate result, requiring further investigations. We report the summary accuracy estimates for the Forns index when used for assessing severe fibrosis (≥ F3) and cirrhosis (F4) in the main text. Comparing FIB‐4 to Forns index There were insufficient studies to meta‐analyse the performance of the Forns index for diagnosing severe fibrosis and cirrhosis. Therefore, comparisons of the two tests' performance were not possible for these target conditions. For diagnosing significant fibrosis and worse, there were no significant differences in their performance when using the high cut‐off. The Forns index performed slightly better than FIB‐4 when using the low/rule‐out cut‐off (relative sensitivity 1.12, 95% CI 1.00 to 1.25; P = 0.0573; relative specificity 0.69, 95% CI 0.57 to 0.84; P = 0.002). Authors' conclusions Both the FIB‐4 score and the Forns index may be considered for the initial assessment of people with CHC. The FIB‐4 score's low cut‐off (1.45) can be used to rule out people with at least severe fibrosis (≥ F3) and cirrhosis (F4). The Forns index's high cut‐off (6.9) can be used to diagnose people with at least significant fibrosis (≥ F2). We judged most of the included studies to be at unclear or high risk of bias. The overall quality of the body of evidence was low or very low, and more high‐quality studies are needed. Our review only captured data from referral centres. Therefore, when generalising our results to a primary care population, the probability of false positives will likely be higher and false negatives will likely be lower. More research is needed in sub‐Saharan Africa, since these tests may be of value in such resource‐poor settings. Plain language summary How accurate are the FIB‐4 score and Forns index (non‐invasive tests) in diagnosing liver fibrosis (scarring) stages in adults with chronic hepatitis C? Key messages • Both the FIB‐4 score and Forns index can be used in the initial phase of investigating whether someone has liver scarring. • It is best to use the FIB‐4 score to rule out stage 3 (severe fibrosis) or stage 4 scarring (cirrhosis). • It is best to use the Forns index to diagnose people with stage 2 scarring (significant fibrosis). Why is improving the diagnosis of liver scarring important? Hepatitis C infection is a common cause of liver scarring (fibrosis). Untreated, liver scarring can progress to a severe form called liver cirrhosis, which is mostly irreversible and can cause the liver to shut down or develop cancer. Currently, the best test to diagnose liver fibrosis is liver biopsy, where liver tissue is taken with a needle and looked at under a microscope. However, liver biopsy is invasive, costly, painful, and carries some serious risks such as bleeding. Accurately diagnosing liver fibrosis through non‐invasive tests such as the FIB‐4 score and Forns index would benefit people and healthcare systems overall. However, their diagnostic accuracy (that is, how good they are at telling us which people have what stage of disease) in people with hepatitis C infection remains unclear. What are the FIB‐4 score and Forns index tests? The FIB‐4 score and Forns index are tests for diagnosing stages of liver fibrosis. They combine standard laboratory results with factors such as age to calculate a score that estimates the amount of scarring in the liver. Compared to liver biopsy, these are simple, inexpensive, widely available, relatively painless, and risk‐free tests. Each test has two cut‐offs: high/rule in and low/rule out. If a person's result is below the low cut‐off, they  do not  have that stage of fibrosis. If a person's result is above the high cut‐off, they  do  have that stage of fibrosis. If someone's score is between the two cut‐offs, the test is unhelpful because it can neither rule in nor rule out fibrosis. This is called the 'grey area'. Someone with a score in the 'grey area' should have further tests, such as a liver biopsy. What did we want to find out? We wanted to determine how well the FIB‐4 score and Forns index can diagnose different liver fibrosis stages in people with chronic hepatitis C, compared to the results from liver biopsy. What did we do? We searched for studies that evaluated the diagnostic accuracy of the FIB‐4 score or Forns index (or both) in people with hepatitis C. We combined the results from these studies. What did we find? We included 84 studies with a total of 107,583 participants. The studies were conducted in 28 countries, and were published between 2002 and 2021. We analysed results from 62 studies with 100,605 participants. We selected this portion of studies because they applied the two tests using comparable low and high cut‐off values. This approach means we can be more confident about the results of our analysis. By combining the studies' results for the FIB‐4 score for diagnosing severe (stage 3) fibrosis, we can say the following for a hypothetical group of 1000 people: • using the high or 'rule‐in' cut‐off, 144 people would correctly be diagnosed with severe fibrosis, whilst 48 people would wrongly be diagnosed with this stage of disease; • using the low or 'rule‐out' cut‐off, 430 people would have severe fibrosis correctly ruled out, whilst 58 people  with  fibrosis would be missed; • by using both cut‐offs together, about one‐third of people will need further tests ('grey area'). By combining the studies' results for the Forns index to diagnose significant (stage 2) fibrosis, we can say the following for a hypothetical group of 1000 people: • using the high or 'rule‐in' cut‐off, 179 people would correctly be diagnosed with significant fibrosis, whilst 13 people would wrongly be diagnosed with this stage of disease; • using the low or 'rule‐out' cut‐off, 218 people would have significant fibrosis correctly ruled out, whilst 83 people would be missed; • by using both cut‐offs together, about half of people will need further tests ('grey area'). What are the limitations of the evidence? Our confidence in the evidence was reduced because many of the studies may have overestimated the diagnostic accuracy of the tests. Also, the numbers described above are a summary based on pooling results from many studies. Because estimates of accuracy varied considerably across individual studies, we cannot be sure that applying the FIB‐4 score or Forns index will always produce these results. How up to date is this evidence? The evidence is current to 13 April 2022.","8","John Wiley & Sons, Ltd","1465-1858","*Hepatitis C, Chronic [complications, pathology]; *Liver Cirrhosis [blood, diagnosis, pathology]; Adult; Alanine Transaminase [blood]; Aspartate Aminotransferases [blood]; Bias; Biomarkers [blood]; Biopsy; Case-Control Studies; Cross-Sectional Studies; Humans; Liver [pathology]; Platelet Count; Sensitivity and Specificity; Severity of Illness Index","10.1002/14651858.CD011929.pub2","http://dx.doi.org/10.1002/14651858.CD011929.pub2","Hepato-Biliary"
"CD015050.PUB2","De Rop, L; Bos, DAG; Stegeman, I; Holtman, G; Ochodo, EA; Spijker, R; Otieno, JA; Alkhlaileh, F; Deeks, JJ; Dinnes, J; Van den Bruel, A; McInnes, MDF; Leeflang, MMG; Verbakel, JY","Accuracy of routine laboratory tests to predict mortality and deterioration to severe or critical COVID‐19 in people with SARS‐CoV‐2","Cochrane Database of Systematic Reviews","2024","Abstract - Background Identifying patients with COVID‐19 disease who will deteriorate can be useful to assess whether they should receive intensive care, or whether they can be treated in a less intensive way or through outpatient care. In clinical care, routine laboratory markers, such as C‐reactive protein, are used to assess a person's health status. Objectives To assess the accuracy of routine blood‐based laboratory tests to predict mortality and deterioration to severe or critical (from mild or moderate) COVID‐19 in people with SARS‐CoV‐2. Search methods On 25 August 2022, we searched the Cochrane COVID‐19 Study Register, encompassing searches of various databases such as MEDLINE via PubMed, CENTRAL, Embase, medRxiv, and ClinicalTrials.gov. We did not apply any language restrictions. Selection criteria We included studies of all designs that produced estimates of prognostic accuracy in participants who presented to outpatient services, or were admitted to general hospital wards with confirmed SARS‐CoV‐2 infection, and studies that were based on serum banks of samples from people. All routine blood‐based laboratory tests performed during the first encounter were included. We included any reference standard used to define deterioration to severe or critical disease that was provided by the authors. Data collection and analysis Two review authors independently extracted data from each included study, and independently assessed the methodological quality using the Quality Assessment of Prognostic Accuracy Studies tool. As studies reported different thresholds for the same test, we used the Hierarchical Summary Receiver Operator Curve model for meta‐analyses to estimate summary curves in SAS 9.4. We estimated the sensitivity at points on the SROC curves that corresponded to the median and interquartile range boundaries of specificities in the included studies. Direct and indirect comparisons were exclusively conducted for biomarkers with an estimated sensitivity and 95% CI of ≥ 50% at a specificity of ≥ 50%. The relative diagnostic odds ratio was calculated as a summary of the relative accuracy of these biomarkers. Main results We identified a total of 64 studies, including 71,170 participants, of which 8169 participants died, and 4031 participants deteriorated to severe/critical condition. The studies assessed 53 different laboratory tests. For some tests, both increases and decreases relative to the normal range were included. There was important heterogeneity between tests and their cut‐off values. None of the included studies had a low risk of bias or low concern for applicability for all domains. None of the tests included in this review demonstrated high sensitivity or specificity, or both. The five tests with summary sensitivity and specificity above 50% were: C‐reactive protein increase, neutrophil‐to‐lymphocyte ratio increase, lymphocyte count decrease, d‐dimer increase, and lactate dehydrogenase increase. Inflammation For mortality, summary sensitivity of a C‐reactive protein increase was 76% (95% CI 73% to 79%) at median specificity, 59% (low‐certainty evidence). For deterioration, summary sensitivity was 78% (95% CI 67% to 86%) at median specificity, 72% (very low‐certainty evidence). For the combined outcome of mortality or deterioration, or both, summary sensitivity was 70% (95% CI 49% to 85%) at median specificity, 60% (very low‐certainty evidence).  For mortality, summary sensitivity of an increase in neutrophil‐to‐lymphocyte ratio was 69% (95% CI 66% to 72%) at median specificity, 63% (very low‐certainty evidence). For deterioration, summary sensitivity was 75% (95% CI 59% to 87%) at median specificity, 71% (very low‐certainty evidence). For mortality, summary sensitivity of a decrease in lymphocyte count was 67% (95% CI 56% to 77%) at median specificity, 61% (very low‐certainty evidence). For deterioration, summary sensitivity of a decrease in lymphocyte count was 69% (95% CI 60% to 76%) at median specificity, 67% (very low‐certainty evidence). For the combined outcome, summary sensitivity was 83% (95% CI 67% to 92%) at median specificity, 29% (very low‐certainty evidence).  For mortality, summary sensitivity of a lactate dehydrogenase increase was 82% (95% CI 66% to 91%) at median specificity, 60% (very low‐certainty evidence). For deterioration, summary sensitivity of a lactate dehydrogenase increase was 79% (95% CI 76% to 82%) at median specificity, 66% (low‐certainty evidence). For the combined outcome, summary sensitivity was 69% (95% CI 51% to 82%) at median specificity, 62% (very low‐certainty evidence). Hypercoagulability For mortality, summary sensitivity of a d‐dimer increase was 70% (95% CI 64% to 76%) at median specificity of 56% (very low‐certainty evidence). For deterioration, summary sensitivity was 65% (95% CI 56% to 74%) at median specificity of 63% (very low‐certainty evidence). For the combined outcome, summary sensitivity was 65% (95% CI 52% to 76%) at median specificity of 54% (very low‐certainty evidence). To predict mortality, neutrophil‐to‐lymphocyte ratio increase had higher accuracy compared to d‐dimer increase (RDOR (diagnostic Odds Ratio) 2.05, 95% CI 1.30 to 3.24), C‐reactive protein increase (RDOR 2.64, 95% CI 2.09 to 3.33), and lymphocyte count decrease (RDOR 2.63, 95% CI 1.55 to 4.46). D‐dimer increase had higher accuracy compared to lymphocyte count decrease (RDOR 1.49, 95% CI 1.23 to 1.80), C‐reactive protein increase (RDOR 1.31, 95% CI 1.03 to 1.65), and lactate dehydrogenase increase (RDOR 1.42, 95% CI 1.05 to 1.90). Additionally, lactate dehydrogenase increase had higher accuracy compared to lymphocyte count decrease (RDOR 1.30, 95% CI 1.13 to 1.49). To predict deterioration to severe disease, C‐reactive protein increase had higher accuracy compared to d‐dimer increase (RDOR 1.76, 95% CI 1.25 to 2.50). The neutrophil‐to‐lymphocyte ratio increase had higher accuracy compared to d‐dimer increase (RDOR 2.77, 95% CI 1.58 to 4.84). Lastly, lymphocyte count decrease had higher accuracy compared to d‐dimer increase (RDOR 2.10, 95% CI 1.44 to 3.07) and lactate dehydrogenase increase (RDOR 2.22, 95% CI 1.52 to 3.26). Authors' conclusions Laboratory tests, associated with hypercoagulability and hyperinflammatory response, were better at predicting severe disease and mortality in patients with SARS‐CoV‐2 compared to other laboratory tests. However, to safely rule out severe disease, tests should have high sensitivity (> 90%), and none of the identified laboratory tests met this criterion. In clinical practice, a more comprehensive assessment of a patient's health status is usually required by, for example, incorporating these laboratory tests into clinical prediction rules together with clinical symptoms, radiological findings, and patient's characteristics. Plain language summary How accurate are routine laboratory tests in predicting mortality and deterioration to severe or critical COVID‐19 in people with SARS‐CoV‐2? What are routine laboratory tests?  Routine laboratory tests are a set of commonly performed blood tests that provide information about a patient's health status. These tests can be used to identify disease or monitor health. What did we want to find out?  It is important to identify patients, presenting at a doctor's appointment at an outpatient service or at the emergency department who are at high risk of developing severe COVID‐19 disease or dying. It can help clinicians in deciding if the patients need hospitalisation. We wanted to know if routine laboratory tests were sufficiently accurate to predict mortality and deterioration in patients with confirmed SARS‐CoV‐2. What did we do?  We searched for studies that assessed how well routine laboratory tests predict mortality and deterioration in patients with confirmed SARS‐CoV‐2. We included studies of any design and set anywhere in the world. Patients of any age or sex were included. What we found  We found 64 studies that looked at 53 different routine laboratory tests. These studies assessed how well these tests could predict mortality, deterioration, or both. A total of 71,170 patients were included, of which 8169 (11.5%) patients died, and 4031 (5.7%) patients deteriorated to severe/critical disease. Adult patients were included in 31 studies, two studies reported on patients more than 60 years, two studies included a mix of children and adults, and one study included only children. Most studies were done in China, followed by Spain and Italy. All studies took place in hospitals. 'Sensitivity' and 'specificity' are often used to report the performance of tests. Sensitivity is the proportion of patients with the outcome (= mortality or deterioration) that are correctly detected by the test, and specificity is the proportion of people without the outcome that are correctly detected by the test. The closer the sensitivity and specificity of a test are to 100%, the more accurate the test is. To safely rule out patients who will not die or deteriorate, a high sensitivity of more than 90% is necessary. When four or more studies assessed the same tests, we pooled the data and analysed them together. We did not find any tests that were accurate enough to safely rule out a severe outcome, such as deterioration or death. We found five tests with both sensitivity and specificity exceeding 50%. Four of these laboratory tests indicate important inflammation in a SARS‐CoV‐2 infection. These four tests are C‐reactive protein, neutrophil‐to‐lymphocyte ratio, lymphocyte count, and lactate dehydrogenase. The fifth test, d‐dimer, reflects a state of increased blood clotting in a SARS‐CoV‐2 infection. How reliable are the results?  We have low confidence in the evidence of this review, because there were important differences between the included studies, and it was, therefore, difficult to compare them. Sensitivity and specificity depend on where the cut‐off point is made between positive (indicative of disease) and negative (disease‐free). For some studies, the authors decided on the cut‐off value (for a test) before doing the test (less likely to create bias) and in others, they chose the cut‐off value after analysis of the test (more likely to be biassed). Who do the results of this review apply to?  Routine laboratory tests can be performed at a doctor's appointment or at the emergency department. However, the included studies only assessed patients presenting to the hospital. We included patients with confirmed SARS‐CoV‐2 infection. Only one study reported on vaccinated patients, and we could not assess the effect of different SARS‐CoV‐2 variants of concern. Therefore, our results might not be representative for vaccinated patients or different variants of concern. What does this mean?  These routine laboratory tests, linked to inflammation and blood clotting in patients with COVID‐19 disease, can be used for risk stratification to assess a patient. However, none of these tests performed well enough to safely rule out progression to severe or deadly disease. These tests might serve to assess the overall health status of the patient. To predict deterioration or mortality, a more comprehensive assessment, including clinical symptoms, radiological findings and patient's characteristics, may be considered. How up‐to‐date is this review?  We searched for all COVID‐19 studies up to 25 August 2022.","8","John Wiley & Sons, Ltd","1465-1858","*C-Reactive Protein [analysis]; *COVID-19 [blood, diagnosis, mortality]; *SARS-CoV-2; Bias; Biomarkers [blood]; COVID-19 Testing [methods]; Clinical Deterioration; Humans; Pandemics; Prognosis; Sensitivity and Specificity; Severity of Illness Index","10.1002/14651858.CD015050.pub2","http://dx.doi.org/10.1002/14651858.CD015050.pub2","Infectious Diseases"
"CD012083.PUB3","Tsujimoto, Y; Kataoka, Y; Banno, M; Anan, K; Shiroshita, A; Jujo, S","Ultrasonography for confirmation of gastric tube placement","Cochrane Database of Systematic Reviews","2024","Abstract - Background Gastric tubes are commonly used for the administration of drugs and tube feeding for people who are unable to swallow. Feeding via a tube misplaced in the trachea can result in severe pneumonia. Therefore, the confirmation of tube placement in the stomach after tube insertion is important. Recent studies have reported that ultrasonography provides good diagnostic accuracy estimates in the confirmation of appropriate tube placement. Hence, ultrasound could provide a promising alternative to X‐rays in the confirmation of tube placement, especially in settings where X‐ray facilities are unavailable or difficult to access. Objectives To assess the diagnostic accuracy of ultrasound alone or in combination with other methods for gastric tube placement confirmation in children and adults. Search methods This systematic review is an update of a previously published Cochrane review. For this update, we searched the Cochrane Library (2021, Issue 6), MEDLINE (to April 2023), Embase (to April 2023), five other databases (to July 2021), and reference lists of articles, and contacted study authors. Selection criteria We included studies that evaluated the diagnostic accuracy of naso‐ and orogastric tube placement confirmed by ultrasound visualization using X‐ray visualization as the reference standard. We included cross‐sectional studies and case‐control studies. We excluded case series or case reports. We excluded studies if X‐ray visualization was not the reference standard or if the tube being placed was a gastrostomy or enteric tube. Data collection and analysis Two review authors independently assessed the methodological quality and extracted data from each of the included studies. We contacted the authors of the included studies to obtain missing data. There were sparse data for specificity. Therefore, we performed a meta‐analysis of only sensitivity using a univariate random‐effects logistic regression model to combine data from studies that used the same method and echo window. Main results We identified 12 new studies in addition to 10 studies included in the earlier version of this review, totalling 1939 participants and 1944 tube insertions. Overall, we judged the risk of bias in the included studies as low or unclear. No study was at low risk of bias or low concern for applicability in every QUADAS‐2 domain. There were limited data (152 participants) for misplacement detection (specificity) due to the low incidence of misplacement. The summary sensitivity of ultrasound on neck and abdomen echo windows were 0.96 (95% confidence interval (CI) 0.92 to 0.98; moderate‐certainty evidence) for air injection and 0.98 (95% CI 0.83 to 1.00; moderate‐certainty evidence) for saline injection. The summary sensitivity of ultrasound on abdomen echo window was 0.96 (95% CI 0.65 to 1.00; very low‐certainty evidence) for air injection and 0.97 (95% CI 0.95 to 0.99; moderate‐certainty evidence) for procedures without injection. The certainty of evidence for specificity across all methods was very low due to the very small sample size. For settings where X‐ray was not readily available and participants underwent gastric tube insertion for drainage (8 studies, 552 participants), sensitivity estimates of ultrasound in combination with other confirmatory tests ranged from 0.86 to 0.98 and specificity estimates of 1.00 with wide CIs. For studies of ultrasound alone (9 studies, 782 participants), sensitivity estimates ranged from 0.77 to 0.98 and specificity estimates were 1.00 with wide CIs or not estimable due to no occurrence of misplacement. Authors' conclusions Of 22 studies that assessed the diagnostic accuracy of gastric tube placement, few studies had a low risk of bias. Based on limited evidence, ultrasound does not have sufficient accuracy as a single test to confirm gastric tube placement. However, in settings where X‐ray is not readily available, ultrasound may be useful to detect misplaced gastric tubes. Larger studies are needed to determine the possibility of adverse events when ultrasound is used to confirm tube placement. Plain language summary Is ultrasound alone or in combination with other methods useful for confirmation of gastric tube placement? Key messages – Whether ultrasound offers a promising alternative to X‐rays for confirming proper gastric tube placement is uncertain. – More research is needed to determine the accuracy of ultrasound to identify misplaced tubes. What are gastric tubes and why are they used? The oesophagus (food pipe) is a muscular tube that connects the mouth to the stomach. If a person cannot swallow properly, a gastric tube may be inserted through the nose or the mouth to provide medicines or liquid food directly into the stomach. Care is needed though, as the oesophagus is very close to the trachea (wind pipe), which allows air to travel to the lungs. If the gastric tube is misplaced and food or medicines passed into the trachea, it can result in a severe infection in the lungs (called pneumonia) or other complications. Therefore, the confirmation of tube placement in the stomach after tube insertion is important. Correct placement is usually checked using X‐rays. Why is replacing X‐rays with ultrasound for confirming gastric tube placement important? Ultrasound is a diagnostic imaging technique that uses sound waves to create images of the inside of the body. It could be more accessible and convenient than X‐rays, especially in locations with limited resources. What did we want to find out? We wanted to determine the accuracy of ultrasound in confirming gastric tube placement and assess its potential to replace X‐rays as the standard method. What did we do? We analyzed 22 studies with 1939 participants looking at the accuracy of ultrasound for confirming gastric tube placement. Study findings Most studies showed that ultrasound performed well in confirming correct tube placement, but there were limited data on incorrect tube placements and potential complications as only 152 participants in the studies had a misplaced tube. The studies used three ultrasound methods: neck approach, upper abdominal approach, and a combination of both. Ultrasound alone was not sufficient for confirming proper placement for feeding tubes but, when combined with other tests, it could be useful for confirming gastric drainage tubes. What are the limitations of the evidence? Many studies had poor or unclear methods, so our confidence in the evidence was reduced. Only eight of the 22 studies were considered representative of people who usually need a gastric tube. Results varied for incorrect tube placement. Future research Larger studies are needed to determine if ultrasound can replace X‐rays for confirming gastric tube placement and whether it can help reduce complications from misplaced tubes. How up to date is this evidence? This review updates our previous review. The evidence is up to date to April 2023.","7","John Wiley & Sons, Ltd","1465-1858","*Intubation, Gastrointestinal [instrumentation, methods]; Adult; Bias; Case-Control Studies; Child; Enteral Nutrition [methods]; Humans; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Stomach [diagnostic imaging]; Ultrasonography [methods]","10.1002/14651858.CD012083.pub3","http://dx.doi.org/10.1002/14651858.CD012083.pub3","Gut"
"CD014715.PUB2","Buijtendijk, MFJ; Bet, BB; Leeflang, MMG; Shah, H; Reuvekamp, T; Goring, T; Docter, D; Timmerman, MGMM; Dawood, Y; Lugthart, MA; Berends, B; Limpens, J; Pajkrt, E; van den Hoff, MJB; de Bakker, BS","Diagnostic accuracy of ultrasound screening for fetal structural abnormalities during the first and second trimester of pregnancy in low‐risk and unselected populations","Cochrane Database of Systematic Reviews","2024","Abstract - Background Prenatal ultrasound is widely used to screen for structural anomalies before birth. While this is traditionally done in the second trimester, there is an increasing use of first‐trimester ultrasound for early detection of lethal and certain severe structural anomalies. Objectives To evaluate the diagnostic accuracy of ultrasound in detecting fetal structural anomalies before 14 and 24 weeks’ gestation in low‐risk and unselected pregnant women and to compare the current two main prenatal screening approaches: a single second‐trimester scan (single‐stage screening) and a first‐ and second‐trimester scan combined (two‐stage screening) in terms of anomaly detection before 24 weeks’ gestation. Search methods We searched MEDLINE, EMBASE, Science Citation Index Expanded (Web of Science), Social Sciences Citation Index (Web of Science), Arts & Humanities Citation Index and Emerging Sources Citation Index (Web of Science) from 1 January 1997 to 22 July 2022. We limited our search to studies published after 1997 and excluded animal studies, reviews and case reports. No further restrictions were applied. We also screened reference lists and citing articles of each of the included studies. Selection criteria Studies were eligible if they included low‐risk or unselected pregnant women undergoing a first‐ and/or second‐trimester fetal anomaly scan, conducted at 11 to 14 or 18 to 24 weeks’ gestation, respectively. The reference standard was detection of anomalies at birth or postmortem. Data collection and analysis Two review authors independently undertook study selection, quality assessment (QUADAS‐2), data extraction and evaluation of the certainty of evidence (GRADE approach). We used univariate random‐effects logistic regression models for the meta‐analysis of sensitivity and specificity. Main results Eighty‐seven studies covering 7,057,859 fetuses (including 25,202 with structural anomalies) were included. No study was deemed low risk across all QUADAS‐2 domains. Main methodological concerns included risk of bias in the reference standard domain and risk of partial verification. Applicability concerns were common in studies evaluating first‐trimester scans and two‐stage screening in terms of patient selection due to frequent recruitment from single tertiary centres without exclusion of referrals. We reported ultrasound accuracy for fetal structural anomalies overall, by severity, affected organ system and for 46 specific anomalies. Detection rates varied widely across categories, with the highest estimates of sensitivity for thoracic and abdominal wall anomalies and the lowest for gastrointestinal anomalies across all tests. The summary sensitivity of a first‐trimester scan was 37.5% for detection of structural anomalies overall (95% confidence interval (CI) 31.1 to 44.3; low‐certainty evidence) and 91.3% for lethal anomalies (95% CI 83.9 to 95.5; moderate‐certainty evidence), with an overall specificity of 99.9% (95% CI 99.9 to 100; low‐certainty evidence). Two‐stage screening had a combined sensitivity of 83.8% (95% CI 74.7 to 90.1; low‐certainty evidence), while single‐stage screening had a sensitivity of 50.5% (95% CI 38.5 to 62.4; very low‐certainty evidence). The specificity of two‐stage screening was 99.9% (95% CI 99.7 to 100; low‐certainty evidence) and for single‐stage screening, it was 99.8% (95% CI 99.2 to 100; moderate‐certainty evidence). Indirect comparisons suggested superiority of two‐stage screening across all analyses regarding sensitivity, with no significant difference in specificity. However, the certainty of the evidence is very low due to the absence of direct comparisons. Authors' conclusions A first‐trimester scan has the potential to detect lethal and certain severe anomalies with high accuracy before 14 weeks’ gestation, despite its limited overall sensitivity. Conversely, two‐stage screening shows high accuracy in detecting most fetal structural anomalies before 24 weeks’ gestation with high sensitivity and specificity. In a hypothetical cohort of 100,000 fetuses, the first‐trimester scan is expected to correctly identify 113 out of 124 fetuses with lethal anomalies (91.3%) and 665 out of 1776 fetuses with any anomaly (37.5%). However, 79 false‐positive diagnoses are anticipated among 98,224 fetuses (0.08%). Two‐stage screening is expected to correctly identify 1448 out of 1776 cases of structural anomalies overall (83.8%), with 118 false positives (0.1%). In contrast, single‐stage screening is expected to correctly identify 896 out of 1776 cases before 24 weeks’ gestation (50.5%), with 205 false‐positive diagnoses (0.2%). This represents a difference of 592 fewer correct identifications and 88 more false positives compared to two‐stage screening. However, it is crucial to acknowledge the uncertainty surrounding the additional benefits of two‐stage versus single‐stage screening, as there are no studies directly comparing them. Moreover, the evidence supporting the accuracy of first‐trimester ultrasound and two‐stage screening approaches primarily originates from studies conducted in single tertiary care facilities, which restricts the generalisability of the results of this meta‐analysis to the broader population. Plain language summary Accuracy of a first‐ and second‐trimester ultrasound scan for identifying fetal anomalies in low‐risk and unselected pregnancies Key messages Prenatal ultrasound is commonly used in the first and second trimesters of pregnancy to identify potential issues with a developing baby (fetus). In this study, we analysed 87 studies covering over 7 million fetuses. While both first‐ and second‐trimester scans confirm normal development well (high specificity), their ability to detect issues (sensitivity) varied. Women with both scans seemed to have more anomalies detected before 24 weeks compared to those with only a second‐trimester scan. However, differences might be due to study setup variations rather than a real difference in detection. What are fetal anomalies? Fetal anomalies are abnormalities that can affect the baby's organs or body parts, which develop during pregnancy. These anomalies can range from severe conditions incompatible with life to less significant ones, some of which may be considered normal variations. How are fetal anomalies detected? Fetal anomalies are primarily detected by ultrasound, which uses sound waves to create detailed images of the baby’s internal organs. Most countries offer one ultrasound scan during pregnancy to check for fetal anomalies, typically conducted between 18 and 24 weeks of pregnancy (second‐trimester scan). Some countries also offer an early anomaly scan to identify some major anomalies at an earlier stage. This scan is typically performed at 11 to 14 weeks (first‐trimester scan). What did we want to find out? The goal was to understand how accurate ultrasound scans are in detecting structural anomalies in low‐risk and unselected pregnant women when conducted in the first and second trimesters. The study also aimed to compare the accuracy of two different approaches: a single‐stage screening approach involving only a second‐trimester scan and a two‐stage approach involving both first‐ and second‐trimester scans. What did we do? We reviewed 87 studies, covering over 7 million fetuses. These studies focused on low‐risk pregnant and unselected women who had undergone first‐ and/or second‐trimester ultrasound scans as part of routine prenatal care. We assessed the quality of the studies, extracted relevant data and used statistical methods to analyse the accuracy of the ultrasound scans. What did we find? A first‐trimester scan appears accurate in early detection of lethal and some severe fetal anomalies. However, its overall ability to detect anomalies is limited. In a hypothetical group of 100,000 fetuses, this scan is expected to correctly identify 113 out of 124 fetuses with lethal anomalies (91.3%) and 665 out of 1776 fetuses with any anomaly (37.5%). Unfortunately, about 79 out of 98,224 healthy fetuses (0.08%) might mistakenly receive a diagnosis of a fetal anomaly when, in reality, there isn't one (false‐positive diagnosis). Although the chance of receiving a false‐positive diagnosis is very low, in the cases where this occurs, this can lead to unnecessary anxiety and investigations. The combination of a first‐ and second‐trimester scan seems highly sensitive, expecting to identify 1448 out of 1776 cases (83.8%) before 24 weeks in a hypothetical group of 100,000 pregnancies. However, around 118 out of 98,224 healthy fetuses (0.1%) may receive a false‐positive diagnosis. Fewer fetal anomalies seem to be identified before 24 weeks in groups of women undergoing only a second‐trimester scan (single‐stage screening) compared to those also undergoing a first‐trimester scan (two‐stage screening). A single second‐trimester scan is expected to detect 896 out of 1776 cases (50.5%, 592 fewer than two‐stage screening), potentially resulting in false‐positive diagnoses for around 205 out of 98,224 healthy fetuses (0.2%, 88 more than two‐stage screening). However, studies solely focusing on the second‐trimester scan were designed differently. Women generally entered these studies after the first trimester. Easily detectable anomalies might have been identified before study entry through other investigations, leaving only the more subtle cases in the study populations. This difference may have led to underestimation of overall anomaly detection in studies assessing the accuracy of a single second‐trimester scan. It is crucial to note varying anomaly detection rates across organ systems. Abdominal wall anomalies had the highest detection rates: 95.6% (first‐trimester scan), 99.0% (first‐ and second‐trimester scan combined) and 90.8% (single second‐trimester scan). Digestive tract anomalies had the lowest rates: 8.3%, 46.5% and 33.3%, respectively. What are the limitations of the evidence? The results of the included studies varied widely, making it challenging to draw consistent conclusions. Additionally, none of the studies were entirely free from potential issues in how they were conducted. Concerns mainly focused on confirming normal and abnormal prenatal findings after birth and how well the results applied to the general population, as most studies were conducted in major university hospitals. Lastly, no studies directly compared detection rates between groups receiving both scans and those with only a second‐trimester scan. Although the results of the review indicate that the combination of a first‐ and second‐trimester scan might be better at detecting anomalies before 24 weeks of pregnancy than a single second‐trimester scan, this difference could be due to variations in study designs and entry times. How up‐to‐date is this evidence? The search for evidence was conducted up to 22 July 2022.","5","John Wiley & Sons, Ltd","1465-1858","*Pregnancy Trimester, First; *Pregnancy Trimester, Second; *Ultrasonography, Prenatal [statistics & numerical data]; Bias; Congenital Abnormalities [diagnostic imaging]; Female; Humans; Pregnancy; Sensitivity and Specificity","10.1002/14651858.CD014715.pub2","http://dx.doi.org/10.1002/14651858.CD014715.pub2","Pregnancy and Childbirth"
"CD011686.PUB3","Tavender, E; Eapen, N; Wang, J; Rausa, VC; Babl, FE; Phillips, N","Triage tools for detecting cervical spine injury in paediatric trauma patients","Cochrane Database of Systematic Reviews","2024","Abstract - Background Paediatric cervical spine injury (CSI) after blunt trauma is rare but can have severe consequences. Clinical decision rules (CDRs) have been developed to guide clinical decision‐making, minimise unnecessary tests and associated risks, whilst detecting all significant CSIs. Several validated CDRs are used to guide imaging decision‐making in adults following blunt trauma and clinical criteria have been proposed as possible paediatric‐specific CDRs. Little information is known about their accuracy. Objectives To assess and compare the diagnostic accuracy of CDRs or sets of clinical criteria, alone or in comparison with each other, for the evaluation of CSI following blunt trauma in children. Search methods For this update, we searched CENTRAL, MEDLINE, Embase, and six other databases from 1 January 2015 to 13 December 2022. As we expanded the index test eligibility for this review update, we searched the excluded studies from the previous version of the review for eligibility. We contacted field experts to identify ongoing studies and studies potentially missed by the search. There were no language restrictions. Selection criteria We included cross‐sectional or cohort designs (retrospective and prospective) and randomised controlled trials that compared the diagnostic accuracy of any CDR or clinical criteria compared with a reference standard for the evaluation of paediatric CSI following blunt trauma. We included studies evaluating one CDR or comparing two or more CDRs (directly and indirectly). We considered X‐ray, computed tomography (CT) or magnetic resonance imaging (MRI) of the cervical spine, and clinical clearance/follow‐up as adequate reference standards. Data collection and analysis Two review authors independently screened titles and abstracts for relevance, and carried out eligibility, data extraction and quality assessment. A third review author arbitrated. We extracted data on study design, participant characteristics, inclusion/exclusion criteria, index test, target condition, reference standard and data (diagnostic two‐by‐two tables) and calculated and plotted sensitivity and specificity on forest plots for visual examination of variation in test accuracy. We assessed methodological quality using the Quality Assessment of Diagnostic Accuracy Studies Version 2 tool. We graded the certainty of the evidence using the GRADE approach. Main results We included five studies with 21,379 enrolled participants, published between 2001 and 2021. Prevalence of CSI ranged from 0.5% to 1.85%. Seven CDRs were evaluated. Three studies reported on direct comparisons of CDRs. One study (973 participants) directly compared the accuracy of three index tests with the sensitivities of NEXUS, Canadian C‐Spine Rule and the PECARN retrospective criteria being 1.00 (95% confidence interval (CI) 0.48 to 1.00), 1.00 (95% CI 0.48 to 1.00) and 1.00 (95% CI 0.48 to 1.00), respectively. The specificities were 0.56 (95% CI 0.53 to 0.59), 0.52 (95% CI 0.49 to 0.55) and 0.32 (95% CI 0.29 to 0.35), respectively (moderate‐certainty evidence). One study (4091 participants) compared the accuracy of the PECARN retrospective criteria with the Leonard de novo model; the sensitivities were 0.91 (95% CI 0.81 to 0.96) and 0.92 (95% CI 0.83 to 0.97), respectively. The specificities were 0.46 (95% CI 0.44 to 0.47) and 0.50 (95% CI 0.49 to 0.52) (moderate‐ and low‐certainty evidence, respectively). One study (270 participants) compared the accuracy of two NICE (National Institute for Health and Care Excellence) head injury guidelines; the sensitivity of the CG56 guideline was 1.00 (95% CI 0.48 to 1.00) compared to 1.00 (95% CI 0.48 to 1.00) with the CG176 guideline. The specificities were 0.46 (95% CI 0.40 to 0.52) and 0.07 (95% CI 0.04 to 0.11), respectively (very low‐certainty evidence). Two additional studies were indirect comparison studies. One study (3065 participants) tested the accuracy of the NEXUS criteria; the sensitivity was 1.00 (95% CI 0.88 to 1.00) and specificity was 0.20 (95% CI 0.18 to 0.21) (low‐certainty evidence). One retrospective study (12,537 participants) evaluated the PEDSPINE criteria and found a sensitivity of 0.93 (95% CI 0.78 to 0.99) and specificity of 0.70 (95% CI 0.69 to 0.72) (very low‐certainty evidence). We did not pool data within the broader CDR categories or investigate heterogeneity due to the small quantity of data and the clinical heterogeneity of studies. Two studies were at high risk of bias. We identified two studies that are awaiting classification pending further information and two ongoing studies. Authors' conclusions There is insufficient evidence to determine the diagnostic test accuracy of CDRs to detect CSIs in children following blunt trauma, particularly for children under eight years of age. Although most studies had a high sensitivity, this was often achieved at the expense of low specificity and should be interpreted with caution due to a small number of CSIs and wide CIs. Well‐designed, large studies are required to evaluate the accuracy of CDRs for the cervical spine clearance in children following blunt trauma, ideally in direct comparison with each other. Plain language summary Clinical tools for detecting cervical spine injury (CSI) in children with injuries Key message – There is currently insufficient evidence to determine which clinical decision tools should be used to assist in deciding whether children with potential cervical spine injuries (CSI) require imaging tests to aid diagnosis. What is a cervical spine injury and how is it detected? The cervical spine is the upper part of the spine between the head and shoulders (the neck). The incidence of traumatic CSI in children is very low. However, it is very important not to miss this injury as the consequences can be devastating, including death or lifelong disability. To detect CSI, several types of imaging techniques can be used: computed tomography (CT), magnetic resonance imaging (MRI) and X‐rays. A CT scan uses detailed X‐rays to produce cross‐sectional images of the body and MRI uses radio waves and a powerful magnet to generate the images. While CT scans and X‐rays are useful in detecting bone injuries, they do use radiation that can increase the risk of developing cancer, especially in children. To avoid exposing children to unnecessary radiation, it is important to find clinical tests that can determine whether children are at risk for CSI, how accurate they are (called diagnostic accuracy) and whether radiographic imaging is needed. What was the aim of this review? Clinical decision rules (CDRs) are tools that clinicians use to decide whether a diagnostic test is needed or another clinical action should be taken. We wanted to find out which CDRs are useful in determining which children are at risk for CSI after blunt trauma (for example, in motor vehicle‐related accidents and falls), and whether radiographic imaging should be used to help diagnosis. Tools that have been developed for adults are also often used for children, but little information is known about their accuracy in children. The aim of this review was to evaluate all CDRs and tools used in this decision‐making process and if they can be used safely and effectively in children. What did we do? We searched medical databases for studies that compared the diagnostic accuracy of any CDR with another CDR for the evaluation of CSI following blunt trauma in children. What did we find? We included five studies recruiting 21,379 children, published between 2001 and 2021, that assessed the accuracy of seven CDRs (NEXUS, Canadian C‐Spine Rule, PECARN retrospective criteria, NICE guidelines CG56 and CG176, Leonard de novo model and PEDSPINE) to evaluate CSIs following blunt trauma in children. Main results There is currently insufficient evidence to determine which CDRs are most effective at detecting CSIs following blunt trauma in children, particularly for those younger than eight years of age. Although most CDRs accurately identified children who had a CSI (called high sensitivity), they frequently did not correctly identify children who did not have a CSI (called low specificity). If these CDRs were applied as a rule, a large proportion of children without CSI attending the emergency department for a blunt trauma assessment would receive imaging potentially exposing them to unnecessary radiation. These CDRs are at best a guide to clinical assessment with current evidence not supporting strict use of CDRs in trauma care for children. More research is needed to evaluate the accuracy of CDRs for use in cervical spine assessment in children. What are the limitations of the evidence? The quality of the studies was variable as there were differences in the children recruited, the number of CSIs, and the methods used making us uncertain about the results. There are currently two large ongoing studies that should contribute to the evidence of the accuracy of CDRs in children. How up to date is the evidence? The evidence is up to date to 13 December 2022.","3","John Wiley & Sons, Ltd","1465-1858","*Cervical Vertebrae [diagnostic imaging, injuries]; *Sensitivity and Specificity; *Spinal Injuries [diagnostic imaging]; *Triage [methods]; *Wounds, Nonpenetrating [diagnostic imaging]; Adolescent; Bias; Child; Child, Preschool; Clinical Decision Rules; Humans; Infant; Magnetic Resonance Imaging; Randomized Controlled Trials as Topic; Tomography, X-Ray Computed","10.1002/14651858.CD011686.pub3","http://dx.doi.org/10.1002/14651858.CD011686.pub3","Back and Neck"
"CD013199.PUB2","Pouncey, AL; Yeldham, G; Magan, T; Lucenteforte, E; Jaffer, U; Virgili, G","Halo sign on temporal artery ultrasound versus temporal artery biopsy for giant cell arteritis","Cochrane Database of Systematic Reviews","2024","Abstract - Background Giant cell arteritis (GCA) is a systemic, inflammatory vasculitis primarily affecting people over the age of 50 years. GCA is treated as a medical emergency due to the potential for sudden, irreversible visual loss. Temporal artery biopsy (TAB) is one of the five criteria of the American College of Rheumatology (ACR) 1990 classification, which is used to aid the diagnosis of GCA. TAB is an invasive test, and it can be slow to obtain a result due to delays in performing the procedure and the time taken for histopathologic assessment. Temporal artery ultrasonography (US) has been demonstrated to show findings in people with GCA such as the halo sign (a hypoechoic circumferential wall thickening due to oedema), stenosis or occlusion that can help to confirm a diagnosis more swiftly and less invasively, but requiring more subjective interpretation. This review will help to determine the role of these investigations in clinical practice. Objectives To evaluate the sensitivity and specificity of the halo sign on temporal artery US, using the ACR 1990 classification as a reference standard, to investigate whether US could be used as triage for TAB. To compare the accuracy of US with TAB in the subset of paired studies that have obtained both tests on the same patients, to investigate whether it could replace TAB as one of the criteria in the ACR 1990 classification. Search methods We used standard Cochrane search methods for diagnostic accuracy. The date of the search was 13 September 2022. Selection criteria We included all participants with clinically suspected GCA who were investigated for the presence of the halo sign on temporal artery US, using the ACR 1990 criteria as a reference standard. We included studies with participants with a prior diagnosis of polymyalgia rheumatica. We excluded studies if participants had had two or more weeks of steroid treatment prior to the investigations. We also included any comparative test accuracy studies of the halo sign on temporal artery US versus TAB, with use of the 1990 ACR diagnostic criteria as a reference standard. Although we have chosen to use this classification for the purpose of the meta‐analysis, we accept that it incorporates unavoidable incorporation bias, as TAB is itself one of the five criteria. This increases the specificity of TAB, making it difficult to compare with US. We excluded case‐control studies, as they overestimate accuracy, as well as case series in which all participants had a prior diagnosis of GCA, as they can only address sensitivity and not specificity. Data collection and analysis Two review authors independently assessed the studies for inclusion in the review. They extracted data using a standardised data collection form and employed the QUADAS‐2 tool to assess methodological quality. As not enough studies reported data at our prespecified halo threshold of 0.3 mm, we fitted hierarchical summary receiver operating characteristic (ROC) models to estimate US sensitivity and also to compare US with TAB. We graded the certainty of the evidence using the GRADE approach. Main results Temporal artery ultrasound was investigated in 15 studies (617 participants with GCA out of 1479, 41.7%), with sample sizes ranging from 20 to 381 participants (median 69). There was wide variation in sensitivity with a median value of 0.78 (interquartile range (IQR) 0.45 to 0.83; range 0.03 to 1.00), while specificity was fair to good in most studies with a median value of 0.91 (IQR 0.78 to 1.00; range 0.40 to 1.00) and four studies with a specificity of 1.00. The hierarchical summary receiver operating characteristic (HSROC) estimate of sensitivity (95% confidence interval (CI)) at the high specificity of 0.95 was 0.51 (0.21 to 0.81), and 0.84 (0.58 to 0.95) at 0.80 specificity. We considered the evidence on sensitivity and specificity as of very low certainty due to risk of bias (−1), imprecision (−1), and inconsistency (−1). Only four studies reported data at a halo cut‐off > 0.3 mm, finding the following sensitivities and specificities (95% CI): 0.80 (0.56 to 0.94) and 0.94 (0.81 to 0.99) in 55 participants; 0.10 (0.00 to 0.45) and 1.00 (0.84 to 1.00) in 31 participants; 0.73 (0.54 to 0.88) and 1.00 (0.93 to 1.00) in 82 participants; 0.83 (0.63 to 0.95) and 0.72 (0.64 to 0.79) in 182 participants. Data on a direct comparison of temporal artery US with biopsy were obtained from 11 studies (808 participants; 460 with GCA, 56.9%). The sensitivity of US ranged between 0.03 and 1.00 with a median of 0.75, while that of TAB ranged between 0.33 and 0.92 with a median of 0.73. The specificity was 1.00 in four studies for US and in seven for TAB. At high specificity (0.95), the sensitivity of US and TAB were 0.50 (95% CI 0.24 to 0.76) versus 0.80 (95% CI 0.57 to 0.93), respectively, and at low specificity (0.80) they were 0.73 (95% CI 0.49 to 0.88) versus 0.92 (95% CI 0.69 to 0.98). We considered the comparative evidence on the sensitivity of US versus TAB to be of very low certainty because specificity was overestimated for TAB since it is one of the criteria used in the reference standard (−1), together with downgrade due to risk of bias (−1), imprecision (−1), and inconsistency (−1) for both sensitivity and specificity. Authors' conclusions There is limited published evidence on the accuracy of temporal artery US for detecting GCA. Ultrasound seems to be moderately sensitive when the specificity is good, but data were heterogeneous across studies and either did not use the same halo thickness threshold or did not report it. We can draw no conclusions from accuracy studies on whether US can replace TAB for diagnosing GCA given the very low certainty of the evidence. Future research could consider using the 2016 revision of the ACR criteria as a reference standard, which will limit incorporation bias of TAB into the reference standard. Plain language summary How useful is ultrasound for diagnosing giant cell arteritis? Key messages • Whether ultrasound (US) can replace biopsy for the diagnosis of giant cell arteritis (GCA) is uncertain. • We suggest that future studies use a different set of criteria to determine whether patients have GCA to allow US and biopsy to be compared more reliably. What is giant cell arteritis? Giant cell arteritis (GCA) is an inflammatory disease that affects blood vessels. It causes swelling and blockage of the arteries, particularly around the eye and at the side of the head, which can lead to sudden and permanent loss of vision. Why is it important to diagnose giant cell arteritis accurately? It is important to diagnose GCA accurately and quickly. Vision loss caused by the disease cannot be reversed, and it can affect both eyes within a few days. This may be prevented by starting one of a group of medications called steroids, which are used to reduce inflammation. It is important to have an accurate diagnosis, as a long course of treatment is required, and steroids can have a number of unpleasant side effects. What tests can be used to diagnose giant cell arteritis? The American College of Rheumatology (ACR) has written a list of five clinical and laboratory criteria that can be used to diagnose GCA. One of these is the results of a biopsy of an artery on the temple (side of the head). With biopsy, a small sample of the artery is taken and examined under a microscope for signs of inflammation. A biopsy is a small operation, which can take time to be arranged, performed, and interpreted, and it can be quite a while before the results are known. Why is the new test potentially better than the reference standard? Ultrasound (US) is a painless and relatively quick procedure that can be used to look at the arteries in both temples and assist with the diagnosis. Who will benefit, why and how? If US is accurate, people who have symptoms and blood results suggestive of GCA will find out if they have the condition more quickly, without needing to undergo an operation. What did we want to find out? We wanted to find out how accurate temporal artery US is at detecting features of GCA and whether it can potentially replace the biopsy altogether as one of the ACR's five criteria used for diagnosing GCA. What did we do? We searched for published studies that measured the accuracy of US for diagnosing GCA and studies that also compared the accuracy of US and biopsy. What did we find? We looked in depth at 16 studies with a total of 1479 participants, about 40% of whom were diagnosed as having GCA. In a group of 100 people with symptoms suggestive of GCA, we would expect 40 to actually have the condition. If we set a threshold for US where 3 of the 60 people without the disease get a false‐positive result, US will correctly identify 20 people that do have GCA, but give a false‐negative result to the other 20. If a threshold is set where 12 of the 60 people without the disease get a false‐positive result, US will correctly identify 34 people with GCA, but miss the remaining 6. There were 11 studies with 808 participants in whom both US and TAB were performed and compared. In a group of 100 people where 40 have GCA and 60 do not, if a threshold is set where 3 out of the 60 people without the condition are wrongly given a positive diagnosis of GCA, US will identify 20 out of the 40 people with GCA, whereas TAB will identify 32. If a threshold is set whereby 12 of the 60 people are wrongly diagnosed as having GCA, US will identify 29 out of the 40 people with true GCA, and TAB will identify 7. What are the limitations of the evidence? These results are not reliable since most studies had flaws in the way they were designed, and their accuracy for detecting people with GCA was variable and uncertain. Additionally, as biopsy is currently part of the criteria used to diagnose GCA, it was always likely to compare favourably to US in the included studies. How up‐to‐date is this evidence? The evidence is current to September 2022.","2","John Wiley & Sons, Ltd","1465-1858","*Giant Cell Arteritis; Biopsy; Humans; Sensitivity and Specificity; Temporal Arteries [diagnostic imaging, pathology]; Ultrasonography","10.1002/14651858.CD013199.pub2","http://dx.doi.org/10.1002/14651858.CD013199.pub2","Eyes and Vision"
"CD008643.PUB3","Williams, CM; Henschke, N; Maher, CG; van Tulder, MW; Koes, BW; Macaskill, P; Irwig, L","Red flags to screen for vertebral fracture in patients presenting with low‐back pain","Cochrane Database of Systematic Reviews","2023","Abstract - Background Low‐back pain (LBP) is a common condition seen in primary care. A principal aim during a clinical examination is to identify patients with a higher likelihood of underlying serious pathology, such as vertebral fracture, who may require additional investigation and specific treatment. All 'evidence‐based' clinical practice guidelines recommend the use of red flags to screen for serious causes of back pain. However, it remains unclear if the diagnostic accuracy of red flags is sufficient to support this recommendation. Objectives To assess the diagnostic accuracy of red flags obtained in a clinical history or physical examination to screen for vertebral fracture in patients presenting with LBP. Search methods Electronic databases were searched for primary studies between the earliest date and 7 March 2012. Forward and backward citation searching of eligible studies was also conducted. Selection criteria Studies were considered if they compared the results of any aspect of the history or test conducted in the physical examination of patients presenting for LBP or examination of the lumbar spine, with a reference standard (diagnostic imaging). The selection criteria were independently applied by two review authors. Data collection and analysis Three review authors independently conducted 'Risk of bias' assessment and data extraction. Risk of bias was assessed using the 11‐item QUADAS tool. Characteristics of studies, patients, index tests and reference standards were extracted. Where available, raw data were used to calculate sensitivity and specificity with 95% confidence intervals (CI). Due to the heterogeneity of studies and tests, statistical pooling was not appropriate and the analysis for the review was descriptive only. Likelihood ratios for each test were calculated and used as an indication of clinical usefulness. Main results Eight studies set in primary (four), secondary (one) and tertiary care (accident and emergency = three) were included in the review. Overall, the risk of bias of studies was moderate with high risk of selection and verification bias the predominant flaws. Reporting of index and reference tests was poor. The prevalence of vertebral fracture in accident and emergency settings ranged from 6.5% to 11% and in primary care from 0.7% to 4.5%. There were 29 groups of index tests investigated however, only two featured in more than two studies. Descriptive analyses revealed that three red flags in primary care were potentially useful with meaningful positive likelihood ratios (LR+) but mostly imprecise estimates (significant trauma, older age, corticosteroid use; LR+ point estimate ranging 3.42 to 12.85, 3.69 to 9.39, 3.97 to 48.50 respectively). One red flag in tertiary care appeared informative (contusion/abrasion; LR+ 31.09, 95% CI 18.25 to 52.96). The results of combined tests appeared more informative than individual red flags with LR+ estimates generally greater in magnitude and precision. Authors' conclusions The available evidence does not support the use of many red flags to specifically screen for vertebral fracture in patients presenting for LBP. Based on evidence from single studies, few individual red flags appear informative as most have poor diagnostic accuracy as indicated by imprecise estimates of likelihood ratios. When combinations of red flags were used the performance appeared to improve. From the limited evidence, the findings give rise to a weak recommendation that a combination of a small subset of red flags may be useful to screen for vertebral fracture. It should also be noted that many red flags have high false positive rates; and if acted upon uncritically there would be consequences for the cost of management and outcomes of patients with LBP. Further research should focus on appropriate sets of red flags and adequate reporting of both index and reference tests. Plain language summary Physician use of red flags to screen for fractured vertebrae for patients with new back pain This review describes the understanding of a common practice for checking for spinal injuries when patients come to a family practice doctor, back pain clinic or emergency room with new back pain. Doctors usually ask a few questions and examine the back to check for the possibility of a spinal fracture. The reason for this check for fractures is that the treatment is different for common back pain and fractures. Fractures are usually diagnosed with an x‐ray, then treated with rest, a back brace and pain relievers. Common back pain is treated with exercise, chiropractic manipulation, and pain relievers; x‐rays, computed tomography (CT) and magnetic resonance imaging scans are not useful for diagnosis. Fractures are rare, being the cause of back pain in the range of 1% to 4.5% of new back pain visits to family doctors. Eight studies including several thousand patients described 29 different questions and physical exam tests that have been used to look for spinal fractures. Most of the 29 were not accurate. The best four questions asked about use of steroids (which can cause weak bones), the patient’s age (age above 74 increases the risk of fractures) and recent trauma such as a fall. Using a combination of the best questions appears to improve the accuracy. For example, women above age 74 are more likely to have a fracture when they come to the physician complaining of back pain. In the emergency room, the best indication of a spinal fracture was a bruise or scrape on the painful area of the back. Fractures are rare and generally do not require emergency treatment, even if red flags exist clinicians and patients can watch and wait. During the waiting period, patients should avoid treatments like exercise and manipulation that are not recommended for spinal fractures. The worst effects of low quality red flag screening are overtreatment and undertreatment. If the tests are not accurate, patients without a fracture may get an x‐ray or CT scan that they don’t need—unnecessary exposure to x‐rays, extra worry for the patient and extra cost. At the other extreme (and much less common), it might be possible to miss a real fracture, and cause the patient to have extra time without the best treatment. Most of the studies were of low or moderate quality, so more research is needed to identify the best combination of questions and examination methods.","11","John Wiley & Sons, Ltd","1465-1858","*Low Back Pain [diagnosis, etiology]; *Spinal Fractures [complications, diagnosis]; Humans; Physical Examination; Sensitivity and Specificity","10.1002/14651858.CD008643.pub3","http://dx.doi.org/10.1002/14651858.CD008643.pub3","Back and Neck"
"CD013126.PUB2","Miranda, F; Gonzalez, F; Plana, MN; Zamora, J; Quinn, TJ; Seron, P","Confusion Assessment Method for the Intensive Care Unit (CAM‐ICU) for the diagnosis of delirium in adults in critical care settings","Cochrane Database of Systematic Reviews","2023","Abstract - Background Delirium is an underdiagnosed clinical syndrome typified by an acute alteration of mental state. It is an important problem in critical care and intensive care units (ICU) due to its high prevalence and its association with adverse outcomes. Delirium is a very distressing condition for patients, with a huge impact on their well‐being. Diagnosis of delirium in the critical care setting is challenging. This is especially true for patients who are mechanically ventilated and are therefore unable to engage in a verbal interview. The Confusion Assessment Method for the Intensive Care Unit (CAM‐ICU) is a tool specifically designed to assess for delirium in the context of ICU patients, including those on mechanical ventilation. CAM‐ICU can be administered by non‐specialists to give a dichotomous delirium present/absent result. Objectives To determine the diagnostic accuracy of the CAM‐ICU for the diagnosis of delirium in adult patients in critical care units. Search methods We searched MEDLINE (Ovid SP, 1946 to 8 July 2022), Embase (Ovid SP, 1982 to 8 July 2022), Web of Science Core Collection (ISI Web of Knowledge, 1945 to 8 July 2022), PsycINFO (Ovid SP, 1806 to 8 July 2022), and LILACS (BIREME, 1982 to 8 July 2022). We checked the reference lists of included studies and other resources for additional potentially relevant studies. We also searched the Health Technology Assessment database, the Cochrane Library, Aggressive Research Intelligence Facility database, WHO ICTRP, ClinicalTrials.gov, and websites of scientific associations to access any annual meetings and abstracts of conference proceedings in the field. Selection criteria We included diagnostic studies enrolling adult ICU patients assessed using the CAM‐ICU tool, regardless of language or publication status and reporting sufficient data on delirium diagnosis for the construction of 2 x 2 tables. Eligible studies evaluated the diagnostic performance of the CAM‐ICU versus a clinical reference standard based on any iteration of the  Diagnostic and Statistical Manual of Mental Disorders  ( DSM ) criteria applied by a clinical expert. Data collection and analysis Two review authors independently selected and collated study data. We assessed the methodological quality of studies using the QUADAS‐2 tool. We used two univariate fixed‐effect or random‐effects models to determine summary estimates of sensitivity and specificity. We performed sensitivity analyses that excluded studies considered to be at high risk of bias and high concerns in applicability, due mainly to the target population included (e.g. patients with traumatic brain injury). We also investigated potential sources of heterogeneity, assessing the effect of reference standard diagnosis and proportion of patients ventilated. Main results We included 25 studies (2817 participants). The mean age of participants ranged from 48 to 69 years; 15 of the studies included critical care units admitting mixed populations (e.g. medical, trauma, surgery patients). The percentage of patients receiving mechanical ventilation ranged from 11.8% to 100%. The prevalence of delirium in the studies included ranged from 12.5% to 83.9%. Presence of delirium was determined by the application of  DSM‐IV  criteria in 13 out of 25 included studies. We assessed 13 studies as at low risk of bias and low applicability concerns for all QUADAS‐2 domains. The most common issue of concern was flow and timing of the tests, followed by patient selection. Overall, we estimated a pooled sensitivity of 0.78 (95% confidence interval (CI) 0.72 to 0.83) and a pooled specificity of 0.95 (95% CI 0.92 to 0.97). Sensitivity analysis restricted to studies at low risk of bias and without any applicability concerns (n = 13 studies) gave similar summary accuracy indices (sensitivity 0.80 (95% CI 0.72 to 0.86), specificity 0.95 (95% CI 0.93 to 0.97)). Subgroup analyses based on diagnostic assessment found summary estimates of sensitivity and specificity for studies using  DSM‐IV  of 0.79 (95% CI 0.72 to 0.85) and 0.94 (95% CI 0.90 to 0.96). For studies that used  DSM‐5  criteria, summary estimates of sensitivity and specificity were 0.75 (95% CI 0.67 to 0.82) and 0.98 (95% CI 0.95 to 0.99).  DSM  criteria had no significant effect on sensitivity (P = 0.421), but the specificity for detection of delirium was higher when  DSM‐5  criteria were used (P = 0.024). The relative specificity comparing  DSM‐5  versus  DSM‐IV  criteria was 1.05 (95% CI 1.02 to 1.08). Summary estimates of sensitivity and specificity for studies recruiting < 100% of patients with mechanical ventilation were 0.81 (95% CI 0.75 to 0.85) and 0.95 (95% CI 0.91 to 0.98). For studies that exclusively recruited patients with mechanical ventilation, summary estimates of sensitivity and specificity were 0.91 (95% CI 0.76 to 0.97) and 0.98 (95% CI 0.92 to 0.99). Although there was a suggestion of differential performance of CAM‐ICU in ventilated patients, the differences were not significant in sensitivity (P = 0.316) or in specificity (P = 0.493). Authors' conclusions The CAM‐ICU tool may have a role in the early identification of delirium, in adult patients hospitalized in intensive care units, including those on mechanical ventilation, when non‐specialized, properly trained clinical personnel apply the CAM‐ICU. The test is most useful for exclusion of delirium. The test may miss a proportion of patients with incident delirium, therefore in situations where detection of all delirium cases is desirable, it may be best to repeat the test or combine CAM‐ICU with another assessment. Future studies should compare different screening tests proposed for bedside assessment of delirium, as this approach will reveal which tool yields superior accuracy. In addition, future studies should consider and report the flow and timing of the tests and clearly report key characteristics related to patient selection. Finally, future research should focus on the impact of CAM‐ICU screening on patient outcomes. Plain language summary How accurate is the CAM‐ICU tool for the diagnosis of delirium in adult patients admitted to critical care units? Key messages Our review suggests that the CAM‐ICU is a reasonable test to look for delirium in critical care settings. The test may be especially good for ruling out delirium. If the CAM‐ICU suggests no delirium, it is unlikely the person has delirium. What was the aim of the review? To evaluate the accuracy of the CAM‐ICU tool for detecting delirium in patients in critical care units. Why is recognizing delirium important? When people are unwell, they can experience changes in their thinking, level of consciousness, or ability to sustain attention. This is called delirium. Delirium becomes more likely as people become more unwell. Delirium is very common when people are so unwell that they need enhanced monitoring and treatment, for example in an intensive care unit. Areas of the hospital providing this enhanced care are called ‘critical care’. Compared to people who don’t experience delirium, people with delirium are more likely to require a longer stay in hospital and are more likely to die. Delirium is associated with distress for both the patient and their family. Prompt recognition of when a person has delirium would allow for treatment. However, assessing for the presence of delirium is difficult. It is even more complicated in a critical care setting, as the patient may have equipment to help them breathe, which means they can’t easily speak with the assessor. What is the CAM‐ICU? The Confusion Assessment Method for the Intensive Care Unit (CAM‐ICU) is a test to look for delirium. It is based on a series of questions and commands that are designed to detect the features of delirium. Certain aspects of the CAM‐ICU test make it suitable for busy critical care settings. The CAM‐ICU test can be completed even if the patient being tested can’t speak. The CAM‐ICU is quick to perform and requires minimal training, thus it can be used by any member of the healthcare team. What was studied in the review? We reviewed studies that assessed the accuracy of the CAM‐ICU when used in a critical care setting. We included those studies that compared the results of the CAM‐ICU test, performed by a CAM‐ICU trained professional, with a more detailed ‘gold standard’ diagnostic assessment. What are the main results of the review? We included 25 studies with a total of 2817 participants. CAM‐ICU correctly identified patients with delirium about 78% of the time. The test was better at identifying those without delirium, giving correct results about 95% of the time. If the CAM‐ICU was applied to a group of 100 patients in critical care, where 40% (40) of them have delirium, it is estimated that the CAM‐ICU tool would correctly detect around 31 patients with delirium and miss nine cases. The CAM‐ICU would suggest that three patients have delirium when they do not. How reliable are the results of the studies in the review? Despite the low risk of bias and the high numbers of participants in the included studies, we are uncertain about the results due to differences among studies in the way the test was applied and in the interpretation of the ‘gold standard’ comparison. To whom do the results of this review apply? We only looked at papers that studied the CAM‐ICU test when used in a critical care setting. CAM‐ICU is used in other healthcare settings, but that was not the focus of this review. How up‐to‐date is this review? Our search of the literature included papers published up to 8 July 2022.","11","John Wiley & Sons, Ltd","1465-1858","*Delirium [diagnosis]; *Intensive Care Units; Adult; Aged; Critical Care; Humans; Middle Aged; Sensitivity and Specificity","10.1002/14651858.CD013126.pub2","http://dx.doi.org/10.1002/14651858.CD013126.pub2","Dementia and Cognitive Improvement"
"CD014911.PUB2","Vandevenne, MMS; Favuzza, E; Veta, M; Lucenteforte, E; Berendschot, TTJM; Mencucci, R; Nuijts, RMMA; Virgili, G; Dickman, MM","Artificial intelligence for detecting keratoconus","Cochrane Database of Systematic Reviews","2023","Abstract - Background Keratoconus remains difficult to diagnose, especially in the early stages. It is a progressive disorder of the cornea that starts at a young age. Diagnosis is based on clinical examination and corneal imaging; though in the early stages, when there are no clinical signs, diagnosis depends on the interpretation of corneal imaging (e.g. topography and tomography) by trained cornea specialists. Using artificial intelligence (AI) to analyse the corneal images and detect cases of keratoconus could help prevent visual acuity loss and even corneal transplantation. However, a missed diagnosis in people seeking refractive surgery could lead to weakening of the cornea and keratoconus‐like ectasia. There is a need for a reliable overview of the accuracy of AI for detecting keratoconus and the applicability of this automated method to the clinical setting. Objectives To assess the diagnostic accuracy of artificial intelligence (AI) algorithms for detecting keratoconus in people presenting with refractive errors, especially those whose vision can no longer be fully corrected with glasses, those seeking corneal refractive surgery, and those suspected of having keratoconus. AI could help ophthalmologists, optometrists, and other eye care professionals to make decisions on referral to cornea specialists. Secondary objectives To assess the following potential causes of heterogeneity in diagnostic performance across studies. • Different AI algorithms (e.g. neural networks, decision trees, support vector machines) • Index test methodology (preprocessing techniques, core AI method, and postprocessing techniques) • Sources of input to train algorithms (topography and tomography images from Placido disc system, Scheimpflug system, slit‐scanning system, or optical coherence tomography (OCT); number of training and testing cases/images; label/endpoint variable used for training) • Study setting • Study design • Ethnicity, or geographic area as its proxy • Different index test positivity criteria provided by the topography or tomography device • Reference standard, topography or tomography, one or two cornea specialists • Definition of keratoconus • Mean age of participants • Recruitment of participants • Severity of keratoconus (clinically manifest or subclinical) Search methods We searched CENTRAL (which contains the Cochrane Eyes and Vision Trials Register), Ovid MEDLINE, Ovid Embase, OpenGrey, the ISRCTN registry, ClinicalTrials.gov, and the World Health Organization International Clinical Trials Registry Platform (WHO ICTRP). There were no date or language restrictions in the electronic searches for trials. We last searched the electronic databases on 29 November 2022. Selection criteria We included cross‐sectional and diagnostic case‐control studies that investigated AI for the diagnosis of keratoconus using topography, tomography, or both. We included studies that diagnosed manifest keratoconus, subclinical keratoconus, or both. The reference standard was the interpretation of topography or tomography images by at least two cornea specialists. Data collection and analysis Two review authors independently extracted the study data and assessed the quality of studies using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. When an article contained multiple AI algorithms, we selected the algorithm with the highest Youden's index. We assessed the certainty of evidence using the GRADE approach. Main results We included 63 studies, published between 1994 and 2022, that developed and investigated the accuracy of AI for the diagnosis of keratoconus. There were three different units of analysis in the studies: eyes, participants, and images. Forty‐four studies analysed 23,771 eyes, four studies analysed 3843 participants, and 15 studies analysed 38,832 images. Fifty‐four articles evaluated the detection of manifest keratoconus, defined as a cornea that showed any clinical sign of keratoconus. The accuracy of AI seems almost perfect, with a summary sensitivity of 98.6% (95% confidence interval (CI) 97.6% to 99.1%) and a summary specificity of 98.3% (95% CI 97.4% to 98.9%). However, accuracy varied across studies and the certainty of the evidence was low. Twenty‐eight articles evaluated the detection of subclinical keratoconus, although the definition of subclinical varied. We grouped subclinical keratoconus, forme fruste, and very asymmetrical eyes together. The tests showed good accuracy, with a summary sensitivity of 90.0% (95% CI 84.5% to 93.8%) and a summary specificity of 95.5% (95% CI 91.9% to 97.5%). However, the certainty of the evidence was very low for sensitivity and low for specificity. In both groups, we graded most studies at high risk of bias, with high applicability concerns, in the domain of patient selection, since most were case‐control studies. Moreover, we graded the certainty of evidence as low to very low due to selection bias, inconsistency, and imprecision. We could not explain the heterogeneity between the studies. The sensitivity analyses based on study design, AI algorithm, imaging technique (topography versus tomography), and data source (parameters versus images) showed no differences in the results. Authors' conclusions AI appears to be a promising triage tool in ophthalmologic practice for diagnosing keratoconus. Test accuracy was very high for manifest keratoconus and slightly lower for subclinical keratoconus, indicating a higher chance of missing a diagnosis in people without clinical signs. This could lead to progression of keratoconus or an erroneous indication for refractive surgery, which would worsen the disease. We are unable to draw clear and reliable conclusions due to the high risk of bias, the unexplained heterogeneity of the results, and high applicability concerns, all of which reduced our confidence in the evidence. Greater standardization in future research would increase the quality of studies and improve comparability between studies. Plain language summary How accurate is artificial intelligence for diagnosing keratoconus? Key messages • The included studies suggest that artificial intelligence (AI) can identify keratoconus. This may lead to early detection and prevention of vision loss. • Estimates were similar for different types of AI algorithms. • We have little confidence in the evidence; there is a need for more research on this topic. What is keratoconus and why is (early) diagnosis so important? Keratoconus is a disease of the cornea (the clear window at the front of the eye) that affects people between the ages of 10 and 40 years. In those affected, the cornea weakens and thins over the years, gradually bulging into the typical cone‐like shape, which leads to reduced vision. Glasses can resolve this problem in the early stages of keratoconus, but no longer offer a satisfying solution as the disease becomes more severe. Early diagnosis is imperative to ensure follow‐up and treatment and thus prevent loss of vision. The diagnosis of keratoconus is based on an eye exam (measuring the eye and evaluating the cornea with a vertical beam of light and a microscope) and imaging (computer‐assisted techniques that create three‐dimensional pictures or maps of the cornea). Interpreting the images can be challenging, especially in primary eye care settings and in the early stages of the disease. Not recognizing keratoconus could lead to worsening of the disease and worsening of vision. For example, people at risk of developing keratoconus who undergo refractive surgery (surgery to correct their vision) could end up with worse vision. What is artificial intelligence and how can it help detect keratoconus? Detecting keratoconus based on images is challenging, especially for untrained clinicians. AI gives machines the ability to adapt, reason, and find solutions. Algorithms can be developed and trained to analyse images of the cornea and recognize keratoconus. These tests could help ophthalmologists, optometrists, and other eye care professionals to make a diagnosis and refer people with keratoconus to cornea specialists in time to preserve their vision. There are many different types of algorithms, but they all distinguish between healthy eyes and keratoconus based on images of the cornea. What did we want to find out? The aim of the review was to find out whether AI can correctly diagnose keratoconus in people seeking refractive surgery and people whose vision can no longer be corrected fully with glasses. What did we do? We searched for studies that investigated the accuracy of AI for diagnosing keratoconus, preferably in people seeking refractive surgery or people whose vision can no longer be corrected fully with glasses. We compared and summarized the results of the studies to calculate two measures of accuracy: sensitivity (the ability of AI to correctly identify keratoconus) and specificity (the ability of AI to correctly rule out keratoconus). The closer sensitivity and specificity were to 100%, the better the algorithm. What did we find? We found 63 studies that used three different units (eyes, participants, and images) to analyse the accuracy of AI for detecting keratoconus: 44 studies analysed 23,771 eyes, four studies analysed 3843 participants, and 15 studies analysed 38,832 images. The accuracy of AI for detecting manifest keratoconus (keratoconus that can be detected through a clinical examination) was high. If 1000 people were tested, 30 people with keratoconus would be correctly referred to a cornea specialist, and none would be missed. Of the remaining 970 people (without keratoconus), only 17 would be wrongly referred. These people would receive additional non‐invasive tests to verify whether they had keratoconus. The accuracy of AI for detecting early keratoconus was lower. If 1000 people were tested, nine people with keratoconus would be correctly referred to a cornea specialist and one would be missed. If this person received refractive surgery, it would aggravate the disease and worsen their vision. Of the remaining 990 people (without keratoconus), 941 would be reassured that they did not have the disease and would receive refractive surgery or glasses; 49 people would be wrongly referred. The evidence suggests that AI may be good at detecting manifest keratoconus but may not be ideal for screening early keratoconus. What are the limitations of the evidence? We have little confidence in the evidence on the accuracy of AI for detecting manifest keratoconus, and we have little to no confidence in the evidence related to early keratoconus. There were problems with how the studies were conducted, which may result in AI appearing more accurate than it really is. How up‐to‐date is this evidence? The evidence is up‐to‐date to 29 November 2022.","11","John Wiley & Sons, Ltd","1465-1858","*Artificial Intelligence; *Keratoconus [diagnostic imaging]; Case-Control Studies; Cross-Sectional Studies; Humans; Physical Examination","10.1002/14651858.CD014911.pub2","http://dx.doi.org/10.1002/14651858.CD014911.pub2","Eyes and Vision"
"CD015089.PUB2","Gottlieb, M; Peksa, GD; Carlson, JN","Head impulse, nystagmus, and test of skew examination for diagnosing central causes of acute vestibular syndrome","Cochrane Database of Systematic Reviews","2023","Abstract - Background Dizziness is a common reason for people to seek medical care. Acute vestibular syndrome (AVS) is a specific type of dizziness, which can include severe vertigo, nausea and vomiting, nystagmus, or unsteadiness. Acute vestibular syndrome can be due to peripheral or central causes. It is important to determine the cause, as the intervention and outcomes differ if it is from a peripheral or central cause. Clinicians can assess for the cause using risk factors, patient history, examination findings, or advanced imaging, such as a magnetic resonance imaging (MRI). The head impulse, nystagmus, test of skew (HINTS) examination is a three‐part examination performed by clinicians to determine if AVS is due to a peripheral or central cause. This includes assessing how the eyes move in response to rapidly turning a person's head (head impulse), assessing the direction of involuntary eye movements (nystagmus), and assessing whether the eyes are aligned or misaligned (test of skew). The HINTS Plus examination includes an additional assessment of auditory function. Objectives To assess the diagnostic accuracy of the HINTS and HINTS Plus examinations, with or without video assistance, for identifying a central etiology for AVS. Search methods We searched CENTRAL, MEDLINE, Embase, Google Scholar, the International HTA database, and two trials registers to September 2022. Selection criteria We included all retrospective and prospective diagnostic test accuracy studies that evaluated the HINTS or HINTS Plus test used in a primary care clinic, an urgent care clinic, the emergency department, or during inpatient hospitalization against a final diagnosis of a central etiology of AVS, as defined by the reference standard of advanced imaging or final diagnosis by a neurologist. Data collection and analysis Two review authors independently determined eligibility of each study according to eligibility criteria, extracted data, assessed the risk of bias, and determined the certainty of evidence. Disagreements were adjudicated by consensus or a third review author if needed. The primary outcome was the diagnostic accuracy of the HINTS and HINTS Plus examinations for identifying a central etiology for AVS, conducted clinically (clinician visual assessment) or with video assistance (e.g. video recording with goggles); we independently assessed the clinical and video‐assisted examinations. Subgroup analyses were performed by provider type (e.g. physicians, non‐physicians), time from symptom onset to presentation (e.g. less than 24 hours, longer than 24 hours), reference standard (e.g. advanced imaging, discharge diagnosis), underlying etiology (e.g. ischemic stroke, alternative etiologies [hemorrhagic stroke, intracranial mass]), study setting (e.g. outpatient [outpatient clinic, urgent care clinic, emergency department], inpatient), physician level of training (e.g. resident, fellow/attending), physician specialty (e.g. otolaryngology, emergency medicine, neurology, and neurologic subspecialist [e.g. neuro‐ophthalmology, neuro‐otology]), and individual diagnostic accuracy of each component of the examination (e.g. head impulse, direction‐changing nystagmus, test of skew). We created 2 x 2 tables of the true positives, true negatives, false positives, and false negatives and used these data to calculate the sensitivity, specificity, positive likelihood ratio, and negative likelihood ratio with 95% confidence intervals (95% CI) for each outcome. Main results We included 16 studies with a total of 2024 participants (981 women and 1043 men) with a mean age of 60 years. Twelve studies assessed the HINTS examination; five assessed the HINTS Plus examination. Thirteen studies were performed in the emergency department; half were performed by neurologists. The clinical HINTS examination (12 studies, 1890 participants) was 94.0% (95% confidence interval [CI] 82.0% to 98.2%) sensitive, and 86.9% (95% CI 75.3% to 93.6%) specific (low‐certainty evidence). The video‐assisted HINTS examination (3 studies, 199 participants) was 85.0% to 100% sensitive (low‐certainty evidence), and 38.9% to 100% specific (very low‐certainty evidence). The clinical HINTS Plus examination (5 studies, 451 participants) was 95.3% (95% CI 78.4% to 99.1%) sensitive, and 72.9% (95% CI 44.4% to 90.1%) specific (low‐certainty evidence). The video‐assisted HINTS Plus examination (2 studies, 163 participants) was 85.0% to 93.8% sensitive, and 28.6% to 38.9% specific (moderate‐certainty evidence). Subgroup analyses were limited, as most studies were conducted in the emergency department, by physicians, and with MRI as a reference standard. Time from symptom onset to presentation varied across studies. Three studies were at high risk of bias and three studies were at unclear risk of bias for participant selection. Three studies were at unclear risk of bias for the index test. Four studies were at unclear risk of bias for the reference standard. Two studies were at unclear risk of bias for flow and timing. One study had unclear applicability concerns for participant selection. Two studies had high applicability concerns for the index test and two studies had unclear applicability concerns for the index test. No studies had applicability concerns for the reference standard. Authors' conclusions The HINTS and HINTS Plus examinations had good sensitivity and reasonable specificity for diagnosing a central cause for AVS in the emergency department when performed by trained clinicians. Overall, the evidence was of low certainty. There were limited data for the role of video‐assistance or specific subgroups. Future research should include more high‐quality studies of the HINTS and HINTS Plus examination; assessment of inter‐rater reliability across users; accuracy across different providers, specialties, and experience; and direct comparison with no HINTS or MRI to assess the effect on clinical care. Plain language summary Head impulse, nystagmus, and test of skew (HINTS) examination for causes in the brain of acute dizziness Key Messages The HINTS and HINTS Plus examinations were reasonably accurate for diagnosing a central cause for acute vestibular syndrome (AVS). What is acute vestibular syndrome? Acute vestibular syndrome is a specific type of dizziness, which can include severe vertigo (spinning sensation), nausea and vomiting, nystagmus (involuntary eye movements), or unsteadiness. Acute vestibular syndrome can be due to peripheral causes (those within the ear canal) or central causes (those within the brain). Clinicians can assess for the cause using risk factors, personal history, examination findings, or advanced imaging, such as magnetic resonance imaging (MRI). What are the HINTS and HINTS Plus examinations? The head impulse, nystagmus, test of skew (HINTS) examination is a three‐step examination performed by clinicians to determine if AVS is due to a peripheral or central cause. This includes assessing how the eyes move in response to rapidly turning a person's head (head impulse), assessing the direction of involuntary eye movements (nystagmus), and assessing whether the eyes are aligned or misaligned (test of skew). The HINTS Plus examination includes an assessment of auditory function (hearing). What did we want to find out? This review looked at how accurate the HINTS or HINTS Plus examination is in determining if a person's AVS is due to a central cause. What did we do? We looked for studies of adults (aged 16 years or older) that assessed the diagnostic accuracy of the HINTS or HINTS Plus examination compared with a gold standard of advanced imaging or final diagnosis by a neurologist. What did we find? We found 16 studies with a total of 2024 participants. Twelve studies assessed the HINTS examination; five assessed the HINTS Plus examination. Most studies were performed in the emergency department; half were performed by neurologists. The HINTS and HINTS Plus examinations had good sensitivity and reasonable specificity for diagnosing a central cause for AVS. What are the limitations of the evidence? The included participants, clinician training, clinician specialty, and clinician experience varied across studies. Study outcomes focused primarily on a single central cause (stroke), while only a few assessed for broader causes, such as intracranial bleeding, intracranial mass, or multiple sclerosis. How up to date is this evidence? This evidence is up‐to‐date to 26 September 2022.","11","John Wiley & Sons, Ltd","1465-1858","*Dizziness [diagnosis, etiology]; *Nystagmus, Pathologic [diagnosis, etiology]; Female; Humans; Male; Middle Aged; Nausea [etiology]; Prospective Studies; Reproducibility of Results; Retrospective Studies; Vertigo [diagnosis, etiology]; Vomiting [etiology]","10.1002/14651858.CD015089.pub2","http://dx.doi.org/10.1002/14651858.CD015089.pub2","Stroke"
"CD014461.PUB2","Han, CS; Hancock, MJ; Downie, A; Jarvik, JG; Koes, BW; Machado, GC; Verhagen, AP; Williams, CM; Chen, Q; Maher, CG","Red flags to screen for vertebral fracture in people presenting with low back pain","Cochrane Database of Systematic Reviews","2023","Abstract - Background Low back pain is a common presentation across different healthcare settings. Clinicians need to confidently be able to screen and identify people presenting with low back pain with a high suspicion of serious or specific pathology (e.g. vertebral fracture). Patients identified with an increased likelihood of having a serious pathology will likely require additional investigations and specific treatment. Guidelines recommend a thorough history and clinical assessment to screen for serious pathology as a cause of low back pain. However, the diagnostic accuracy of recommended red flags (e.g. older age, trauma, corticosteroid use) remains unclear, particularly those used to screen for vertebral fracture. Objectives To assess the diagnostic accuracy of red flags used to screen for vertebral fracture in people presenting with low back pain. Where possible, we reported results of red flags separately for different types of vertebral fracture (i.e. acute osteoporotic vertebral compression fracture, vertebral traumatic fracture, vertebral stress fracture, unspecified vertebral fracture). Search methods We used standard, extensive Cochrane search methods. The latest search date was 26 July 2022. Selection criteria We considered primary diagnostic studies if they compared results of history taking or physical examination (or both) findings (index test) with a reference standard test (e.g. X‐ray, magnetic resonance imaging (MRI), computed tomography (CT), single‐photon emission computerised tomography (SPECT)) for the identification of vertebral fracture in people presenting with low back pain. We included index tests that were presented individually or as part of a combination of tests. Data collection and analysis Two review authors independently extracted data for diagnostic two‐by‐two tables from the publications or reconstructed them using information from relevant parameters to calculate sensitivity, specificity, and positive (+LR) and negative (−LR) likelihood ratios with 95% confidence intervals (CIs). We extracted aspects of study design, characteristics of the population, index test, reference standard, and type of vertebral fracture. Meta‐analysis was not possible due to heterogeneity of studies and index tests, therefore the analysis was descriptive. We calculated sensitivity, specificity, and LRs for each test and used these as an indication of clinical usefulness. Two review authors independently conducted risk of bias and applicability assessment using the QUADAS‐2 tool. Main results This review is an update of a previous Cochrane Review of red flags to screen for vertebral fracture in people with low back pain. We included 14 studies in this review, six based in primary care, five in secondary care, and three in tertiary care. Four studies reported on 'osteoporotic vertebral fractures', two studies reported on 'vertebral compression fracture', one study reported on 'osteoporotic and traumatic vertebral fracture', two studies reported on 'vertebral stress fracture', and five studies reported on 'unspecified vertebral fracture'. Risk of bias was only rated as low in one study for the domains reference standard and flow and timing. The domain patient selection had three studies and the domain index test had six studies rated at low risk of bias. Meta‐analysis was not possible due to heterogeneity of the data. Results from single studies suggest only a small number of the red flags investigated may be informative. In the primary healthcare setting, results from single studies suggest 'trauma' demonstrated informative +LRs (range: 1.93 to 12.85) for 'unspecified vertebral fracture' and 'osteoporotic vertebral fracture' (+LR: 6.42, 95% CI 2.94 to 14.02). Results from single studies suggest 'older age' demonstrated informative +LRs for studies in primary care for 'unspecified vertebral fracture' (older age greater than 70 years: 11.19, 95% CI 5.33 to 23.51). Results from single studies suggest 'corticosteroid use' may be an informative red flag in primary care for 'unspecified vertebral fracture' (+LR range: 3.97, 95% CI 0.20 to 79.15 to 48.50, 95% CI 11.48 to 204.98) and 'osteoporotic vertebral fracture' (+LR: 2.46, 95% CI 1.13 to 5.34); however, diagnostic values varied and CIs were imprecise. Results from a single study suggest red flags as part of a combination of index tests such as 'older age and female gender' in primary care demonstrated informative +LRs for 'unspecified vertebral fracture' (16.17, 95% CI 4.47 to 58.43). In the secondary healthcare setting, results from a single study suggest 'trauma' demonstrated informative +LRs for 'unspecified vertebral fracture' (+LR: 2.18, 95% CI 1.86 to 2.54) and 'older age' demonstrated informative +LRs for 'osteoporotic vertebral fracture' (older age greater than 75 years: 2.51, 95% CI 1.48 to 4.27). Results from a single study suggest red flags as part of a combination of index tests such as 'older age and trauma' in secondary care demonstrated informative +LRs for 'unspecified vertebral fracture' (+LR: 4.35, 95% CI 2.92 to 6.48). Results from a single study suggest when '4 of 5 tests' were positive in secondary care, they demonstrated informative +LRs for 'osteoporotic vertebral fracture' (+LR: 9.62, 95% CI 5.88 to 15.73). In the tertiary care setting, results from a single study suggest 'presence of contusion/abrasion' was informative for 'vertebral compression fracture' (+LR: 31.09, 95% CI 18.25 to 52.96). Authors' conclusions The available evidence suggests that only a few red flags are potentially useful in guiding clinical decisions to further investigate people suspected to have a vertebral fracture. Most red flags were not useful as screening tools to identify vertebral fracture in people with low back pain. In primary care, 'older age' was informative for 'unspecified vertebral fracture', and 'trauma' and 'corticosteroid use' were both informative for 'unspecified vertebral fracture' and 'osteoporotic vertebral fracture'. In secondary care, 'older age' was informative for 'osteoporotic vertebral fracture' and 'trauma' was informative for 'unspecified vertebral fracture'. In tertiary care, 'presence of contusion/abrasion' was informative for 'vertebral compression fracture'. Combinations of red flags were also informative and may be more useful than individual tests alone. Unfortunately, the challenge to provide clear guidance on which red flags should be used routinely in clinical practice remains. Further research with primary studies is needed to improve and consolidate our current recommendations for screening for vertebral fractures to guide clinical care. Plain language summary Use of red flags to screen for vertebral fractures in people with low back pain Key messages – The four best red flags for vertebral fractures in people with low back pain were corticosteroid use (e.g. medicines that can weaken bones), older age (e.g. aged above 70 years), trauma (e.g. a fall), and a contusion (bruising) or abrasion (cuts and grazes). – More research is needed to identify the best red flags or combination of red flags to screen for spinal fractures. Red flags to screen for spinal fractures Red flags for spinal fractures are signs and symptoms found by a health professional (e.g. doctor, physiotherapist) during an examination that warn that something is wrong within the spine (backbone). The accuracy of red flags is important, as low‐quality tests can lead to incorrect diagnosis and treatment. On the one hand, if the tests are not accurate, people without a spinal fracture (break) may undergo unnecessary imaging (e.g. X‐ray, magnetic resonance imaging that uses radio waves to produce detailed images of the inside of the body). Some of these imaging methods lead to radiation exposure, extra costs, and added worry for the patient. On the other hand, missing a spinal fracture will result in a delay in receiving treatment and reduce quality of life. Therefore, identifying the most accurate red flags to screen for spinal fractures is needed. What did we want to find out? We wanted to assess how accurate the red flags used to screen for spinal fracture are in people presenting with low back pain. Where possible, we reported results of red flags separately for the different types of spinal fracture, such as osteoporotic vertebral compression fractures (e.g. fractures due to osteoporosis), vertebral traumatic fracture (e.g. due to falls), vertebral stress fracture (e.g. rapid increase in load on the spine), or unspecified vertebral fracture (e.g. no specific cause reported). What did we do? We updated a previous Cochrane Review. We searched for studies that investigated the accuracy of red flags across different healthcare settings. We included studies that compared results of history taking and physical examination (or both) (known as index tests or red flags) with different types of imaging (known as the reference standard) to identify spinal fractures in people with low back pain. We also included studies if they reported on the results of red flags separately for the different types of spinal fractures. What did we find? Fourteen studies investigated different red flags used to identify spinal fractures, and most of the red flags were not accurate or useful. Overall, the four best red flags found were corticosteroid use (e.g. medicines that can weaken bones), person's age (e.g. aged above 70 years), trauma (e.g. a fall), and a contusion (bruising) or abrasion (cuts and grazes). In the primary healthcare setting (e.g. general practitioners), 'trauma' as a red flag was best to screen for 'unspecified spinal fracture' and 'osteoporotic spinal fracture'. 'Older age' as a red flag was best to screen for 'unspecified spinal fracture' in primary care. 'Corticosteroid use' may be useful as a red flag in primary care to screen for 'unspecified spinal fracture' and 'osteoporotic spinal fracture'. Red flags as part of a combination of index tests such as 'older age and female gender' as a red flag in primary care is best to screen for 'unspecified spinal fracture'. In the secondary healthcare setting (e.g. specialists and consultants), 'trauma' as a red flag is best to screen for 'unspecified spinal fracture' and 'older age' for 'osteoporotic spinal fracture'. Red flags as part of a combination of index tests such as 'older age and trauma' in secondary care as a red flag is best to screen for 'unspecified spinal fracture'. When 'four of five tests' are positive in secondary care as a red flag, it may be used to screen for 'osteoporotic spinal fracture'. In the tertiary care setting (e.g. specialised care in a hospital setting), the 'presence of contusion/abrasion' as a red flag was best to screen for 'spinal compression fracture'. What are the limitations of the evidence? A limitation of our review is that most of the included studies were different in terms of the healthcare setting they were performed in, used different study designs, or presented data for different types of spinal fracture, which made drawing conclusions difficult. Many of the red flags investigated were from single studies and few studies investigated the same index tests. There was also little uniform agreement on the definition of red flags (e.g. for corticosteroids, it was not clear how long and how much was used), which may explain why the accuracy of some red flags varied from study to study. Some red flags may also affect different types of spinal fractures differently; however, this was not clearly reported in most cases. How up to date is this evidence? The evidence is up to date to July 2022.","8","John Wiley & Sons, Ltd","1465-1858","*Contusions; *Fractures, Compression [diagnosis, diagnostic imaging]; *Fractures, Stress; *Low Back Pain [diagnosis, etiology]; *Spinal Fractures [diagnosis, diagnostic imaging]; Adrenal Cortex Hormones; Aged; Female; Humans","10.1002/14651858.CD014461.pub2","http://dx.doi.org/10.1002/14651858.CD014461.pub2","Back and Neck"
"CD012660.PUB2","Okwundu, CI; Olowoyeye, A; Uthman, OA; Smith, J; Wiysonge, CS; Bhutani, VK; Fiander, M; Gautham, KS","Transcutaneous bilirubinometry versus total serum bilirubin measurement for newborns","Cochrane Database of Systematic Reviews","2023","Abstract - Background Jaundice is a very common condition in newborns, affecting up to 60% of term newborns and 80% of preterm newborns in the first week of life. Jaundice is caused by increased bilirubin in the blood from the breakdown of red blood cells. The gold standard for measuring bilirubin levels is obtaining a blood sample and processing it in a laboratory. However, noninvasive transcutaneous bilirubin (TcB) measurement devices are widely available and used in many settings to estimate total serum bilirubin (TSB) levels. Objectives To determine the diagnostic accuracy of transcutaneous bilirubin measurement for detecting hyperbilirubinaemia in newborns. Search methods We searched CENTRAL, MEDLINE, Embase, CINAHL and trial registries up to 18 August 2022. We also checked the reference lists of all included studies and relevant systematic reviews for other potentially eligible studies. Selection criteria We included cross‐sectional and prospective cohort studies that evaluated the accuracy of any TcB device compared to TSB measurement in term or preterm newborn infants (0 to 28 days postnatal age). All included studies provided sufficient data and information to create a 2 × 2 table for the calculation of measures of diagnostic accuracy, including sensitivities and specificities. We excluded studies that only reported correlation coefficients. Data collection and analysis Two review authors independently applied the eligibility criteria to all citations from the search and extracted data from the included studies using a standard data extraction form. We summarised the available results narratively and, where possible, we combined study data in a meta‐analysis. Main results We included 23 studies, involving 5058 participants. All studies had low risk of bias as measured by the QUADAS 2 tool. The studies were conducted in different countries and settings, included newborns of different gestational and postnatal ages, compared various TcB devices (including the JM 101, JM 102, JM 103, BiliChek, Bilitest and JH20‐1C) and used different cutoff values for a positive result. In most studies, the TcB measurement was taken from the forehead, sternum, or both. The sensitivity of various TcB cutoff values to detect significant hyperbilirubinaemia ranged from 74% to 100%, and specificity ranged from 18% to 89%. Authors' conclusions The high sensitivity of TcB to detect hyperbilirubinaemia suggests that TcB devices are reliable screening tests for ruling out hyperbilirubinaemia in newborn infants. Positive test results would require confirmation through serum bilirubin measurement. Plain language summary Is measuring bilirubin levels through the skin a reliable alternative to measuring bilirubin levels in the blood of newborns? Key messages The studies included in this review suggest that measuring bilirubin levels through the skin without a needle can identify high bilirubin levels in newborns. Why is it important to diagnose high bilirubin levels in newborns? Bilirubin is a substance produced through the breakdown of red blood cells. Jaundice is a very common problem in the newborn period and results from high levels of bilirubin in the blood (hyperbilirubinaemia). It is important to detect hyperbilirubinaemia early to prevent unwanted consequences such as brain damage. What is transcutaneous bilirubin measurement? The usual procedure for measuring bilirubin in newborns is to collect a sample of blood (by making a small cut in the heel or inserting a needle into a vein, which can be painful for the infant) and test it in the laboratory (total serum bilirubin measurement). However, there are devices that measure bilirubin by sending a flash of light through the skin (transcutaneous bilirubin measurement). This method is painless and gives an almost immediate result. What did we want to find out? We wanted to find out whether transcutaneous bilirubin measurement devices could accurately diagnose hyperbilirubinaemia. What did we do? We searched for studies that had investigated the accuracy of transcutaneous bilirubin measurement compared with total serum bilirubin measurement. We had intended to combine the results across studies using statistical methods but were unable to; instead, we presented the results narratively. What did we find? We found 23 studies (5058 participants) that were conducted in different countries and settings, used different transcutaneous bilirubin measuring devices, and defined hyperbilirubinaemia with different bilirubin values. Some of the infants were premature and others were born at term (from 37 weeks' pregnancy); their ages ranged from birth to one month of life. Overall, the findings of the studies suggest that transcutaneous bilirubin measurement is a good screening tool for detecting hyperbilirubinaemia in newborns. The included studies found different degrees or levels of accuracy for the use of transcutaneous bilirubin measurement. However, due to the differences between studies, we could not provide an overall combined summary of the accuracy of the different tests. The differences in these studies included factors like the threshold values for hyperbilirubinaemia, the types of transcutaneous bilirubin measuring devices, and age and ethnicity/skin colour of the included infants. What are the limitations of the evidence? The included studies were of high methodological quality. However, we reported the results narratively and did not formally evaluate the quality of evidence using GRADE. How up to date is this evidence? The evidence is up to date to August 2022.","5","John Wiley & Sons, Ltd","1465-1858","*Bilirubin; *Jaundice, Neonatal [diagnosis]; Cross-Sectional Studies; Humans; Hyperbilirubinemia [diagnosis]; Infant; Infant, Newborn; Neonatal Screening [methods]; Prospective Studies","10.1002/14651858.CD012660.pub2","http://dx.doi.org/10.1002/14651858.CD012660.pub2","Neonatal"
"CD013129.PUB2","Gokulakrishnan, G; Kulkarni, M; He, S; Leeflang, MMG; Cabrera, AG; Fernandes, CJ; Pammi, M","Brain natriuretic peptide and N‐terminal brain natriuretic peptide for the diagnosis of haemodynamically significant patent ductus arteriosus in preterm neonates","Cochrane Database of Systematic Reviews","2022","Abstract - Background Echocardiogram is the reference standard for the diagnosis of haemodynamically significant patent ductus arteriosus (hsPDA) in preterm infants. A simple blood assay for brain natriuretic peptide (BNP) or amino‐terminal pro‐B‐type natriuretic peptide (NT‐proBNP) may be useful in the diagnosis and management of hsPDA, but a summary of the diagnostic accuracy has not been reviewed recently. Objectives Primary objective: To determine the diagnostic accuracy of the cardiac biomarkers BNP and NT‐proBNP for diagnosis of haemodynamically significant patent ductus arteriosus (hsPDA) in preterm neonates. Our secondary objectives were: to compare the accuracy of BNP and NT‐proBNP; and to explore possible sources of heterogeneity among studies evaluating BNP and NT‐proBNP, including type of commercial assay, chronological age of the infant at testing, gestational age at birth, whether used to initiate medical or surgical treatment, test threshold, and criteria of the reference standard (type of echocardiographic parameter used for diagnosis, clinical symptoms or physical signs if data were available). Search methods We searched the following databases in September 2021: MEDLINE, Embase, Cumulative Index to Nursing and Allied Health Literature (CINAHL) and Web of Science. We also searched clinical trial registries and conference abstracts. We checked references of included studies and conducted cited reference searches of included studies. We did not apply any language or date restrictions to the electronic searches or use methodological filters, so as to maximise sensitivity. Selection criteria We included prospective or retrospective, cohort or cross‐sectional studies, which evaluated BNP or NT‐proBNP (index tests) in preterm infants (participants) with suspected hsPDA (target condition) in comparison with echocardiogram (reference standard). Data collection and analysis Two authors independently screened title/abstracts and full‐texts, resolving any inclusion disagreements through discussion or with a third reviewer. We extracted data from included studies to create 2 × 2 tables. Two independent assessors performed quality assessment using the Quality Assessment of Diagnostic‐Accuracy Studies‐2 (QUADAS 2) tool. We excluded studies that did not report data in sufficient detail to construct 2 × 2 tables, and where this information was not available from the primary investigators. We used bivariate and hierarchical summary receiver operating characteristic (HSROC) random‐effects models for meta‐analysis and generated summary receiver operating characteristic space (ROC) curves. Since both BNP and NTproBNP are continuous variables, sensitivity and specificity were reported at multiple thresholds. We dealt with the threshold effect by reporting summary ROC curves without summary points. Main results We included 34 studies: 13 evaluated BNP and 21 evaluated NT‐proBNP in the diagnosis of hsPDA. Studies varied by methodological quality, type of commercial assay, thresholds, age at testing, gestational age and whether the assay was used to initiate medical or surgical therapy. We noted some variability in the definition of hsPDA among the included studies. For BNP, the summary curve is reported in the ROC space (13 studies, 768 infants, low‐certainty evidence). The estimated specificities from the ROC curve at fixed values of sensitivities at median (83%), lower and upper quartiles (79% and 92%) were 93.6% (95% confidence interval (CI) 77.8 to 98.4), 95.5% (95% CI 83.6 to 98.9) and 81.1% (95% CI 50.6 to 94.7), respectively. Subgroup comparisons revealed differences by type of assay and better diagnostic accuracy at lower threshold cut‐offs (< 250 pg/ml compared to ≥ 250 pg/ml), testing at gestational age < 30 weeks and chronological age at testing at one to three days. Data were insufficient for subgroup analysis of whether the BNP testing was indicated for medical or surgical management of PDA. For NT‐proBNP, the summary ROC curve is reported in the ROC space (21 studies, 1459 infants, low‐certainty evidence). The estimated specificities from the ROC curve at fixed values of sensitivities at median (92%), lower and upper quartiles (85% and 94%) were 83.6% (95% CI 73.3 to 90.5), 90.6% (95% CI 83.8 to 94.7) and 79.4% (95% CI 67.5 to 87.8), respectively. Subgroup analyses by threshold (< 6000 pg/ml and ≥ 6000 pg/ml) did not reveal any differences. Subgroup analysis by mean gestational age (< 30 weeks vs 30 weeks and above) showed better accuracy with < 30 weeks, and chronological age at testing (days one to three vs over three) showed testing at days one to three had better diagnostic accuracy. Data were insufficient for subgroup analysis of whether the NTproBNP testing was indicated for medical or surgical management of PDA. We performed meta‐regression for BNP and NT‐proBNP using the covariates: assay type, threshold, mean gestational age and chronological age; none of the covariates significantly affected summary sensitivity and specificity. Authors' conclusions Low‐certainty evidence suggests that BNP and NT‐proBNP have moderate accuracy in diagnosing hsPDA and may work best as a triage test to select infants for echocardiography. The studies evaluating the diagnostic accuracy of BNP and NT‐proBNP for hsPDA varied considerably by assay characteristics (assay kit and threshold) and infant characteristics (gestational and chronological age); hence, generalisability between centres is not possible. We recommend that BNP or NT‐proBNP assays be locally validated for specific populations and outcomes, to initiate therapy or follow response to therapy. Plain language summary In preterm infants, how accurate are the blood tests BNP and NT‐proBNP in identifying PDAs (patent ductus arteriosus) that require further treatment? A number of preterm infants have a heart condition called patent ductus arteriosus (PDA). A PDA is a blood vessel that usually closes spontaneously in term and most preterm infants, but in some the PDA tends to remain open longer. Preterm infants who have an open PDA often remain stable; but in some, the large blood flow through the PDA can cause breathing and blood pressure problems and may need treatment. Usually, a large PDA is identified by a scan of the heart called an echocardiogram (echo). Performing an echo is time consuming and requires several resources; equipment, personnel trained to operate it and a paediatric cardiologist to review the images. Given how challenging it can be for many hospitals to have all these resources in place, alternate blood tests (BNP and NT‐proBNP) that can identify a large PDA in a timelier fashion may be useful. What is the aim of this review? To see if BNP and NT‐proBNP can identify a large PDA accurately so an echo need not be performed in all infants. What was studied in the review? The review looked at studies, which measured blood levels of BNP and NT‐proBNP in preterm infants and compared results to an echo. What are the main results of the review? We identified a total of 34 studies (13 studies for BNP and 21 studies for NT‐proBNP) to answer this question. The results of these studies suggest that if the BNP blood test was performed in a group of 100 preterm infants who may have a large problematic PDA, the BNP blood test would miss only two of these infants but may wrongly detect large PDA in 13 of the 100 infants. Similarly, if the NT‐proBNP blood test was performed in a group of 100 preterm infants who may have a large problematic PDA, the NT‐proBNP blood test would miss only two of these infants but may wrongly detect large PDA in 11 of the 100 infants. How reliable are the results of the studies in this review? We have little confidence in the evidence because the results of the studies were inconsistent and variable. Who do the results of this review apply to? Preterm infants who are in the hospital for prematurity and are suspected to have a large PDA. What are the implications of this review? The blood tests were accurate enough to consider being used as first‐line tests, even though they were variable. Depending on the results, an echo could then be considered if needed. Such an approach may decrease the need for echo, which can be expensive and time consuming, requiring specialised equipment and trained personnel. How up‐to‐date is this review? The review included studies published up to September 2021.","12","John Wiley & Sons, Ltd","1465-1858","*Infant, Premature; *Natriuretic Peptide, Brain; Cross-Sectional Studies; Humans; Infant; Infant, Newborn; Prospective Studies; Retrospective Studies","10.1002/14651858.CD013129.pub2","http://dx.doi.org/10.1002/14651858.CD013129.pub2","Neonatal"
"CD013652.PUB2","Fox, T; Geppert, J; Dinnes, J; Scandrett, K; Bigio, J; Sulis, G; Hettiarachchi, D; Mathangasinghe, Y; Weeratunga, P; Wickramasinghe, D; Bergman, H; Buckley, BS; Probyn, K; Sguassero, Y; Davenport, C; Cunningham, J; Dittrich, S; Emperador, D; Hooft, L; Leeflang, MMG; McInnes, MDF; Spijker, R; Struyf, T; Van den Bruel, A; Verbakel, JY; Takwoingi, Y; Taylor-Phillips, S; Deeks, JJ","Antibody tests for identification of current and past infection with SARS‐CoV‐2","Cochrane Database of Systematic Reviews","2022","Abstract - Background The diagnostic challenges associated with the COVID‐19 pandemic resulted in rapid development of diagnostic test methods for detecting SARS‐CoV‐2 infection. Serology tests to detect the presence of antibodies to SARS‐CoV‐2 enable detection of past infection and may detect cases of SARS‐CoV‐2 infection that were missed by earlier diagnostic tests. Understanding the diagnostic accuracy of serology tests for SARS‐CoV‐2 infection may enable development of effective diagnostic and management pathways, inform public health management decisions and understanding of SARS‐CoV‐2 epidemiology. Objectives To assess the accuracy of antibody tests, firstly, to determine if a person presenting in the community, or in primary or secondary care has current SARS‐CoV‐2 infection according to time after onset of infection and, secondly, to determine if a person has previously been infected with SARS‐CoV‐2. Sources of heterogeneity investigated included: timing of test, test method, SARS‐CoV‐2 antigen used, test brand, and reference standard for non‐SARS‐CoV‐2 cases. Search methods The COVID‐19 Open Access Project living evidence database from the University of Bern (which includes daily updates from PubMed and Embase and preprints from medRxiv and bioRxiv) was searched on 30 September 2020. We included additional publications from the Evidence for Policy and Practice Information and Co‐ordinating Centre (EPPI‐Centre) ‘COVID‐19: Living map of the evidence’ and the Norwegian Institute of Public Health ’NIPH systematic and living map on COVID‐19 evidence’. We did not apply language restrictions. Selection criteria We included test accuracy studies of any design that evaluated commercially produced serology tests, targeting IgG, IgM, IgA alone, or in combination. Studies must have provided data for sensitivity, that could be allocated to a predefined time period after onset of symptoms, or after a positive RT‐PCR test. Small studies with fewer than 25 SARS‐CoV‐2 infection cases were excluded. We included any reference standard to define the presence or absence of SARS‐CoV‐2 (including reverse transcription polymerase chain reaction tests (RT‐PCR), clinical diagnostic criteria, and pre‐pandemic samples). Data collection and analysis We use standard screening procedures with three reviewers. Quality assessment (using the QUADAS‐2 tool) and numeric study results were extracted independently by two people. Other study characteristics were extracted by one reviewer and checked by a second. We present sensitivity and specificity with 95% confidence intervals (CIs) for each test and, for meta‐analysis, we fitted univariate random‐effects logistic regression models for sensitivity by eligible time period and for specificity by reference standard group. Heterogeneity was investigated by including indicator variables in the random‐effects logistic regression models. We tabulated results by test manufacturer and summarised results for tests that were evaluated in 200 or more samples and that met a modification of UK Medicines and Healthcare products Regulatory Agency (MHRA) target performance criteria. Main results We included 178 separate studies (described in 177 study reports, with 45 as pre‐prints) providing 527 test evaluations. The studies included 64,688 samples including 25,724 from people with confirmed SARS‐CoV‐2; most compared the accuracy of two or more assays (102/178, 57%). Participants with confirmed SARS‐CoV‐2 infection were most commonly hospital inpatients (78/178, 44%), and pre‐pandemic samples were used by 45% (81/178) to estimate specificity. Over two‐thirds of studies recruited participants based on known SARS‐CoV‐2 infection status (123/178, 69%). All studies were conducted prior to the introduction of SARS‐CoV‐2 vaccines and present data for naturally acquired antibody responses. Seventy‐nine percent (141/178) of studies reported sensitivity by week after symptom onset and 66% (117/178) for convalescent phase infection. Studies evaluated enzyme‐linked immunosorbent assays (ELISA) (165/527; 31%), chemiluminescent assays (CLIA) (167/527; 32%) or lateral flow assays (LFA) (188/527; 36%). Risk of bias was high because of participant selection (172, 97%); application and interpretation of the index test (35, 20%); weaknesses in the reference standard (38, 21%); and issues related to participant flow and timing (148, 82%). We judged that there were high concerns about the applicability of the evidence related to participants in 170 (96%) studies, and about the applicability of the reference standard in 162 (91%) studies. Average sensitivities for current SARS‐CoV‐2 infection increased by week after onset for all target antibodies. Average sensitivity for the combination of either IgG or IgM was 41.1% in week one (95% CI 38.1 to 44.2; 103 evaluations; 3881 samples, 1593 cases), 74.9% in week two (95% CI 72.4 to 77.3; 96 evaluations, 3948 samples, 2904 cases) and 88.0% by week three after onset of symptoms (95% CI 86.3 to 89.5; 103 evaluations, 2929 samples, 2571 cases). Average sensitivity during the convalescent phase of infection (up to a maximum of 100 days since onset of symptoms, where reported) was 89.8% for IgG (95% CI 88.5 to 90.9; 253 evaluations, 16,846 samples, 14,183 cases), 92.9% for IgG or IgM combined (95% CI 91.0 to 94.4; 108 evaluations, 3571 samples, 3206 cases) and 94.3% for total antibodies (95% CI 92.8 to 95.5; 58 evaluations, 7063 samples, 6652 cases). Average sensitivities for IgM alone followed a similar pattern but were of a lower test accuracy in every time slot. Average specificities were consistently high and precise, particularly for pre‐pandemic samples which provide the least biased estimates of specificity (ranging from 98.6% for IgM to 99.8% for total antibodies). Subgroup analyses suggested small differences in sensitivity and specificity by test technology however heterogeneity in study results, timing of sample collection, and smaller sample numbers in some groups made comparisons difficult. For IgG, CLIAs were the most sensitive (convalescent‐phase infection) and specific (pre‐pandemic samples) compared to both ELISAs and LFAs (P < 0.001 for differences across test methods). The antigen(s) used (whether from the Spike‐protein or nucleocapsid) appeared to have some effect on average sensitivity in the first weeks after onset but there was no clear evidence of an effect during convalescent‐phase infection. Investigations of test performance by brand showed considerable variation in sensitivity between tests, and in results between studies evaluating the same test. For tests that were evaluated in 200 or more samples, the lower bound of the 95% CI for sensitivity was 90% or more for only a small number of tests (IgG, n = 5; IgG or IgM, n = 1; total antibodies, n = 4). More test brands met the MHRA minimum criteria for specificity of 98% or above (IgG, n = 16; IgG or IgM, n = 5; total antibodies, n = 7). Seven assays met the specified criteria for both sensitivity and specificity. In a low‐prevalence (2%) setting, where antibody testing is used to diagnose COVID‐19 in people with symptoms but who have had a negative PCR test, we would anticipate that 1 (1 to 2) case would be missed and 8 (5 to 15) would be falsely positive in 1000 people undergoing IgG or IgM testing in week three after onset of SARS‐CoV‐2 infection. In a seroprevalence survey, where prevalence of prior infection is 50%, we would anticipate that 51 (46 to 58) cases would be missed and 6 (5 to 7) would be falsely positive in 1000 people having IgG tests during the convalescent phase (21 to 100 days post‐symptom onset or post‐positive PCR) of SARS‐CoV‐2 infection. Authors' conclusions Some antibody tests could be a useful diagnostic tool for those in whom molecular‐ or antigen‐based tests have failed to detect the SARS‐CoV‐2 virus, including in those with ongoing symptoms of acute infection (from week three onwards) or those presenting with post‐acute sequelae of COVID‐19. However, antibody tests have an increasing likelihood of detecting an immune response to infection as time since onset of infection progresses and have demonstrated adequate performance for detection of prior infection for sero‐epidemiological purposes. The applicability of results for detection of vaccination‐induced antibodies is uncertain. Plain language summary What is the diagnostic accuracy of antibody tests for the detection of infection with the COVID‐19 virus? Background COVID‐19 is an infectious disease caused by the SARS‐CoV‐2 virus that spreads easily between people in a similar way to the common cold or ‘flu’. Most people with COVID‐19 have a mild‐to‐moderate respiratory illness, and some may have no symptoms (asymptomatic infection). Others experience severe symptoms and need specialist treatment and intensive care. In response to COVID‐19 infection, the immune system develops proteins called antibodies that can attack the virus as it circulates in their blood. People who have been vaccinated against COVID‐19 also produce these antibodies against the virus. Tests are available to detect antibodies in peoples' blood, which may indicate that they currently have COVID‐19 or have had it previously, or it may indicate that they have been vaccinated (although this group was not the focus of this review). Why are accurate tests important? Accurate testing allows identification of people who need to isolate themselves to prevent the spread of infection, or who might need treatment for their infection. Failure of diagnostic tests to detect infection with COVID‐19 when it is present (a false negative result) may delay treatment and risk further spread of infection to others. Incorrect diagnosis of COVID‐19 when it is not present (a false positive result) may lead to unnecessary further testing, treatment, and isolation of the person and close contacts. Accurate identification of people who have previously had COVID‐19 is important in measuring disease spread and assessing the success of public health interventions. To determine the accuracy of an antibody test in identifying COVID‐19, test results are compared in people known to have (or have had) COVID‐19 and in people known not to have (or have had) COVID‐19. The criteria used to determine whether people are known or not known to have COVID‐19 is called the ‘reference standard’. Many studies use a test called reverse transcriptase polymerase chain reaction (RT‐PCR) as the reference standard, with samples taken from the nose and throat. Additional tests that can be used include measuring symptoms, like coughing or high temperature, or ‘imaging’ tests like chest X‐rays. People known not to have COVID‐19 are sometimes identified from stored blood samples taken before COVID‐19 existed, or from patients with symptoms confirmed to be caused by other diseases. What did the review study? We wanted to find out whether antibody tests: ‐ are able to diagnose infection in people with or without symptoms of COVID‐19, and ‐ can be used to find out if someone has already had COVID‐19. The studies we included in our review looked at three types of antibodies. Most commonly, antibody tests measure two types known as IgG and IgM, but some tests only measure a single type of antibody or different combinations of the three types of antibodies (IgA, IgG, IgM). What did we do? We looked for studies that measured the diagnostic accuracy of antibody tests to detect current or past COVID‐19 infection and compared them with reference standard criteria. Since there are many antibody tests available, we included studies assessing any antibody test compared with any reference standard. People could be tested in hospital or in the community. The people tested may have been confirmed to have, or not to have, COVID‐19 infection, or they may be suspected of having COVID‐19. Study characteristics We found 178 relevant studies. Studies took place in Europe (94), Asia (45), North America (35), Australia (2), and South America (2). Seventy‐eight studies included people who were in hospital with suspected or confirmed COVID‐19 infection and 14 studies included people in community settings. Several studies included people from multiple settings (35) or did not report where the participants were recruited from (39). One hundred and forty‐one studies included recent infection cases (mainly week 1 to week 3 after onset of symptoms), and many also included people tested later (from day 21 onwards after infection) (117). Main results In participants that had COVID‐19 and were tested one week after symptoms developed, antibody tests detected only 27% to 41% of infections. In week 2 after first symptoms, 64% to 79% of infections were detected, rising to 78% to 88% in week 3. Tests that specifically detected IgG or IgM antibodies were the most accurate and, when testing people from 21 days after first symptoms, they detected 93% of people with COVID‐19. Tests gave false positive results for 1% of those without COVID‐19. Below we illustrate results for two different scenarios. If 1000 people were tested for IgG or IgM antibodies during the third week after onset of symptoms and only 20 (2%) of them actually had COVID‐19: ‐ 26 people would test positive. Of these, 8 people (31%) would not have COVID‐19 (false positive result). ‐ 974 people would test negative. Of these, 2 people (0.2%) would actually have COVID‐19 (false negative result). If 1000 people with no symptoms for COVID‐19 were tested for IgG antibodies and 500 (50%) of them had previously had COVID‐19 infection more than 21 days previously: ‐ 455 people would test positive. Of these, 6 people (1%) would not have been infected (false positive result). ‐ 545 people would test negative. Of these, 51 (9%) would actually have had a prior COVID‐19 infection (false negative result). How reliable were the results of the studies of this review? We have limited confidence in the evidence for several reasons. The number of samples contributed by studies for each week post‐symptom onset was often small, and there were sometimes problems with how studies were conducted. Participants included in the studies were often hospital patients who were more likely to have experienced severe symptoms of COVID‐19. The accuracy of antibody tests for detecting COVID‐19 in these patients may be different from the accuracy of the tests in people with mild or moderate symptoms. It is not possible to identify by how much the test results would differ in other populations. Who do the results of this review apply to? A high percentage of participants were in hospital with COVID‐19, so were likely to have more severe disease than people with COVID‐19 who were not hospitalised. Only a small number of studies assessed these tests in people with no symptoms. The results of the review may therefore be more applicable to those with severe disease than people with mild symptoms. Studies frequently did not report whether participants had symptoms at the time samples were taken for testing making it difficult to fully separate test results for early‐phase infection as opposed to later‐phase infections. The studies in our review assessed several test methods across a global population, therefore it is likely that test results would be similar in most areas of the world. What are the implications of this review? The review shows that antibody tests could have a useful role in detecting if someone has had COVID‐19, but the timing of test use is important. Some antibody tests may help to confirm COVID‐19 infection in people who have had symptoms for more than two weeks but who have been unable to confirm their infection using other methods. This is particularly useful if they are experiencing potentially serious symptoms that may be due to COVID‐19 as they may require specific treatment. Antibody tests may also be useful to determine how many people have had a previous COVID‐19 infection. We could not be certain about how well the tests work for people who have milder disease or no symptoms, or for detecting antibodies resulting from vaccination. How up‐to‐date is this review? This review updates our previous review. The evidence is up‐to‐date to September 2020.","11","John Wiley & Sons, Ltd","1465-1858","*COVID-19 [diagnosis, epidemiology]; *SARS-CoV-2; Antibodies, Viral; COVID-19 Vaccines; Humans; Immunoglobulin G; Immunoglobulin M; Pandemics; Seroepidemiologic Studies","10.1002/14651858.CD013652.pub2","http://dx.doi.org/10.1002/14651858.CD013652.pub2","Infectious Diseases"
"CD013359.PUB3","Kay, AW; Ness, T; Verkuijl, SE; Viney, K; Brands, A; Masini, T; González Fernández, L; Eisenhut, M; Detjen, AK; Mandalakas, AM; Steingart, KR; Takwoingi, Y","Xpert MTB/RIF Ultra assay for tuberculosis disease and rifampicin resistance in children","Cochrane Database of Systematic Reviews","2022","Abstract - Background Every year, an estimated one million children and young adolescents become ill with tuberculosis, and around 226,000 of those children die. Xpert MTB/RIF Ultra (Xpert Ultra) is a molecular World Health Organization (WHO)‐recommended rapid diagnostic test that simultaneously detects  Mycobacterium tuberculosis  complex and rifampicin resistance. We previously published a Cochrane Review 'Xpert MTB/RIF and Xpert MTB/RIF Ultra assays for tuberculosis disease and rifampicin resistance in children'. The current review updates evidence on the diagnostic accuracy of Xpert Ultra in children presumed to have tuberculosis disease. Parts of this review update informed the 2022 WHO updated guidance on management of tuberculosis in children and adolescents. Objectives To assess the diagnostic accuracy of Xpert Ultra for detecting: pulmonary tuberculosis, tuberculous meningitis, lymph node tuberculosis, and rifampicin resistance, in children with presumed tuberculosis. Secondary objectives To investigate potential sources of heterogeneity in accuracy estimates. For detection of tuberculosis, we considered age, comorbidity (HIV, severe pneumonia, and severe malnutrition), and specimen type as potential sources. To summarize the frequency of Xpert Ultra trace results. Search methods We searched the Cochrane Infectious Diseases Group Specialized Register, MEDLINE, Embase, three other databases, and three trial registers without language restrictions to 9 March 2021. Selection criteria Cross‐sectional and cohort studies and randomized trials that evaluated Xpert Ultra in HIV‐positive and HIV‐negative children under 15 years of age. We included ongoing studies that helped us address the review objectives. We included studies evaluating sputum, gastric, stool, or nasopharyngeal specimens (pulmonary tuberculosis), cerebrospinal fluid (tuberculous meningitis), and fine needle aspirate or surgical biopsy tissue (lymph node tuberculosis). For detecting tuberculosis, reference standards were microbiological (culture) or composite reference standard; for stool, we also included Xpert Ultra performed on a routine respiratory specimen. For detecting rifampicin resistance, reference standards were drug susceptibility testing or MTBDR plus . Data collection and analysis Two review authors independently extracted data and, using QUADAS‐2, assessed methodological quality judging risk of bias separately for each target condition and reference standard. For each target condition, we used the bivariate model to estimate summary sensitivity and specificity with 95% confidence intervals (CIs). We stratified all analyses by type of reference standard. We summarized the frequency of Xpert Ultra trace results; trace represents detection of a very low quantity of  Mycobacterium tuberculosis  DNA. We assessed certainty of evidence using GRADE. Main results We identified 14 studies (11 new studies since the previous review). For detection of pulmonary tuberculosis, 335 data sets (25,937 participants) were available for analysis. We did not identify any studies that evaluated Xpert Ultra accuracy for tuberculous meningitis or lymph node tuberculosis. Three studies evaluated Xpert Ultra for detection of rifampicin resistance. Ten studies (71%) took place in countries with a high tuberculosis burden based on WHO classification. Overall, risk of bias was low. Detection of pulmonary tuberculosis Sputum, 5 studies Xpert Ultra summary sensitivity verified by culture was 75.3% (95% CI 64.3 to 83.8; 127 participants; high‐certainty evidence), and specificity was 97.1% (95% CI 94.7 to 98.5; 1054 participants; high‐certainty evidence). Gastric aspirate, 7 studies Xpert Ultra summary sensitivity verified by culture was 70.4% (95% CI 53.9 to 82.9; 120 participants; moderate‐certainty evidence), and specificity was 94.1% (95% CI 84.8 to 97.8; 870 participants; moderate‐certainty evidence). Stool, 6 studies Xpert Ultra summary sensitivity verified by culture was 56.1% (95% CI 39.1 to 71.7; 200 participants; moderate‐certainty evidence), and specificity was 98.0% (95% CI 93.3 to 99.4; 1232 participants; high certainty‐evidence). Nasopharyngeal aspirate, 4 studies Xpert Ultra summary sensitivity verified by culture was 43.7% (95% CI 26.7 to 62.2; 46 participants; very low‐certainty evidence), and specificity was 97.5% (95% CI 93.6 to 99.0; 489 participants; high‐certainty evidence). Xpert Ultra sensitivity was lower against a composite than a culture reference standard for all specimen types other than nasopharyngeal aspirate, while specificity was similar against both reference standards. Interpretation of results In theory, for a population of 1000 children: • where 100 have pulmonary tuberculosis in sputum (by culture): ‐ 101 would be Xpert Ultra‐positive, and of these, 26 (26%) would not have pulmonary tuberculosis (false positive); and ‐ 899 would be Xpert Ultra‐negative, and of these, 25 (3%) would have tuberculosis (false negative). • where 100 have pulmonary tuberculosis in gastric aspirate (by culture): ‐ 123 would be Xpert Ultra‐positive, and of these, 53 (43%) would not have pulmonary tuberculosis (false positive); and ‐ 877 would be Xpert Ultra‐negative, and of these, 30 (3%) would have tuberculosis (false negative). • where 100 have pulmonary tuberculosis in stool (by culture): ‐ 74 would be Xpert Ultra‐positive, and of these, 18 (24%) would not have pulmonary tuberculosis (false positive); and ‐ 926 would be Xpert Ultra‐negative, and of these, 44 (5%) would have tuberculosis (false negative). • where 100 have pulmonary tuberculosis in nasopharyngeal aspirate (by culture): ‐ 66 would be Xpert Ultra‐positive, and of these, 22 (33%) would not have pulmonary tuberculosis (false positive); and ‐ 934 would be Xpert Ultra‐negative, and of these, 56 (6%) would have tuberculosis (false negative). Detection of rifampicin resistance Xpert Ultra sensitivity was 100% (3 studies, 3 participants; very low‐certainty evidence), and specificity range was 97% to 100% (3 studies, 128 participants; low‐certainty evidence). Trace results Xpert Ultra trace results, regarded as positive in children by WHO standards, were common. Xpert Ultra specificity remained high in children, despite the frequency of trace results. Authors' conclusions We found Xpert Ultra sensitivity to vary by specimen type, with sputum having the highest sensitivity, followed by gastric aspirate and stool. Nasopharyngeal aspirate had the lowest sensitivity. Xpert Ultra specificity was high against both microbiological and composite reference standards. However, the evidence base is still limited, and findings may be imprecise and vary by study setting. Although we found Xpert Ultra accurate for detection of rifampicin resistance, results were based on a very small number of studies that included only three children with rifampicin resistance. Therefore, findings should be interpreted with caution. Our findings provide support for the use of Xpert Ultra as an initial rapid molecular diagnostic in children being evaluated for tuberculosis. Plain language summary Xpert Ultra for diagnosing tuberculosis and rifampicin resistance in children Why is improving the diagnosis of tuberculosis important? Every year, an estimated one million children and young adolescents become ill with tuberculosis, and around 226,000 die from the disease. Tuberculosis is caused by the bacterium  Mycobacterium tuberculosis  and mostly affects the lungs (pulmonary tuberculosis), though it can affect other sites in the body (extrapulmonary tuberculosis). Signs and symptoms of pulmonary tuberculosis include cough, fever, night sweats, and weight loss. Signs and symptoms of extrapulmonary tuberculosis depend on the site of disease. When detected early and treated effectively, tuberculosis is largely curable. Not recognizing tuberculosis (false negative) early may result in delayed diagnosis and treatment, severe illness, and death. An incorrect tuberculosis diagnosis (false positive) may result in anxiety, unnecessary treatment (which can involve medication side effects), and the possibility of missing alternative diagnoses which warrant treatment. What was the aim of this review? To determine the accuracy of Xpert Ultra in children with symptoms of tuberculosis for diagnosing pulmonary tuberculosis, tuberculous meningitis (affecting membranes that surround the brain and spinal cord), lymph node tuberculosis (a painful swelling of one or more lymph nodes, which are bean‐shaped structures that help fight infection), and rifampicin resistance. What did this review study? Xpert Ultra, a World Health Organization‐recommended rapid test that simultaneously detects tuberculosis and rifampicin resistance in adults and children with tuberculosis symptoms. Rifampicin is an important medicine used to treat tuberculosis. For tuberculosis diagnosis, we assessed results against two different benchmarks: tuberculosis culture (a method used to grow bacteria on nutrient‐rich media) and a composite definition based on symptoms, chest X‐ray, sputum microscopy (examination under a microscope of mucus and other matter coughed up from the lungs), and culture. For rifampicin resistance detection, we assessed results against drug susceptibility testing or line probe assay (a rapid laboratory‐based test for detecting tuberculosis bacteria). What were the main results in this review? We included 14 studies. For pulmonary tuberculosis, we analysed 335 data sets (around 26,000 participants). No studies evaluated Xpert Ultra accuracy for tuberculous meningitis or lymph node tuberculosis. Three studies evaluated Xpert Ultra accuracy for detection of rifampicin resistance. For a population of 1000 children: • where 100 have pulmonary tuberculosis in sputum according to culture results: ‐ 101 would be Xpert Ultra‐positive, and of these, 26 (26%) would not have pulmonary tuberculosis (false positive); and ‐ 899 would be Xpert Ultra‐negative, and of these, 25 (3%) would have tuberculosis (false negative). • where 100 have pulmonary tuberculosis in gastric aspirate (collection of lung and oral secretions from the stomach) according to culture results: ‐ 97 would be Xpert Ultra‐positive, and of these, 27 (28%) would not have pulmonary tuberculosis (false positive); and ‐ 903 would be Xpert Ultra‐negative, and of these, 30 (3%) would have tuberculosis (false negative). • where 100 have pulmonary tuberculosis in stool according to culture results: ‐ 74 would be Xpert Ultra‐positive and of these, 18 (24%) would not have pulmonary tuberculosis (false positive); and ‐ 926 would be Xpert Ultra‐negative, and of these, 44 (5%) would have tuberculosis (false negative). • where 100 have pulmonary tuberculosis in nasopharyngeal aspirate (secretions from the uppermost part of the throat, behind the nose) according to culture results: ‐ 66 would be Xpert Ultra‐positive, and of these, 22 (33%) would not have pulmonary tuberculosis (false positive); and ‐ 934 would be Xpert Ultra‐negative, and of these, 56 (6%) would have tuberculosis (false negative). Xpert Ultra accurately detected rifampicin resistance, but there were few studies and only three children with rifampicin resistance included. How confident are we in the results of this review? For pulmonary tuberculosis, we are fairly confident because we included studies from different countries and used two different benchmarks, though neither is perfect. However, the evidence base is still limited and there were few studies with few children for one of the specimen types (nasopharyngeal aspirate). For rifampicin resistance, we identified few studies with very few children with rifampicin resistance, so we are less confident. What children do the results of this review apply to? Children and young adolescents (birth to 14 years) who are HIV‐positive or HIV‐negative, with signs or symptoms of pulmonary tuberculosis. The results also apply to children with severe pneumonia or malnutrition and tuberculosis symptoms. In this review, we did not identify any studies that evaluated Xpert Ultra accuracy for tuberculous meningitis or lymph node tuberculosis. What are the implications of this review? The results suggest that Xpert Ultra in sputum, gastric aspirate, stool, and nasopharyngeal aspirate is an accurate method for detecting pulmonary tuberculosis and rifampicin resistance in children. Using Xpert Ultra in sputum, gastric aspirate, stool, and nasopharyngeal aspirate, the risk of missing a diagnosis of pulmonary tuberculosis (confirmed by culture) is low, suggesting that only a small number of children will not receive treatment. The risk of incorrectly diagnosing a child as having pulmonary tuberculosis is slightly higher. This may result in some children receiving unnecessary treatment. How up to date is this review? This review updates our previous review and includes evidence published up to 9 March 2021.","9","John Wiley & Sons, Ltd","1465-1858","*Antibiotics, Antitubercular [therapeutic use]; *HIV Infections [drug therapy]; *Mycobacterium tuberculosis [genetics]; *Tuberculosis, Lymph Node [diagnosis, drug therapy]; *Tuberculosis, Meningeal [cerebrospinal fluid, diagnosis, drug therapy]; *Tuberculosis, Pulmonary [diagnosis, drug therapy, microbiology]; Adolescent; Child; Cross-Sectional Studies; Humans; Microbial Sensitivity Tests; Rifampin [pharmacology]; Sensitivity and Specificity; Sputum [microbiology]","10.1002/14651858.CD013359.pub3","http://dx.doi.org/10.1002/14651858.CD013359.pub3","Infectious Diseases"
"CD013483.PUB2","Fraquelli, M; Nadarevic, T; Colli, A; Manzotti, C; Giljaca, V; Miletic, D; Štimac, D; Casazza, G","Contrast‐enhanced ultrasound for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease","Cochrane Database of Systematic Reviews","2022","Abstract - Background Hepatocellular carcinoma occurs mostly in people with chronic liver disease. Worldwide, it ranks sixth in terms of incidence of cancer, and fourth in terms of cancer‐related deaths. Contrast‐enhanced ultrasound (CEUS) is used as an add‐on test to confirm the presence of focal liver lesions suspected as hepatocellular carcinoma after prior diagnostic tests such as abdominal ultrasound or measurement of alpha‐foetoprotein, or both. According to guidelines, a single contrast‐enhanced imaging investigation, with either computed tomography (CT) or magnetic resonance imaging (MRI), may show the typical hepatocellular carcinoma hallmarks in people with cirrhosis, which will be sufficient to diagnose hepatocellular carcinoma. However, a significant number of hepatocellular carcinomas show atypical imaging features, and therefore, are missed at imaging. Dynamic CEUS images are obtained similarly to CT and MRI images. CEUS differentiates between arterial and portal venous phases, in which sonographic hepatocellular carcinoma hallmarks, such as arterial hyperenhancement and subsequent washout appearance, are investigated. The advantages of CEUS over CT and MRI include real‐time imaging, use of contrast agents that do not contain iodine and are not nephrotoxic, and quick image acquisition. Despite the advantages, the use of CEUS in the diagnostic algorithm for HCC remains controversial, with disagreement on relevant guidelines. There is no clear evidence of the benefit of surveillance programmes in terms of overall survival as the conflicting results can be a consequence of an inaccurate detection, ineffective treatment, or both. Therefore, assessing the diagnostic accuracy of CEUS may clarify whether the absence of benefit could be related to underdiagnosis. Furthermore, an assessment of the accuracy of CEUS for the diagnosis of hepatocellular carcinoma is needed for either diagnosing hepatocellular carcinoma or ruling it out in people with chronic liver disease who are not included in surveillance programmes. Objectives 1. To assess the diagnostic accuracy of contrast‐enhanced ultrasound (CEUS) for the diagnosis of hepatocellular carcinoma of any size and at any stage in adults with chronic liver disease, in a surveillance programme or in a clinical setting. 2. To assess the diagnostic accuracy of CEUS for the diagnosis of resectable hepatocellular carcinoma in people with chronic liver disease and identify potential sources of heterogeneity in the results. Search methods We used standard, extensive Cochrane search methods. The last date of search was 5 November 2021. Selection criteria We included studies assessing the diagnostic accuracy of CEUS for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease, with cross‐sectional designs, using one of the acceptable reference standards, such as pathology of the explanted liver, and histology of resected or biopsied focal liver lesion with at least a six‐month follow‐up. Data collection and analysis We used standard Cochrane methods to screen studies, extract data, and assess the risk of bias and applicability concerns, using the QUADAS‐2 checklist. We used the bivariate model and provided estimates of summary sensitivity and specificity. We assessed the certainty of the evidence using GRADE. We presented uncertainty‐of‐the‐accuracy estimates using 95% confidence intervals (CIs). Main results We included 23 studies with 6546 participants. Studies were published between 2001 and 2021. We judged all 23 studies at high‐risk of bias in at least one domain, and 13/23 studies at high concern for applicability. Most studies used different reference standards to exclude the presence of the target condition. The time interval between the index test and the reference standard was rarely defined. We also had major concerns on their applicability due to the characteristics of the participants. – CEUS for hepatocellular carcinoma of any size and stage: sensitivity 77.8% (95% CI 69.4% to 84.4%) and specificity 93.8% (95% CI 89.1% to 96.6%) (23 studies, 6546 participants; very low‐certainty evidence). – CEUS for resectable hepatocellular carcinoma: sensitivity 77.5% (95% CI 62.9% to 87.6%) and specificity 92.7% (95% CI 86.8% to 96.1%) (13 studies, 1257 participants; low‐certainty evidence). The observed heterogeneity in the results remains unexplained. The sensitivity analyses, including only studies with clearly prespecified positivity criteria and only studies in which the reference standard results were interpreted with no knowledge of the results about the index test, showed no differences in the results. Authors' conclusions We found that by using CEUS, as an add‐on test following abdominal ultrasound, to diagnose hepatocellular carcinoma of any size and stage, 22% of people with hepatocellular carcinoma would be missed, and 6% of people without hepatocellular carcinoma would unnecessarily undergo further testing or inappropriate treatment. As to resectable hepatocellular carcinoma, we found that 23% of people with resectable hepatocellular carcinoma would incorrectly be unresected, while 8% of people without hepatocellular carcinoma would undergo further inappropriate testing or treatment. The uncertainty resulting from the high risk of bias of the included studies, heterogeneity, and imprecision of the results and concerns on their applicability limit our ability to draw confident conclusions. Plain language summary How accurate are contrast‐enhanced ultrasound scans for detecting hepatocellular carcinoma? Key messages In adults with chronic liver disease, contrast‐enhanced ultrasound (CEUS) can miss diagnosing hepatocellular carcinoma in around 22.2% of people who would not then receive timely or appropriate treatment, and would fail to diagnose hepatocellular carcinoma in 6.2% of people who could receive unnecessary further testing or treatment. In the subset of people who are able have the hepatocellular carcinoma removed by surgery, CEUS probably misses hepatocellular carcinoma in 22.5% of people who could undergo surgery to remove part of their liver while it would erroneously find cancer in 7.3% of people who would undergo unnecessary further test or surgery. As there were some problems with the way the studies were conducted, CEUS to detect hepatocellular carcinoma may appear more accurate than it actually is. Why is improving the diagnosis of hepatocellular carcinoma important? Hepatocellular carcinoma is cancer originating in the liver. It is sixth in terms of occurrences of cancer and fourth in terms of cancer‐related deaths worldwide. It occurs mostly in people with chronic liver disease regardless of the exact cause. People with blood test or ultrasound results that suggest they may have hepatocellular carcinoma may go on to have further tests, such as imaging or a biopsy (where a small piece of the liver is removed and examined). If the cancer is detected early, people may have part of the liver removed or have a liver transplant. In advanced hepatocellular carcinoma, they may need chemotherapy. If hepatocellular carcinoma is missed at diagnostic testing, people will not receive appropriate treatment. However, incorrectly diagnosing hepatocellular carcinoma when it is not present means that people may undergo unnecessary testing or treatment. What is contrast‐enhanced ultrasound and how can it diagnose hepatocellular carcinoma? CEUS can detect abnormalities in the liver that might be due to cancer and, using contrast agents, confirm the diagnosis of hepatocellular carcinoma. These contrast agents are safe. CEUS is used in clinical practice to confirm the presence of hepatocellular carcinoma in people in whom suspicion was raised by prior performed abdominal ultrasound or a blood test to measure alpha‐foetoprotein. The role of CEUS in the diagnosis of hepatocellular carcinoma remains controversial between guidelines. Previous systematic reviews have assessed the performance of CEUS in detecting hepatocellular carcinoma but they have included different studies and found different results. What did we want to find out? We wanted to find out if CEUS is accurate enough to diagnose hepatocellular carcinoma in adults with chronic liver disease (a progressive deterioration of liver functions for more than six months). We were interested first in hepatocellular carcinoma of any size and severity and, second, in hepatocellular carcinomas that were suitable for surgical removal (resection). What did we do? We searched for studies that assessed the accuracy of diagnostic tests of CEUS scans compared to the best available tests to confirm hepatocellular carcinoma in adults with chronic liver disease. The best available test is examination of the liver, or part of it, under a microscope. What did we find? We found 23 studies with 6546 adults. Around 690 (69%) out of 1000 adults with chronic liver disease had a confirmed hepatocellular carcinoma of any size and severity. Considering these 1000 people, CEUS: – correctly detected liver cancer in 537 people; – missed liver cancer in 153 people; – incorrectly detected cancer in 19 people; – correctly detected no cancer in 291 people. Around 690 (69%) out of 1000 adults with chronic liver disease had a confirmed hepatocellular carcinoma that could be removed by surgery. Considering these 1000 people, CEUS: – correctly detected liver cancer in 535 people; – missed liver cancer in 155 people; – incorrectly detected cancer in 23 people; – correctly detected no cancer in 287 people. What are the limitations of the evidence ? Our confidence in the evidence is limited as the studies used different methods to select study participants and used different reference standards. This means that CEUS scans may be more or less accurate than what the evidence suggests. How up to date is this evidence? The evidence is up to date to 5 November 2021.","9","John Wiley & Sons, Ltd","1465-1858","*Carcinoma, Hepatocellular [diagnostic imaging]; *Liver Neoplasms [diagnostic imaging]; Adult; Cross-Sectional Studies; Humans; Sensitivity and Specificity; Tomography, X-Ray Computed; Ultrasonography","10.1002/14651858.CD013483.pub2","http://dx.doi.org/10.1002/14651858.CD013483.pub2","Hepato-Biliary"
"CD011964.PUB2","Davenport, C; Rai, N; Sharma, P; Deeks, JJ; Berhane, S; Mallett, S; Saha, P; Champaneria, R; Bayliss, SE; Snell, KIE; Sundar, S","Menopausal status, ultrasound and biomarker tests in combination for the diagnosis of ovarian cancer in symptomatic women","Cochrane Database of Systematic Reviews","2022","Abstract - Background Ovarian cancer (OC) has the highest case fatality rate of all gynaecological cancers. Diagnostic delays are caused by non‐specific symptoms. Existing systematic reviews have not comprehensively covered tests in current practice, not estimated accuracy separately in pre‐ and postmenopausal women, or used inappropriate meta‐analytic methods. Objectives To establish the accuracy of combinations of menopausal status, ultrasound scan (USS) and biomarkers for the diagnosis of ovarian cancer in pre‐ and postmenopausal women and compare the accuracy of different test combinations. Search methods We searched CENTRAL, MEDLINE (Ovid), Embase (Ovid), five other databases and three trial registries from 1991 to 2015 and MEDLINE (Ovid) and Embase (Ovid) from June 2015 to June 2019. We also searched conference proceedings from the European Society of Gynaecological Oncology, International Gynecologic Cancer Society, American Society of Clinical Oncology and Society of Gynecologic Oncology, ZETOC and Conference Proceedings Citation Index (Web of Knowledge). We searched reference lists of included studies and published systematic reviews. Selection criteria We included cross‐sectional diagnostic test accuracy studies evaluating single tests or comparing two or more tests, randomised trials comparing two or more tests, and studies validating multivariable models for the diagnosis of OC investigating test combinations, compared with a reference standard of histological confirmation or clinical follow‐up in women with a pelvic mass (detected clinically or through USS) suspicious for OC. Data collection and analysis Two review authors independently extracted data and assessed quality using QUADAS‐2. We used the bivariate hierarchical model to indirectly compare tests at commonly reported thresholds in pre‐ and postmenopausal women separately. We indirectly compared tests across all thresholds and estimated sensitivity at fixed specificities of 80% and 90% by fitting hierarchical summary receiver operating characteristic (HSROC) models in pre‐ and postmenopausal women separately. Main results We included 59 studies (32,059 women, 9545 cases of OC). Five studies evaluated the accuracy of a combination of menopausal status and USS findings (IOTA Logistic Regression Model 2 (LR2), four studies evaluated the Assessment of Different NEoplasias in the adneXa model (ADNEX)); 19 studies evaluated the accuracy of a combination of menopausal status, USS findings and serum biomarker CA125 (Risk of Malignancy Index (RMI)); and 42 studies evaluated the accuracy of a combination of menopausal status and two serum biomarkers (CA125 and HE4) (Risk of Ovarian Malignancy Algorithm (ROMA)). Most studies were at high or unclear risk of bias in participant, reference standard, and flow and timing domains. All studies were in hospital settings. Mean prevalence was 16% (RMI, ROMA), 22% (LR2) and 27% (ADNEX) in premenopausal women and 38% (RMI), 45% (ROMA), 52% (LR2) and 55% (ADNEX) in postmenopausal women. The prevalence of OC in the studies was considerably higher than would be expected in symptomatic women presenting in community‐based settings, or in women referred from the community to hospital with a suspicion of OC. Studies were at high or unclear applicability because presenting features were not reported, or USS was performed by experienced ultrasonographers for RMI, LR2 and ADNEX. The higher sensitivity and lower specificity observed in postmenopausal compared to premenopausal women across all index tests and at all thresholds may reflect highly selected patient cohorts in the included studies. In premenopausal women, ROMA at a threshold of 13.1 (± 2), LR2 at a threshold to achieve a post‐test probability of OC of 10% and ADNEX (post‐test probability 10%) demonstrated a higher sensitivity (ROMA: 77.4%, 95% CI 72.7% to 81.5%; LR2: 83.3%, 95% CI 74.7% to 89.5%; ADNEX: 95.5%, 95% CI 91.0% to 97.8%) compared to RMI (57.2%, 95% CI 50.3% to 63.8%). The specificity of ROMA and ADNEX were lower in premenopausal women (ROMA: 84.3%, 95% CI 81.2% to 87.0%; ADNEX: 77.8%, 95% CI 67.4% to 85.5%) compared to RMI 92.5% (95% CI 90.3% to 94.2%). The specificity of LR2 was comparable to RMI (90.4%, 95% CI 84.6% to 94.1%). In postmenopausal women, ROMA at a threshold of 27.7 (± 2), LR2 (post‐test probability 10%) and ADNEX (post‐test probability 10%) demonstrated a higher sensitivity (ROMA: 90.3%, 95% CI 87.5% to 92.6%; LR2: 94.8%, 95% CI 92.3% to 96.6%; ADNEX: 97.6%, 95% CI 95.6% to 98.7%) compared to RMI (78.4%, 95% CI 74.6% to 81.7%). Specificity of ROMA at a threshold of 27.7 (± 2) (81.5, 95% CI 76.5% to 85.5%) was comparable to RMI (85.4%, 95% CI 82.0% to 88.2%), whereas for LR2 (post‐test probability 10%) and ADNEX (post‐test probability 10%) specificity was lower (LR2: 60.6%, 95% CI 50.5% to 69.9%; ADNEX: 55.0%, 95% CI 42.8% to 66.6%). Authors' conclusions In specialist healthcare settings in both premenopausal and postmenopausal women, RMI has poor sensitivity. In premenopausal women, ROMA, LR2 and ADNEX offer better sensitivity (fewer missed cancers), but for ROMA and ADNEX this is off‐set by a decrease in specificity and increase in false positives. In postmenopausal women, ROMA demonstrates a higher sensitivity and comparable specificity to RMI. ADNEX has the highest sensitivity in postmenopausal women, but reduced specificity. The prevalence of OC in included studies is representative of a highly selected referred population, rather than a population in whom referral is being considered. The comparative accuracy of tests observed here may not be transferable to non‐specialist settings. Ultimately health systems need to balance accuracy and resource implications to identify the most suitable test. Plain language summary What is the accuracy of different combinations of ultrasound imaging and blood tests to diagnose ovarian cancer in women before and after the menopause? Why is improving the diagnosis of ovarian cancer important? Many women diagnosed with ovarian cancer (OC) die from the disease, because it has usually spread outside the tubes/ovaries at the time of diagnosis. Missing OC (a false‐negative result) may need major surgery and a lower chance of survival. An incorrect diagnosis of OC (a false‐positive result) may result in anxiety, unnecessary further tests and surgery. What did we aim to do? We aimed to find out how accurate ultrasounds and blood tests are for diagnosing OC in premenopausal women and postmenopausal women. What did we study? We included 59 studies that compared four tests: Risk of Malignancy Index (RMI) (ultrasound and CA125 blood test); Risk of Ovarian Malignancy Algorithm (ROMA) (CA125 and HE4 blood tests); the IOTA Logistic Regression model 2 (LR2) ultrasound and the Assessment of Different NEoplasias in the adneXa model (ADNEX) (CA125 blood test and ultrasound). What were the main results? Premenopausal women The sensitivities (proportion of women  with  OC correctly identified) of ROMA (77.4%), LR2 (83.3%) and ADNEX (95.5%) are higher than RMI (57.2%). The specificities (proportion of women  without  OC correctly identified) of ROMA (84.3%) and ADNEX (77.8%) were lower than RMI (92.5%) and LR2 (90.4%). The results indicate that if these tests were to be used in hospital settings in a group of 1000 premenopausal women, of whom 30 (3%) actually have OC: – for RMI 13 women, for ROMA 7 women, for LR2 5 women and for ADNEX 1 woman would have their cancer missed by the test (false‐negative result); – for RMI 73 women, for ROMA 152 women, for LR2 93 women and for ADNEX 215 women would test positive when they do not have OC (false‐positive result). Postmenopausal women The sensitivities of ROMA (90.3%), LR2 (94.8%) and ADNEX (97.6%) are higher than RMI (78.4%). The specificities of ROMA (81.5%) and RMI (85.4%) are higher than LR2 (60.6%) and ADNEX (55.0%). The results of these studies indicate that if these tests were to be used in hospital settings in a group of 1000 postmenopausal women, of whom 30 (3%) actually have OC: – for RMI 6 women, for ROMA 3 women, for LR2 2 women and for ADNEX 1 woman would have their cancer missed by the test (false‐negative result); – for RMI 142 women, for ROMA 179 women, for LR2 382 women and for ADNEX 437 women would test positive when they do not have OC (false‐positive result). How reliable are the results? OC was diagnosed by histology (looking at surgically removed specimens under a microscope) or following up women for one year to see if they remained free of OC. In some studies, women with negative test results were not followed up for long enough to be sure a cancer had not been missed, and some studies excluded women with types of OC that are harder to diagnose. This may make tests appear more accurate than they are in practice. Who do the results apply to? Most studies were conducted in European hospitals in women with a confirmed pelvic mass. The occurrence of OC in included studies was much higher than seen in the community and so the accuracy of these tests may be different for women being tested in non‐specialist healthcare settings. What are the implications? This review suggests that in both pre‐ and postmenopausal women referred to hospital with a pelvic mass, ADNEX appears to miss the fewest cases of OC and RMI misses the most cases of OC. RMI appears to result in the fewest incorrect diagnoses of OC and ADNEX results in the most incorrect diagnoses of OC. Incorrect diagnoses of OC, when no cancer is present (false‐positive test), may result in anxiety, unnecessary further tests and surgery. When choosing which test to use, the potential for missed cancers must be balanced against unnecessary testing and surgery. How up‐to‐date is this review? The review includes studies published up to June 2019.","7","John Wiley & Sons, Ltd","1465-1858","*Ovarian Neoplasms [diagnostic imaging]; Biomarkers; Carcinoma, Ovarian Epithelial; Cross-Sectional Studies; Female; Humans; Menopause; Sensitivity and Specificity","10.1002/14651858.CD011964.pub2","http://dx.doi.org/10.1002/14651858.CD011964.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD013080.PUB2","Smith, R; Villanueva, G; Probyn, K; Sguassero, Y; Ford, N; Orrell, C; Cohen, K; Chaplin, M; Leeflang, MMG; Hine, P","Accuracy of measures for antiretroviral adherence in people living with HIV","Cochrane Database of Systematic Reviews","2022","Abstract - Background Good patient adherence to antiretroviral (ART) medication determines effective HIV viral suppression, and thus reduces the risk of progression and transmission of HIV. With accurate methods to monitor treatment adherence, we could use simple triage to target adherence support interventions that could help in the community or at health centres in resource‐limited settings. Objectives To determine the accuracy of simple measures of ART adherence (including patient self‐report, tablet counts, pharmacy records, electronic monitoring, or composite methods) for detecting non‐suppressed viral load in people living with HIV and receiving ART treatment. Search methods The Cochrane Infectious Diseases Group Information Specialists searched CENTRAL, MEDLINE, Embase, LILACS, CINAHL, African‐Wide information, and Web of Science up to 22 April 2021. They also searched the World Health Organization (WHO) International Clinical Trials Registry Platform (ICTRP) and ClinicalTrials.gov for ongoing studies. No restrictions were placed on the language or date of publication when searching the electronic databases. Selection criteria We included studies of all designs that evaluated a simple measure of adherence (index test) such as self‐report, tablet counts, pharmacy records or secondary database analysis, or both, electronic monitoring or composite measures of any of those tests, in people living with HIV and receiving ART treatment. We used a viral load assay with a limit of detection ranging from 10 copies/mL to 400 copies/mL as the reference standard. We created 2 × 2 tables to calculate sensitivity and specificity. Data collection and analysis We screened studies, extracted data, and assessed risk of bias using QUADAS‐2 independently and in duplicate. We assessed the certainty of evidence using the GRADE method. The results of estimated sensitivity and specificity were presented using paired forest plots and tabulated summaries. We encountered a high level of variation among studies which precluded a meaningful meta‐analysis or comparison of adherence measures. We explored heterogeneity using pre‐defined subgroup analysis. Main results We included 51 studies involving children and adults with HIV, mostly living in low‐ and middle‐income settings, conducted between 2003 and 2021. Several studies assessed more than one index test, and the most common measure of adherence to ART was self‐report. ‐  Self‐report questionnaires  (25 studies, 9211 participants; very low‐certainty): sensitivity ranged from 10% to 85% and specificity ranged from 10% to 99%. ‐  Self‐report using a visual analogue scale (VAS)  (11 studies, 4235 participants; very low‐certainty): sensitivity ranged from 0% to 58% and specificity ranged from 55% to 100%. ‐  Tablet counts  (12 studies, 3466 participants; very low‐certainty): sensitivity ranged from 0% to 100% and specificity ranged from 5% to 99%. ‐  Electronic monitoring devices  (3 studies, 186 participants; very low‐certainty): sensitivity ranged from 60% to 88% and the specificity ranged from 27% to 67%. ‐  Pharmacy records or secondary databases  (6 studies, 2254 participants; very low‐certainty): sensitivity ranged from 17% to 88% and the specificity ranged from 9% to 95%. ‐  Composite measures  (9 studies, 1513 participants; very low‐certainty): sensitivity ranged from 10% to 100% and specificity ranged from 49% to 100%. Across all included studies, the ability of adherence measures to detect viral non‐suppression showed a large variation in both sensitivity and specificity that could not be explained by subgroup analysis. We assessed the overall certainty of the evidence as very low due to risk of bias, indirectness, inconsistency, and imprecision. The risk of bias and the applicability concerns for patient selection, index test, and reference standard domains were generally low or unclear due to unclear reporting. The main methodological issues identified were related to flow and timing due to high numbers of missing data. For all index tests, we assessed the certainty of the evidence as very low due to limitations in the design and conduct of the studies, applicability concerns and inconsistency of results. Authors' conclusions We encountered high variability for all index tests, and the overall certainty of evidence in all areas was very low. No measure consistently offered either a sufficiently high sensitivity or specificity to detect viral non‐suppression. These concerns limit their value in triaging patients for viral load monitoring or enhanced adherence support interventions. Plain language summary Are there good ways to find out if people living with HIV are taking their medicines every day? The issue For people with HIV, taking their HIV medicines every day (adherence), is vital to keep HIV under control. The best way to measure peoples’ adherence to HIV medicines is with ‘viral load testing’, which tells us how much virus there is in the blood. Viral load testing is not available everywhere, such as in places where there is lack of funds. If we could measure adherence with a more readily available measure, this might help detect people who need more help with taking their medicines. Aim of this review To find out if simple measures of adherence can tell us whether people might not be taking their medication every day and might then have higher (detectable) viral loads. These people might be helped by extra viral load monitoring. This could then prevent them developing complications from HIV or passing HIV to other people. What we found We looked at 51 studies involving children and adults with HIV that happened between 2003 and 2021. These studies tested different ways to measure adherence, including surveys or rating scales filled out by patients, counting of patients’ pills, pharmacy notes, or gadgets. All the measures we looked at did not help find patients who might not be taking their medications and who had higher viral loads. Different studies showed very different results. We could not explain these differences by whether the studies included children or adults, whether they were in richer or poorer areas, or what cut off they used to say if the viral load was high. This also meant that we could not combine the studies. What are the implications of this review? Based on the results, it is uncertain that simple measures of adherence to ART treatment can help find people living with HIV who may have a higher viral load. Still, there may be other values to trying to measure adherence that this review cannot show. Reporting how current the evidence is The evidence is up‐to‐date to 22 April 2021.","7","John Wiley & Sons, Ltd","1465-1858","*Anti-Retroviral Agents [therapeutic use]; *HIV Infections [complications, drug therapy]; Adult; Child; Humans; Reference Standards; Sensitivity and Specificity; Viral Load","10.1002/14651858.CD013080.pub2","http://dx.doi.org/10.1002/14651858.CD013080.pub2","Infectious Diseases"
"CD013705.PUB3","Dinnes, J; Sharma, P; Berhane, S; van Wyk, SS; Nyaaba, N; Domen, J; Taylor, M; Cunningham, J; Davenport, C; Dittrich, S; Emperador, D; Hooft, L; Leeflang, MMG; McInnes, MDF; Spijker, R; Verbakel, JY; Takwoingi, Y; Taylor-Phillips, S; Van den Bruel, A; Deeks, JJ","Rapid, point‐of‐care antigen tests for diagnosis of SARS‐CoV‐2 infection","Cochrane Database of Systematic Reviews","2022","Abstract - Background Accurate rapid diagnostic tests for SARS‐CoV‐2 infection would be a useful tool to help manage the COVID‐19 pandemic. Testing strategies that use rapid antigen tests to detect current infection have the potential to increase access to testing, speed detection of infection, and inform clinical and public health management decisions to reduce transmission. This is the second update of this review, which was first published in 2020. Objectives To assess the diagnostic accuracy of rapid, point‐of‐care antigen tests for diagnosis of SARS‐CoV‐2 infection. We consider accuracy separately in symptomatic and asymptomatic population groups. Sources of heterogeneity investigated included setting and indication for testing, assay format, sample site, viral load, age, timing of test, and study design. Search methods We searched the COVID‐19 Open Access Project living evidence database from the University of Bern (which includes daily updates from PubMed and Embase and preprints from medRxiv and bioRxiv) on 08 March 2021. We included independent evaluations from national reference laboratories, FIND and the Diagnostics Global Health website. We did not apply language restrictions. Selection criteria We included studies of people with either suspected SARS‐CoV‐2 infection, known SARS‐CoV‐2 infection or known absence of infection, or those who were being screened for infection. We included test accuracy studies of any design that evaluated commercially produced, rapid antigen tests. We included evaluations of single applications of a test (one test result reported per person) and evaluations of serial testing (repeated antigen testing over time). Reference standards for presence or absence of infection were any laboratory‐based molecular test (primarily reverse transcription polymerase chain reaction (RT‐PCR)) or pre‐pandemic respiratory sample. Data collection and analysis We used standard screening procedures with three people. Two people independently carried out quality assessment (using the QUADAS‐2 tool) and extracted study results. Other study characteristics were extracted by one review author and checked by a second. We present sensitivity and specificity with 95% confidence intervals (CIs) for each test, and pooled data using the bivariate model. We investigated heterogeneity by including indicator variables in the random‐effects logistic regression models. We tabulated results by test manufacturer and compliance with manufacturer instructions for use and according to symptom status. Main results We included 155 study cohorts (described in 166 study reports, with 24 as preprints). The main results relate to 152 evaluations of single test applications including 100,462 unique samples (16,822 with confirmed SARS‐CoV‐2). Studies were mainly conducted in Europe (101/152, 66%), and evaluated 49 different commercial antigen assays. Only 23 studies compared two or more brands of test. Risk of bias was high because of participant selection (40, 26%); interpretation of the index test (6, 4%); weaknesses in the reference standard for absence of infection (119, 78%); and participant flow and timing 41 (27%). Characteristics of participants (45, 30%) and index test delivery (47, 31%) differed from the way in which and in whom the test was intended to be used. Nearly all studies (91%) used a single RT‐PCR result to define presence or absence of infection. The 152 studies of single test applications reported 228 evaluations of antigen tests. Estimates of sensitivity varied considerably between studies, with consistently high specificities. Average sensitivity was higher in symptomatic (73.0%, 95% CI 69.3% to 76.4%; 109 evaluations; 50,574 samples, 11,662 cases) compared to asymptomatic participants (54.7%, 95% CI 47.7% to 61.6%; 50 evaluations; 40,956 samples, 2641 cases). Average sensitivity was higher in the first week after symptom onset (80.9%, 95% CI 76.9% to 84.4%; 30 evaluations, 2408 cases) than in the second week of symptoms (53.8%, 95% CI 48.0% to 59.6%; 40 evaluations, 1119 cases). For those who were asymptomatic at the time of testing, sensitivity was higher when an epidemiological exposure to SARS‐CoV‐2 was suspected (64.3%, 95% CI 54.6% to 73.0%; 16 evaluations; 7677 samples, 703 cases) compared to where COVID‐19 testing was reported to be widely available to anyone on presentation for testing (49.6%, 95% CI 42.1% to 57.1%; 26 evaluations; 31,904 samples, 1758 cases). Average specificity was similarly high for symptomatic (99.1%) or asymptomatic (99.7%) participants. We observed a steady decline in summary sensitivities as measures of sample viral load decreased. Sensitivity varied between brands. When tests were used according to manufacturer instructions, average sensitivities by brand ranged from 34.3% to 91.3% in symptomatic participants (20 assays with eligible data) and from 28.6% to 77.8% for asymptomatic participants (12 assays). For symptomatic participants, summary sensitivities for seven assays were 80% or more (meeting acceptable criteria set by the World Health Organization (WHO)). The WHO acceptable performance criterion of 97% specificity was met by 17 of 20 assays when tests were used according to manufacturer instructions, 12 of which demonstrated specificities above 99%. For asymptomatic participants the sensitivities of only two assays approached but did not meet WHO acceptable performance standards in one study each; specificities for asymptomatic participants were in a similar range to those observed for symptomatic people. At 5% prevalence using summary data in symptomatic people during the first week after symptom onset, the positive predictive value (PPV) of 89% means that 1 in 10 positive results will be a false positive, and around 1 in 5 cases will be missed. At 0.5% prevalence using summary data for asymptomatic people, where testing was widely available and where epidemiological exposure to COVID‐19 was suspected, resulting PPVs would be 38% to 52%, meaning that between 2 in 5 and 1 in 2 positive results will be false positives, and between 1 in 2 and 1 in 3 cases will be missed. Authors' conclusions Antigen tests vary in sensitivity. In people with signs and symptoms of COVID‐19, sensitivities are highest in the first week of illness when viral loads are higher. Assays that meet appropriate performance standards, such as those set by WHO, could replace laboratory‐based RT‐PCR when immediate decisions about patient care must be made, or where RT‐PCR cannot be delivered in a timely manner. However, they are more suitable for use as triage to RT‐PCR testing. The variable sensitivity of antigen tests means that people who test negative may still be infected. Many commercially available rapid antigen tests have not been evaluated in independent validation studies. Evidence for testing in asymptomatic cohorts has increased, however sensitivity is lower and there is a paucity of evidence for testing in different settings. Questions remain about the use of antigen test‐based repeat testing strategies. Further research is needed to evaluate the effectiveness of screening programmes at reducing transmission of infection, whether mass screening or targeted approaches including schools, healthcare setting and traveller screening. Plain language summary How accurate are rapid antigen tests for diagnosing COVID‐19? Key messages • Rapid antigen tests are most accurate when they are used in people who have signs or symptoms of COVID‐19, especially during the first week of illness. People who test negative may still be infected. • Rapid antigen tests are considerably less accurate when they are used in people with no signs or symptoms of infection, but do perform better in people who have been in contact with someone who has confirmed COVID‐19. • The accuracy of rapid antigen tests varies between tests that are produced by different manufacturers and there is a lack of evidence for many commercially available tests. What are rapid point‐of‐care antigen tests for COVID‐19? Rapid point‐of‐care tests aim to confirm or rule out COVID‐19 infection in people with or without COVID‐19 symptoms. They: • are portable, so they can be used wherever the patient is (at the point‐of‐care) or in non‐healthcare settings such as in the home; • are easy to perform, with a minimum amount of extra equipment or complicated preparation steps; • are less expensive than standard laboratory tests; • do not require a specialist operator or setting; and • provide results ‘while you wait’. For this review we were interested in rapid antigen tests, sometimes referred to as ‘lateral flow tests’. These tests identify proteins on the virus in samples taken from the nose or throat. They come in disposable plastic cassettes, similar to over‐the‐counter pregnancy tests. Why is this question important? People with suspected COVID‐19 need to know quickly whether they are infected, so that they can self‐isolate, receive treatment, and inform close contacts. Currently, COVID‐19 infection is confirmed by a laboratory test called RT‐PCR, which uses specialist equipment and often takes at least 24 hours to produce a result. In many places, rapid antigen tests have opened access to testing for many more people, with and without symptoms, and in locations other than healthcare settings. Faster diagnosis of COVID‐19 infection could allow people to take appropriate action more quickly, with the potential to reduce the spread of COVID‐19, but it is important to understand how accurate they are and the best way to use them. What did we want to find out? We wanted to know whether commercially available, rapid point‐of‐care antigen tests are accurate enough to diagnose COVID‐19 infection reliably, and to find out if accuracy differs in people with and without symptoms. What did we do? We looked for studies that measured the accuracy of any commercially produced rapid antigen test in people who were also tested for COVID‐19 using RT‐PCR. People could be tested in hospital, in the community or in their own homes. Studies could test people with or without symptoms. What did we find? We included 155 studies in the review. The main results are based on 152 studies investigating a total of 100,462 nose or throat samples; COVID‐19 was confirmed in 16,822 of these samples. Studies investigated 49 different antigen tests. Around 60% of studies took place in Europe. Main results In people with confirmed COVID‐19, antigen tests correctly identified COVID‐19 infection in an average of 73% of people with symptoms, compared to 55% of people without symptoms. Tests were most accurate when used in the first week after symptoms began (an average of 82% of confirmed cases had positive antigen tests). This is likely to be because people have the most virus in their system in the first days after they are infected. For people with no symptoms, tests were most accurate in people likely to have been in contact with a case of COVID‐19 infection (an average of 64% of confirmed cases had positive antigen tests). In people who did not have COVID‐19, antigen tests correctly ruled out infection in 99.6% of people with symptoms and 99.7% of people without symptoms. Different brands of tests varied in accuracy. Summary results (combined from more than one study per test brand) for seven tests met World Health Organization (WHO) standards as ‘acceptable’ for confirming and ruling out COVID‐19 in people with signs and symptoms of COVID‐19. Two more tests met the WHO acceptable standard in one study each. No test met this standard when evaluated in people without symptoms. Using summary results for symptomatic people tested during the first week after symptoms began, if 1000 people with symptoms had the antigen test, and 50 (5%) of them really had COVID‐19: • 45 people would test positive for COVID‐19. Of these, 5 people (11%) would not have COVID‐19 (false positive result). • 955 people would test negative for COVID‐19. Of these, 10 people (1.0%) would actually have COVID‐19 (false negative result). In people with no symptoms of COVID‐19 the number of confirmed cases is expected to be much lower than in people with symptoms. Using summary results for people with no known exposure to COVID‐19 in a bigger population of 10,000 people with no symptoms, where 50 (0.5%) of them really had COVID‐19: • 62 people would test positive for COVID‐19. Of these, 30 people (48%) would not have COVID‐19 (false positive result). • 9938 people would test negative for COVID‐19. Of these, 18 people (0.2%) would actually have COVID‐19 (false negative result). What are the limitations of the evidence? In general, studies used relatively rigorous methods, particularly for selecting participants and performing the tests. Sometimes studies did not perform the test on the people for whom it was intended and did not follow the manufacturers’ instructions for using the test. Sometimes the tests were not carried out at the point of care. Studies used less rigorous methods for confirming the presence or absence of COVID‐19 infection; 91% of studies relied on a single negative RT‐PCR result as evidence of no COVID‐19 infection. Results from different test brands varied, and relatively few studies directly compared one test brand with another. Finally, not all studies gave enough information about their participants for us to judge how long they had had symptoms, or even whether or not they had symptoms. What does this mean? In people with symptoms, some rapid antigen tests are accurate enough to replace RT‐PCR, especially for ruling in the presence of infection. Alternatively, where RT‐PCR is available, rapid antigen tests could be used to select which people with symptoms require further testing with RT‐PCR, thereby reducing the burden on laboratory services. This would be most useful when quick decisions are needed about patient care, to identify outbreaks, to allow people to self‐isolate more quickly, or to initiate contact tracing. Rapid antigen tests are less good at ruling out infection in symptomatic people ‐ individuals who receive a negative rapid antigen test result may still be infected. Rapid antigen tests are less accurate when used in people with no symptoms of COVID‐19. More evidence is needed to understand the accuracy of rapid testing in people without symptoms and the extent to which repeated testing strategies can lead to reduced transmission, either for tests carried out at home or in non‐healthcare settings such as schools. There is no independent evidence to support the use of many test brands. More direct comparisons of test brands are needed, with testers following manufacturers’ instructions. How up‐to‐date is this review? This review updates our previous review and includes evidence published up to 8 March 2021.","7","John Wiley & Sons, Ltd","1465-1858","*COVID-19 [diagnosis]; COVID-19 Testing; Humans; Pandemics; Point-of-Care Systems; SARS-CoV-2; Sensitivity and Specificity","10.1002/14651858.CD013705.pub3","http://dx.doi.org/10.1002/14651858.CD013705.pub3","Infectious Diseases"
"CD013172.PUB2","Cassola, N; Baptista-Silva, JCC; Nakano, LCU; Flumignan, CDQ; Sesso, R; Vasconcelos, V; Carvas Junior, N; Flumignan, RLG","Duplex ultrasound for diagnosing symptomatic carotid stenosis in the extracranial segments","Cochrane Database of Systematic Reviews","2022","Abstract - Background Carotid artery stenosis is an important cause of stroke and transient ischemic attack. Correctly and rapidly identifying patients with symptomatic carotid artery stenosis is essential for adequate treatment with early cerebral revascularization. Doubts about the diagnostic value regarding the accuracy of duplex ultrasound (DUS) and the possibility of using DUS as the single diagnostic test before carotid revascularization are still debated. Objectives To estimate the accuracy of DUS in individuals with symptomatic carotid stenosis verified by either digital subtraction angiography (DSA), computed tomography angiography (CTA), or magnetic resonance angiography (MRA). Search methods We searched CRDTAS, CENTRAL, MEDLINE (Ovid), Embase (Ovid), ISI Web of Science, HTA, DARE, and LILACS up to 15 February 2021. We handsearched the reference lists of all included studies and other relevant publications and contacted experts in the field to identify additional studies or unpublished data. Selection criteria We included studies assessing DUS accuracy against an acceptable reference standard (DSA, MRA, or CTA) in symptomatic patients. We considered the classification of carotid stenosis with DUS defined with validated duplex velocity criteria, and the NASCET criteria for carotid stenosis measures on DSA, MRA, and CTA. We excluded studies that included < 70% of symptomatic patients; the time between the index test and the reference standard was longer than four weeks or not described, or that presented no objective criteria to estimate carotid stenosis. Data collection and analysis The review authors independently screened articles, extracted data, and assessed the risk of bias and applicability concerns using the QUADAS‐2 domain list. We extracted data with an effort to complete a 2 × 2 table (true positives, true negatives, false positives, and false negatives) for each of the different categories of carotid stenosis and reference standards. We produced forest plots and summary receiver operating characteristic (ROC) plots to summarize the data. Where meta‐analysis was possible, we used a bivariate meta‐analysis model. Main results We identified 25,087 unique studies, of which 22 were deemed eligible for inclusion (4957 carotid arteries). The risk of bias varied considerably across the studies, and studies were generally of moderate to low quality. We narratively described the results without meta‐analysis in seven studies in which the criteria used to determine stenosis were too different from the duplex velocity criteria proposed in our protocol or studies that provided insufficient data to complete a 2 × 2 table for at least in one category of stenosis. Nine studies (2770 carotid arteries) presented DUS versus DSA results for 70% to 99% carotid artery stenosis, and two (685 carotid arteries) presented results from DUS versus CTA in this category. Seven studies presented results for occlusion with DSA as the reference standard and three with CTA as the reference standard. Five studies compared DUS versus DSA for 50% to 99% carotid artery stenosis. Only one study presented results from 50% to 69% carotid artery stenosis. For DUS versus DSA, for < 50% carotid artery stenosis, the summary sensitivity was 0.63 (95% confidence interval [CI] 0.48 to 0.76) and the summary specificity was 0.99 (95% CI 0.96 to 0.99); for the 50% to 69% range, only one study was included and meta‐analysis not performed; for the 50% to 99% range, the summary sensitivity was 0.97 (95% CI 0.95 to 0.98) and the summary specificity was 0.70 (95% CI 0.67 to 0.73); for the 70% to 99% range, the summary sensitivity was 0.85 (95% CI 0.77 to 0.91) and the summary specificity was 0.98 (95% CI 0.74 to 0.90); for occlusion, the summary sensitivity was 0.91 (95% CI 0.81 to 0.97) and the summary specificity was 0.95 (95% CI 0.76 to 0.99). For sensitivity analyses, excluding studies in which participants were selected based on the presence of occlusion on DUS had an impact on specificity: 0.98 (95% CI 0.97 to 0.99). For DUS versus CTA, we found two studies in the range of 70% to 99%; the sensitivity varied from 0.57 to 0.94 and the specificity varied from 0.87 to 0.98. For occlusion, the summary sensitivity was 0.95 (95% CI 0.80 to 0.99) and the summary specificity was 0.91 (95% CI 0.09 to 0.99). For DUS versus MRA, there was one study with results for 50% to 99% carotid artery stenosis, with a sensitivity of 0.88 (95% CI 0.70 to 0.98) and specificity of 0.60 (95% CI 0.15 to 0.95); in the 70% to 99% range, two studies were included, with sensitivity that varied from 0.54 to 0.99 and specificity that varied from 0.78 to 0.89. We could perform only a few of the proposed sensitivity analyses because of the small number of studies included. Authors' conclusions This review provides evidence that the diagnostic accuracy of DUS is high, especially at discriminating between the presence or absence of significant carotid artery stenosis (< 50% or 50% to 99%). This evidence, plus its less invasive nature, supports the early use of DUS for the detection of carotid artery stenosis. The accuracy for 70% to 99% carotid artery stenosis and occlusion is high. Clinicians should exercise caution when using DUS as the single preoperative diagnostic method, and the limitations should be considered. There was little evidence of the accuracy of DUS when compared with CTA or MRA. The results of this review should be interpreted with caution because they are based on studies of low methodological quality, mainly due to the patient selection method. Methodological problems in participant inclusion criteria from the studies discussed above apparently influenced an overestimated estimate of prevalence values. Most of the studies included failed to precisely describe inclusion criteria and previous testing. Future diagnostic accuracy studies should include direct comparisons of the various modalities of diagnostic tests (mainly DUS, CTA, and MRA) for carotid artery stenosis since DSA is no longer considered to be the best method for diagnosing carotid stenosis and less invasive tests are now used as reference standards in clinical practice. Also, for future studies, the participant inclusion criteria require careful attention. Plain language summary How accurate is duplex ultrasound (DUS) imaging for diagnosing carotid artery stenosis in symptomatic patients? Carotid artery stenosis (CAS) is a narrowing of the lumen (the inside space) of the carotid artery (usually due to cholesterol deposits called plaque). CAS is responsible for 8% of all strokes due to a blocked blood vessel (ischemic strokes) and is associated with a high chance of recurrence. In such circumstances, the treatment is to re‐establish adequate blood flow (by surgery or other approaches to open the artery) to prevent further neurologic episodes. Duplex ultrasound (DUS) can help identify the appropriate patients who will benefit from a more invasive treatment and those who should be with drugs alone. What is the aim of this review? To determine how accurate DUS is for diagnosing different grades of CAS in individuals with neurologic symptoms. What was studied in the review? DUS is used in clinical practice as the first test to detect carotid artery stenosis, usually with the result confirmed by other more expensive and invasive tests, such as computed tomography angiography (CTA), magnetic resonance angiography (MRA), or digital subtraction angiography (DSA). The advantage of DUS is that it is less expensive and helps to reduce the time required to select patients for treatment. We included studies assessing the accuracy of DUS compared with DSA, MRA, or CTA in patients with recent stroke symptoms. We grouped the results from studies that used approximately the same method and threshold to assess accuracy in the following categories of carotid artery stenosis: < 50%, 50% to 99%, 50% to 69%, 70% to 99%, and occlusion (blockage of the vessel). What are the main results of this review? This review included 22 studies (4957 carotid arteries tested). The searches were performed up to 15 February 2021. The results indicate the following: If DUS were to be used in a standardized cohort of 1000 patients: For DUS versus DSA < 50% CAS (4 studies, 1495 carotid arteries): Estimated 299 patients would have a DUS result indicating the presence of non‐significant CAS, of whom eight (2.7%) would be incorrectly classified. Of the 701 people with a result indicating that < 50% carotid stenosis is not present, 169 (24.1%) would be incorrectly classified. 50% to 99% CAS (5 studies, 1536 carotid arteries): Estimated 642 patients would have a DUS result indicating the presence of 50% to 99% CAS; of these, 147 (22.8%) would be incorrectly classified. Of the 358 people with a result indicating that 50% to 99% carotid stenosis is not present, 15 (4.2%) would be incorrectly classified. 70% to 99% CAS (9 studies, 2770 carotid arteries): Estimated 390 patients would have a DUS result indicating the presence of 70% to 99% CAS; of these, eight (2%) would be incorrectly classified. Of the 610 people with a result indicating that 70% to 99% carotid stenosis is not present, 68 (11.1%) would be incorrectly classified. Occlusion (7 studies, 1212 carotid arteries): Estimated 205 patients would have a DUS result indicating carotid artery occlusion; of these, 41 (20%) would be incorrectly classified. Of the 795 people with a result indicating that carotid occlusion is not present, 16 (2%) would be incorrectly classified. For DUS versus CTA Occlusion (3 studies, 833 carotid arteries): An estimated 606 patients would have a DUS result indicating carotid artery occlusion; of these, 36 (6%) would be incorrectly classified. 394 people with a result indicating that carotid occlusion is not present, 30 (8%) would be incorrectly classified. For DUS versus MRA Meta‐analysis was not performed. How reliable are the results of the studies in this review? There were some problems with how the studies were conducted that could impair the correct estimates of the diagnostic accuracy. Many of the studies were of poor or unclear quality. Who do the results of this review apply to? The results are relevant for patients with neurologic symptoms who are suspected of having carotid artery stenosis. What are the implications of this review? The diagnostic accuracy of DUS is high, especially at discriminating between the presence or absence of significant carotid artery stenosis. This evidence, plus its less invasive nature, supports the early use of DUS for the detection of carotid artery stenosis.","7","John Wiley & Sons, Ltd","1465-1858","*Carotid Stenosis [diagnostic imaging, surgery]; Constriction, Pathologic; Humans; Magnetic Resonance Angiography; Sensitivity and Specificity; Ultrasonography, Doppler, Duplex","10.1002/14651858.CD013172.pub2","http://dx.doi.org/10.1002/14651858.CD013172.pub2","Stroke"
"CD012558.PUB2","Creavin, ST; Noel-Storr, AH; Langdon, RJ; Richard, E; Creavin, AL; Cullum, S; Purdy, S; Ben-Shlomo, Y","Clinical judgement by primary care physicians for the diagnosis of all‐cause dementia or cognitive impairment in symptomatic people","Cochrane Database of Systematic Reviews","2022","Abstract - Background In primary care, general practitioners (GPs) unavoidably reach a clinical judgement about a patient as part of their encounter with patients, and so clinical judgement can be an important part of the diagnostic evaluation. Typically clinical decision making about what to do next for a patient incorporates clinical judgement about the diagnosis with severity of symptoms and patient factors, such as their ideas and expectations for treatment. When evaluating patients for dementia, many GPs report using their own judgement to evaluate cognition, using information that is immediately available at the point of care, to decide whether someone has or does not have dementia, rather than more formal tests. Objectives To determine the diagnostic accuracy of GPs’ clinical judgement for diagnosing cognitive impairment and dementia in symptomatic people presenting to primary care. To investigate the heterogeneity of test accuracy in the included studies. Search methods We searched MEDLINE (Ovid SP), Embase (Ovid SP), PsycINFO (Ovid SP), Web of Science Core Collection (ISI Web of Science), and LILACs (BIREME) on 16 September 2021. Selection criteria We selected cross‐sectional and cohort studies from primary care where clinical judgement was determined by a GP either prospectively (after consulting with a patient who has presented to a specific encounter with the doctor) or retrospectively (based on knowledge of the patient and review of the medical notes, but not relating to a specific encounter with the patient). The target conditions were dementia and cognitive impairment (mild cognitive impairment and dementia) and we included studies with any appropriate reference standard such as the Diagnostic and Statistical Manual of Mental Disorders (DSM), International Classification of Diseases (ICD), aetiological definitions, or expert clinical diagnosis. Data collection and analysis Two review authors screened titles and abstracts for relevant articles and extracted data separately with differences resolved by consensus discussion. We used QUADAS‐2 to evaluate the risk of bias and concerns about applicability in each study using anchoring statements. We performed meta‐analysis using the bivariate method. Main results We identified 18,202 potentially relevant articles, of which 12,427 remained after de‐duplication. We assessed 57 full‐text articles and extracted data on 11 studies (17 papers), of which 10 studies had quantitative data. We included eight studies in the meta‐analysis for the target condition dementia and four studies for the target condition cognitive impairment. Most studies were at low risk of bias as assessed with the QUADAS‐2 tool, except for the flow and timing domain where four studies were at high risk of bias, and the reference standard domain where two studies were at high risk of bias. Most studies had low concern about applicability to the review question in all QUADAS‐2 domains. Average age ranged from 73 years to 83 years (weighted average 77 years). The percentage of female participants in studies ranged from 47% to 100%. The percentage of people with a final diagnosis of dementia was between 2% and 56% across studies (a weighted average of 21%). For the target condition dementia, in individual studies sensitivity ranged from 34% to 91% and specificity ranged from 58% to 99%. In the meta‐analysis for dementia as the target condition, in eight studies in which a total of 826 of 2790 participants had dementia, the summary diagnostic accuracy of clinical judgement of general practitioners was sensitivity 58% (95% confidence interval (CI) 43% to 72%), specificity 89% (95% CI 79% to 95%), positive likelihood ratio 5.3 (95% CI 2.4 to 8.2), and negative likelihood ratio 0.47 (95% CI 0.33 to 0.61). For the target condition cognitive impairment, in individual studies sensitivity ranged from 58% to 97% and specificity ranged from 40% to 88%. The summary diagnostic accuracy of clinical judgement of general practitioners in four studies in which a total of 594 of 1497 participants had cognitive impairment was sensitivity 84% (95% CI 60% to 95%), specificity 73% (95% CI 50% to 88%), positive likelihood ratio 3.1 (95% CI 1.4 to 4.7), and negative likelihood ratio 0.23 (95% CI 0.06 to 0.40). It was impossible to draw firm conclusions in the analysis of heterogeneity because there were small numbers of studies. For specificity we found the data were compatible with studies that used ICD‐10, or applied retrospective judgement, had higher reported specificity compared to studies with DSM definitions or using prospective judgement. In contrast for sensitivity, we found studies that used a prospective index test may have had higher sensitivity than studies that used a retrospective index test. Authors' conclusions Clinical judgement of GPs is more specific than sensitive for the diagnosis of dementia. It would be necessary to use additional tests to confirm the diagnosis for either target condition, or to confirm the absence of the target conditions, but clinical judgement may inform the choice of further testing. Many people who a GP judges as having dementia will have the condition. People with false negative diagnoses are likely to have less severe disease and some could be identified by using more formal testing in people who GPs judge as not having dementia. Some false positives may require similar practical support to those with dementia, but some ‐ such as some people with depression ‐ may suffer delayed intervention for an alternative treatable pathology. Plain language summary Why is improving dementia diagnosis important? Dementia refers to a group of brain conditions that lead to progressive problems with memory, working‐things‐out, or functioning in everyday life. Doctors use a variety of tests to diagnose dementia. People have often reported that it can take a long time to get a diagnosis of dementia from initially presenting to a healthcare provider with symptoms suggestive of dementia. Cognitive impairment is a broader term that includes people whose brain is not functioning as well as expected given their age, but they do not have dementia, as well as people with dementia. Some people with cognitive impairment who do not have dementia may have a condition called mild cognitive impairment (MCI). Some people with MCI (but not all) will develop dementia over time. What is the aim of this review? The review authors aimed to investigate the diagnostic accuracy of clinical judgement of general practitioners (GPs) for diagnosing dementia, and cognitive impairment, in primary care. What was studied in the review? The authors included extracted data from 11 studies, including 10 with complete data on diagnostic accuracy. The authors included eight studies in the statistical summary with a total of 2790 people, of whom 826 (30%) had dementia. The authors included four studies that investigated cognitive impairment as the condition to diagnose, with a total of 1497 people of whom 594 had cognitive impairment (40%). What are the main results of the review? The results of the review indicate that in theory, if GPs used their clinical judgement in practice for dementia, they would correctly identify 58% of the people who have dementia as having the condition (sensitivity) and 89% of the people who do not have dementia as being free of the condition (specificity). The results of the review indicate that in theory, if GPs used their clinical judgement in practice for cognitive impairment, they would correctly identify 84% of the people who have cognitive impairment as having the condition (sensitivity) and 73% of the people who do not have cognitive impairment as being free of the condition (specificity). How reliable are the results of the studies in this review? In this review there were some technical problems with the design of the original studies, and there were differences between studies that made it difficult to compare them to each other. This means that it is difficult to be certain how applicable these findings are in clinical practice. Who do the results of this review apply to? Researchers who conducted the studies in the review carried out their investigations mostly in Europe, with one study in the USA and one study in Australia. All studies included people attending their GP. Average age ranged from 73 years to 83 years (weighted average 77 years). The percentage of female participants in studies ranged from 47% to 100%. The percentage of people with a final diagnosis of dementia was between 2% and 56% across studies (a weighted average of 21%). If applying these findings in settings with fewer number of people with dementia then the accuracy of the test may be different. What are the implications of this review? If these studies are indeed representative of GPs practice, then if GPs used their clinical judgement alone to diagnose dementia then this might mean that some people with dementia are incorrectly 'missed', and it is important to do further tests to confirm that the person does not have a problem. However, if a GP thinks someone has dementia there is a good chance that the diagnosis is correct and the test to confirm dementia might be different and potentially less time consuming and burdensome. The studies included in this review suggest that clinical judgement could be a useful test to determine what to do next. How up‐to‐date is this review? The review authors searched for and used studies published up to 16 September 2021.","6","John Wiley & Sons, Ltd","1465-1858","*Alzheimer Disease [diagnosis]; *Cognitive Dysfunction [diagnosis]; *Dementia [diagnosis]; *Physicians, Primary Care; Aged; Clinical Reasoning; Cross-Sectional Studies; Female; Humans; Prospective Studies; Retrospective Studies; Sensitivity and Specificity","10.1002/14651858.CD012558.pub2","http://dx.doi.org/10.1002/14651858.CD012558.pub2","Dementia and Cognitive Improvement"
"CD009276.PUB2","Grobbee, EJ; Wisse, PHA; Schreuders, EH; van Roon, A; van Dam, L; Zauber, AG; Lansdorp-Vogelaar, I; Bramer, W; Berhane, S; Deeks, JJ; Steyerberg, EW; van Leerdam, ME; Spaander, MCW; Kuipers, EJ","Guaiac‐based faecal occult blood tests versus faecal immunochemical tests for colorectal cancer screening in average‐risk individuals","Cochrane Database of Systematic Reviews","2022","Abstract - Background Worldwide, many countries have adopted colorectal cancer (CRC) screening programmes, often based on faecal occult blood tests (FOBTs). CRC screening aims to detect advanced neoplasia (AN), which is defined as CRC or advanced adenomas. FOBTs fall into two categories based on detection technique and the detected blood component: qualitative guaiac‐based FOBTs (gFOBTs) and faecal immunochemical tests (FITs), which can be qualitative and quantitative. Screening with gFOBTs reduces CRC‐related mortality. Objectives To compare the diagnostic test accuracy of gFOBT and FIT screening for detecting advanced colorectal neoplasia in average‐risk individuals. Search methods We searched CENTRAL, MEDLINE, Embase, BIOSIS Citation Index, Science Citation Index Expanded, and Google Scholar. We searched the reference lists and PubMed‐related articles of included studies to identify additional studies. Selection criteria We included prospective and retrospective studies that provided the number of true positives, false positives, false negatives, and true negatives for gFOBTs, FITs, or both, with colonoscopy as reference standard. We excluded case‐control studies. We included studies in which all participants underwent both index test and reference standard (""reference standard: all""), and studies in which only participants with a positive index test underwent the reference standard while participants with a negative test were followed for at least one year for development of interval carcinomas (""reference standard: positive""). The target population consisted of asymptomatic, average‐risk individuals undergoing CRC screening. The target conditions were CRC and advanced neoplasia (advanced adenomas and CRC combined). Data collection and analysis Two review authors independently screened and selected studies for inclusion. In case of disagreement, a third review author made the final decision. We used the Rutter and Gatsonis hierarchical summary receiver operating characteristic model to explore differences between tests and identify potential sources of heterogeneity, and the bivariate hierarchical model to estimate sensitivity and specificity at common thresholds: 10 µg haemoglobin (Hb)/g faeces and 20 µg Hb/g faeces. We performed indirect comparisons of the accuracy of the two tests and direct comparisons when both index tests were evaluated in the same population. Main results We ran the initial search on 25 June 2019, which yielded 63 studies for inclusion. We ran a top‐up search on 14 September 2021, which yielded one potentially eligible study, currently awaiting classification. We included a total of 33 ""reference standard: all"" published articles involving 104,640 participants. Six studies evaluated only gFOBTs, 23 studies evaluated only FITs, and four studies included both gFOBTs and FITs. The cut‐off for positivity of FITs varied between 2.4 μg and 50 µg Hb/g faeces. For each Quality Assessment of Diagnostic Accuracy Studies (QUADAS)‐2 domain, we assessed risk of bias as high in less than 20% of studies. The summary curve showed that FITs had a higher discriminative ability than gFOBTs for AN (P < 0.001) and CRC (P = 0.004). For the detection of AN, the summary sensitivity of gFOBTs was 15% (95% confidence interval (CI) 12% to 20%), which was significantly lower than FITs at both 10 μg and 20 μg Hb/g cut‐offs with summary sensitivities of 33% (95% CI 27% to 40%; P < 0.001) and 26% (95% CI 21% to 31%, P = 0.002), respectively. Results were simulated in a hypothetical cohort of 10,000 screening participants with 1% CRC prevalence and 10% AN prevalence. Out of 1000 participants with AN, gFOBTs missed 850, while FITs missed 670 (10 μg Hb/g cut‐off) and 740 (20 μg Hb/g cut‐off). No significant differences in summary specificity for AN detection were found between gFOBTs (94%; 95% CI 92% to 96%), and FITs at 10 μg Hb/g cut‐off (93%; 95% CI 90% to 95%) and at 20 μg Hb/g cut‐off (97%; 95% CI 95% to 98%). So, among 9000 participants without AN, 540 were offered (unnecessary) colonoscopy with gFOBTs compared to 630 (10 μg Hb/g) and 270 (20 μg Hb/g) with FITs. Similarly, for the detection of CRC, the summary sensitivity of gFOBTs, 39% (95% CI 25% to 55%), was significantly lower than FITs at 10 μg and 20 μg Hb/g cut‐offs: 76% (95% CI 57% to 88%: P = 0.001) and 65% (95% CI 46% to 80%; P = 0.035), respectively. So, out of 100 participants with CRC, gFOBTs missed 61, and FITs missed 24 (10 μg Hb/g) and 35 (20 μg Hb/g). No significant differences in summary specificity for CRC were found between gFOBTs (94%; 95% CI 91% to 96%), and FITs at the 10 μg Hb/g cut‐off (94%; 95% CI 87% to 97%) and 20 μg Hb/g cut‐off (96%; 95% CI 91% to 98%). So, out of 9900 participants without CRC, 594 were offered (unnecessary) colonoscopy with gFOBTs versus 594 (10 μg Hb/g) and 396 (20 μg Hb/g) with FITs. In five studies that compared FITs and gFOBTs in the same population, FITs showed a higher discriminative ability for AN than gFOBTs (P = 0.003). We included a total of 30 ""reference standard: positive"" studies involving 3,664,934 participants. Of these, eight were gFOBT‐only studies, 18 were FIT‐only studies, and four studies combined both gFOBTs and FITs. The cut‐off for positivity of FITs varied between 5 µg to 250 µg Hb/g faeces. For each QUADAS‐2 domain, we assessed risk of bias as high in less than 20% of studies. The summary curve showed that FITs had a higher discriminative ability for detecting CRC than gFOBTs (P < 0.001). The summary sensitivity for CRC of gFOBTs, 59% (95% CI 55% to 64%), was significantly lower than FITs at the 10 μg Hb/g cut‐off, 89% (95% CI 80% to 95%; P < 0.001) and the 20 μg Hb/g cut‐off, 89% (95% CI 85% to 92%; P < 0.001). So, in the hypothetical cohort with 100 participants with CRC, gFOBTs missed 41, while FITs missed 11 (10 μg Hb/g) and 11 (20 μg Hb/g). The summary specificity of gFOBTs was 98% (95% CI 98% to 99%), which was higher than FITs at both 10 μg and 20 μg Hb/g cut‐offs: 94% (95% CI 92% to 95%; P < 0.001) and 95% (95% CI 94% to 96%; P < 0.001), respectively. So, out of 9900 participants without CRC, 198 were offered (unnecessary) colonoscopy with gFOBTs compared to 594 (10 μg Hb/g) and 495 (20 μg Hb/g) with FITs. At a specificity of 90% and 95%, FITs had a higher sensitivity than gFOBTs. Authors' conclusions FITs are superior to gFOBTs in detecting AN and CRC in average‐risk individuals. Specificity of both tests was similar in ""reference standard: all"" studies, whereas specificity was significantly higher for gFOBTs than FITs in ""reference standard: positive"" studies. However, at pre‐specified specificities, the sensitivity of FITs was significantly higher than gFOBTs. Plain language summary Which faecal blood test is more accurate in detecting bowel cancer and large polyps in population screening? Background  One of the most common types of cancer diagnosed is large bowel or colorectal cancer (CRC). Early detection, before symptoms appear, makes it easier to treat bowel cancer and increases the chance of survival. Taking part in a bowel cancer screening program can lead to early detection and removal of large or advanced polyps (advanced adenomas), which are considered to be a precursor to bowel cancer. Simple faecal tests are used to detect the presence of blood in stool, which could be an early sign of bowel cancer or polyps. Two types of faecal blood tests used in population screening are: guaiac‐based faecal occult blood tests (gFOBTs) and faecal immunochemical tests (FITs). Large, older studies have shown that screening with gFOBTs can reduce mortality. In a systematic review of the literature, we compared the accuracy of these two tests in order to assess which test gives the best results in population screening for bowel cancer, and, secondarily, for advanced neoplasia (which comprises bowel cancer and advanced polyps together). Study characteristics  We carried out a detailed search of online databases for studies that evaluated or compared (one of) these two tests in CRC screening. The review included only studies in average‐risk individuals over 40 years of age without symptoms. The reference standard to compare the test results with was a total endoscopic examination of the large bowel with a camera on a flexible tube passed through the anus (colonoscopy). We reviewed two types of studies: those in which all participants underwent both the stool test and colonoscopy; and those in which only participants with an unfavourable result on the stool test underwent colonoscopy (in these studies, participants who did not have a colonoscopy after the stool test were followed for at least one year to see if they would be diagnosed with colorectal cancer). The evidence is current until 25 June 2019. We ran a top‐up search on 14 September 2021, which yielded only one potentially eligible study, currently awaiting classification. Test characteristics The gFOBT 'screenees' – i.e. those who participate in screening – are instructed to collect two faecal samples from three consecutive bowel movements and to smear this on six stool panels. If there is blood in the stool, the panel changes colour. The number of coloured panels for referral to colonoscopy varies between screening programs. In most programs, a single coloured panel is sufficient for referral; however, in others, the number of panels is set at five out of six. The FIT screenees are instructed to collect one faecal sample from one bowel movement, and to collect this with a brush or spatula into a tube. This tube is then send to a laboratory where the concentration of blood in the stool can be measured. Depending on the height of this concentration, above or below the so‐called cut‐off or threshold, the screenee is referred for colonoscopy. This cut‐off differs per screening program. Key results  We analysed 63 studies including almost 4 million individuals. The results of this review indicate that if, in theory, 10,000 people take part in screening with a faecal blood test and 100 people in this group have CRC: ‐ out of the 100 people with CRC, 24 will be missed in those being screened with FITs. ‐ out of the 100 people with CRC, 61 will be missed in those being screened with gFOBTs. We also looked at participants with large polyps, CRC, or both. If, in theory, 10,000 people take part in screening with a faecal blood test and 1000 people in this group have large polyps, CRC, or both: ‐ out of the 1000 people with large polyps, CRC, or both, 850 will be missed in those being screened with gFOBTs. ‐ out of the 1000 people with large polyps, CRC, or both, 670 will be missed in those being screened with FITs. In this theoretical group of 10,000 screenees: ‐ 594 people being screened with FITs will be offered an 'unnecessary' colonoscopy – unnecessary because they do not have CRC; and ‐ 594 people being screened with gFOBTs will be offered an 'unnecessary' colonoscopy. From the results described above, we can see that FITs miss less CRC than gFOBTs, while an equal number of screenees from each type of blood test undergo an unnecessary colonoscopy. How reliable are the results of the studies in this review?  The results of the studies are reliable, as the included studies mostly met the quality criteria we specified before commencing the review. Future research  More research is needed to investigate whether, in the long term, FIT screening can reduce the number of bowel cancer cases and deaths, and to compare these findings with those from gFOBT screening.","6","John Wiley & Sons, Ltd","1465-1858","*Adenoma [diagnosis]; *Colorectal Neoplasms [diagnosis]; Early Detection of Cancer [methods]; Guaiac; Hemoglobins; Humans; Occult Blood; Prospective Studies; Retrospective Studies; Sensitivity and Specificity","10.1002/14651858.CD009276.pub2","http://dx.doi.org/10.1002/14651858.CD009276.pub2","Colorectal"
"CD013665.PUB3","Struyf, T; Deeks, JJ; Dinnes, J; Takwoingi, Y; Davenport, C; Leeflang, MMG; Spijker, R; Hooft, L; Emperador, D; Domen, J; Tans, A; Janssens, S; Wickramasinghe, D; Lannoy, V; Horn, SR A; Van den Bruel, A","Signs and symptoms to determine if a patient presenting in primary care or hospital outpatient settings has COVID‐19","Cochrane Database of Systematic Reviews","2022","Abstract - Background COVID‐19 illness is highly variable, ranging from infection with no symptoms through to pneumonia and life‐threatening consequences. Symptoms such as fever, cough, or loss of sense of smell (anosmia) or taste (ageusia), can help flag early on if the disease is present. Such information could be used either to rule out COVID‐19 disease, or to identify people who need to go for COVID‐19 diagnostic tests. This is the second update of this review, which was first published in 2020. Objectives To assess the diagnostic accuracy of signs and symptoms to determine if a person presenting in primary care or to hospital outpatient settings, such as the emergency department or dedicated COVID‐19 clinics, has COVID‐19. Search methods We undertook electronic searches up to 10 June 2021 in the University of Bern living search database. In addition, we checked repositories of COVID‐19 publications. We used artificial intelligence text analysis to conduct an initial classification of documents. We did not apply any language restrictions. Selection criteria Studies were eligible if they included people with clinically suspected COVID‐19, or recruited known cases with COVID‐19 and also controls without COVID‐19 from a single‐gate cohort. Studies were eligible when they recruited people presenting to primary care or hospital outpatient settings. Studies that included people who contracted SARS‐CoV‐2 infection while admitted to hospital were not eligible. The minimum eligible sample size of studies was 10 participants. All signs and symptoms were eligible for this review, including individual signs and symptoms or combinations. We accepted a range of reference standards. Data collection and analysis Pairs of review authors independently selected all studies, at both title and abstract, and full‐text stage. They resolved any disagreements by discussion with a third review author. Two review authors independently extracted data and assessed risk of bias using the QUADAS‐2 checklist, and resolved disagreements by discussion with a third review author. Analyses were restricted to prospective studies only. We presented sensitivity and specificity in paired forest plots, in receiver operating characteristic (ROC) space and in dumbbell plots. We estimated summary parameters using a bivariate random‐effects meta‐analysis whenever five or more primary prospective studies were available, and whenever heterogeneity across studies was deemed acceptable. Main results We identified 90 studies; for this update we focused on the results of 42 prospective studies with 52,608 participants. Prevalence of COVID‐19 disease varied from 3.7% to 60.6% with a median of 27.4%. Thirty‐five studies were set in emergency departments or outpatient test centres (46,878 participants), three in primary care settings (1230 participants), two in a mixed population of in‐ and outpatients in a paediatric hospital setting (493 participants), and two overlapping studies in nursing homes (4007 participants). The studies did not clearly distinguish mild COVID‐19 disease from COVID‐19 pneumonia, so we present the results for both conditions together. Twelve studies had a high risk of bias for selection of participants because they used a high level of preselection to decide whether reverse transcription polymerase chain reaction (RT‐PCR) testing was needed, or because they enrolled a non‐consecutive sample, or because they excluded individuals while they were part of the study base. We rated 36 of the 42 studies as high risk of bias for the index tests because there was little or no detail on how, by whom and when, the symptoms were measured. For most studies, eligibility for testing was dependent on the local case definition and testing criteria that were in effect at the time of the study, meaning most people who were included in studies had already been referred to health services based on the symptoms that we are evaluating in this review. The applicability of the results of this review iteration improved in comparison with the previous reviews. This version has more studies of people presenting to ambulatory settings, which is where the majority of assessments for COVID‐19 take place. Only three studies presented any data on children separately, and only one focused specifically on older adults. We found data on 96 symptoms or combinations of signs and symptoms. Evidence on individual signs as diagnostic tests was rarely reported, so this review reports mainly on the diagnostic value of symptoms. Results were highly variable across studies. Most had very low sensitivity and high specificity. RT‐PCR was the most often used reference standard (40/42 studies). Only cough (11 studies) had a summary sensitivity above 50% (62.4%, 95% CI 50.6% to 72.9%)); its specificity was low (45.4%, 95% CI 33.5% to 57.9%)). Presence of fever had a sensitivity of 37.6% (95% CI 23.4% to 54.3%) and a specificity of 75.2% (95% CI 56.3% to 87.8%). The summary positive likelihood ratio of cough was 1.14 (95% CI 1.04 to 1.25) and that of fever 1.52 (95% CI 1.10 to 2.10). Sore throat had a summary positive likelihood ratio of 0.814 (95% CI 0.714 to 0.929), which means that its presence increases the probability of having an infectious disease other than COVID‐19. Dyspnoea (12 studies) and fatigue (8 studies) had a sensitivity of 23.3% (95% CI 16.4% to 31.9%) and 40.2% (95% CI 19.4% to 65.1%) respectively. Their specificity was 75.7% (95% CI 65.2% to 83.9%) and 73.6% (95% CI 48.4% to 89.3%). The summary positive likelihood ratio of dyspnoea was 0.96 (95% CI 0.83 to 1.11) and that of fatigue 1.52 (95% CI 1.21 to 1.91), which means that the presence of fatigue slightly increases the probability of having COVID‐19. Anosmia alone (7 studies), ageusia alone (5 studies), and anosmia or ageusia (6 studies) had summary sensitivities below 50% but summary specificities over 90%. Anosmia had a summary sensitivity of 26.4% (95% CI 13.8% to 44.6%) and a specificity of 94.2% (95% CI 90.6% to 96.5%). Ageusia had a summary sensitivity of 23.2% (95% CI 10.6% to 43.3%) and a specificity of 92.6% (95% CI 83.1% to 97.0%). Anosmia or ageusia had a summary sensitivity of 39.2% (95% CI 26.5% to 53.6%) and a specificity of 92.1% (95% CI 84.5% to 96.2%). The summary positive likelihood ratios of anosmia alone and anosmia or ageusia were 4.55 (95% CI 3.46 to 5.97) and 4.99 (95% CI 3.22 to 7.75) respectively, which is just below our arbitrary definition of a 'red flag', that is, a positive likelihood ratio of at least 5. The summary positive likelihood ratio of ageusia alone was 3.14 (95% CI 1.79 to 5.51). Twenty‐four studies assessed combinations of different signs and symptoms, mostly combining olfactory symptoms. By combining symptoms with other information such as contact or travel history, age, gender, and a local recent case detection rate, some multivariable prediction scores reached a sensitivity as high as 90%. Authors' conclusions Most individual symptoms included in this review have poor diagnostic accuracy. Neither absence nor presence of symptoms are accurate enough to rule in or rule out the disease. The presence of anosmia or ageusia may be useful as a red flag for the presence of COVID‐19. The presence of cough also supports further testing. There is currently no evidence to support further testing with PCR in any individuals presenting only with upper respiratory symptoms such as sore throat, coryza or rhinorrhoea. Combinations of symptoms with other readily available information such as contact or travel history, or the local recent case detection rate may prove more useful and should be further investigated in an unselected population presenting to primary care or hospital outpatient settings. The diagnostic accuracy of symptoms for COVID‐19 is moderate to low and any testing strategy using symptoms as selection mechanism will result in both large numbers of missed cases and large numbers of people requiring testing. Which one of these is minimised, is determined by the goal of COVID‐19 testing strategies, that is, controlling the epidemic by isolating every possible case versus identifying those with clinically important disease so that they can be monitored or treated to optimise their prognosis. The former will require a testing strategy that uses very few symptoms as entry criterion for testing, the latter could focus on more specific symptoms such as fever and anosmia. Plain language summary How accurate are symptoms and medical examination to diagnose COVID‐19? Key messages ‐ The results suggest that a single symptom included in this review cannot accurately diagnose COVID‐19. ‐ Loss of sense of taste or smell could be a 'red flag' for the presence of COVID‐19. Cough or fever might be useful to identify people who might have COVID‐19. These symptoms might be useful to prompt further testing when they are present. ‐ We need more research to investigate combinations of symptoms and signs with other information such as recent contact or travel history, or vaccination status, and in children, and adults aged 65 years and over. What are symptoms or signs of COVID‐19? Symptoms are experienced by patients. COVID‐19 symptoms include cough, sore throat, high temperature, diarrhoea, headache, muscle or joint pain, fatigue, and loss of sense of smell and taste. Signs are measured by healthcare workers during clinical examination. They include lung sounds, blood pressure, blood oxygen level and heart rate. Symptoms and signs of COVID‐19 might be important to help people know whether they and the people they come into contact with should isolate at home, undergo testing with a rapid lateral flow test or PCR (laboratory‐based) test, or be hospitalised. What did we want to find out? Symptoms and signs of COVID‐19 are varied and may indicate other diseases, not just COVID‐19. We wanted to know how accurate diagnosis of COVID‐19 is, based on symptoms and signs from medical examination. We were interested in people with suspected COVID‐19, who go to their doctor, outpatient test centres or hospital. What did we do? We searched for studies that assessed the accuracy of symptoms and signs to diagnose COVID‐19. Studies had to be conducted in general practice, outpatient test centres or hospital outpatient settings only. We only included studies of people in hospital if signs and symptoms were recorded when they were admitted to the hospital, for example through the emergency department. What did we find? We focused on 42 studies with 52,608 participants in this review. The studies assessed 96 separate or combined symptoms and signs. Thirty‐five studies were conducted in emergency departments or outpatient COVID‐19 test centres (46,878 participants), 3 studies in general practice (1230 participants), 2 studies in children’s hospitals (493 in‐ and outpatients), and 2 studies in nursing homes (4007 participants). The studies were conducted in 18 different countries around the world. Twenty‐three studies were conducted in Europe, 8 in North‐America, 5 in Asia, and 3 in South‐America and 3 in Australia. We didn’t find any studies conducted in Africa. Three focused specifically on children, and only 1 focused on adults aged 65 years and over. Most studies did not clearly distinguish between mild and severe COVID‐19, so we present the results for mild, moderate and severe disease together. Few studies reported individual signs as diagnostic tests, so we focus mainly on the diagnostic value of symptoms. The most frequently reported symptoms were cough, fever, shortness of breath and sore throat. According to the studies in our review, in a group of 1000 people with suspected COVID‐19 of whom 270 (27%) would actually have COVID‐19, around 567 people would have a cough. Of these 567, 168 would actually have COVID‐19. Of the 433 who do not have a cough, 102 would have COVID‐19. In the same 1000 people, around 283 people would have a fever. Of these 283, 102 would actually have COVID‐19. Of the 717 people without fever, 168 would have COVID‐19. Someone who has lost their sense of smell or taste is five times more likely to have COVID‐19 than someone who hasn’t. Other symptoms, such as a sore throat or runny nose, are more likely to indicate the presence of an infectious disease other than COVID‐19. In the same 1000 people as described above, around 362 people would have a sore throat. Of these, only 84 would actually have COVID‐19. Of the 638 patients without sore throat, 186 would have COVID‐19. We found similar figures for having a runny nose. What are the limitations of the evidence? The results of this updated review are more reliable than those in previous versions as we included more high‐quality studies. However, the accuracy of individual symptoms varied across studies and the diagnostic value of symptoms such as fever, cough or other respiratory symptoms might still be overestimated, as most studies deliberately included participants because they had these symptoms. The results do not clearly differentiate between people with mild, moderate or severe COVID‐19. Only a few studies investigated the symptom‐based diagnosis of COVID‐19 in children or older adults. How up to date is this review? This review updates our previous review. The evidence is up to date to June 2021.","5","John Wiley & Sons, Ltd","1465-1858","*Ageusia [complications]; *COVID-19 [diagnosis, epidemiology]; *Pharyngitis; Aged; Anosmia [diagnosis, etiology]; Artificial Intelligence; COVID-19 Testing; Child; Cough [etiology]; Dyspnea; Fatigue [etiology]; Fever [diagnosis, etiology]; Hospitals; Humans; Outpatients; Primary Health Care; Prospective Studies; SARS-CoV-2; Sensitivity and Specificity","10.1002/14651858.CD013665.pub3","http://dx.doi.org/10.1002/14651858.CD013665.pub3","Infectious Diseases"
"CD014841.PUB2","Pillay, S; Steingart, KR; Davies, GR; Chaplin, M; De Vos, M; Schumacher, SG; Warren, R; Theron, G","Xpert MTB/XDR for detection of pulmonary tuberculosis and resistance to isoniazid, fluoroquinolones, ethionamide, and amikacin","Cochrane Database of Systematic Reviews","2022","Abstract - Background The World Health Organization (WHO) End TB Strategy stresses universal access to drug susceptibility testing (DST). DST determines whether  Mycobacterium tuberculosis  bacteria are susceptible or resistant to drugs. Xpert MTB/XDR is a rapid nucleic acid amplification test for detection of tuberculosis and drug resistance in one test suitable for use in peripheral and intermediate level laboratories. In specimens where tuberculosis is detected by Xpert MTB/XDR, Xpert MTB/XDR can also detect resistance to isoniazid, fluoroquinolones, ethionamide, and amikacin. Objectives To assess the diagnostic accuracy of Xpert MTB/XDR for pulmonary tuberculosis in people with presumptive pulmonary tuberculosis (having signs and symptoms suggestive of tuberculosis, including cough, fever, weight loss, night sweats). To assess the diagnostic accuracy of Xpert MTB/XDR for resistance to isoniazid, fluoroquinolones, ethionamide, and amikacin in people with tuberculosis detected by Xpert MTB/XDR, irrespective of rifampicin resistance (whether or not rifampicin resistance status was known) and with known rifampicin resistance. Search methods We searched multiple databases to 23 September 2021. We limited searches to 2015 onwards as Xpert MTB/XDR was launched in 2020. Selection criteria Diagnostic accuracy studies using sputum in adults with presumptive or confirmed pulmonary tuberculosis. Reference standards were culture (pulmonary tuberculosis detection); phenotypic DST (pDST), genotypic DST (gDST),composite (pDST and gDST) (drug resistance detection). Data collection and analysis Two review authors independently reviewed reports for eligibility and extracted data using a standardized form. For multicentre studies, we anticipated variability in the type and frequency of mutations associated with resistance to a given drug at the different centres and considered each centre as an independent study cohort for quality assessment and analysis. We assessed methodological quality with QUADAS‐2, judging risk of bias separately for each target condition and reference standard. For pulmonary tuberculosis detection, owing to heterogeneity in participant characteristics and observed specificity estimates, we reported a range of sensitivity and specificity estimates and did not perform a meta‐analysis. For drug resistance detection, we performed meta‐analyses by reference standard using bivariate random‐effects models. Using GRADE, we assessed certainty of evidence of Xpert MTB/XDR accuracy for detection of resistance to isoniazid and fluoroquinolones in people irrespective of rifampicin resistance and to ethionamide and amikacin in people with known rifampicin resistance, reflecting real‐world situations. We used pDST, except for ethionamide resistance where we considered gDST a better reference standard. Main results We included two multicentre studies from high multidrug‐resistant/rifampicin‐resistant tuberculosis burden countries, reporting on six independent study cohorts, involving 1228 participants for pulmonary tuberculosis detection and 1141 participants for drug resistance detection. The proportion of participants with rifampicin resistance in the two studies was 47.9% and 80.9%. For tuberculosis detection, we judged high risk of bias for patient selection owing to selective recruitment. For ethionamide resistance detection, we judged high risk of bias for the reference standard, both pDST and gDST, though we considered gDST a better reference standard. Pulmonary tuberculosis detection ‐ Xpert MTB/XDR sensitivity range, 98.3% (96.1 to 99.5) to 98.9% (96.2 to 99.9) and specificity range, 22.5% (14.3 to 32.6) to 100.0% (86.3 to 100.0); median prevalence of pulmonary tuberculosis 91.3%, (interquartile range, 89.3% to 91.8%), (2 studies; 1 study reported on 2 cohorts, 1228 participants; very low‐certainty evidence, sensitivity and specificity). Drug resistance detection People irrespective of rifampicin resistance ‐ Isoniazid resistance: Xpert MTB/XDR summary sensitivity and specificity (95% confidence interval (CI)) were 94.2% (87.5 to 97.4) and 98.5% (92.6 to 99.7) against pDST, (6 cohorts, 1083 participants, moderate‐certainty evidence, sensitivity and specificity). ‐ Fluoroquinolone resistance: Xpert MTB/XDR summary sensitivity and specificity were 93.2% (88.1 to 96.2) and 98.0% (90.8 to 99.6) against pDST, (6 cohorts, 1021 participants; high‐certainty evidence, sensitivity; moderate‐certainty evidence, specificity). People with known rifampicin resistance ‐ Ethionamide resistance: Xpert MTB/XDR summary sensitivity and specificity were 98.0% (74.2 to 99.9) and 99.7% (83.5 to 100.0) against gDST, (4 cohorts, 434 participants; very low‐certainty evidence, sensitivity and specificity). ‐ Amikacin resistance: Xpert MTB/XDR summary sensitivity and specificity were 86.1% (75.0 to 92.7) and 98.9% (93.0 to 99.8) against pDST, (4 cohorts, 490 participants; low‐certainty evidence, sensitivity; high‐certainty evidence, specificity). Of 1000 people with pulmonary tuberculosis, detected as tuberculosis by Xpert MTB/XDR: ‐ where 50 have isoniazid resistance, 61 would have an Xpert MTB/XDR result indicating isoniazid resistance: of these, 14/61 (23%) would not have isoniazid resistance (FP); 939 (of 1000 people) would have a result indicating the absence of isoniazid resistance: of these, 3/939 (0%) would have isoniazid resistance (FN). ‐ where 50 have fluoroquinolone resistance, 66 would have an Xpert MTB/XDR result indicating fluoroquinolone resistance: of these, 19/66 (29%) would not have fluoroquinolone resistance (FP); 934 would have a result indicating the absence of fluoroquinolone resistance: of these, 3/934 (0%) would have fluoroquinolone resistance (FN). ‐ where 300 have ethionamide resistance, 296 would have an Xpert MTB/XDR result indicating ethionamide resistance: of these, 2/296 (1%) would not have ethionamide resistance (FP); 704 would have a result indicating the absence of ethionamide resistance: of these, 6/704 (1%) would have ethionamide resistance (FN). ‐ where 135 have amikacin resistance, 126 would have an Xpert MTB/XDR result indicating amikacin resistance: of these, 10/126 (8%) would not have amikacin resistance (FP); 874 would have a result indicating the absence of amikacin resistance: of these, 19/874 (2%) would have amikacin resistance (FN). Authors' conclusions Review findings suggest that, in people determined by Xpert MTB/XDR to be tuberculosis‐positive, Xpert MTB/XDR provides accurate results for detection of isoniazid and fluoroquinolone resistance and can assist with selection of an optimised treatment regimen. Given that Xpert MTB/XDR targets a limited number of resistance variants in specific genes, the test may perform differently in different settings. Findings in this review should be interpreted with caution. Sensitivity for detection of ethionamide resistance was based only on Xpert MTB/XDR detection of mutations in the  inhA  promoter region, a known limitation. High risk of bias limits our confidence in Xpert MTB/XDR accuracy for pulmonary tuberculosis. Xpert MTB/XDR's impact will depend on its ability to detect tuberculosis (required for DST), prevalence of resistance to a given drug, health care infrastructure, and access to other tests. Plain language summary Xpert MTB/XDR, a rapid test for resistance to tuberculosis drugs Why is improving the diagnosis of tuberculosis drug resistance important? Tuberculosis tests, like Xpert MTB/RIF, Xpert MTB/RIF Ultra, and Truenat, only diagnose rifampicin resistance, but do not provide information about resistance to other drugs used to treat tuberculosis. This information is needed to allow for effective treatment to be started quickly. Not recognizing tuberculosis drug resistance when present (false negative, FN) may result in severe illness and death. An incorrect diagnosis of tuberculosis drug resistance (false positive, FP) may result in stigma and prolonged and unnecessary treatment with less effective drugs that have more side effects. What is the aim of this review? How accurate is Xpert MTB/XDR for detecting pulmonary tuberculosis and resistance to tuberculosis drugs (i.e. isoniazid, fluoroquinolones, ethionamide, and amikacin) in adults? What was studied in the review? Xpert MTB/XDR is a rapid test for detecting tuberculosis and drug resistance in one test, suitable for laboratories that do not require advanced skills and infrastructure. We assessed Xpert MTB/XDR accuracy against three reference standards. What are the main results of the review? We identified two multicentre studies reporting on six separate cohorts (groups of study participants), 1228 participants for pulmonary tuberculosis detection and 1141 participants for drug resistance detection. For pulmonary tuberculosis detection, we included two studies (one reporting on two separate cohorts). We did not determine an overall summary of Xpert MTB/XDR accuracy. If Xpert MTB/XDR were to be used in 1000 people with suspected tuberculosis of whom 100 have tuberculosis: ‐ an estimated 98 to 99 people would have an Xpert MTB/XDR result indicating tuberculosis: of these 1 to 2 (1%) would not have tuberculosis (FP); and 203 to 900 people would have a result indicating the absence of tuberculosis: of these 0 to 697 (0% to 77%) would have tuberculosis (FN). Drug resistance detection Of 1000 people detected as tuberculosis positive by Xpert MTB/XDR: ‐ where 50 have isoniazid resistance, an estimated 61 would have an Xpert MTB/XDR result indicating isoniazid resistance: of these, 14/61 (23%) would not have isoniazid resistance (FP); and 939 (of the 1000 people) would have an Xpert MTB/XDR result indicating the absence of isoniazid resistance: of these, 3/939 (0%) would have isoniazid resistance (FN); ‐ where 50 have isoniazid resistance, 61 (of 1000 people) would have an Xpert MTB/XDR result indicating isoniazid resistance: of these, 14/61 (23%) would not have isoniazid resistance (FP); and 939 (of 1000 people) would have a result indicating the absence of isoniazid resistance: of these, 3/939 (0%) would have isoniazid resistance (FN); ‐ where 50 have fluoroquinolone resistance, 66 would have an Xpert MTB/XDR result indicating fluoroquinolone resistance: of these, 19/66 (29%) would not have fluoroquinolone resistance (FP); and 934 would have a result indicating the absence of fluoroquinolone resistance: of these, 3/934 (0%) would have fluoroquinolone resistance (FN); ‐ where 300 have ethionamide resistance, 296 would have an Xpert MTB/XDR result indicating ethionamide resistance: of these, 2/296 (1%) would not have ethionamide resistance (FP); and 704 would have a result indicating the absence of ethionamide resistance: of these, 6/704 (1%) would have ethionamide resistance (FN); ‐ where 135 have amikacin resistance, 126 would have an Xpert MTB/XDR result indicating amikacin resistance: of these, 10/126 (8%) would not have amikacin resistance (FP); and 874 would have a result indicating the absence of amikacin resistance: of these, 19/874 (2%) would have amikacin resistance (FN). How reliable are the results of the studies in this review? For pulmonary tuberculosis detection, we did not consider the results reliable because around 90% of the participants had Xpert‐detected pulmonary tuberculosis to begin with due to the way people were chosen to participate in the studies. For drug resistance detection, we were confident in the results, except for results for ethionamide resistance detection, where the reference standards were not ideal. Who do the results of this review apply to? People with suspected pulmonary tuberculosis and tuberculosis drug resistance living in countries with a high burden of tuberculosis drug resistance. How up‐to‐date is this review? We searched for studies up to 23 September 2021. Searches were limited to 2015 onwards as Xpert MTB/XDR was launched in July 2020.","5","John Wiley & Sons, Ltd","1465-1858","*Antibiotics, Antitubercular [pharmacology, therapeutic use]; *Mycobacterium tuberculosis [genetics]; *Tuberculosis, Lymph Node [diagnosis]; *Tuberculosis, Multidrug-Resistant [diagnosis, drug therapy]; *Tuberculosis, Pulmonary [diagnosis, drug therapy]; Adult; Amikacin [pharmacology, therapeutic use]; Drug Resistance, Bacterial [genetics]; Ethionamide [pharmacology, therapeutic use]; Fluoroquinolones [pharmacology, therapeutic use]; Humans; Isoniazid [pharmacology, therapeutic use]; Microbial Sensitivity Tests; Rifampin [pharmacology, therapeutic use]; Sensitivity and Specificity","10.1002/14651858.CD014841.pub2","http://dx.doi.org/10.1002/14651858.CD014841.pub2","Infectious Diseases"
"CD013639.PUB5","Ebrahimzadeh, S; Islam, N; Dawit, H; Salameh, J-P; Kazi, S; Fabiano, N; Treanor, L; Absi, M; Ahmad, F; Rooprai, P; Al Khalil, A; Harper, K; Kamra, N; Leeflang, MMG; Hooft, L; van der Pol, CB; Prager, R; Hare, SS; Dennie, C; Spijker, R; Deeks, JJ; Dinnes, J; Jenniskens, K; Korevaar, DA; Cohen, JF; Van den Bruel, A; Takwoingi, Y; van de Wijgert, J; Wang, J; Pena, E; Sabongui, S; McInnes, MDF","Thoracic imaging tests for the diagnosis of COVID‐19","Cochrane Database of Systematic Reviews","2022","Abstract - Background Our March 2021 edition of this review showed thoracic imaging computed tomography (CT) to be sensitive and moderately specific in diagnosing COVID‐19 pneumonia. This new edition is an update of the review. Objectives Our objectives were to evaluate the diagnostic accuracy of thoracic imaging in people with suspected COVID‐19; assess the rate of positive imaging in people who had an initial reverse transcriptase polymerase chain reaction (RT‐PCR) negative result and a positive RT‐PCR result on follow‐up; and evaluate the accuracy of thoracic imaging for screening COVID‐19 in asymptomatic individuals. The secondary objective was to assess threshold effects of index test positivity on accuracy. Search methods We searched the COVID‐19 Living Evidence Database from the University of Bern, the Cochrane COVID‐19 Study Register, The Stephen B. Thacker CDC Library, and repositories of COVID‐19 publications through to 17 February 2021. We did not apply any language restrictions. Selection criteria We included diagnostic accuracy studies of all designs, except for case‐control, that recruited participants of any age group suspected to have COVID‐19. Studies had to assess chest CT, chest X‐ray, or ultrasound of the lungs for the diagnosis of COVID‐19, use a reference standard that included RT‐PCR, and report estimates of test accuracy or provide data from which we could compute estimates. We excluded studies that used imaging as part of the reference standard and studies that excluded participants with normal index test results. Data collection and analysis The review authors independently and in duplicate screened articles, extracted data and assessed risk of bias and applicability concerns using QUADAS‐2. We presented sensitivity and specificity per study on paired forest plots, and summarized pooled estimates in tables. We used a bivariate meta‐analysis model where appropriate. Main results We included 98 studies in this review. Of these, 94 were included for evaluating the diagnostic accuracy of thoracic imaging in the evaluation of people with suspected COVID‐19. Eight studies were included for assessing the rate of positive imaging in individuals with initial RT‐PCR negative results and positive RT‐PCR results on follow‐up, and 10 studies were included for evaluating the accuracy of thoracic imaging for imagining asymptomatic individuals. For all 98 included studies, risk of bias was high or unclear in 52 (53%) studies with respect to participant selection, in 64 (65%) studies with respect to reference standard, in 46 (47%) studies with respect to index test, and in 48 (49%) studies with respect to flow and timing. Concerns about the applicability of the evidence to: participants were high or unclear in eight (8%) studies; index test were high or unclear in seven (7%) studies; and reference standard were high or unclear in seven (7%) studies. Imaging in people with suspected COVID‐19 We included 94 studies. Eighty‐seven studies evaluated one imaging modality, and seven studies evaluated two imaging modalities. All studies used RT‐PCR alone or in combination with other criteria (for example, clinical signs and symptoms, positive contacts) as the reference standard for the diagnosis of COVID‐19. For chest CT (69 studies, 28285 participants, 14,342 (51%) cases), sensitivities ranged from 45% to 100%, and specificities from 10% to 99%. The pooled sensitivity of chest CT was 86.9% (95% confidence interval (CI) 83.6 to 89.6), and pooled specificity was 78.3% (95% CI 73.7 to 82.3). Definition for index test positivity was a source of heterogeneity for sensitivity, but not specificity. Reference standard was not a source of heterogeneity. For chest X‐ray (17 studies, 8529 participants, 5303 (62%) cases), the sensitivity ranged from 44% to 94% and specificity from 24 to 93%. The pooled sensitivity of chest X‐ray was 73.1% (95% CI 64.1 to 80.5), and pooled specificity was 73.3% (95% CI 61.9 to 82.2). Definition for index test positivity was not found to be a source of heterogeneity. Definition for index test positivity and reference standard were not found to be sources of heterogeneity. For ultrasound of the lungs (15 studies, 2410 participants, 1158 (48%) cases), the sensitivity ranged from 73% to 94% and the specificity ranged from 21% to 98%. The pooled sensitivity of ultrasound was 88.9% (95% CI 84.9 to 92.0), and the pooled specificity was 72.2% (95% CI 58.8 to 82.5). Definition for index test positivity and reference standard were not found to be sources of heterogeneity. Indirect comparisons of modalities evaluated across all 94 studies indicated that chest CT and ultrasound gave higher sensitivity estimates than X‐ray (P = 0.0003 and P = 0.001, respectively). Chest CT and ultrasound gave similar sensitivities (P = 0.42). All modalities had similar specificities (CT versus X‐ray P = 0.36; CT versus ultrasound P = 0.32; X‐ray versus ultrasound P = 0.89). Imaging in PCR‐negative people who subsequently became positive For rate of positive imaging in individuals with initial RT‐PCR negative results, we included 8 studies (7 CT, 1 ultrasound) with a total of 198 participants suspected of having COVID‐19, all of whom had a final diagnosis of COVID‐19. Most studies (7/8) evaluated CT. Of 177 participants with initially negative RT‐PCR who had positive RT‐PCR results on follow‐up testing, 75.8% (95% CI 45.3 to 92.2) had positive CT findings. Imaging in asymptomatic PCR‐positive people For imaging asymptomatic individuals, we included 10 studies (7 CT, 1 X‐ray, 2 ultrasound) with a total of 3548 asymptomatic participants, of whom 364 (10%) had a final diagnosis of COVID‐19. For chest CT (7 studies, 3134 participants, 315 (10%) cases), the pooled sensitivity was 55.7% (95% CI 35.4 to 74.3) and the pooled specificity was 91.1% (95% CI 82.6 to 95.7). Authors' conclusions Chest CT and ultrasound of the lungs are sensitive and moderately specific in diagnosing COVID‐19. Chest X‐ray is moderately sensitive and moderately specific in diagnosing COVID‐19. Thus, chest CT and ultrasound may have more utility for ruling out COVID‐19 than for differentiating SARS‐CoV‐2 infection from other causes of respiratory illness. The uncertainty resulting from high or unclear risk of bias and the heterogeneity of included studies limit our ability to confidently draw conclusions based on our results. Plain language summary How accurate is chest imaging for diagnosing COVID‐19? Why is this question important? People with suspected COVID‐19 need to know quickly whether they are infected, so they can receive appropriate treatment, self‐isolate, and inform close contacts. Currently, a formal diagnosis of COVID‐19 requires a laboratory test (RT‐PCR) of nose and throat samples. RT‐PCR requires specialist equipment and takes at least 24 hours to produce a result. It is not completely accurate, and may require a second RT‐PCR or a different test to confirm diagnosis. Clinicians may use chest imaging to diagnose people who have COVID‐19 symptoms, while awaiting RT‐PCR results or when RT‐PCR results are negative, and the person has COVID‐19 symptoms. This is the fourth version of this review. What did we want to find out? We wanted to know whether chest imaging is accurate enough to diagnose COVID‐19 in people with suspected infection; we included studies in people with suspected COVID‐19 only and excluded studies in people with confirmed COVID‐19. We also wanted to assess the accuracy of chest imaging for screening asymptomatic people. The evidence is up to date to 17 February 2021. What are chest imaging tests? X‐rays or scans produce an image of the organs and structures in the chest. ‐ X‐rays (radiography) use radiation to produce a 2‐D image. Usually done in hospitals, using fixed equipment by a radiographer; they can also be done on portable machines. ‐ Computed tomography (CT) scans use a computer to merge 2‐D X‐ray images and convert them to a 3‐D image. They require highly‐specialized equipment and are done in hospital by a specialist radiographer. ‐ Ultrasound scans use high‐frequency sound waves to produce an image. They can be done in hospitals or other healthcare settings, such as a doctor’s office. What did we do? We searched for studies that assessed the accuracy of chest imaging to diagnose COVID‐19 in people of any age with suspected COVID‐19. We included studies with ‘symptomatic' or 'mixed populations'. What did we find? We found 94 studies with 37,631 participants (of whom 19,768 (53%) had a final diagnosis of COVID‐19) for evaluating the diagnostic accuracy of thoracic imaging in the evaluation of people with suspected COVID‐19. Eighty‐seven studies evaluated one imaging modality, and seven studies evaluated two imaging modalities. All 94 studies used RT‐PCR either alone or in combination with other criteria (such as clinical signs and symptoms, or positive contacts) as the reference standard for the diagnosis of COVID‐19. Chest CT: suspected people Pooled results showed that chest CT (69 studies) correctly diagnosed COVID‐19 in 87% of people who had COVID‐19. However, it incorrectly identified COVID‐19 in 21% of people who did not have COVID‐19. Chest X‐ray: suspected people Pooled results showed that chest X‐ray (17 studies) correctly diagnosed COVID‐19 in 73 % of people who had COVID‐19. However, it incorrectly identified COVID‐19 in 27% of people who did not have COVID‐19. Lung ultrasound: suspected people Pooled results showed that lung ultrasound (15 studies) correctly diagnosed COVID‐19 in 87% of people with COVID‐19. However, it incorrectly diagnosed COVID‐19 in 24% of people who did not have COVID‐19. Screening asymptomatic people We included 10 studies (7 CT, 1 X‐ray, 2 ultrasound) with 3548 asymptomatic participants, of whom 364 (10%) had a final diagnosis of COVID‐19. Pooled results of seven studies, showed that CT correctly diagnosed COVID‐19 in 56% of people who had COVID‐19, and incorrectly identified COVID‐19 in 8% of people who did not have COVID‐19. How reliable are the results? The studies differed from each other and used different methods to report their results. Very few studies directly compared one type of imaging test with another. Also, the risk of bias was high or unclear in about half of all included studies. Therefore, it is difficult to draw confident conclusions. What does this mean? The evidence suggests that chest CT and ultrasound are better at ruling out COVID‐19 infection than distinguishing it from other respiratory problems. So, their usefulness may be limited to excluding COVID‐19 infection rather than differentiating it from other causes of lung infection. In addition, chest CT imaging had poor sensitivity and high specificity for detecting asymptomatic individuals.","5","John Wiley & Sons, Ltd","1465-1858","*COVID-19 [diagnostic imaging]; Humans; SARS-CoV-2; Sensitivity and Specificity; Tomography, X-Ray Computed; Ultrasonography","10.1002/14651858.CD013639.pub5","http://dx.doi.org/10.1002/14651858.CD013639.pub5","Infectious Diseases"
"CD012809.PUB2","Tsujimoto, Y; Kumasawa, J; Shimizu, S; Nakano, Y; Kataoka, Y; Tsujimoto, H; Kono, M; Okabayashi, S; Imura, H; Mizuta, T","Doppler trans‐thoracic echocardiography for detection of pulmonary hypertension in adults","Cochrane Database of Systematic Reviews","2022","Abstract - Background Pulmonary hypertension (PH) is an important cause of morbidity and mortality, which leads to a substantial loss of exercise capacity. PH ultimately leads to right ventricular overload and subsequent heart failure and early death. Although early detection and treatment of PH are recommended, due to the limited responsiveness to therapy at late disease stages, many patients are diagnosed at a later stage of the disease because symptoms and signs of PH are nonspecific at earlier stages. While direct pressure measurement with right‐heart catheterisation is the clinical reference standard for PH, it is not routinely used due to its invasiveness and complications. Trans‐thoracic Doppler echocardiography is less invasive, less expensive, and widely available compared to right‐heart catheterisation; it is therefore recommended that echocardiography be used as an initial diagnosis method in guidelines. However, several studies have questioned the accuracy of noninvasively measured pulmonary artery pressure. There is substantial uncertainty about the diagnostic accuracy of echocardiography for the diagnosis of PH. Objectives To determine the diagnostic accuracy of trans‐thoracic Doppler echocardiography for detecting PH. Search methods We searched MEDLINE, Embase, Web of Science Core Collection, ClinicalTrials.gov, World Health Organization International Clinical Trials Registry Platform from database inception to August 2021, reference lists of articles, and contacted study authors. We applied no restrictions on language or type of publication. Selection criteria We included studies that evaluated the diagnostic accuracy of trans‐thoracic Doppler echocardiography for detecting PH, where right‐heart catheterisation was the reference standard. We excluded diagnostic case‐control studies (two‐gate design), studies where right‐heart catheterisation was not the reference standard, and those in which the reference standard threshold differed from 25 mmHg. We also excluded studies that did not provide sufficient diagnostic test accuracy data (true‐positive [TP], false‐positive [FP], true‐negative [TN], and false‐negative [FN] values, based on the reference standard). We included studies that provided data from which we could extract TP, FP, TN, and FN values, based on the reference standard. Two authors independently screened and assessed the eligibility based on the titles and abstracts of records identified by the search. After the title and abstract screening, the full‐text reports of all potentially eligible studies were obtained, and two authors independently assessed the eligibility of the full‐text reports. Data collection and analysis Two review authors independently assessed the risk of bias and extracted data from each of the included studies. We contacted the authors of the included studies to obtain missing data. We assessed the methodological quality of studies using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. We estimated a summary receiver operating characteristic (SROC) curve by fitting a hierarchical summary ROC (HSROC) non‐linear mixed model. We explored sources of heterogeneity regarding types of PH, methods to estimate the right atrial pressure, and threshold of index test to diagnose PH. All analyses were performed using the Review Manager 5, SAS and STATA statistical software. Main results We included 17 studies (comprising 3656 adult patients) assessing the diagnostic accuracy of Doppler trans‐thoracic echocardiography for the diagnosis of PH. The included studies were heterogeneous in terms of patient distribution of age, sex, WHO classification, setting, country, positivity threshold, and year of publication. The prevalence of PH reported in the included studies varied widely (from 6% to 88%). The threshold of index test for PH diagnosis varied widely (from 30 mmHg to 47 mmHg) and was not always prespecified. No study was assigned low risk of bias or low concern in each QUADAS‐2 domain assessed. Poor reporting, especially in the index test and reference standard domains, hampered conclusive judgement about the risk of bias. There was little consistency in the thresholds used in the included studies; therefore, common thresholds contained very sparse data, which prevented us from calculating summary points of accuracy estimates. With a fixed specificity of 86% (the median specificity), the estimated sensitivity derived from the median value of specificity using HSROC model was 87% (95% confidence interval [CI]: 78% to 96%). Using a prevalence of PH of 68%, which was the median among the included studies conducted mainly in tertiary hospitals, diagnosing a cohort of 1000 adult patients under suspicion of PH would result in 88 patients being undiagnosed with PH (false negatives) and 275 patients would avoid unnecessary referral for a right‐heart catheterisation (true negatives). In addition, 592 of 1000 patients would receive an appropriate and timely referral for a right‐heart catheterisation (true positives), while 45 patients would be wrongly considered to have PH (false positives). Conversely, when we assumed low prevalence of PH (10%), as in the case of preoperative examinations for liver transplantation, the number of false negatives and false positives would be 13 and 126, respectively. Authors' conclusions Our evidence assessment of echocardiography for the diagnosis of PH in adult patients revealed several limitations. We were unable to determine the average sensitivity and specificity at any particular index test threshold and to explain the observed variability in results. The high heterogeneity of the collected data and the poor methodological quality would constrain the implementation of this result into clinical practice. Further studies relative to the accuracy of Doppler trans‐thoracic echocardiography for the diagnosis of PH in adults, that apply a rigorous methodology for conducting diagnostic test accuracy studies, are needed. Plain language summary Doppler trans‐thoracic echocardiography for detection of pulmonary hypertension in adults What was studied in this review? Pulmonary hypertension is high blood pressure in the blood vessels that supply blood from the right half of the heart to the lungs. It is a serious condition that can damage the right side of the heart. The walls in the blood vessels become thick and stiff which makes it harder for the blood to flow. This can lead to heart failure. Symptoms can include shortness of breath, tiredness, chest pain, a racing heartbeat or swelling in the lower limbs and abdomen. The symptoms can be similar to other heart and lung diseases, so diagnosis can take time. Early diagnosis is beneficial because treatment can start early. Starting treatment early is better because people respond better to treatment in the early stages of the disease. Not being diagnosed early can have severe consequences such as disability in daily life or death. The most accurate way to diagnose pulmonary hypertension is using a pressure measurement called right‐heart catheterisation. However, this is invasive and can cause complications. Another technique, called Doppler echocardiography is noninvasive, cheaper and more widely available in hospitals. Therefore, many guidelines recommend the use of echocardiography as an initial diagnosis method. We wanted to do this review because several studies have questioned the accuracy of echocardiography. We wanted to find out how good echocardiography is compared to right‐heart catheterisation for the diagnosis of pulmonary hypertension. What was the aim of this review? To evaluate evidence on the ability of echocardiography to identify pulmonary hypertension in adults compared to right‐heart catheterisation. What were the main results of the review? We found 17 studies involving 3656 people who had suspected pulmonary hypertension. There was a lot of variation in the studies. We found the characteristics of participants varied in terms of age, sex, cause of pulmonary hypertension, setting and country. The cut‐off values for the echocardiography readings chosen to diagnose pulmonary hypertension also varied. We used the available data to estimate how well the echocardiography performed compared to right‐heart catheterisation. In tertiary care hospitals, where most of the included studies were conducted, it is assumed that 680 of 1000 patients have pulmonary hypertension. We found that 592 people of 1000 would be correctly diagnosed with pulmonary hypertension using echocardiography. But 45 of 1000 patients would be wrongly considered as having pulmonary hypertension (false positive), while 88 of 1000 patients might be incorrectly considered as not having pulmonary hypertension (false negative) and 275 of 1000 patients would avoid unnecessary referral for a right‐heart catheterisation. In a scenario where the preoperative examination for liver transplantation is conducted, 100 of 1000 are assumed to have pulmonary hypertension. We found the number of false negatives and false positives would be 13 and 126, respectively. How reliable were the results of the studies? We judged the included studies to have important limitations in their validity, which means that they were at high risk of providing distorted results. Therefore, we cannot be certain if the number of false negatives is correct (i.e. it could be even higher). Who do the results of this review apply to? These results apply to adults who are suspected of having pulmonary hypertension. However, the diagnostic accuracy of echocardiography varied considerably among studies and it is yet unclear what causes this diversity of test accuracy. Of note, the results are from studies conducted in relatively high prevalence settings. Therefore, care should be taken when applying this result to individual situations. How up‐to‐date was the review? This review is current to August 2021.","5","John Wiley & Sons, Ltd","1465-1858","*Hypertension, Pulmonary [diagnostic imaging]; Adult; Echocardiography; Echocardiography, Doppler; Humans; Physical Examination [methods]; Sensitivity and Specificity","10.1002/14651858.CD012809.pub2","http://dx.doi.org/10.1002/14651858.CD012809.pub2","Airways"
"CD014798.PUB2","Nadarevic, T; Colli, A; Giljaca, V; Fraquelli, M; Casazza, G; Manzotti, C; Štimac, D; Miletic, D","Magnetic resonance imaging for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease","Cochrane Database of Systematic Reviews","2022","Abstract - Background Hepatocellular carcinoma occurs mostly in people with chronic liver disease and ranks sixth in terms of global incidence of cancer, and third in terms of cancer deaths. In clinical practice, magnetic resonance imaging (MRI) is used as a second‐line diagnostic imaging modality to confirm the presence of focal liver lesions suspected as hepatocellular carcinoma on prior diagnostic test such as abdominal ultrasound or alpha‐fetoprotein, or both, either in surveillance programmes or in clinical settings. According to current guidelines, a single contrast‐enhanced imaging study (computed tomography (CT) or MRI) showing typical hallmarks of hepatocellular carcinoma in people with cirrhosis is considered valid to diagnose hepatocellular carcinoma. The detection of hepatocellular carcinoma amenable to surgical resection could improve the prognosis. However, a significant number of hepatocellular carcinomas do not show typical hallmarks on imaging modalities, and hepatocellular carcinoma may, therefore, be missed. There is no clear evidence of the benefit of surveillance programmes in terms of overall survival: the conflicting results can be a consequence of inaccurate detection, ineffective treatment, or both. Assessing the diagnostic accuracy of MRI may clarify whether the absence of benefit could be related to underdiagnosis. Furthermore, an assessment of the accuracy of MRI in people with chronic liver disease who are not included in surveillance programmes is needed for either ruling out or diagnosing hepatocellular carcinoma. Objectives Primary: to assess the diagnostic accuracy of MRI for the diagnosis of hepatocellular carcinoma of any size and at any stage in adults with chronic liver disease. Secondary: to assess the diagnostic accuracy of MRI for the diagnosis of resectable hepatocellular carcinoma in adults with chronic liver disease, and to identify potential sources of heterogeneity in the results. Search methods We searched the Cochrane Hepato‐Biliary Group Controlled Trials Register, the Cochrane Hepato‐Biliary Group Diagnostic Test of Accuracy Studies Register, the Cochrane Library, MEDLINE, Embase, and three other databases to 9 November 2021. We manually searched articles retrieved, contacted experts, handsearched abstract books from meetings held during the last 10 years, and searched for literature in OpenGrey (9 November 2021). Further information was requested by e‐mails, but no additional information was provided. No data was obtained through correspondence with investigators. We applied no language or document‐type restrictions. Selection criteria Studies assessing the diagnostic accuracy of MRI for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease, with cross‐sectional designs, using one of the acceptable reference standards, such as pathology of the explanted liver and histology of resected or biopsied focal liver lesion with at least a six‐month follow‐up. Data collection and analysis At least two review authors independently screened studies, extracted data, and assessed the risk of bias and applicability concerns, using the QUADAS‐2 checklist. We presented the results of sensitivity and specificity, using paired forest plots, and we tabulated the results. We used a hierarchical meta‐analysis model where appropriate. We presented uncertainty of the accuracy estimates using 95% confidence intervals (CIs). We double‐checked all data extractions and analyses. Main results We included 34 studies, with 4841 participants. We judged all studies to be at high risk of bias in at least one domain because most studies used different reference standards, often inappropriate to exclude the presence of the target condition, and the time interval between the index test and the reference standard was rarely defined. Regarding applicability, we judged 15% (5/34) of studies to be at low concern and 85% (29/34) of studies to be at high concern mostly owing to characteristics of the participants, most of whom were on waiting lists for orthotopic liver transplantation, and due to pathology of the explanted liver being the only reference standard. MRI for hepatocellular carcinoma of any size and stage: sensitivity 84.4% (95% CI 80.1% to 87.9%) and specificity 93.8% (95% CI 90.1% to 96.1%) (34 studies, 4841 participants; low‐certainty evidence). MRI for resectable hepatocellular carcinoma: sensitivity 84.3% (95% CI 77.6% to 89.3%) and specificity 92.9% (95% CI 88.3% to 95.9%) (16 studies, 2150 participants; low‐certainty evidence). The observed heterogeneity in the results remains mostly unexplained. The sensitivity analyses, which included only studies with clearly prespecified positivity criteria and only studies in which the reference standard results were interpreted without knowledge of the results of the index test, showed no variation in the results. Authors' conclusions We found that using MRI as a second‐line imaging modality to diagnose hepatocellular carcinoma of any size and stage, 16% of people with hepatocellular carcinoma would be missed, and 6% of people without hepatocellular carcinoma would be unnecessarily treated. For resectable hepatocellular carcinoma, we found that 16% of people with resectable hepatocellular carcinoma would improperly not be resected, while 7% of people without hepatocellular carcinoma would undergo inappropriate surgery. The uncertainty resulting from the high risk of bias in the included studies and concerns regarding their applicability limit our ability to confidently draw conclusions based on our results. Plain language summary How accurate are magnetic resonance imaging (MRI) scans for detecting liver cancer? Key messages In people with chronic liver disease, magnetic resonance imaging (MRI: cross‐sectional scans inside the body) probably misses liver cancer in 16% of people, who would not receive timely or appropriate treatment, and incorrectly finds liver cancer in 6% of people, who would receive unnecessary treatment. MRI probably misses liver cancer in 16% of people with liver cancer who could have surgery to remove part of their liver, and incorrectly finds liver cancer in 7% of people who undergo inappropriate surgery. The studies were at high risk of bias and too different from each other to allow us to draw firm conclusions based on the evidence. Why is it important to diagnose liver cancer accurately? Liver cancer, or 'hepatocellular carcinoma', occurs mostly in people with chronic liver disease, regardless of the cause. It is the sixth most common cancer in the world and the third most common cause of deaths due to cancer. It is difficult to diagnose because early symptoms are similar to those of liver disease. People with blood test or ultrasound results that suggest liver cancer may go on to have further tests, such as scans that produce images of the liver, or biopsy where a small piece of the liver is removed and examined. If liver cancer is detected early, people may be treated with surgery to remove part of the liver (called a liver resection) or with a liver transplant. If the liver cancer is more advanced, they may need chemotherapy. If liver cancer is missed at the diagnostic test, people will not receive appropriate treatment. However, incorrectly diagnosing liver cancer when it is not present means that people may undergo unnecessary testing or treatment. What is magnetic resonance imaging (MRI) and how might it diagnose liver cancer? MRI produces images that show a cross‐section or 'slice' of the bones, blood vessels, and tissues inside the body. The images are a series of signal intensities that are directed and combined by a computer. MRI scans can detect the presence of abnormalities in the liver that might be cancer. Current guidelines recommend using either MRI or another type of imaging, computed tomography, or a combination to confirm the presence of liver cancer in people who might have liver cancer. What did we want to find out? We wanted to find out if MRI is accurate enough to diagnose liver cancer in adults with chronic liver disease. We were interested first, in liver cancers of any size and stage and second, in liver cancers that were suitable for resection. What did we do? We searched for studies that assessed the accuracy of MRI scans compared to the best available tests to confirm liver cancer in adults with chronic liver disease. The best available tests are examination of the liver, or part of the liver under a microscope. What did we find? We found 34 studies assessing 4841 people. Around 560 of 1000 (56%) adults with chronic liver disease have confirmed liver cancer. Of these 1000 people, MRI may: ‐ correctly detect liver cancer in 473 people; ‐ miss liver cancer in 87 people; ‐ incorrectly detect liver cancer in 27 cancer‐free people; ‐ correctly detect no liver cancer in 413 people. Based on the studies, around 560 of 1000 (56%) adults with chronic liver disease have confirmed resectable liver cancer. Of these 1000 people, MRI may: ‐ correctly detect resectable liver cancer in 472 people; ‐ miss resectable liver cancer in 88 people; ‐ incorrectly detect resectable liver cancer in 31 people; ‐ correctly detect no resectable liver cancer in 409 people. What are the limitations of the evidence? Our confidence in the evidence is limited because the studies used different methods to select study participants and used different definitions for the presence of liver disease. This means MRI scans could be more or less accurate than suggested by our analyses of the evidence. How up to date is this evidence? The evidence is up to date to 9 November 2021.","5","John Wiley & Sons, Ltd","1465-1858","*Carcinoma, Hepatocellular [complications, diagnostic imaging]; *Liver Neoplasms [complications, diagnostic imaging]; Adult; Cross-Sectional Studies; Humans; Magnetic Resonance Imaging; Sensitivity and Specificity; Ultrasonography","10.1002/14651858.CD014798.pub2","http://dx.doi.org/10.1002/14651858.CD014798.pub2","Hepato-Biliary"
"CD013724.PUB2","Beishon, LC; Elliott, E; Hietamies, TM; Mc Ardle, R; O'Mahony, A; Elliott, AR; Quinn, TJ","Diagnostic test accuracy of remote, multidomain cognitive assessment (telephone and video call) for dementia","Cochrane Database of Systematic Reviews","2022","Abstract - Background Remote cognitive assessments are increasingly needed to assist in the detection of cognitive disorders, but the diagnostic accuracy of telephone‐ and video‐based cognitive screening remains unclear. Objectives To assess the test accuracy of any multidomain cognitive test delivered remotely for the diagnosis of any form of dementia. To assess for potential differences in cognitive test scoring when using a remote platform, and where a remote screener was compared to the equivalent face‐to‐face test. Search methods We searched ALOIS, the Cochrane Dementia and Cognitive Improvement Group Specialized Register, CENTRAL, MEDLINE, Embase, PsycINFO, CINAHL, Web of Science, LILACS, and ClinicalTrials.gov ( www.clinicaltrials.gov/ ) databases on 2 June 2021. We performed forward and backward searching of included citations. Selection criteria We included cross‐sectional studies, where a remote, multidomain assessment was administered alongside a clinical diagnosis of dementia or equivalent face‐to‐face test. Data collection and analysis Two review authors independently assessed risk of bias and extracted data; a third review author moderated disagreements. Our primary analysis was the accuracy of remote assessments against a clinical diagnosis of dementia. Where data were available, we reported test accuracy as sensitivity and specificity. We did not perform quantitative meta‐analysis as there were too few studies at individual test level. For those studies comparing remote versus in‐person use of an equivalent screening test, if data allowed, we described correlations, reliability, differences in scores and the proportion classified as having cognitive impairment for each test. Main results The review contains 31 studies (19 differing tests, 3075 participants), of which seven studies (six telephone, one video call, 756 participants) were relevant to our primary objective of describing test accuracy against a clinical diagnosis of dementia. All studies were at unclear or high risk of bias in at least one domain, but were low risk in applicability to the review question. Overall, sensitivity of remote tools varied with values between 26% and 100%, and specificity between 65% and 100%, with no clearly superior test. Across the 24 papers comparing equivalent remote and in‐person tests (14 telephone, 10 video call), agreement between tests was good, but rarely perfect (correlation coefficient range: 0.48 to 0.98). Authors' conclusions Despite the common and increasing use of remote cognitive assessment, supporting evidence on test accuracy is limited. Available data do not allow us to suggest a preferred test. Remote testing is complex, and this is reflected in the heterogeneity seen in tests used, their application, and their analysis. More research is needed to describe accuracy of contemporary approaches to remote cognitive assessment. While data comparing remote and in‐person use of a test were reassuring, thresholds and scoring rules derived from in‐person testing may not be applicable when the equivalent test is adapted for remote use. Plain language summary How accurate are remote, virtual assessments at diagnosing dementia? Why is this question important? Dementia is a chronic and progressive condition that affects peoples' memory and ability to function day‐to‐day. A clinical diagnosis of dementia usually involves brain scans, physical examinations and history taking. As a first step, we often use memory and thinking tests to identify people who need further assessment. Traditionally these tests are performed in‐person, but modifications of the tests allow them to be used over the telephone or via video calls – sometimes called 'remote assessment'. The need for remote assessment has become particularly urgent due to COVID‐19. However, there are potential benefits of remote assessment beyond the COVID‐19 pandemic. Physically attending appointments can be difficult for some people and remote assessments offer greater convenience. Remote assessments are also useful in research, as a large number of people can be reached in a fairly short amount of time. A test delivered by telephone may not be as good as the in‐person equivalent, and getting these tests right is important. One the one hand, If a test suggests someone has dementia when they do not (called a false positive), this can have an emotional impact on the person and their family. On the other hand, not identifying memory and thinking problems when they are present (called a false negative), mean that the person does not get the treatment and support that they need. What was the aim of this review? We aimed to assess whether memory and thinking tests carried out by telephone or video call can detect dementia. What was studied in this review? We looked at various memory and thinking tests. Many tests have been developed over time and they differ in their content and application, but most are based on a modification of a traditional in‐person test. What were the main results of this review? The review included 31 studies, using 19 different memory tests, with a total of 3075 participants. Only seven tests were relevant to our question regarding accuracy of remote testing. With the limited number of studies, estimates on the accuracy of these tests are imprecise. Our review suggests that remote tests could correctly identify people with dementia between 26% and 100% of the time, and could correctly rule out dementia 65% to 100% of the time. The remaining 24 studies compared a remote test with the face‐to‐face equivalent. These studies suggested that remote test scores usually agreed with in‐person testing, but this was not perfect. How reliable are the results of the studies in this review? In these studies, a clinical diagnosis of dementia was used as the reference (gold) standard. We identified a number of issues in the design, conduct and reporting of the studies. A particular issue was around the selection of participants for the studies. Studies often did not include people with hearing or language impairments that may have complicated remote testing. Who do the results of this study apply to? Most studies investigated older adults (over 65 years). The findings may not be representative of all older adults with dementia, as some studies only examined specific groups of people, for example, after stroke. The studies were usually performed in specialist centres by experts. So, we do not know how well these tests identify dementia in routine community practice. What are the implications of this review? The review highlights the lack of high‐quality research describing accuracy of telephone‐ and video call‐based memory and thinking tests. There were many differences between the studies included in this review such as the type of test used, participants included, the setting in which the study is carried out and language studied. This made comparisons between studies difficult. Our review suggests that remote assessments and in‐person assessments are not always equivalent. In situations where access to in‐person assessment is difficult, remote testing could be used as a useful first step. Ideally, this should be followed up with an in‐person assessment before a diagnosis is made. Due to limited studies, and differences in the way studies were carried out, we cannot recommend one particular remote test for the assessment of dementia. How up to date is this review? This search was performed in June 2021.","4","John Wiley & Sons, Ltd","1465-1858","*Dementia [diagnosis]; Cognition; Cross-Sectional Studies; Diagnostic Tests, Routine; Humans; Reproducibility of Results; Sensitivity and Specificity; Telephone","10.1002/14651858.CD013724.pub2","http://dx.doi.org/10.1002/14651858.CD013724.pub2","Dementia and Cognitive Improvement"
"CD010890.PUB2","van't Hoog, A; Viney, K; Biermann, O; Yang, B; Leeflang, MMG; Langendam, MW","Symptom‐ and chest‐radiography screening for active pulmonary tuberculosis in HIV‐negative adults and adults with unknown HIV status","Cochrane Database of Systematic Reviews","2022","Abstract - Background Systematic screening in high‐burden settings is recommended as a strategy for early detection of pulmonary tuberculosis disease, reducing mortality, morbidity and transmission, and improving equity in access to care. Questioning for symptoms and chest radiography (CXR) have historically been the most widely available tools to screen for tuberculosis disease. Their accuracy is important for the design of tuberculosis screening programmes and determines, in combination with the accuracy of confirmatory diagnostic tests, the yield of a screening programme and the burden on individuals and the health service. Objectives To assess the sensitivity and specificity of questioning for the presence of one or more tuberculosis symptoms or symptom combinations, CXR, and combinations of these as screening tools for detecting bacteriologically confirmed pulmonary tuberculosis disease in HIV‐negative adults and adults with unknown HIV status who are considered eligible for systematic screening for tuberculosis disease. Second, to investigate sources of heterogeneity, especially in relation to regional, epidemiological, and demographic characteristics of the study populations. Search methods We searched the MEDLINE, Embase, LILACS, and HTA (Health Technology Assessment) databases using pre‐specified search terms and consulted experts for unpublished reports, for the period 1992 to 2018. The search date was 10 December 2018. This search was repeated on 2 July 2021. Selection criteria Studies were eligible if participants were screened for tuberculosis disease using symptom questions, or abnormalities on CXR, or both, and were offered confirmatory testing with a reference standard. We included studies if diagnostic two‐by‐two tables could be generated for one or more index tests, even if not all participants were subjected to a microbacteriological reference standard. We excluded studies evaluating self‐reporting of symptoms. Data collection and analysis We categorized symptom and CXR index tests according to commonly used definitions. We assessed the methodological quality of included studies using the QUADAS‐2 instrument. We examined the forest plots and receiver operating characteristic plots visually for heterogeneity. We estimated summary sensitivities and specificities (and 95% confidence intervals (CI)) for each index test using bivariate random‐effects methods. We analyzed potential sources of heterogeneity in a hierarchical mixed‐model. Main results The electronic database search identified 9473 titles and abstracts. Through expert consultation, we identified 31 reports on national tuberculosis prevalence surveys as eligible (of which eight were already captured in the search of the electronic databases), and we identified 957 potentially relevant articles through reference checking. After removal of duplicates, we assessed 10,415 titles and abstracts, of which we identified 430 (4%) for full text review, whereafter we excluded 364 articles. In total, 66 articles provided data on 59 studies. We assessed the 2 July 2021 search results; seven studies were potentially eligible but would make no material difference to the review findings or grading of the evidence, and were not added in this edition of the review. We judged most studies at high risk of bias in one or more domains, most commonly because of incorporation bias and verification bias. We judged applicability concerns low in more than 80% of studies in all three domains. The three most common symptom index tests, cough for two or more weeks (41 studies), any cough (21 studies), and any tuberculosis symptom (29 studies), showed a summary sensitivity of 42.1% (95% CI 36.6% to 47.7%), 51.3% (95% CI 42.8% to 59.7%), and 70.6% (95% CI 61.7% to 78.2%, all very low‐certainty evidence), and a specificity of 94.4% (95% CI 92.6% to 95.8%, high‐certainty evidence), 87.6% (95% CI 81.6% to 91.8%, low‐certainty evidence), and 65.1% (95% CI 53.3% to 75.4%, low‐certainty evidence), respectively. The data on symptom index tests were more heterogenous than those for CXR. The studies on any tuberculosis symptom were the most heterogeneous, but had the lowest number of variables explaining this variation. Symptom index tests also showed regional variation. The summary sensitivity of any CXR abnormality (23 studies) was 94.7% (95% CI 92.2% to 96.4%, very low‐certainty evidence) and 84.8% (95% CI 76.7% to 90.4%, low‐certainty evidence) for CXR abnormalities suggestive of tuberculosis (19 studies), and specificity was 89.1% (95% CI 85.6% to 91.8%, low‐certainty evidence) and 95.6% (95% CI 92.6% to 97.4%, high‐certainty evidence), respectively. Sensitivity was more heterogenous than specificity, and could be explained by regional variation. The addition of cough for two or more weeks, whether to any (pulmonary) CXR abnormality or to CXR abnormalities suggestive of tuberculosis, resulted in a summary sensitivity and specificity of 99.2% (95% CI 96.8% to 99.8%) and 84.9% (95% CI 81.2% to 88.1%) (15 studies; certainty of evidence not assessed). Authors' conclusions The summary estimates of the symptom and CXR index tests may inform the choice of screening and diagnostic algorithms in any given setting or country where screening for tuberculosis is being implemented. The high sensitivity of CXR index tests, with or without symptom questions in parallel, suggests a high yield of persons with tuberculosis disease. However, additional considerations will determine the design of screening and diagnostic algorithms, such as the availability and accessibility of CXR facilities or the resources to fund them, and the need for more or fewer diagnostic tests to confirm the diagnosis (depending on screening test specificity), which also has resource implications. These review findings should be interpreted with caution due to methodological limitations in the included studies and regional variation in sensitivity and specificity. The sensitivity and specificity of an index test in a specific setting cannot be predicted with great precision due to heterogeneity. This should be borne in mind when planning for and implementing tuberculosis screening programmes. Plain language summary How accurate are asking about symptoms and doing a chest X‐ray to screen for tuberculosis of the lungs among adults who are HIV‐negative or with unknown HIV status? Why is improving screening for tuberculosis of the lungs important? Systematic screening in settings where tuberculosis is common is a recommended strategy for early detection of tuberculosis. Screening helps identify people who are more likely to have tuberculosis so they can have confirmatory testing. These are additional tests to confirm the presence of  Mycobacterium tuberculosis , the bacterium that causes tuberculosis. Asking about tuberculosis symptoms (for example, cough, coughing up blood, fever, and fatigue) and doing a chest X‐ray (CXR), which shows lung abnormalities, are commonly used screening methods. Tuberculosis is treatable with antibiotics, which means that early detection may result in lower mortality and morbidity, less transmission of tuberculosis, and more equitable access to care. Not recognizing lung tuberculosis when it is present (a false‐negative result) may result in delayed treatment and further transmission. Conversely, a screening result that is thought to be positive while it is not may result in unnecessary confirmatory tests, which burdens both the individual and the public health system. Knowing how often screening tests lead to false‐positive and false‐negative results – this is called accuracy ‐ may help in choosing a screening method. What is the aim of this review? To find out how accurate asking about symptoms and CXR are as screening tests for lung tuberculosis in adults with unknown or negative HIV status. What was studied in the review? We studied the accuracy of three types of symptom questions: (i) cough for two or more weeks, (ii) cough of any duration, and (iii) any tuberculosis symptom. For CXR, we studied two definitions for a positive result: (i) any CXR lung abnormality and (ii) CXR lung abnormalities suggestive of tuberculosis. The results are interpreted by staff trained in radiology. What are the main results in this review? The review included 59 studies, of which 48 reported on one or more symptom screening questions and 37 reported on CXR. The results below indicate a situation in which five individuals (0.5%) have lung tuberculosis among a group of 1000 screened individuals. Cough for two weeks or more: if 1000 individuals were screened, 58 would screen positive, meaning they report cough for two weeks or more and, of these, 56 (97%) would not have lung tuberculosis. Of 1000 individuals, 942 would screen negative, meaning they do not report cough for two weeks or more and, of these, three (0.3%) would have lung tuberculosis. Cough of any duration: of 1000 individuals, 127 would screen positive and, of these, 124 (98%) would not have lung tuberculosis. Of 1000 individuals, 873 would screen negative and, of these, two (0.2%) would have lung tuberculosis. Any tuberculosis symptom: of 1000 individuals, 351 would screen positive and, of these, 348 (99%) would not have lung tuberculosis. Of 1000 individuals, 649 would screen negative and, of these, one (0.2%) would have lung tuberculosis. Any CXR lung abnormality: of 1000 individuals, 113 would show lung abnormalities on CXR and, of these, 108 (96%) would not have lung tuberculosis. Of 1000 individuals, 887 would not show lung abnormalities and, of these, no one (0%) would have lung tuberculosis. CXR lung abnormalities suggestive of tuberculosis: of 1000 individuals, 48 would screen positive and, of these, 44 (92%) would not have lung tuberculosis. Of 1000 individuals, 952 would screen negative and, of these, one (0.1%) would have lung tuberculosis. How reliable are the results of the studies in this review? In the included studies, the diagnosis of tuberculosis was made by assessing the study participants with confirmatory tests (the reference standard). This is the best available method for deciding whether participants really had lung tuberculosis. However, there were problems with how the studies were conducted. In many studies, those without symptoms or CXR abnormalities were not tested with a confirmatory test. Therefore, the numbers of those without symptoms or CXR abnormalities, but nevertheless having tuberculosis (people who tested falsely negative), may have been underestimated in these studies. Consequently, screening for symptoms or CXR abnormalities might appear more accurate than it really is. In addition, results from individual studies included in the review varied, for example, because of regional variation. Therefore, we cannot be sure that screening for symptoms and CXR abnormalities will always have the same accuracy. What are the implications of this review? The results of the review suggest that screening for tuberculosis with symptom questions or CXR might result in a high yield of persons with tuberculosis disease. However, this screening might also result in a high proportion of persons without the disease screening positive. Additional considerations for the best design of screening programmes include the local epidemiological situation, availability and accessibility of CXR, and the need for confirmatory tests. How up to date is this review? The review authors searched for and included studies published from 1 January 1992 to 10 December 2018. A repeat of the search to 2 July 2021 revealed no further studies that would inform the results of the analysis.","3","John Wiley & Sons, Ltd","1465-1858","*HIV Infections [complications]; *Tuberculosis, Pulmonary [diagnostic imaging, epidemiology]; Adult; Cough; Humans; Mass Screening; Radiography; Sensitivity and Specificity","10.1002/14651858.CD010890.pub2","http://dx.doi.org/10.1002/14651858.CD010890.pub2","Infectious Diseases"
"CD013208.PUB2","Ochodo, EA; Olwanda, EElizabeth; Deeks, JJ; Mallett, S","Point‐of‐care viral load tests to detect high HIV viral load in people living with HIV/AIDS attending health facilities","Cochrane Database of Systematic Reviews","2022","Abstract - Background Viral load (VL) testing in people living with HIV (PLHIV) helps to monitor antiretroviral therapy (ART). VL is still largely tested using central laboratory‐based platforms, which have long test turnaround times and involve sophisticated equipment. VL tests with point‐of‐care (POC) platforms capable of being used near the patient are potentially easy to use, give quick results, are cost‐effective, and could replace central or reference VL testing platforms. Objectives To estimate the diagnostic accuracy of POC tests to detect high viral load levels in PLHIV attending healthcare facilities. Search methods We searched eight electronic databases using standard, extensive Cochrane search methods, and did not use any language, document type, or publication status limitations. We also searched the reference lists of included studies and relevant systematic reviews, and consulted an expert in the field from the World Health Organization (WHO) HIV Department for potentially relevant studies. The latest search was 23 November 2020. Selection criteria We included any primary study that compared the results of a VL test with a POC platform to that of a central laboratory‐based reference test to detect high viral load in PLHIV on HIV/AIDS care or follow‐up. We included all forms of POC tests for VL as defined by study authors, regardless of the healthcare facility in which the test was conducted. We excluded diagnostic case‐control studies with healthy controls and studies that did not provide sufficient data to create the 2 × 2 tables to calculate sensitivity and specificity. We did not limit our study inclusion to age, gender, or geographical setting. Data collection and analysis Two review authors independently screened the titles, abstracts, and full texts of the search results to identify eligible articles. They also independently extracted data using a standardized data extraction form and conducted risk of bias assessment using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. Using participants as the unit of analysis, we fitted simplified univariable models for sensitivity and specificity separately, employing a random‐effects model to estimate the summary sensitivity and specificity at the current and commonly reported World Health Organization (WHO) threshold (≥ 1000 copies/mL). The bivariate models did not converge to give a model estimate. Main results We identified 18 studies (24 evaluations, 10,034 participants) defining high viral loads at main thresholds ≥ 1000 copies/mL (n = 20), ≥ 5000 copies/mL (n = 1), and ≥ 40 copies/mL (n = 3). All evaluations were done on samples from PLHIV retrieved from routine HIV/AIDS care centres or health facilities. For clinical applicability, we included 14 studies (20 evaluations, 8659 participants) assessing high viral load at the clinical threshold of ≥ 1000 copies/mL in the meta‐analyses. Of these, sub‐Saharan Africa, Europe, and Asia contributed 16, three, and one evaluation respectively. All included participants were on ART in only nine evaluations; in the other 11 evaluations the proportion of participants on ART was either partial or not clearly stated. Thirteen evaluations included adults only (n = 13), five mixed populations of adults and children, whilst in the remaining two the age of included populations was not clearly stated. The majority of evaluations included commercially available tests (n = 18). Ten evaluations were POC VL tests conducted near the patient in a peripheral or onsite laboratory, whilst the other 10 were evaluations of POC VL tests in a central or reference laboratory setting. The test types evaluated as POC VL tests included Xpert HIV‐1 Viral Load test (n = 8), SAMBA HIV‐1 Semi‐Q Test (n = 9), Alere Q NAT prototype assay for HIV‐1 (n = 2) and m‐PIMA HIV‐1/2 Viral Load test (n = 1). The majority of evaluations (n = 17) used plasma samples, whilst the rest (n = 3) utilized whole blood samples. Pooled sensitivity (95% confidence interval (CI)) of POC VL at a threshold of ≥ 1000 copies/mL was 96.6% (94.8 to 97.8) (20 evaluations, 2522 participants), and pooled specificity (95% CI) was 95.7% (90.8 to 98.0) (20 evaluations, 6137 participants). Median prevalence for high viral load (≥ 1000 copies/mL) (n = 20) was 33.4% (range 6.9% to 88.5%). Limitations The risk of bias was mostly assessed as unclear across the four domains due to incomplete reporting. Authors' conclusions We found POC VL to have high sensitivity and high specificity for the diagnosis of high HIV viral load in PLHIV attending healthcare facilities at a clinical threshold of ≥ 1000 copies/mL. Plain language summary Point‐of‐care tests for detecting high viral load in people living with HIV attending healthcare facilities Why is improving the diagnosis of high HIV viral load infection important? It helps to monitor the HIV virus levels in people living with HIV (PLHIV) who are receiving antiretroviral therapy (ART). High virus levels indicate that the medications are failing to suppress the virus, a condition known as ART treatment failure, which has a risk of severe illness and death. Rapid diagnostic tests that detect high HIV virus levels quickly near the patient (point‐of‐care) can increase access to early changes in ART. What is the aim of this review? To determine the accuracy of point‐of‐care (POC) tests for diagnosing high HIV virus levels in PLHIV attending healthcare facilities. What was studied in this review? Point‐of‐care tests for viral load detection with results measured against central laboratory tests (reference test). We included all forms of tests with POC platforms for VL regardless of the healthcare facility in which the test was conducted. What are the main results in this review? Fourteen studies that completed 20 evaluations involving 8659 participants compared molecular POC tests for diagnosing high virus levels at the clinically recommended positivity threshold of ≥ 1000 copies/mL. What are the strengths and limitations of this review? The review included sufficient studies done on samples from PLHIV retrieved from routine HIV/AIDS care centres or health facilities, but it was unclear if all included participants were on ART. Also, none of the included tests was a true POC test conducted at the patient's side: half of the included studies (n = 10) evaluated POC tests in onsite laboratories near the patient, and the other half were tests with POC platforms evaluated in a central or reference laboratory (n = 10). To whom do the results of this review apply? PLHIV with suspected high viral loads attending healthcare facilities. What are the implications of this review? In theory, for a population of 1000 PLHIV where 100 have high virus levels, 136 people would receive a positive result with the molecular POC test; of these, 39 will not have high viral levels (false‐positive result) and would be incorrectly identified as not responding to ART treatment, possibly leading to unnecessary testing or further treatment; and 864 would receive a negative test result with the molecular POC test; of these, three will actually have high virus levels (false‐negative result) and would be missed whilst failing ART treatment. How up‐to‐date is this review? The evidence is current to 23 November 2020.","3","John Wiley & Sons, Ltd","1465-1858","*HIV Infections [diagnosis]; *Point-of-Care Systems; Adult; Child; Health Facilities; Humans; Sensitivity and Specificity; Serologic Tests; Viral Load","10.1002/14651858.CD013208.pub2","http://dx.doi.org/10.1002/14651858.CD013208.pub2","Infectious Diseases"
"CD013387.PUB2","McAleenan, A; Jones, HE; Kernohan, A; Robinson, T; Schmidt, L; Dawson, S; Kelly, C; Spencer Leal, E; Faulkner, CL; Palmer, A; Wragg, C; Jefferies, S; Brandner, S; Vale, L; Higgins, JPT; Kurian, KM","Diagnostic test accuracy and cost‐effectiveness of tests for codeletion of chromosomal arms 1p and 19q in people with glioma","Cochrane Database of Systematic Reviews","2022","Abstract - Background Complete deletion of both the short arm of chromosome 1 (1p) and the long arm of chromosome 19 (19q), known as 1p/19q codeletion, is a mutation that can occur in gliomas. It occurs in a type of glioma known as oligodendroglioma and its higher grade counterpart known as anaplastic oligodendroglioma. Detection of 1p/19q codeletion in gliomas is important because, together with another mutation in an enzyme known as isocitrate dehydrogenase, it is needed to make the diagnosis of an oligodendroglioma. Presence of 1p/19q codeletion also informs patient prognosis and prediction of the best drug treatment. The main two tests in use are fluorescent in situ hybridisation (FISH) and polymerase chain reaction (PCR)‐based loss of heterozygosity (LOH) assays (also known as PCR‐based short tandem repeat or microsatellite analysis). Many other tests are available. None of the tests is perfect, although PCR‐based LOH is expected to have very high sensitivity. Objectives To estimate the sensitivity and specificity and cost‐effectiveness of different deoxyribonucleic acid (DNA)‐based techniques for determining 1p/19q codeletion status in glioma. Search methods We searched MEDLINE, Embase and BIOSIS up to July 2019. There were no restrictions based on language or date of publication. We sought economic evaluation studies from the results of this search and using the National Health Service Economic Evaluation Database. Selection criteria We included cross‐sectional studies in adults with glioma or any subtype of glioma, presenting raw data or cross‐tabulations of two or more DNA‐based tests for 1p/19q codeletion. We also sought economic evaluations of these tests. Data collection and analysis We followed procedures outlined in the  Cochrane Handbook for Diagnostic Test Accuracy Reviews . Two review authors independently screened titles/abstracts/full texts, performed data extraction, and undertook applicability and risk of bias assessments using QUADAS‐2. Meta‐analyses used the hierarchical summary ROC model to estimate and compare test accuracy. We used FISH and PCR‐based LOH as alternate reference standards to examine how tests compared with those in common use, and conducted a latent class analysis comparing FISH and PCR‐based LOH. We constructed an economic model to evaluate cost‐effectiveness. Main results We included 53 studies examining: PCR‐based LOH, FISH, single nucleotide polymorphism (SNP) array, next‐generation sequencing (NGS), comparative genomic hybridisation (CGH), array comparative genomic hybridisation (aCGH), multiplex‐ligation‐dependent probe amplification (MLPA), real‐time PCR, chromogenic in situ hybridisation (CISH), mass spectrometry (MS), restriction fragment length polymorphism (RFLP) analysis, G‐banding, methylation array and NanoString. Risk of bias was low for only one study; most gave us concerns about how patients were selected or about missing data. We had applicability concerns about many of the studies because only patients with specific subtypes of glioma were included. 1520 participants contributed to analyses using FISH as the reference, 1304 participants to analyses involving PCR‐based LOH as the reference and 262 participants to analyses of comparisons between methods from studies not including FISH or PCR‐based LOH. Most evidence was available for comparison of FISH with PCR‐based LOH (15 studies, 915 participants): PCR‐based LOH detected 94% of FISH‐determined codeletions (95% credible interval (CrI) 83% to 98%) and FISH detected 91% of codeletions determined by PCR‐based LOH (CrI 78% to 97%). Of tumours determined not to have a deletion by FISH, 94% (CrI 87% to 98%) had a deletion detected by PCR‐based LOH, and of those determined not to have a deletion by PCR‐based LOH, 96% (CrI 90% to 99%) had a deletion detected by FISH. The latent class analysis suggested that PCR‐based LOH may be slightly more accurate than FISH. Most other techniques appeared to have high sensitivity (i.e. produced few false‐negative results) for detection of 1p/19q codeletion when either FISH or PCR‐based LOH was considered as the reference standard, although there was limited evidence. There was some indication of differences in specificity (false‐positive rate) with some techniques. Both NGS and SNP array had high specificity when considered against FISH as the reference standard (NGS: 6 studies, 243 participants; SNP: 6 studies, 111 participants), although we rated certainty in the evidence as low or very low. NGS and SNP array also had high specificity when PCR‐based LOH was considered the reference standard, although with much more uncertainty as these results were based on fewer studies (just one study with 49 participants for NGS and two studies with 33 participants for SNP array). G‐banding had low sensitivity and specificity when PCR‐based LOH was the reference standard. Although MS had very high sensitivity and specificity when both FISH and PCR‐based LOH were considered the reference standard, these results were based on only one study with a small number of participants. Real‐time PCR also showed high specificity with FISH as a reference standard, although there were only two studies including 40 participants. We found no relevant economic evaluations. Our economic model using FISH as the reference standard suggested that the resource‐optimising test depends on which measure of diagnostic accuracy is most important. With FISH as the reference standard, MLPA is likely to be cost‐effective if society was willing to pay GBP 1000 or less for a true positive detected. However, as the value placed on a true positive increased, CISH was most cost‐effective. Findings differed when the outcome measure changed to either true negative detected or correct diagnosis. When PCR‐based LOH was used as the reference standard, MLPA was likely to be cost‐effective for all measures of diagnostic accuracy at lower threshold values for willingness to pay. However, as the threshold values increased, none of the tests were clearly more likely to be considered cost‐effective. Authors' conclusions In our review, most techniques (except G‐banding) appeared to have good sensitivity (few false negatives) for detection of 1p/19q codeletions in glioma against both FISH and PCR‐based LOH as a reference standard. However, we judged the certainty of the evidence low or very low for all the tests. There are possible differences in specificity, with both NGS and SNP array having high specificity (fewer false positives) for 1p/19q codeletion when considered against FISH as the reference standard. The economic analysis should be interpreted with caution due to the small number of studies. Plain language summary Comparing different methods of determining whether gliomas are missing arms 1p and 19q of the chromosomes Why is improving the detection of 1p/19q codeletion in glioma important? Gliomas are a type of brain tumour (cancer). There are different types of glioma, with different changes in their genetic material. One of the possible genetic changes is the loss of parts of two of our 23 chromosomes. When both a specific part of chromosome 1 and a specific part of chromosome 19 are missing, it is known as '1p/19q codeletion'. 1p/19q codeletion is used to diagnose a glioma known as an oligodendroglioma. Presence of 1p/19q codeletion can also tell us how long a patient with a glioma may survive and which is the best medicine to treat that patient. What is the aim of this review? We wanted to find out which is the most accurate and cost‐effective way to identify 1p/19q codeletion in gliomas. What is studied in the review? The review examined and compared all methods to detect 1p/19q codeletion that are based on the deoxyribonucleic acid (DNA, which contains the information for an organism to develop, survive and reproduce) of the tumour. These include tests known as FISH and CISH, which are performed directly on tumour tissue and a number of other tests that are based on DNA extracted from the tumour tissue including: PCR‐based LOH, real‐time PCR, MLPA, SNP array, CGH array and NGS. None of these tests is perfect, so there is no 'gold standard' against which to compare them. The two most commonly used tests (FISH and PCR‐based LOH) were used as the best available reference tests against which to examine the others. What are the main results of the review? We found 53 studies. Most tests were good at identifying instances of 1p/19q codeletion (meaning they were tests with good 'sensitivity') that had been identified by either of the two common tests. However, there were some differences in how well the tests were able to rule out 1p/19q codeletion when it did not seem to be present (the 'specificity' of the test). NGS and SNP arrays were better at this (i.e. having fewer 'false‐positives' results) when considered against FISH as the reference test. The cost per correct diagnosis was lowest for MLPA, although this was not a firm finding because the amount of evidence was small. How reliable are results of the studies in this review? Our certainty in the evidence was low or very low, because there were few studies for most of the tests and there were limitations to almost all the studies. Similarly, the economic analysis must be interpreted with caution due to the relatively small number of studies. To whom do the results of this review apply? The ways in which the tests were performed were thought to be representative of how they would be performed in practice. However, many of the studies included people with specific types of gliomas, so the results might not be representative of all people with gliomas. What are the implications of this review? The limited evidence suggests that currently used techniques show good sensitivity for detection of 1p/19q codeletion. NGS and SNP arrays may have higher specificity when FISH is the reference standard, but this comes at greater cost per test. How up‐to‐date is this review? The latest search for studies took place in August 2019.","3","John Wiley & Sons, Ltd","1465-1858","*Brain Neoplasms [genetics]; *Glioma [diagnosis, genetics]; *Oligodendroglioma; Chromosomes, Human, Pair 1 [genetics]; Cost-Benefit Analysis; Cross-Sectional Studies; DNA; Diagnostic Tests, Routine; Humans; State Medicine","10.1002/14651858.CD013387.pub2","http://dx.doi.org/10.1002/14651858.CD013387.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD012028.PUB2","D'Souza, N; Hicks, G; Beable, R; Higginson, A; Rud, B","Magnetic resonance imaging (MRI) for diagnosis of acute appendicitis","Cochrane Database of Systematic Reviews","2021","Abstract - Background Appendicitis remains a difficult disease to diagnose, and imaging adjuncts are commonly employed. Magnetic resonance imaging (MRI) is an imaging test that can be used to diagnose appendicitis. It is not commonly regarded as a first‐line imaging test for appendicitis, but the reported diagnostic accuracy in some studies is equivalent to computed tomography (CT) scans. As it does not expose patients to radiation, it is an attractive imaging modality, particularly in women and children. Objectives The primary objective was to determine the diagnostic accuracy of MRI for detecting appendicitis in all patients. Secondary objectives: To investigate the accuracy of MRI in subgroups of pregnant women, children, and adults. To investigate the potential influence of MRI scanning variables such as sequences, slice thickness, or field of view. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, and Embase until February 2021. We searched the references of included studies and other systematic reviews to identify further studies. We did not exclude studies that were unpublished, published in another language, or retrospective. Selection criteria We included studies that compared the outcome of an MRI scan for suspected appendicitis with a reference standard of histology, intraoperative findings, or clinical follow‐up. Three study team members independently filtered search results for eligible studies. Data collection and analysis We independently extracted study data and assessed study quality using the Quality Assessment of Studies of Diagnostic Accuracy ‐ Revised (QUADAS‐2) tool. We used the bivariate model to calculate pooled estimates of sensitivity and specificity. Main results We identified 58 studies with sufficient data for meta‐analysis including a total of 7462 participants (1980 with and 5482 without acute appendicitis). Estimates of sensitivity ranged from 0.18 to 1.0; estimates of specificity ranged from 0.4 to 1.0. Summary sensitivity was 0.95 (95% confidence interval (CI) 0.94 to 0.97); summary specificity was 0.96 (95% CI 0.95 to 0.97). Sensitivity and specificity remained high on subgroup analysis for pregnant women (sensitivity 0.96 (95% CI 0.88 to 0.99); specificity 0.97 (95% CI 0.95 to 0.98); 21 studies, 2282 women); children (sensitivity 0.96 (95% CI 0.95 to 0.97); specificity 0.96 (95% CI 0.92 to 0.98); 17 studies, 2794 children); and adults (sensitivity 0.96 (95% CI 0.93 to 0.97); specificity 0.93 (95% CI 0.80 to 0.98); 9 studies, 1088 participants), as well as different scanning techniques. In a hypothetical cohort of 1000 patients, there would be 12 false‐positive results and 30 false‐negative results. Methodological quality of the included studies was poor, and the risk of bias was high or unclear in 53% to 83% of the QUADAS‐2 domains. Authors' conclusions MRI appears to be highly accurate in confirming and excluding acute appendicitis in adults, children, and pregnant women regardless of protocol. The methodological quality of the included studies was generally low due to incomplete and low standards of follow‐up, so summary estimates of sensitivity and specificity may be biased. We could not assess the impact and direction of potential bias given the very low number of high‐quality studies. Studies comparing MRI protocols were few, and although we found no influence of MRI protocol variables on the summary estimates of accuracy, our results do not rule out that some MRI protocols are more accurate than others. Plain language summary Magnetic resonance imaging (MRI) for diagnosis of acute appendicitis Review question To check the accuracy of magnetic resonance imaging (MRI), a medical imaging tool used for taking detailed pictures of the inside of the body, to test for appendicitis. Why is diagnosing appendicitis important? Appendicitis is a very common condition that is usually treated with emergency surgery, but it can be difficult to diagnose. Up to one in four patients may be incorrectly diagnosed with appendicitis. Tools such as MRI can help diagnose appendicitis quickly and early. What was studied in this review? We studied the accuracy of MRI for appendicitis in all patients. What are the main results of the review? We analysed the results of 58 studies with 7462 participants to calculate the accuracy of MRI. The results of these studies indicate that in theory, if MRI were to be used in 1000 patients with suspected appendicitis, where 250 patients actually had appendicitis, then: • an estimated 250 patients will have an MRI result indicating appendicitis, 12 of whom will not actually have appendicitis; and • of the 750 patients with a result indicating that appendicitis is not present, 30 will actually have appendicitis. MRI remained very accurate when looking specifically at adults, pregnant women, and children. How reliable are the results of the studies in this review? There were problems with how most of the studies were conducted that may have resulted in MRI appearing more accurate than it actually is. To whom do the results of this review apply? The results apply to people with suspected appendicitis, including adults, pregnant women, and children. Most studies were conducted in Europe and North America in large university hospitals. Patients had often undergone an ultrasound scan without a clear result. What are the key messages of this review? Based on the studies included in this review, MRI seems to be a very accurate test for appendicitis. The chance of wrongly diagnosing someone with appendicitis or missing appendicitis was less than 5%. However, as most of the included studies had problems, we cannot trust their results completely. Although MRI is promising, until better studies have been performed, we cannot firmly recommend the use of MRI for the diagnosis of appendicitis. How up‐to‐date is this review? We searched for and used studies published up to February 2021.","12","John Wiley & Sons, Ltd","1465-1858","*Appendicitis [diagnostic imaging]; Adult; Child; Female; Humans; Magnetic Resonance Imaging; Pregnancy; Retrospective Studies; Sensitivity and Specificity; Tomography, X-Ray Computed","10.1002/14651858.CD012028.pub2","http://dx.doi.org/10.1002/14651858.CD012028.pub2","Colorectal"
"CD010173.PUB3","Walsh, T; Warnakulasuriya, S; Lingen, MW; Kerr, AR; Ogden, GR; Glenny, A-M; Macey, R","Clinical assessment for the detection of oral cavity cancer and potentially malignant disorders in apparently healthy adults","Cochrane Database of Systematic Reviews","2021","Abstract - Background The early detection of oral cavity squamous cell carcinoma (OSCC) and oral potentially malignant disorders (OPMD), followed by appropriate treatment, may improve survival and reduce the risk for malignant transformation respectively. This is an update of a Cochrane Review first published in 2013. Objectives To estimate the diagnostic test accuracy of conventional oral examination, vital rinsing, light‐based detection, mouth self‐examination, remote screening, and biomarkers, used singly or in combination, for the early detection of OPMD or OSCC in apparently healthy adults. Search methods Cochrane Oral Health's Information Specialist searched the following databases: Cochrane Oral Health's Trials Register (to 20 October 2020), MEDLINE Ovid (1946 to 20 October 2020), and Embase Ovid (1980 to 20 October 2020). The US National Institutes of Health Trials Registry (ClinicalTrials.gov) and the World Health Organization International Clinical Trials Registry Platform were searched for ongoing trials. No restrictions were placed on the language or date of publication when searching the electronic databases. We conducted citation searches, and screened reference lists of included studies for additional references. Selection criteria We selected studies that reported the test accuracy of any of the aforementioned tests in detecting OPMD or OSCC during a screening procedure. Diagnosis of OPMD or OSCC was provided by specialist clinicians or pathologists, or alternatively through follow‐up. Data collection and analysis Two review authors independently screened titles and abstracts for relevance. Eligibility, data extraction, and quality assessment were carried out by at least two authors independently and in duplicate. Studies were assessed for methodological quality using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2). We reported the sensitivity and specificity of the included studies. We provided judgement of the certainty of the evidence using a GRADE assessment. Main results We included 18 studies, recruiting 72,202 participants, published between 1986 and 2019. These studies evaluated the diagnostic test accuracy of conventional oral examination (10 studies, none new to this update), mouth self‐examination (four studies, two new to this update), and remote screening (three studies, all new to this update). One randomised controlled trial of test accuracy directly evaluated conventional oral examination plus vital rinsing versus conventional oral examination alone. There were no eligible studies evaluating light‐based detection or blood or salivary sample analysis (which tests for the presence of biomarkers for OPMD and OSCC). Only one study of conventional oral examination was judged as at overall low risk of bias and overall low concern regarding applicability. Given the clinical heterogeneity of the included studies in terms of the participants recruited, setting, prevalence of the target condition, the application of the index test and reference standard, and the flow and timing of the process, the data could not be pooled within the broader categories of index test. For conventional oral examination (10 studies, 25,568 participants), prevalence in the test accuracy sample ranged from 1% to 51%. For the seven studies with prevalence of 10% or lower, a prevalence more comparable to the general population, the sensitivity estimates were variable, and ranged from 0.50 (95% confidence interval (CI) 0.07 to 0.93) to 0.99 (95% CI 0.97 to 1.00); the specificity estimates were more consistent and ranged from 0.94 (95% CI 0.88 to 0.97) to 0.99 (95% CI 0.98 to 1.00). We judged the overall certainty of the evidence to be low, and downgraded for inconsistency and indirectness. Evidence for mouth self‐examination and remote screening was more limited. We judged the overall certainty of the evidence for these index tests to be very low, and downgraded for imprecision, inconsistency, and indirectness. We judged the evidence for vital rinsing (toluidine blue) as an adjunct to conventional oral examination compared to conventional oral examination to be moderate, and downgraded for indirectness as the trial was undertaken in a high‐risk population. Authors' conclusions There is a lack of high‐certainty evidence to support the use of screening programmes for oral cavity cancer and OPMD in the general population. Frontline screeners such as general dentists, dental hygienists, other allied professionals, and community healthcare workers should remain vigilant for signs of OPMD and OSCC. Plain language summary What are the most accurate tests for screening for cancer of the mouth (oral cancer) and conditions that may lead to oral cancer? Key messages ‐ There is a lack of high‐certainty evidence to support the use of screening tests for cancer of the mouth and conditions that may lead to mouth cancer in the general population. ‐ General dental practitioners and healthcare professionals should be watchful for signs of oral potentially malignant disorders (OPMD) and malignancies whilst performing routine oral examinations in practice for other common oral lesions/conditions. Detection of oral cancer Cancer of the mouth (oral cancer) is a serious condition, and only half of those that develop the disease will survive after 5 years. This is because it is often detected late. Early detection when the oral cancer is small or as a 'preceding' condition or lesion (which can become cancer) can result in simpler treatment and much better outcomes. As a result, there is a need to understand how good different types of tests are at the early detection of oral cancer and the lesions that precede it. What did we want to find out? The aim of this review was to find out the accuracy of different screening tests for cancer of the mouth and conditions that may lead to mouth cancer. What did we do? We searched for studies that reported the test accuracy of different screening tests in detecting cancer of the mouth or OPMDs during a screening procedure. Diagnosis of cancer of the mouth or OPMDs was provided by specialist clinicians or pathologists, or alternatively through follow‐up. We compared and summarised the results of the studies and rated our confidence in the evidence, based on factors such as study methods and sizes. What did we find? We included 18 studies recruiting 72,202 participants, published between 1986 and 2019. These studies evaluated a conventional oral examination (COE) or visual inspection (10 studies), mouth self‐examination (four studies), and remote screening (three studies). One randomised controlled trial of test accuracy directly compared conventional oral examination plus vital rinsing with conventional oral examination alone. No eligible studies evaluated the accuracy of tests of blood or saliva. There was substantial variation in the participants that were recruited, the setting, the prevalence of mouth cancer or OPMDs, and how the different tests were carried out, and so we were unable to pool the data. ‐ Most studies evaluated the accuracy of the different COEs (10 studies, 25,568 participants). The prevalence of mouth cancer or OPMDs in these studies ranged from 1% to 51%. For the seven COE studies with a prevalence of 10% or lower, a prevalence more comparable to the general population, the sensitivity estimates (proportion of true positives) ranged from 0.50 to 0.99 with specificity estimates (proportion of true negatives) from 0.94 to 0.99. ‐ Evidence for mouth self‐examination (4 studies, 35,059 participants) and remote screening (3 studies, 3600 participants) was more limited. What are the limitations of the evidence? We judged the overall certainty of the evidence for COE to be low and downgraded for the variation across studies and applicability of the study samples. We judged the overall certainty of the evidence for mouth self‐examination and remote screening to be very low, and downgraded for variation across studies, applicability of the study samples, and imprecise accuracy estimates. How up to date is this evidence? The evidence is up to date to October 2020.","12","John Wiley & Sons, Ltd","1465-1858","*Carcinoma, Squamous Cell [diagnosis]; *Early Detection of Cancer; Bias; Humans; Mouth; Sensitivity and Specificity; United States","10.1002/14651858.CD010173.pub3","http://dx.doi.org/10.1002/14651858.CD010173.pub3","Oral Health"
"CD012325.PUB2","Vaarwerk, B; Breunis, WB; Haveman, LM; de Keizer, B; Jehanno, N; Borgwardt, L; van Rijn, RR; van den Berg, H; Cohen, JF; van Dalen, EC; Merks, JHM","Fluorine‐18‐fluorodeoxyglucose (FDG) positron emission tomography (PET) computed tomography (CT) for the detection of bone, lung, and lymph node metastases in rhabdomyosarcoma","Cochrane Database of Systematic Reviews","2021","Abstract - Background Rhabdomyosarcoma (RMS) is the most common paediatric soft‐tissue sarcoma and can emerge throughout the whole body. For patients with newly diagnosed RMS, prognosis for survival depends on multiple factors such as histology, tumour site, and extent of the disease. Patients with metastatic disease at diagnosis have impaired prognosis compared to those with localised disease. Appropriate staging at diagnosis therefore plays an important role in choosing the right treatment regimen for an individual patient. Fluorine‐18‐fluorodeoxyglucose ( 18 F‐FDG) positron emission tomography (PET) is a functional molecular imaging technique that uses the increased glycolysis of cancer cells to visualise both structural information and metabolic activity.  18 F‐FDG‐PET combined with computed tomography (CT) could help to accurately stage the extent of disease in patients with newly diagnosed RMS. In this review we aimed to evaluate whether  18 F‐FDG‐PET could replace other imaging modalities for the staging of distant metastases in RMS. Objectives To determine the diagnostic accuracy of  18 F‐FDG‐PET/CT imaging for the detection of bone, lung, and lymph node metastases in RMS patients at first diagnosis. Search methods We searched MEDLINE in PubMed (from 1966 to 23 December 2020) and Embase in Ovid (from 1980 to 23 December 2020) for potentially relevant studies. We also checked the reference lists of relevant studies and review articles; scanned conference proceedings; and contacted the authors of included studies and other experts in the field of RMS for information about any ongoing or unpublished studies. We did not impose any language restrictions. Selection criteria We included cross‐sectional studies involving patients with newly diagnosed proven RMS, either prospective or retrospective, if they reported the diagnostic accuracy of  18 F‐FDG‐PET/CT in diagnosing lymph node involvement or bone metastases or lung metastases or a combination of these metastases. We included studies that compared the results of the  18 F‐FDG‐PET/CT imaging with those of histology or with evaluation by a multidisciplinary tumour board as reference standard. Data collection and analysis Two review authors independently performed study selection, data extraction, and methodological quality assessement according to Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2). We analysed data for the three outcomes (nodal involvement and lung and bone metastases) separately. We used data from the 2 × 2 tables (consisting of true positives, false positives, true negatives, and false negatives) to calculate sensitivity and specificity in each study and corresponding 95% confidence intervals. We did not consider a formal meta‐analysis to be relevant because of the small number of studies and substantial heterogeneity between studies. Main results Two studies met our inclusion criteria. The diagnostic accuracy of  18 F‐FDG‐PET/CT was reported in both studies, which included a total of 36 participants. We considered both studies to be at high risk of bias for the domain reference standard. We considered one study to be at high risk of bias for the domain index test and flow and timing. Sensitivity and specificity of  18 F‐FDG‐PET/CT for the detection of bone metastases was 100% in both studies (95% confidence interval (CI) for sensitivity was 29% to 100% in study one and 40% to 100% in study two; 95% CI for specificity was 83% to 100% in study one and 66% to 100% in study two). The reported sensitivity of  18 F‐FDG‐PET/CT for the detection of lung metastases was not calculated since only two participants in study two showed lung metastases, of which one was detected by  18 F‐FDG‐PET/CT. Reported specificity was 96% in study one (95% CI 78% to 100%) and 100% (95% CI 72% to 100%) in study two. The reported sensitivity for the detection of nodal involvement was 100% (95% CI 63% to 100% in study one and 40% to 100% in study two); the reported specificity was 100% (95% CI 78% to 100%) in study one and 89% (95% CI 52% to 100%) in study two. Authors' conclusions The diagnostic accuracy of  18 F‐FDG‐PET/CT for the detection of bone, lung, and lymph node metastases was reported in only two studies including a total of only 36 participants with newly diagnosed RMS. Because of the small number of studies (and participants), there is currently insufficient evidence to reliably determine the diagnostic accuracy of  18 F‐FDG‐PET/CT in the detection of distant metastases. Larger series evaluating the diagnostic accuracy of  18 F‐FDG‐PET/CT for the detection of metastases in patients with RMS are necessary. Plain language summary The accuracy of  18 F‐FDG‐PET/CT for the detection of metastatic rhabdomyosarcoma in newly diagnosed patients Why is accurate staging of rhabdomyosarcoma important? Rhabdomyosarcoma (RMS) accounts for 3% to 5% of all childhood cancers. Treatment consists of multidrug chemotherapy and surgery and/or radiotherapy. Treatment for newly diagnosed patients depends on the extent of the cancer. Five‐year survival for patients with localised disease is around 75%, whereas it is below 30% for patients with disease that has spread to different part(s) of the body (metastatic disease). Survival for metastatic patients depends on the number of metastases as well as the site(s) of metastases. It is of great importance to accurately stage the extent of the disease because not recognising patients with metastatic disease would lead to undertreatment, whereas incorrectly identifying lesions as being metastatic would lead to overtreatment.  18 F‐FDG‐PET/CT is a special imaging technique that could be helpful in visualising the extent of the disease in patients with newly diagnosed RMS. However, the accuracy (the ability to separate RMS metastases from other lesions) of  18 F‐FDG‐PET/CT is currently unknown. What was the aim of this review? We wanted to find out how accurate  18 F‐FDG‐PET/CT is for the detection of bone and lung metastases and lymph node involvement in patients with newly diagnosed RMS. What was studied in this review? We searched scientific literature databases for studies comparing the results of  18 F‐FDG‐PET/CT to standard staging evaluation (histologic examinations (using a microscope to examine tissues and cells) or results of other imaging methods discussed by a multidisciplinary tumour board). The advantage of using  18 F‐FDG‐PET/CT compared to standard staging evaluations would be the use of  18 F‐FDG‐PET/CT as a single diagnostic test to detect metastases, thus reducing patient burden and lowering radiation exposure. Main results We included two studies involving a total of 36 participants with RMS. Because of the low number of participants in the included studies and the differences in quality between studies, we were not able to calculate summary values of sensitivity (the proportion of patients who have the condition and who received a positive result on the test) and specificity (the proportion of patients who do not have the condition and who received a negative result on the test), and the reported results should be viewed with caution. The two included studies reported a sensitivity and specificity of 100% of  18 F‐FDG‐PET/CT for the detection of bone metastases. The sensitivity for the detection of lung metastases was 50% in one study, and could not be estimated in the other study; specificity ranged from 96% to 100%. In both studies, the sensitivity for the detection of lymph node involvement was 100%, and specificity ranged from 89% to 100%. How reliable are the results of the studies in this review? The main limitation of this review was that the findings were based on only two studies with a total of 36 participants and due to the design of the studies we considered them to be at high risk of bias on specific domains. In the included studies, histopathological (microscopic examination of tissue) confirmation was considered the optimal reference standard (the best available method to determine whether the condition is present or absent) to determine RMS metastases from other lesions; however, this was not done in all participants. In cases where no histopathological confirmation was done, the judgement of a multidisciplinary tumour board was considered the reference standard. Judgement of the multidisciplinary tumour boards was partly based on  18 F‐FDG‐PET/CT imaging results, potentially resulting in overestimation of sensitivity and specificity. Furthermore, in one included study the participants all underwent the same diagnostic procedures, whereas in the other study this was not the case for all participants; this study did not clearly define what was considered a positive test result for  18 F‐FDG‐PET/CT imaging. This might have biased the results. What are the implications of this review? The total number of studies and participants was too low to allow us to draw any firm conclusions. Large studies evaluating the accuracy of  18 F‐FDG‐PET/CT in patients with RMS are needed. How up‐to‐date is this review? We searched for and used studies published from 1966 to 23 December 2020.","11","John Wiley & Sons, Ltd","1465-1858","*Fluorodeoxyglucose F18; *Rhabdomyosarcoma [diagnostic imaging]; Cross-Sectional Studies; Humans; Lung; Lymphatic Metastasis; Neoplasm Staging; Positron Emission Tomography Computed Tomography; Positron-Emission Tomography; Prospective Studies; Radiopharmaceuticals; Retrospective Studies; Sensitivity and Specificity; Tomography, X-Ray Computed","10.1002/14651858.CD012325.pub2","http://dx.doi.org/10.1002/14651858.CD012325.pub2","Childhood Cancer"
"CD012679.PUB2","Boaden, E; Burnell, J; Hives, L; Dey, P; Clegg, A; Lyons, MW; Lightbody, CE; Hurley, MA; Roddam, H; McInnes, E; Alexandrov, A; Watkins, CL","Screening for aspiration risk associated with dysphagia in acute stroke","Cochrane Database of Systematic Reviews","2021","Abstract - Background Stroke can affect people’s ability to swallow, resulting in passage of some food and drink into the airway. This can cause choking, chest infection, malnutrition and dehydration, reduced rehabilitation, increased risk of anxiety and depression, longer hospital stay, increased likelihood of discharge to a care home, and increased risk of death. Early identification and management of disordered swallowing reduces risk of these difficulties. Objectives Primary objective • To determine the diagnostic accuracy and the sensitivity and specificity of bedside screening tests for detecting risk of aspiration associated with dysphagia in people with acute stroke Secondary objectives • To assess the influence of the following sources of heterogeneity on the diagnostic accuracy of bedside screening tools for dysphagia ‐ Patient demographics (e.g. age, gender) ‐ Time post stroke that the study was conducted (from admission to 48 hours) to ensure only hyperacute and acute stroke swallow screening tools are identified ‐ Definition of dysphagia used by the study ‐ Level of training of nursing staff (both grade and training in the screening tool) ‐ Low‐quality studies identified from the methodological quality checklist ‐ Type and threshold of index test ‐ Type of reference test Search methods In June 2017 and December 2019, we searched CENTRAL, MEDLINE, Embase, CINAHL, and the Health Technology Assessment (HTA) database via the Centre for Reviews and Dissemination; the reference lists of included studies; and grey literature sources. We contacted experts in the field to identify any ongoing studies and those potentially missed by the search strategy. Selection criteria We included studies that were single‐gate or two‐gate studies comparing a bedside screening tool administered by nurses or other healthcare professionals (HCPs) with expert or instrumental assessment for detection of aspiration associated with dysphagia in adults with acute stroke admitted to hospital. Data collection and analysis Two review authors independently screened each study using the eligibility criteria and then extracted data, including the sensitivity and specificity of each index test against the reference test. A third review author was available at each stage to settle disagreements. The methodological quality of each study was assessed using the Quality Assessment of Studies of Diagnostic Accuracy (QUADAS‐2) tool. We identified insufficient studies for each index test, so we performed no meta‐analysis. Diagnostic accuracy data were presented as sensitivities and specificities for the index tests. Main results Overall, we included 25 studies in the review, four of which we included as narratives (with no accuracy statistics reported). The included studies involved 3953 participants and 37 screening tests. Of these, 24 screening tests used water only, six used water and other consistencies, and seven used other methods. For index tests using water only, sensitivity and specificity ranged from 46% to 100% and from 43% to 100%, respectively; for those using water and other consistencies, sensitivity and specificity ranged from 75% to 100% and from 69% to 90%, respectively; and for those using other methods, sensitivity and specificity ranged from 29% to 100% and from 39% to 86%, respectively. Twenty screening tests used expert assessment or the Mann Assessment of Swallowing Ability (MASA) as the reference, six used fibreoptic endoscopic evaluation of swallowing (FEES), and 11 used videofluoroscopy (VF). Fifteen screening tools had an outcome of aspiration risk, 20 screening tools had an outcome of dysphagia, and two narrative papers did not report the outcome. Twenty‐one screening tests were carried out by nurses, and 16 were carried out by other HCPs (not including speech and language therapists (SLTs)). We assessed a total of six studies as low risk across all four QUADAS‐2 risk of bias domains, and we rated 15 studies as low concern across all three applicability domains. No single study demonstrated 100% sensitivity and specificity with low risk of bias for all domains. The best performing combined water swallow and instrumental tool was the Bedside Aspiration test (n = 50), the best performing water plus other consistencies tool was the Gugging Swallowing Screen (GUSS; n = 30), and the best water only swallow screening tool was the Toronto Bedside Swallowing Screening Test (TOR‐BSST; n = 24). All tools demonstrated combined highest sensitivity and specificity and low risk of bias for all domains. However, clinicians should be cautious in their interpretation of these findings, as these tests are based on single studies with small sample sizes, which limits the estimates of reliability of screening tests. Authors' conclusions We were unable to identify a single swallow screening tool with high and precisely estimated sensitivity and specificity based on at least one trial with low risk of bias. However, we were able to offer recommendations for further high‐quality studies that are needed to improve the accuracy and clinical utility of bedside screening tools. Plain language summary Screening for aspiration risk associated with dysphagia in acute stroke Question How accurate are swallow screening tools for detecting when food and drink enter the airway in people with acute stroke? Background Stroke often affects a person’s ability to swallow, allowing food and drink into the airway. This can cause choking, chest infection, malnutrition, dehydration, and reduced rehabilitation, with increased risk of anxiety, depression, discharge to a care home, and death. Early identification and management of disordered swallowing through the most accurate testing reduces these risks. If the test fails to identify swallowing difficulties, the person will continue oral intake and may experience the difficulties identified above. If the test incorrectly identifies swallowing difficulties, the person may not be given anything to eat or drink, significantly impacting quality of life, until a more detailed assessment is undertaken (usually the next day). Study characteristics We identified 25 studies that used a total of 37 tools. Seven tools did not use water or other consistencies, 24 used only water, and six considered water and other consistencies. Key results We were unable to identify a tool that could accurately identify everyone with food and drink entering their airway, as well as detect all those who definitely did not. Many studies involved different healthcare professionals, food and fluid testing consistencies, and time between stroke onset and the screening test, so it is unclear which tool is best. We were unable to directly compare the different tools because most studies used different methods. We were able to identify the tools most able to detect people with and without risk of swallowing difficulties from studies with good quality evidence. The best combined water swallow and instrumental test was the Bedside Aspiration test, the best water plus other consistencies tool was the Gugging Swallowing Screen, and the best water only tool was the Toronto Bedside Swallowing Screening Test. However, clinicians should be cautious in their interpretation of these findings, as these tests are based on single studies with small sample sizes. Quality of the evidence Most included studies were poorly conducted or were unclear in reporting what they did (i.e. unclear or high risk of bias). Conclusion We were unable to identify a single tool with combined high levels of accuracy and good quality evidence. However, we are able to offer recommendations for further high‐quality studies that are needed to improve the accuracy and clinical utility of swallow screening tools.","10","John Wiley & Sons, Ltd","1465-1858","*Deglutition Disorders [diagnosis, etiology]; *Stroke [complications]; Humans; Mass Screening; Reproducibility of Results; Sensitivity and Specificity","10.1002/14651858.CD012679.pub2","http://dx.doi.org/10.1002/14651858.CD012679.pub2","Stroke"
"CD013362.PUB2","Nadarevic, T; Giljaca, V; Colli, A; Fraquelli, M; Casazza, G; Miletic, D; Štimac, D","Computed tomography for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease","Cochrane Database of Systematic Reviews","2021","Abstract - Background Hepatocellular carcinoma occurs mostly in people with chronic liver disease and ranks sixth in terms of global incidence of cancer, and fourth in terms of cancer deaths. In clinical practice, computed tomography (CT) is used as a second‐line diagnostic imaging modality to confirm the presence of focal liver lesions suspected as hepatocellular carcinoma on prior diagnostic test such as abdominal ultrasound or alpha‐foetoprotein, or both, either in surveillance programmes or in clinical settings. According to current guidelines, a single contrast‐enhanced imaging study CT or magnetic resonance imaging (MRI) showing typical hallmarks of hepatocellular carcinoma in people with cirrhosis is valid to diagnose hepatocellular carcinoma. However, a significant number of hepatocellular carcinomas do not show typical hallmarks on imaging modalities, and hepatocellular carcinoma is, therefore, missed. There is no clear evidence of the benefit of surveillance programmes in terms of overall survival: the conflicting results can be a consequence of inaccurate detection, ineffective treatment, or both. Assessing the diagnostic accuracy of CT may clarify whether the absence of benefit could be related to underdiagnosis. Furthermore, an assessment of the accuracy of CT in people with chronic liver disease, who are not included in surveillance programmes is needed for either ruling out or diagnosing hepatocellular carcinoma. Objectives Primary: to assess the diagnostic accuracy of multidetector, multiphasic contrast‐enhanced CT for the diagnosis of hepatocellular carcinoma of any size and at any stage in adults with chronic liver disease, either in a surveillance programme or in a clinical setting. Secondary: to assess the diagnostic accuracy of CT for the diagnosis of resectable hepatocellular carcinoma in adults with chronic liver disease. Search methods We searched the Cochrane Hepato‐Biliary Trials Register, Cochrane Hepato‐Biliary Diagnostic‐Test‐Accuracy Studies Register, the Cochrane Library, MEDLINE, Embase, LILACS, Science Citation Index Expanded, and Conference Proceedings Citation Index – Science until 4 May 2021. We applied no language or document‐type restrictions. Selection criteria Studies assessing the diagnostic accuracy of CT for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease, with cross‐sectional designs, using one of the acceptable reference standards, such as pathology of the explanted liver and histology of resected or biopsied focal liver lesion with at least a six‐month follow‐up. Data collection and analysis At least two review authors independently screened studies, extracted data, and assessed the risk of bias and applicability concerns, using the QUADAS‐2 checklist. We presented the results of sensitivity and specificity, using paired forest plots, and tabulated the results. We used a hierarchical meta‐analysis model where appropriate. We presented uncertainty of the accuracy estimates using 95% confidence intervals (CIs). We double‐checked all data extractions and analyses. Main results We included 21 studies, with a total of 3101 participants. We judged all studies to be at high risk of bias in at least one domain because most studies used different reference standards, often inappropriate to exclude the presence of the target condition, and the time‐interval between the index test and the reference standard was rarely defined. Regarding applicability in the patient selection domain, we judged 14% (3/21) of studies to be at low concern and 86% (18/21) of studies to be at high concern owing to characteristics of the participants who were on waiting lists for orthotopic liver transplantation. CT for hepatocellular carcinoma of any size and stage: sensitivity 77.5% (95% CI 70.9% to 82.9%) and specificity 91.3% (95% CI 86.5% to 94.5%) (21 studies, 3101 participants; low‐certainty evidence). CT for resectable hepatocellular carcinoma: sensitivity 71.4% (95% CI 60.3% to 80.4%) and specificity 92.0% (95% CI 86.3% to 95.5%) (10 studies, 1854 participants; low‐certainty evidence). In the three studies at low concern for applicability (861 participants), we found sensitivity 76.9% (95% CI 50.8% to 91.5%) and specificity 89.2% (95% CI 57.0% to 98.1%). The observed heterogeneity in the results remains mostly unexplained. The sensitivity analyses, which included only studies with clearly prespecified positivity criteria and only studies in which the reference standard results were interpreted without knowledge of the results of the index test, showed no variation in the results. Authors' conclusions In the clinical pathway for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease, CT has roles as a confirmatory test for hepatocellular carcinoma lesions, and for staging assessment. We found that using CT in detecting hepatocellular carcinoma of any size and stage, 22.5% of people with hepatocellular carcinoma would be missed, and 8.7% of people without hepatocellular carcinoma would be unnecessarily treated. For resectable hepatocellular carcinoma, we found that 28.6% of people with resectable hepatocellular carcinoma would improperly not be resected, while 8% of people without hepatocellular carcinoma would undergo inappropriate surgery. The uncertainty resulting from the high risk of bias in the included studies and concerns regarding their applicability limit our ability to confidently draw conclusions based on our results. Plain language summary How accurate are computerised tomography (CT) scans for detecting liver cancer? Key messages In people with chronic liver disease, · computerised tomography (CT: cross‐sectional scans inside the body) probably misses liver cancer in 22.5% of people who would not receive timely or appropriate treatment, and also, CT incorrectly finds liver cancer in 8.7% of people who would receive unnecessary treatment. · CT probably misses liver cancer in 28.6% of people with liver cancer who could have surgery to remove part of their liver, and CT incorrectly finds liver cancer in 7.7% of people who undergo inappropriate surgery. · The studies were too different from each other to allow us to draw firm conclusions based on the evidence. Why is it important to diagnose liver cancer accurately? Liver cancer, or ‘hepatocellular carcinoma’ occurs mostly in people with chronic liver disease, regardless of the cause. It is the sixth most common cancer in the world and the fourth most common cause of death due to cancer. It is difficult to diagnose because early symptoms are similar to those of liver disease. People with blood test or ultrasound results that suggest liver cancer may go on to have further tests, such as scans that produce images of the liver, or biopsy where a small piece of the liver is removed and examined. If liver cancer is detected early, people may be treated with surgery to remove part of the liver (a liver resection) or with a liver transplant. If the liver cancer is more advanced, people may need chemotherapy. If liver cancer is missed, people will not receive appropriate treatment. However, incorrectly diagnosing liver cancer when it is not present means that people may undergo unnecessary testing or treatment. What is computed tomography and how might it diagnose liver cancer? Computed tomography (CT) produces images that show a cross‐section or ‘slice’ of the bones, blood vessels and tissues inside the body. The images consist of a series of X‐rays that are directed and combined by a computer. CT scans can detect the presence of abnormalities in the liver that might be cancer. Current guidelines recommend using either CT or another type of imaging, magnetic resonance imaging (MRI), to confirm the presence of liver cancer in people who might have liver cancer, and to judge the size and spread (stage) of the cancer. What did we want to find out? We wanted to find out if CT is accurate enough to diagnose liver cancer in adults with chronic liver disease. We were interested firstly, in liver cancers of any size and stage and secondly, in liver cancers that were suitable for resection. What did we do? We searched for studies that assessed the accuracy of CT scans compared to the best available tests to confirm liver cancer in adults with chronic liver disease. The best available tests are examination of the liver, or part of the liver under a microscope. What did we find? We found a total of 21 studies with 3101 people. Based on the studies, around 520 (52%) out of 1000 adults with chronic liver disease have confirmed liver cancer. Of these 1000 people, CT may: · correctly detect liver cancer in 403 people · miss liver cancer in 117 people · incorrectly detect liver cancer in 42 cancer‐free people · correctly detect no liver cancer in 438 people. Based on the studies, around 350 (35%) out of 1000 adults with chronic liver disease have confirmed resectable liver cancer. Of these 1000 people, CT may: · correctly detect resectable liver cancer in 250 people · miss resectable liver cancer in 100 people · incorrectly detect resectable liver cancer in 50 people; and · correctly detect no resectable liver cancer in 600 people. What are the limitations of the evidence? Our confidence in the evidence is limited because the studies used different methods to select study participants and used different definitions for the presence of liver disease. This means CT scans could be more or less accurate than suggested by the evidence. How up to date is this evidence? The evidence is up to date to 4 May 2021.","10","John Wiley & Sons, Ltd","1465-1858","*Carcinoma, Hepatocellular [diagnostic imaging]; *Liver Neoplasms [diagnostic imaging]; Adult; Cross-Sectional Studies; Humans; Sensitivity and Specificity; Tomography, X-Ray Computed; Ultrasonography","10.1002/14651858.CD013362.pub2","http://dx.doi.org/10.1002/14651858.CD013362.pub2","Hepato-Biliary"
"CD011482.PUB2","van Gruting, IM A; Stankiewicz, A; Thakar, R; Santoro, GA; IntHout, J; Sultan, AH","Imaging modalities for the detection of posterior pelvic floor disorders in women with obstructed defaecation syndrome","Cochrane Database of Systematic Reviews","2021","Abstract - Background Obstructed defaecation syndrome (ODS) is difficulty in evacuating stools, requiring straining efforts at defaecation, having the sensation of incomplete evacuation, or the need to manually assist defaecation. This is due to a physical blockage of the faecal stream during defaecation attempts, caused by rectocele, enterocele, intussusception, anismus or pelvic floor descent. Evacuation proctography (EP) is the most common imaging technique for diagnosis of posterior pelvic floor disorders. It has been regarded as the reference standard because of extensive experience, although it has been proven not to have perfect accuracy. Moreover, EP is invasive, embarrassing and uses ionising radiation. Alternative imaging techniques addressing these issues have been developed and assessed for their accuracy. Because of varying results, leading to a lack of consensus, a systematic review and meta‐analysis of the literature are required. Objectives To determine the diagnostic test accuracy of EP, dynamic magnetic resonance imaging (MRI) and pelvic floor ultrasound for the detection of posterior pelvic floor disorders in women with ODS, using latent class analysis in the absence of a reference standard, and to assess whether MRI or ultrasound could replace EP. The secondary objective was to investigate differences in diagnostic test accuracy in relation to the use of rectal contrast, evacuation phase, patient position and cut‐off values, which could influence test outcome. Search methods We ran an electronic search on 18 December 2019 in the Cochrane Library, MEDLINE, Embase, SCI, CINAHL and CPCI. Reference list, Google scholar. We also searched WHO ICTRP and clinicaltrials.gov for eligible articles. Two review authors conducted title and abstract screening and full‐text assessment, resolving disagreements with a third review author. Selection criteria Diagnostic test accuracy and cohort studies were eligible for inclusion if they evaluated the test accuracy of EP, and MRI or pelvic floor ultrasound, or both, for the detection of posterior pelvic floor disorders in women with ODS. We excluded case‐control studies. If studies partially met the inclusion criteria, we contacted the authors for additional information. Data collection and analysis Two review authors performed data extraction, including study characteristics, 'Risk‐of‐bias' assessment, sources of heterogeneity and test accuracy results. We excluded studies if test accuracy data could not be retrieved despite all efforts. We performed meta‐analysis using Bayesian hierarchical latent class analysis. For the index test to qualify as a replacement test for EP, both sensitivity and specificity should be similar or higher than the historic reference standard (EP), and for a triage test either specificity or sensitivity should be similar or higher. We conducted heterogeneity analysis assessing the effect of different test conditions on test accuracy. We ran sensitivity analyses by excluding studies with high risk of bias, with concerns about applicability, or those published before 2010. We assessed the overall quality of evidence (QoE) according to GRADE. Main results Thirty‐nine studies covering 2483 participants were included into the meta‐analyses. We produced pooled estimates of sensitivity and specificity for all index tests for each target condition. Findings of the sensitivity analyses were consistent with the main analysis. Sensitivity of EP for diagnosis of rectocele was 98% (credible interval (CrI)94%‐99%), enterocele 91%(CrI 83%‐97%), intussusception 89%(CrI 79%‐96%) and pelvic floor descent 98%(CrI 93%‐100%); specificity for enterocele was 96%(CrI 93%‐99%), intussusception 92%(CrI 86%‐97%) and anismus 97%(CrI 94%‐99%), all with high QoE. Moderate to low QoE showed a sensitivity for anismus of 80%(CrI 63%‐94%), and specificity for rectocele of 78%(CrI 63%‐90%) and pelvic floor descent 83%(CrI 59%‐96%). Specificity of MRI for diagnosis of rectocele was 90% (CrI 79%‐97%), enterocele 99% (CrI 96%‐100%) and intussusception 97% (CrI 88%‐100%), meeting the criteria for a triage test with high QoE. MRI did not meet the criteria to replace EP. Heterogeneity analysis showed that sensitivity of MRI performed with evacuation phase was higher than without for rectocele (94%, CrI 87%‐98%) versus 65%, CrI 52% to 89%, and enterocele (87%, CrI 74%‐95% versus 62%, CrI 51%‐88%), and sensitivity of MRI without evacuation phase was significantly lower than EP. Specificity of transperineal ultrasound (TPUS) for diagnosis of rectocele was 89% (CrI 81%‐96%), enterocele 98% (CrI 95%‐100%) and intussusception 96% (CrI 91%‐99%); sensitivity for anismus was 92% (CrI 72%‐98%), meeting the criteria for a triage test with high QoE. TPUS did not meet the criteria to replace EP. Heterogeneity analysis showed that sensitivity of TPUS performed with rectal contrast was not significantly higher than without for rectocele(92%, CrI 69%‐99% versus 81%, CrI 58%‐95%), enterocele (90%, CrI 71%‐99% versus 67%, CrI 51%‐90%) and intussusception (90%, CrI 69%‐98% versus 61%, CrI 51%‐86%), and was lower than EP. Specificity of endovaginal ultrasound (EVUS) for diagnosis of rectocele was 76% (CrI 54%‐93%), enterocele 97% (CrI 80%‐99%) and intussusception 93% (CrI 72%‐99%); sensitivity for anismus was 84% (CrI 59%‐96%), meeting the criteria for a triage test with very low to moderate QoE. EVUS did not meet the criteria to replace EP. Specificity of dynamic anal endosonography (DAE) for diagnosis of rectocele was 88% (CrI 62%‐99%), enterocele 97% (CrI 75%‐100%) and intussusception 93% (CrI 65%‐99%), meeting the criteria for a triage test with very low to moderate QoE. DAE did not meet the criteria to replace EP. Echodefaecography (EDF) had a sensitivity of 89% (CrI 65%‐98%) and specificity of 92% (CrI 72%‐99%) for intussusception, meeting the criteria to replace EP but with very low QoE. Specificity of EDF for diagnosis of rectocele was 89% (CrI 60%‐99%) and for enterocele 97% (CrI 87%‐100%); sensitivity for anismus was 87% (CrI 72%‐96%), meeting the criteria for a triage test with low to very low QoE. Authors' conclusions In a population of women with symptoms of ODS, none of the imaging techniques met the criteria to replace EP. MRI and TPUS met the criteria of a triage test, as a positive test confirms diagnosis of rectocele, enterocele and intussusception, and a negative test rules out diagnosis of anismus. An evacuation phase increased sensitivity of MRI. Rectal contrast did not increase sensitivity of TPUS. QoE of EVUS, DAE and EDF was too low to draw conclusions. More well‐designed studies are required to define their role in the diagnostic pathway of ODS. Plain language summary Is evacuation proctogram still the reference standard for diagnosis of posterior pelvic floor disorders in women with obstructed defaecation syndrome? The issue Obstructed defaecation syndrome is a sensation of obstruction during attempts to empty the bowel, a feeling of incomplete bowel emptying, or the need to use a finger to splint the perineum/vagina or insert into the rectum to remove stool. This can cause embarrassment and frustration, leading to an adverse effect on quality of life. Different imaging techniques exist to examine women with these symptoms. The most commonly performed technique currently used is called evacuation proctography (EP). This test can cause embarrassment, as it requires the woman to have a large amount of a porridge‐like substance introduced via the back passage and then she has to sit on a commode and open her bowels whilst X‐ray images are being taken by the radiologist. Why is this review important? Other imaging techniques to assess women with these symptoms are available, and most of them are less embarrassing. However, it remains unclear how good these imaging techniques are in diagnosing the conditions that cause these symptoms. To be able to provide evidence for potential use of these less embarrassing imaging techniques, existing data of previously‐published studies reporting the accuracy (the ability to detect and exclude a specific disorder) of these imaging techniques need to be analysed. How was this review conducted? We searched the available literature on 18 December 2019. We selected studies that assessed the performance of magnetic resonance imaging (MRI) or pelvic floor ultrasound, or both, and EP in women with symptoms of obstructed defaecation. We assessed the quality of the included studies, as well as possible sources that might influence the performance of imaging techniques. We conducted statistical analysis by assessing all available imaging techniques equally, in the absence of a reference standard, to calculate the test accuracy of all imaging techniques under evaluation. What are the findings? We included 39 studies covering 2483 women in the meta‐analysis. EP was found to have the highest ability to correctly detect most conditions causing symptoms of obstructed defaecation; none of the other diagnostic tests met the criteria to replace EP. MRI and transperineal ultrasound (TPUS) met the criteria for a triage test. They are better able to correctly identify healthy patients than EP. This means that a positive test result suggests the presence of the disease, as the test rarely gives positive results in healthy women, and avoids further testing. The results of the other ultrasound techniques were of too low a quality of evidence to draw conclusions. What does this mean? In a population of women seeking help for their symptoms of obstructed defaecation, EP remains the test of choice. MRI and TPUS can be used for the initial assessment of women with obstructed defaecation as a screening test. TPUS or MRI could therefore potentially reduce the number of women having to undergo EP.","9","John Wiley & Sons, Ltd","1465-1858","*Pelvic Floor Disorders [complications, diagnostic imaging]; Bayes Theorem; Defecation; Defecography; Female; Humans; Ultrasonography","10.1002/14651858.CD011482.pub2","http://dx.doi.org/10.1002/14651858.CD011482.pub2","Colorectal"
"CD013207.PUB2","Ochodo, EA; Guleid, F; Deeks, JJ; Mallett, S","Point‐of‐care tests detecting HIV nucleic acids for diagnosis of HIV‐1 or HIV‐2 infection in infants and children aged 18 months or less","Cochrane Database of Systematic Reviews","2021","Abstract - Background The standard method of diagnosing HIV in infants and children less than 18 months is with a nucleic acid amplification test reverse transcriptase polymerase chain reaction test (NAT RT‐PCR) detecting viral ribonucleic acid (RNA). Laboratory testing using the RT‐PCR platform for HIV infection is limited by poor access, logistical support, and delays in relaying test results and initiating therapy in low‐resource settings. The use of rapid diagnostic tests at or near the point‐of‐care (POC) can increase access to early diagnosis of HIV infection in infants and children less than 18 months of age and timely initiation of antiretroviral therapy (ART). Objectives To summarize the diagnostic accuracy of point‐of‐care nucleic acid‐based testing (POC NAT) to detect HIV‐1/HIV‐2 infection in infants and children aged 18 months or less exposed to HIV infection. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL) (until 2 February 2021), MEDLINE and Embase (until 1 February 2021), and LILACS and Web of Science (until 2 February 2021) with no language or publication status restriction. We also searched conference websites and clinical trial registries, tracked reference lists of included studies and relevant systematic reviews, and consulted experts for potentially eligible studies. Selection criteria We defined POC tests as rapid diagnostic tests conducted at or near the patient site. We included any primary study that compared the results of a POC NAT to a reference standard of laboratory NAT RT‐PCR or total nucleic acid testing to detect the presence or absence of HIV infection denoted by HIV viral nucleic acids in infants and children aged 18 months or less who were exposed to HIV‐1/HIV‐2 infection. We included cross‐sectional, prospective, and retrospective study designs and those that provided sufficient data to create the 2 × 2 table to calculate sensitivity and specificity. We excluded diagnostic case control studies with healthy controls. Data collection and analysis We extracted information on study characteristics using a pretested standardized data extraction form. We used the QUADAS‐2 (Quality Assessment of Diagnostic Accuracy Studies) tool to assess the risk of bias and applicability concerns of the included studies. Two review authors independently selected and assessed the included studies, resolving any disagreements by consensus. The unit of analysis was the participant. We first conducted preliminary exploratory analyses by plotting estimates of sensitivity and specificity from each study on forest plots and in receiver operating characteristic (ROC) space. For the overall meta‐analyses, we pooled estimates of sensitivity and specificity using the bivariate meta‐analysis model at a common threshold (presence or absence of infection). Main results We identified a total of 12 studies (15 evaluations, 15,120 participants). All studies were conducted in sub‐Saharan Africa. The ages of included infants and children in the evaluations were as follows: at birth (n = 6), ≤ 12 months (n = 3), ≤ 18 months (n = 5), and ≤ 24 months (n = 1). Ten evaluations were field evaluations of the POC NAT test at the point of care, and five were laboratory evaluations of the POC NAT tests.The POC NAT tests evaluated included Alere q HIV‐1/2 Detect qualitative test (recently renamed m‐PIMA q HIV‐1/2 Detect qualitative test) (n = 6), Xpert HIV‐1 qualitative test (n = 6), and SAMBA HIV‐1 qualitative test (n = 3). POC NAT pooled sensitivity and specificity (95% confidence interval (CI)) against laboratory reference standard tests were 98.6% (96.1 to 99.5) (15 evaluations, 1728 participants) and 99.9% (99.7 to 99.9) (15 evaluations, 13,392 participants) in infants and children ≤ 18 months. Risk of bias in the included studies was mostly low or unclear due to poor reporting. Five evaluations had some concerns for applicability for the index test, as they were POC tests evaluated in a laboratory setting, but there was no difference detected between settings in sensitivity (−1.3% (95% CI −4.1 to 1.5)); and specificity results were similar. Authors' conclusions For the diagnosis of HIV‐1/HIV‐2 infection, we found the sensitivity and specificity of POC NAT tests to be high in infants and children aged 18 months or less who were exposed to HIV infection. Plain language summary Point‐of‐care tests for detecting HIV viral molecules in infants and children aged 18 months or less Why is improving the diagnosis of HIV infection important? It is estimated that 1.5 million infants are still exposed to HIV every year. If left untreated, about 50% to 60% of HIV‐infected infants will die by the age of two years. Children infected before birth are especially at high risk of death. HIV is incurable; however, there are medications that suppress HIV, known as antiretroviral drugs (ART). When HIV is detected early, severe illness and death from HIV‐related infections can be prevented by taking this medication. A test that detects HIV viral genetic molecules quickly and accurately at or near the patient's side (point‐of‐care) therefore can increase access to early appropriate treatment and minimize missing treatments in those whose HIV remains undetected. What is the aim of this review? To determine the accuracy of molecular point‐of‐care tests for detecting the main types of HIV infection (HIV‐1/HIV‐2) in infants and children aged 18 months or less. What was studied in this review? Published reports of molecular point‐of‐care tests with results measured against laboratory viral‐based tests (benchmark). What are the main results of this review? Twelve studies which completed 15 evaluations involving 15,120 participants compared molecular point‐of‐care tests for diagnosing HIV infection. What are the strengths and limitations of this review? The review included sufficient studies and participants. All studies were conducted in sub‐Saharan Africa, making the results highly applicable for use in communities where the disease is regularly found and where disease control programmes are often targeted. However, one in three included evaluations of the molecular point‐of‐care tests were conducted in a laboratory setting and not near the patient but there was no difference in the test accuracy between settings. To whom do the results of this review apply? Infants and children aged 18 months or less who were exposed to HIV infection. What are the implications of this review? In theory, for a population of 1000 children aged 18 months or less where 100 have HIV infection, 100 children will be positive with the molecular point‐of‐care test, of which one will not have the infection (false‐positive result), and 900 will be negative with the molecular point‐of‐care test, of which one will indeed have the infection (false‐negative result). How up‐to‐date is this review? The evidence is current to 2 February 2021.","8","John Wiley & Sons, Ltd","1465-1858","*Point-of-Care Testing; Cross-Sectional Studies; Female; HIV Infections [*diagnosis]; HIV-1 [*genetics, isolation & purification]; HIV-2 [*genetics, isolation & purification]; Humans; Infant; Infant, Newborn; Male; Polymerase Chain Reaction [*methods]; Reverse Transcriptase Polymerase Chain Reaction; Sensitivity and Specificity","10.1002/14651858.CD013207.pub2","http://dx.doi.org/10.1002/14651858.CD013207.pub2","Infectious Diseases"
"CD010783.PUB3","Arevalo-Rodriguez, I; Smailagic, N; Roqué-Figuls, M; Ciapponi, A; Sanchez-Perez, E; Giannakou, A; Pedraza, OL; Bonfill Cosp, X; Cullum, S","Mini‐Mental State Examination (MMSE) for the early detection of dementia in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2021","Abstract - Background Dementia is a progressive global cognitive impairment syndrome. In 2010, more than 35 million people worldwide were estimated to be living with dementia. Some people with mild cognitive impairment (MCI) will progress to dementia but others remain stable or recover full function. There is great interest in finding good predictors of dementia in people with MCI. The Mini‐Mental State Examination (MMSE) is the best‐known and the most often used short screening tool for providing an overall measure of cognitive impairment in clinical, research and community settings. Objectives To determine the accuracy of the Mini Mental State Examination for the early detection of dementia in people with mild cognitive impairment Search methods We searched ALOIS (Cochrane Dementia and Cognitive Improvement Specialized Register of diagnostic and intervention studies (inception to May 2014); MEDLINE (OvidSP) (1946 to May 2014); EMBASE (OvidSP) (1980 to May 2014); BIOSIS (Web of Science) (inception to May 2014); Web of Science Core Collection, including the Conference Proceedings Citation Index (ISI Web of Science) (inception to May 2014); PsycINFO (OvidSP) (inception to May 2014), and LILACS (BIREME) (1982 to May 2014). We also searched specialized sources of diagnostic test accuracy studies and reviews, most recently in May 2014: MEDION (Universities of Maastricht and Leuven, www.mediondatabase.nl), DARE (Database of Abstracts of Reviews of Effects, via the Cochrane Library), HTA Database (Health Technology Assessment Database, via the Cochrane Library), and ARIF (University of Birmingham, UK, www.arif.bham.ac.uk). No language or date restrictions were applied to the electronic searches and methodological filters were not used as a method to restrict the search overall so as to maximize sensitivity. We also checked reference lists of relevant studies and reviews, tracked citations in Scopus and Science Citation Index, used searches of known relevant studies in PubMed to track related articles, and contacted research groups conducting work on MMSE for dementia diagnosis to try to locate possibly relevant but unpublished data. Selection criteria We considered longitudinal studies in which results of the MMSE administered to MCI participants at baseline were obtained and the reference standard was obtained by follow‐up over time. We included participants recruited and clinically classified as individuals with MCI under Petersen and revised Petersen criteria, Matthews criteria, or a Clinical Dementia Rating = 0.5. We used acceptable and commonly used reference standards for dementia in general, Alzheimer’s dementia, Lewy body dementia, vascular dementia and frontotemporal dementia. Data collection and analysis We screened all titles generated by the electronic database searches. Two review authors independently assessed the abstracts of all potentially relevant studies. We assessed the identified full papers for eligibility and extracted data to create two by two tables for dementia in general and other dementias. Two authors independently performed quality assessment using the QUADAS‐2 tool. Due to high heterogeneity and scarcity of data, we derived estimates of sensitivity at fixed values of specificity from the model we fitted to produce the summary receiver operating characteristic curve. Main results In this review, we included 11 heterogeneous studies with a total number of 1569 MCI patients followed for conversion to dementia. Four studies assessed the role of baseline scores of the MMSE in conversion from MCI to all‐cause dementia and eight studies assessed this test in conversion from MCI to Alzheimer´s disease dementia. Only one study provided information about the MMSE and conversion from MCI to vascular dementia. For conversion from MCI to dementia in general, the accuracy of baseline MMSE scores ranged from sensitivities of 23% to 76% and specificities from 40% to 94%. In relationship to conversion from MCI to Alzheimer’s disease dementia, the accuracy of baseline MMSE scores ranged from sensitivities of 27% to 89% and specificities from 32% to 90%. Only one study provided information about conversion from MCI to vascular dementia, presenting a sensitivity of 36% and a specificity of 80% with an incidence of vascular dementia of 6.2%. Although we had planned to explore possible sources of heterogeneity, this was not undertaken due to the scarcity of studies included in our analysis. Authors' conclusions Our review did not find evidence supporting a substantial role of MMSE as a stand‐alone single‐administration test in the identification of MCI patients who could develop dementia. Clinicians could prefer to request additional and extensive tests to be sure about the management of these patients. An important aspect to assess in future updates is if conversion to dementia from MCI stages could be predicted better by MMSE changes over time instead of single measurements. It is also important to assess if a set of tests, rather than an isolated one, may be more successful in predicting conversion from MCI to dementia. Plain language summary Baseline scores of Mini‐Mental State examination (MMSE) for early prediction of developing dementia in people with mild cognitive impairments (MCI) Patients with MCI should be evaluated and monitored due to their increased risk of progression to dementia. At present there are no agreements about what the best approach is to register the progression to dementia. Several cognitive function tests have been proposed for this task because most of them are easy to administer, take no longer than 10 minutes to complete, involve major executive functions, and yield an objective score. Our review assessed the current evidence related to one of those brief tests, the Mini‐Mental State Examination (MMSE), in the prediction of decline to dementia in people with cognitive impairments. After an extensive search and analysis of available information, we did not find evidence supporting a substantial role of MMSE as a stand‐alone single‐administration test in the identification of patients who will convert to dementia in the future.","7","John Wiley & Sons, Ltd","1465-1858","*Mental Status and Dementia Tests; Alzheimer Disease [diagnosis]; Cognitive Dysfunction [*complications]; Dementia [*diagnosis, etiology]; Dementia, Vascular [diagnosis, etiology]; Disease Progression; Early Diagnosis; Frontotemporal Dementia [diagnosis, etiology]; Humans; Lewy Body Disease [diagnosis, etiology]; Neuropsychological Tests; Sensitivity and Specificity","10.1002/14651858.CD010783.pub3","http://dx.doi.org/10.1002/14651858.CD010783.pub3","Dementia and Cognitive Improvement"
"CD013786.PUB2","McCleery, J; Laverty, J; Quinn, TJ","Diagnostic test accuracy of telehealth assessment for dementia and mild cognitive impairment","Cochrane Database of Systematic Reviews","2021","Abstract - Background Many millions of people living with dementia around the world are not diagnosed, which has a negative impact both on their access to care and treatment and on rational service planning. Telehealth ‐ the use of information and communication technology (ICT) to provide health services at a distance ‐ may be a way to increase access to specialist assessment for people with suspected dementia, especially those living in remote or rural areas. It has also been much used during the COVID‐19 pandemic. It is important to know whether diagnoses made using telehealth assessment are as accurate as those made in conventional, face‐to‐face clinical settings. Objectives Primary objective: to assess the diagnostic accuracy of telehealth assessment for dementia and mild cognitive impairment. Secondary objectives: to identify the quality and quantity of the relevant research evidence; to identify sources of heterogeneity in the test accuracy data; to identify and synthesise any data on patient or clinician satisfaction, resource use, costs or feasibility of the telehealth assessment models in the included studies. Search methods We searched multiple databases and clinical trial registers on 4 November 2020 for published and 'grey' literature and registered trials. We applied no search filters and no language restrictions. We screened the retrieved citations in duplicate and assessed in duplicate the full texts of papers considered potentially relevant. Selection criteria We included in the review cross‐sectional studies with 10 or more participants who had been referred to a specialist service for assessment of a suspected cognitive disorder. Within a period of one month or less, each participant had to undergo two clinical assessments designed to diagnose dementia or mild cognitive impairment (MCI): a telehealth assessment (the index test) and a conventional face‐to‐face assessment (the reference standard). The telehealth assessment could be informed by some data collected face‐to‐face, e.g. by nurses working in primary care, but all contact between the patient and the specialist clinician responsible for synthesising the information and making the diagnosis had to take place remotely using ICT. Data collection and analysis Two review authors independently extracted data from included studies. Data extracted covered study design, setting, participants, details of index test and reference standard, and results in the form of numbers of participants given diagnoses of dementia or MCI. Data were also sought on dementia subtype diagnoses and on quantitative measures of patient or clinician satisfaction, resource use, costs and feasibility. We assessed risk of bias and applicability of each included study using QUADAS‐2. We entered the results into 2x2 tables in order to calculate the sensitivity and specificity of telehealth assessment for the diagnosis of all‐cause dementia, MCI, and any cognitive syndrome (combining dementia and MCI). We presented the results of included studies narratively because there were too few studies to derive summary estimates of sensitivity and specificity. Main results Three studies with 136 participants were eligible for inclusion. Two studies (20 and 100 participants) took place in community settings in Australia and one study (16 participants) was conducted in veterans' homes in the USA. Participants were referred from primary care with undiagnosed cognitive symptoms or were identified as being at high risk of having dementia on a screening test in the care homes. Dementia and MCI were target conditions in the larger study; the other studies targeted dementia diagnosis only. Only one small study used a 'pure' telehealth model, i.e. not involving any elements of face‐to‐face assessment. The studies were generally well‐conducted. We considered two studies to be at high risk of incorporation bias because a substantial amount of information collected face‐to‐face by nurses was used to inform both index test and reference standard assessments. One study was at unclear risk of selection bias. For the diagnosis of all‐cause dementia, sensitivity of telehealth assessment ranged from 0.80 to 1.00 and specificity from 0.80 to 1.00. We considered this to be very low‐certainty evidence due to imprecision, inconsistency between studies and risk of bias. For the diagnosis of MCI, data were available from only one study (100 participants) giving a sensitivity of 0.71 (95% CI 0.54 to 0.84) and a specificity of 0.73 (95% CI 0.60 to 0.84). We considered this to be low‐certainty evidence due to imprecision and risk of bias. For diagnosis of any cognitive syndrome (dementia or MCI), data from the same study gave a sensitivity of 0.97 (95% CI 0.91 to 0.99) and a specificity of 0.22 (95% CI 0.03 to 0.60). The majority of diagnostic disagreements concerned the distinction between MCI and dementia, occurring approximately equally in either direction. There was also a tendency for patients identified as cognitively healthy at face‐to‐face assessment to be diagnosed with MCI at telehealth assessment (but numbers were small). There were insufficient data to make any assessment of the accuracy of dementia subtype diagnosis. One study provided a small amount of data indicating a good level of clinician and especially patient satisfaction with the telehealth model. There were no data on resource use, costs or feasibility. Authors' conclusions We found only very few eligible studies with a small number of participants. An important difference between the studies providing data for the analyses was whether the target condition was dementia only (two studies) or dementia and MCI (one study). The data suggest that telehealth assessment may be highly sensitive and specific for the diagnosis of all‐cause dementia when assessed against a reference standard of conventional face‐to‐face assessment, but the estimates are imprecise due to small sample sizes and between‐study heterogeneity, and may apply mainly to telehealth models which incorporate a considerable amount of face‐to‐face contact with healthcare professionals other than the doctor responsible for making the diagnosis. For the diagnosis of MCI by telehealth assessment, best estimates of both sensitivity and specificity were somewhat lower, but were based on a single study. Errors occurred at the cognitively healthy/MCI and the MCI/dementia boundaries. However, there is no evidence that diagnostic disagreements were more frequent than would be expected due to the known variation between clinicians’ opinions when assigning a dementia diagnosis. Plain language summary Accuracy of telehealth assessment for diagnosing dementia and mild cognitive impairment Background Dementia is an illness in which memory and other thinking skills deteriorate to the point that someone can no longer manage their daily activities without assistance. If the memory and thinking problems are milder, so that independent living is not affected, the condition is described as mild cognitive impairment (MCI). Both conditions usually affect older people. It is considered important that people with dementia or MCI can get an accurate diagnosis at a time and place suitable for them so that they and their families can understand the problem and can access treatment and support. However millions of people with dementia around the world never get a diagnosis. There are many reasons for this, but one may be a lack of accessible diagnostic services, particularly for people in rural areas or those who find it difficult to travel. During the COVID‐19 pandemic, many face‐to‐face services were closed. Telehealth ‐ the use of information and communication technology (ICT) to provide health services at a distance may be a way to increase access to specialist assessment for people with suspected dementia who cannot easily get to clinics. However, it is important to be sure that increased accessibility does not come at the expense of accuracy of diagnosis. Review question We asked how accurate telehealth diagnoses of dementia and MCI were compared to diagnoses made when patients attend traditional clinics for a face‐to‐face assessment. What we did We searched databases of medical studies up to 4 November 2020 for studies in which people had two assessments for suspected dementia or MCI: one telehealth assessment and one conventional face‐to‐face assessment. Both assessments were done by specialists and took place within a month of each other. For the telehealth assessment, all contact between the patient and the diagnosing specialist had to have been done remotely, using ICT, but some of the information needed to make the diagnosis could be collected by other members of the healthcare team who saw the patient in person. We then assessed how closely the results of the telehealth assessments agreed with the face‐to‐face assessments. What we found We included three studies (136 participants) who had suspected dementia. One small study (16 participants) was conducted in veterans' homes in the USA; the other two studies were in community services in Australia. They all used videoconference systems for their telehealth assessments. All three studies aimed to make dementia diagnoses but only one also aimed to diagnose MCI. The quality of the studies was generally good. In two studies, nurses who saw the patients in person played a big role in gathering information used in both assessments, which could bias those studies towards close agreement between the assessments. The studies found that telehealth assessment correctly identified 80% to 100% of the people who were diagnosed with dementia at face‐to‐face assessment and also correctly identified 80% to 100% of people who did not have dementia. Only one study (100 participants) attempted to diagnose MCI. In this study, 71% of participants who had MCI and 73% of participants who did not have MCI were correctly identified using telehealth assessment. Telehealth assessment in this study correctly identified 97% of the participants who had  either  MCI  or  dementia, but correctly identified only 22% of those who did not have either, although again this result was very uncertain because of the very small number of people in this category. It is important to note that diagnoses of dementia and MCI made by two specialists seeing patients face‐to‐face will not show 100% agreement. Therefore perfect agreement between telehealth and face‐to‐face assessments cannot be expected. What we concluded From the evidence we found, telehealth assessment for diagnosing dementia seems to have a good level of accuracy when compared to face‐to‐face assessment, although the small number of studies and participants and differences between the included studies means that there is a lot of uncertainty about this result. Telehealth appeared to be a little less accurate for diagnosing MCI than for diagnosing dementia. Agreement between two face‐to‐face assessments is also not perfect and we cannot say that disagreements between telehealth and face‐to‐face diagnoses were any more common.","7","John Wiley & Sons, Ltd","1465-1858","Bias; COVID-19 [epidemiology]; Cognitive Dysfunction [*diagnosis]; Cross-Sectional Studies; Dementia [*diagnosis]; Health Services Accessibility; Humans; Patient Satisfaction; Reference Standards; Sensitivity and Specificity; Telemedicine [*standards]","10.1002/14651858.CD013786.pub2","http://dx.doi.org/10.1002/14651858.CD013786.pub2","Dementia and Cognitive Improvement"
"CD010276.PUB3","Walsh, T; Macey, R; Kerr, AR; Lingen, MW; Ogden, GR; Warnakulasuriya, S","Diagnostic tests for oral cancer and potentially malignant disorders in patients presenting with clinically evident lesions","Cochrane Database of Systematic Reviews","2021","Abstract - Background Squamous cell carcinoma is the most common form of malignancy of the oral cavity, and is often proceeded by oral potentially malignant disorders (OPMD). Early detection of oral cavity squamous cell carcinoma (oral cancer) can improve survival rates. The current diagnostic standard of surgical biopsy with histology is painful for patients and involves a delay in order to process the tissue and render a histological diagnosis; other diagnostic tests are available that are less invasive and some are able to provide immediate results. This is an update of a Cochrane Review first published in 2015. Objectives Primary objective: to estimate the diagnostic accuracy of index tests for the detection of oral cancer and OPMD, in people presenting with clinically evident suspicious and innocuous lesions. Secondary objective: to estimate the relative accuracy of the different index tests. Search methods Cochrane Oral Health's Information Specialist searched the following databases: MEDLINE Ovid (1946 to 20 October 2020), and Embase Ovid (1980 to 20 October 2020). The US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov) and the World Health Organization International Clinical Trials Registry Platform were also searched for ongoing trials to 20 October 2020. No restrictions were placed on the language or date of publication when searching the electronic databases. We conducted citation searches, and screened reference lists of included studies for additional references. Selection criteria We selected studies that reported the diagnostic test accuracy of the following index tests when used as an adjunct to conventional oral examination in detecting OPMD or oral cavity squamous cell carcinoma: vital staining (a dye to stain oral mucosa tissues), oral cytology, light‐based detection and oral spectroscopy, blood or saliva analysis (which test for the presence of biomarkers in blood or saliva). Data collection and analysis Two review authors independently screened titles and abstracts for relevance. Eligibility, data extraction and quality assessment were carried out by at least two authors, independently and in duplicate. Studies were assessed for methodological quality using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2). Meta‐analysis was used to combine the results of studies for each index test using the bivariate approach to estimate the expected values of sensitivity and specificity. Main results This update included 63 studies (79 datasets) published between 1980 and 2020 evaluating 7942 lesions for the quantitative meta‐analysis. These studies evaluated the diagnostic accuracy of conventional oral examination with: vital staining (22 datasets), oral cytology (24 datasets), light‐based detection or oral spectroscopy (24 datasets). Nine datasets assessed two combined index tests. There were no eligible diagnostic accuracy studies evaluating blood or salivary sample analysis. Two studies were classed as being at low risk of bias across all domains, and 33 studies were at low concern for applicability across the three domains, where patient selection, the index test, and the reference standard used were generalisable across the population attending secondary care. The summary estimates obtained from the meta‐analysis were: ‐ vital staining: sensitivity 0.86 (95% confidence interval (CI) 0.79 to 0.90) specificity 0.68 (95% CI 0.58 to 0.77), 20 studies, sensitivity low‐certainty evidence, specificity very low‐certainty evidence; ‐ oral cytology: sensitivity 0.90 (95% CI 0.82 to 0.94) specificity 0.94 (95% CI 0.88 to 0.97), 20 studies, sensitivity moderate‐certainty evidence, specificity moderate‐certainty evidence; ‐ light‐based: sensitivity 0.87 (95% CI 0.78 to 0.93) specificity 0.50 (95% CI 0.32 to 0.68), 23 studies, sensitivity low‐certainty evidence, specificity very low‐certainty evidence; and ‐ combined tests: sensitivity 0.78 (95% CI 0.45 to 0.94) specificity 0.71 (95% CI 0.53 to 0.84), 9 studies, sensitivity very low‐certainty evidence, specificity very low‐certainty evidence. Authors' conclusions At present none of the adjunctive tests can be recommended as a replacement for the currently used standard of a surgical biopsy and histological assessment. Given the relatively high values of the summary estimates of sensitivity and specificity for oral cytology, this would appear to offer the most potential. Combined adjunctive tests involving cytology warrant further investigation. Potentially eligible studies of blood and salivary biomarkers were excluded from the review as they were of a case‐control design and therefore ineligible. In the absence of substantial improvement in the tests evaluated in this updated review, further research into biomarkers may be warranted. Plain language summary What are the most accurate tests for finding cancer of the mouth (oral cancer) and conditions that may lead to oral cancer? Why is it important to improve the detection of oral cancer? Persistent abnormal patches or sores in the mouth may represent mouth cancer or oral potentially malignant disorders (OPMDs). OPMDs can sometimes turn into mouth cancer, but if identified early enough patient outcomes can be improved. What is the aim of this review? Diagnosing mouth cancer involves the surgical removal of a piece of affected tissue (biopsy) that is then sent to a laboratory for examination of the cells using a microscope. This is painful for patients and involves a delay in finding out the results. The aim of this Cochrane Review review was to find out the accuracy of less invasive diagnostic tests that may provide more timely results. Researchers in Cochrane included 63 studies to answer this question. What was studied in the review? Three tests used in addition to a visual examination were evaluated. ‐ Vital stain: a liquid that can be used as a mouthrinse or applied directly to a suspected abnormal area of the mouth. It is thought that any area that is coloured after applying this liquid has a high chance of being mouth cancer or an OPMD. ‐ Oral cytology: a brush is used to remove cells from the suspected abnormal area that are sent to a laboratory for microscopic examination. ‐ Light‐based detection: a special light shone in the mouth that is believed to make cancerous areas appear different to healthy areas. A small number of studies evaluated a combination of these tests. No studies evaluated the accuracy of tests of blood or saliva. What are the main results of the review? The review included 63 studies involving 7942 abnormal patches or mouth sores. Each study participant underwent one or more diagnostic tests as well as a surgical biopsy. The proportion of people in the included studies with mouth cancer or OPMDs identified through surgical biopsy ranged widely from 4% to 97%. Based on adults attending general dental practices in the UK, in a sample of 1000 lesions 25 would be mouth cancer or an OPMD, and 975 lesions would not be mouth cancer or an OPMD. ‐ Of the 1000 lesions that are tested with vital staining: 22 will be correctly identified as having mouth cancer or an OPMD (true positives), but three lesions that truly are mouth cancer or an OPMD will remain undetected; their 'negative' test results will be incorrect (false negatives). 663 lesions will be correctly identified as not having mouth cancer or an OPMD (true negatives), but 312 people will be incorrectly identified; their 'positive' test results will suggest they have mouth cancer or an OPMD (false positives). ‐ For oral cytology: 23 lesions will be correctly identified as having mouth cancer or an OPMD (true positives), but two lesions that truly are mouth cancer or an OPMD will remain undetected (false negatives). 917 lesions will be correctly identified as not having mouth cancer or an OPMD (true negatives), but 58 lesions will be incorrectly identified (false positives). ‐ For light‐based tests: 22 lesions will be correctly identified as having mouth cancer or an OPMD (true positives), but three lesions that truly are mouth cancer or an OPMD will remain undetected (false negatives). 488 lesions will be correctly identified as not having mouth cancer or an OPMD (true negatives), but 487 lesions will be incorrectly identified (false positives). There was considerable variability in the accuracy of the vital staining and light‐based tests which meant that the results of a future study could take a broad range of values. How reliable are the results of the studies in this review? There were shortcomings in many of the studies that put them at high risk of bias. We rated the certainty of the evidence as moderate for oral cytology and low or very low for the remaining tests. Who do the results of this review apply to? Studies included in the review were carried out in many different countries but no studies originated in Africa. Most studies were completed in hospitals. Studies were published between 1980 and 2020. What are the implications of this review? Although cytology was the most accurate of all the tests, none can be recommended as a replacement for the currently used standard of a surgical biopsy and pathology assessment. Most studies investigated patients that had been referred to a hospital clinic for further investigation and so we have only limited information on how accurate they would be when used by a general dentist or frontline medical provider. How up‐to‐date is this review? Review authors searched for and used studies published up to 20 October 2020.","7","John Wiley & Sons, Ltd","1465-1858","Bias; Biomarkers, Tumor [analysis, blood]; Carcinoma, Squamous Cell [*diagnosis, pathology]; Coloring Agents; Early Detection of Cancer; Humans; Light; Lip Neoplasms [diagnosis, pathology]; Mouth Neoplasms [*diagnosis, pathology]; Mouth [pathology]; Saliva [chemistry]; Sensitivity and Specificity","10.1002/14651858.CD010276.pub3","http://dx.doi.org/10.1002/14651858.CD010276.pub3","Oral Health"
"CD010079.PUB3","Quinn, TJ; Fearon, P; Noel-Storr, AH; Young, C; McShane, R; Stott, DJ","Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE) for the detection of dementia within community dwelling populations","Cochrane Database of Systematic Reviews","2021","Abstract - Background Various tools exist for initial assessment of possible dementia with no consensus on the optimal assessment method. Instruments that use collateral sources to assess change in cognitive function over time may have particular utility. The most commonly used informant dementia assessment is the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE). A synthesis of the available data regarding IQCODE accuracy will help inform cognitive assessment strategies for clinical practice, research and policy. Objectives Our primary obective was to determine the accuracy of the informant‐based questionnaire IQCODE for detection of dementia within community dwelling populations. Our secondary objective was to describe the effect of heterogeneity on the summary estimates. We were particularly interested in the traditional 26‐item scale versus the 16‐item short form; and language of administration. We explored the effect of varying the threshold IQCODE score used to define 'test positivity'. Search methods We searched the following sources on 28 January 2013: ALOIS (Cochrane Dementia and Cognitive Improvement Group), MEDLINE (OvidSP), EMBASE (OvidSP), PsycINFO (OvidSP), BIOSIS Previews (ISI Web of Knowledge), Web of Science with Conference Proceedings (ISI Web of Knowledge), LILACS (BIREME). We also searched sources relevant or specific to diagnostic test accuracy: MEDION (Universities of Maastrict and Leuven); DARE (York University); ARIF (Birmingham University). We used sensitive search terms based on MeSH terms and other controlled vocabulary. Selection criteria We selected those studies performed in community settings that used (not necessarily exclusively) the IQCODE to assess for presence of dementia and, where dementia diagnosis was confirmed with clinical assessment. Our intention with limiting the search to a 'community' setting was to include those studies closest to population level assessment. Within our predefined community inclusion criteria, there were relevant papers that fulfilled our definition of community dwelling but represented a selected population, for example stroke survivors. We included these studies but performed sensitivity analyses to assess the effects of these less representative populations on the summary results. Data collection and analysis We screened all titles generated by the electronic database searches and abstracts of all potentially relevant studies were reviewed. Full papers were assessed for eligibility and data extracted by two independent assessors. For quality assessment (risk of bias and applicability) we used the QUADAS 2 tool. We included test accuracy data on the IQCODE used at predefined diagnostic thresholds. Where data allowed, we performed meta‐analyses to calculate summary values of sensitivity and specificity with corresponding 95% confidence intervals (CIs). We pre‐specified analyses to describe the effect of IQCODE format (traditional or short form) and language of administration for the IQCODE. Main results From 16,144 citations, 71 papers described IQCODE test accuracy. We included 10 papers (11 independent datasets) representing data from 2644 individuals (n = 379 (14%) with dementia). Using IQCODE cut‐offs commonly employed in clinical practice (3.3, 3.4, 3.5, 3.6) the sensitivity and specificity of IQCODE for diagnosis of dementia across the studies were generally above 75%. Taking an IQCODE threshold of 3.3 (or closest available) the sensitivity was 0.80 (95% CI 0.75 to 0.85); specificity was 0.84 (95% CI 0.78 to 0.90); positive likelihood ratio was 5.2 (95% CI 3.7 to 7.5) and the negative likelihood ratio was 0.23 (95% CI 0.19 to 0.29). Comparative analysis suggested no significant difference in the test accuracy of the 16 and 26‐item IQCODE tests and no significant difference in test accuracy by language of administration. There was little difference in sensitivity across our predefined diagnostic cut‐points. There was substantial heterogeneity in the included studies. Sensitivity analyses removing potentially unrepresentative populations in these studies made little difference to the pooled data estimates. The majority of included papers had potential for bias, particularly around participant selection and sampling. The quality of reporting was suboptimal particularly regarding timing of assessments and descriptors of reproducibility and inter‐observer variability. Authors' conclusions Published data suggest that if using the IQCODE for community dwelling older adults, the 16 item IQCODE may be preferable to the traditional scale due to lesser test burden and no obvious difference in accuracy. Although IQCODE test accuracy is in a range that many would consider 'reasonable', in the context of community or population settings the use of the IQCODE alone would result in substantial misdiagnosis and false reassurance. Across the included studies there were issues with heterogeneity, several potential biases and suboptimal reporting quality. Plain language summary A structured collateral interview regarding cognition and function (the IQCODE) for assessment of possible dementia Numbers of people with dementia and other cognitive problems are increasing globally. Early diagnosis of dementia is recommended but there is no agreement on the best approach or how non‐memory specialists should assess patients. A potential strategy is to interview friends or family of the subject to assess for change in function or cognition. Various methods of this collateral interview are available and the most commonly used is the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE). We searched differing databases of published research for all papers relating to the accuracy of IQCODE for selecting those with dementia. We found eleven studies that tested diagnostic accuracy of IQCODE in community dwelling individuals, we were able to combine their findings to give a summary result. We compared two forms of IQCODE questionnaire and found that a short form with fewer questions was just as accurate as the original longer questionnaire. The overall accuracy of IQCODE was reasonable although not perfect. If IQCODE were to be used on its own for assessing large populations of older adults, it would label many people with dementia who do not have the disease and also miss the diagnosis in a substantial proportion.","7","John Wiley & Sons, Ltd","1465-1858","*Independent Living; *Proxy; Aged; Aged, 80 and over; Bias; Cognitive Dysfunction [*diagnosis]; Dementia [*diagnosis]; Health Surveys [*standards]; Humans; Reproducibility of Results; Sensitivity and Specificity","10.1002/14651858.CD010079.pub3","http://dx.doi.org/10.1002/14651858.CD010079.pub3","Dementia and Cognitive Improvement"
"CD010772.PUB3","Burton, JK; Fearon, P; Noel-Storr, AH; McShane, R; Stott, DJ; Quinn, TJ","Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE) for the detection of dementia within a secondary care setting","Cochrane Database of Systematic Reviews","2021","Abstract - Background The diagnosis of dementia relies on the presence of new‐onset cognitive impairment affecting an individual's functioning and activities of daily living. The Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE) is a questionnaire instrument, completed by a suitable 'informant' who knows the patient well, designed to assess change in functional performance secondary to cognitive change; it is used as a tool for identifying those who may have dementia. In secondary care there are two specific instances where patients may be assessed for the presence of dementia. These are in the general acute hospital setting, where opportunistic screening may be undertaken, or in specialist memory services where individuals have been referred due to perceived cognitive problems. To ensure an instrument is suitable for diagnostic use in these settings, its test accuracy must be established. Objectives To determine the accuracy of the informant‐based questionnaire IQCODE for detection of dementia in a secondary care setting. Search methods We searched the following sources on the 28th of January 2013: ALOIS (Cochrane Dementia and Cognitive Improvement Group), MEDLINE (Ovid SP), EMBASE (Ovid SP), PsycINFO (Ovid SP), BIOSIS Previews (Thomson Reuters Web of Science), Web of Science Core Collection (includes Conference Proceedings Citation Index) (Thomson Reuters Web of Science), CINAHL (EBSCOhost) and LILACS (BIREME). We also searched sources specific to diagnostic test accuracy: MEDION (Universities of Maastricht and Leuven); DARE (Database of Abstracts of Reviews of Effects ‐ via the Cochrane Library); HTA Database (Health Technology Assessment Database via the Cochrane Library) and ARIF (Birmingham University). We also checked reference lists of relevant studies and reviews, used searches of known relevant studies in PubMed to track related articles, and contacted research groups conducting work on IQCODE for dementia diagnosis to try to find additional studies. We developed a sensitive search strategy; search terms were designed to cover key concepts using several different approaches run in parallel and included terms relating to cognitive tests, cognitive screening and dementia. We used standardised database subject headings such as MeSH terms (in MEDLINE) and other standardised headings (controlled vocabulary) in other databases, as appropriate. Selection criteria We selected those studies performed in secondary‐care settings, which included (not necessarily exclusively) IQCODE to assess for the presence of dementia and where dementia diagnosis was confirmed with clinical assessment. For the 'secondary care' setting we included all studies which assessed patients in hospital (e.g. acute unscheduled admissions, referrals to specialist geriatric assessment services etc.) and those referred for specialist 'memory' assessment, typically in psychogeriatric services. Data collection and analysis We screened all titles generated by electronic database searches, and reviewed abstracts of all potentially relevant studies. Two independent assessors checked full papers for eligibility and extracted data. We determined quality assessment (risk of bias and applicability) using the QUADAS‐2 tool, and reporting quality using the STARD tool. Main results From 72 papers describing IQCODE test accuracy, we included 13 papers, representing data from 2745 individuals (n = 1413 (51%) with dementia). Pooled analysis of all studies using data presented closest to a cut‐off of 3.3 indicated that sensitivity was 0.91 (95% CI 0.86 to 0.94); specificity 0.66 (95% CI 0.56 to 0.75); the positive likelihood ratio was 2.7 (95% CI 2.0 to 3.6) and the negative likelihood ratio was 0.14 (95% CI 0.09 to 0.22). There was a statistically significant difference in test accuracy between the general hospital setting and the specialist memory setting (P = 0.019), suggesting that IQCODE performs better in a 'general' setting. We found no significant differences in the test accuracy of the short (16‐item) versus the 26‐item IQCODE, or in the language of administration. There was significant heterogeneity in the included studies, including a highly varied prevalence of dementia (10.5% to 87.4%). Across the included papers there was substantial potential for bias, particularly around sampling of included participants and selection criteria, which may limit generalisability. There was also evidence of suboptimal reporting, particularly around disease severity and handling indeterminate results, which are important if considering use in clinical practice. Authors' conclusions The IQCODE can be used to identify older adults in the general hospital setting who are at risk of dementia and require specialist assessment; it is useful specifically for ruling out those without evidence of cognitive decline. The language of administration did not affect test accuracy, which supports the cross‐cultural use of the tool. These findings are qualified by the significant heterogeneity, the potential for bias and suboptimal reporting found in the included studies. Plain language summary Assessment of changes in memory and everyday function in older people using a structured questionnaire, the IQCODE Improving how we assess people who may have dementia is a health and social care priority, recent initiatives to increase dementia diagnosis rates have attracted considerable attention. At present we do not have an agreed approach to dementia testing. There are many tests which can help us identify people with the memory and thinking problems suggestive of dementia, but there is no agreement on which tests are best. It is possible that some tests may be better suited to certain healthcare settings than others. Our review was interested in the accuracy of a questionnaire‐based assessment for dementia, called the IQCODE (Informant Questionnaire for Cognitive Decline in the Elderly). We describe how useful the IQCODE is when used in a hospital setting. Under the umbrella term 'hospital' we include specialist memory clinics and old‐age psychiatry units as well as general hospital clinics and wards and the older people's services within them. We searched electronic databases of published research studies, looking for all studies of IQCODE in a hospital setting. We searched from the first available papers in scientific databases up to and including January 2013. We found 13 relevant studies which had results suitable to be combined in a single analysis. Of these papers, six (1352 participants) described studies conducted in “specialist” services such as memory clinics or wards. Three papers (566 participants) described studies conducted in general older adult services and four studies (827 participants) included both specialist and general services. Summarising the available papers, we found that IQCODE was useful for 'ruling out' possible dementia in the general hospital setting. This means if a person has a low score on IQCODE testing they probably do not have dementia. IQCODE was less useful in specialist memory clinics and psychiatry wards. We also found that a short version of the IQCODE gave similar results to the traditional longer version. As part of our assessment we looked at whether the design of the available studies was suitable for the study question. We found several instances where the design of the study could be improved. For example, seven of the thirteen studies only included a selection of all the people attending the service who could have been assessed with IQCODE. We also looked at how well researchers reported the conduct and results of their studies. Again, there were many instances where the reporting could be improved. A common issue was not describing the severity of memory and thinking problems in those thought to have dementia, only reported in three of the included studies. In summary, IQCODE may be a useful tool for assessing adults for possible dementia. There are still a number of unanswered questions around how useful IQCODE may be in hospital settings. For example, before we start using IQCODE routinely we need to describe if it is practical and acceptable to hospital staff, to patients and to their carers. The review was performed by a team based in research centres in the UK (Glasgow, Leicester, Oxford). We had no external funding specific to this study and we have no conflicts of interest that may have influenced our assessment of the research data.","7","John Wiley & Sons, Ltd","1465-1858","*Proxy; *Secondary Care; Activities of Daily Living; Adult; Aged; Cognition Disorders [diagnosis]; Cognitive Dysfunction [*diagnosis]; Confidence Intervals; Dementia [*diagnosis]; Diagnosis, Differential; Health Surveys [*standards]; Hospitals; Humans; Language; Middle Aged; Sensitivity and Specificity","10.1002/14651858.CD010772.pub3","http://dx.doi.org/10.1002/14651858.CD010772.pub3","Dementia and Cognitive Improvement"
"CD010771.PUB3","Burton, JK; Fearon, P; Noel-Storr, AH; McShane, R; Stott, DJ; Quinn, TJ","Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE) for the detection of dementia within a general practice (primary care) setting","Cochrane Database of Systematic Reviews","2021","Abstract - Background The IQCODE (Informant Questionnaire for Cognitive Decline in the Elderly) is a commonly used questionnaire based tool that uses collateral information to assess for cognitive decline and dementia. Brief tools that can be used for dementia ""screening"" or ""triage"" may have particular utility in primary care / general practice healthcare settings but only if they have suitable test accuracy. A synthesis of the available data regarding IQCODE accuracy in a primary care setting should help inform cognitive assessment strategies for clinical practice; research and policy. Objectives To determine the accuracy of the informant‐based questionnaire IQCODE, for detection of dementia in a primary care setting. Search methods A search was performed in the following sources on the 28th of January 2013: ALOIS (Cochrane Dementia and Cognitive Improvement Group), MEDLINE (Ovid SP), EMBASE (Ovid SP), PsycINFO (Ovid SP), BIOSIS (Ovid SP), ISI Web of Science and Conference Proceedings (ISI Web of Knowledge), CINHAL (EBSCOhost) and LILACs (BIREME). We also searched sources specific to diagnostic test accuracy: MEDION (Universities of Maastricht and Leuven); DARE (York University); HTA Database (Health Technology Assessments Database via The Cochrane Library) and ARIF (Birmingham University). We developed a sensitive search strategy; search terms were designed to cover key concepts using several different approaches run in parallel and included terms relating to cognitive tests, cognitive screening and dementia. We used standardized database subject headings such as MeSH terms (in MEDLINE) and other standardized headings (controlled vocabulary) in other databases, as appropriate. Selection criteria We selected those studies performed in primary care settings, which included (not necessarily exclusively) IQCODE to assess for the presence of dementia and where dementia diagnosis was confirmed with clinical assessment. For the ""primary care"" setting, we included those healthcare settings where unselected patients, present for initial, non‐specialist assessment of memory or non‐memory related symptoms; often with a view to onward referral for more definitive assessment. Data collection and analysis We screened all titles generated by electronic database searches and abstracts of all potentially relevant studies were reviewed. Full papers were assessed for eligibility and data extracted by two independent assessors. Quality assessment (risk of bias and applicability) was determined using the QUADAS‐2 tool. Reporting quality was determined using the STARDdem extension to the STARD tool. Main results From 71 papers describing IQCODE test accuracy, we included 1 paper, representing data from 230 individuals (n=16 [7%] with dementia). The paper described those patients consulting a primary care service who self‐identified as Japanese‐American. Dementia diagnosis was made using Benson & Cummings criteria and the IQCODE was recorded as part of a longer interview with the informant. IQCODE accuracy was assessed at various test thresholds, with a ""trade‐off"" between sensitivity and specificity across these cutpoints. At an IQCODE threshold of 3.2 sensitivity: 100%, specificity: 76%; for IQCODE 3.7 sensitivity: 75%, specificity: 98%. Applying the QUADAS‐2 assessments, the study was at high risk of bias in all categories. In particular degree of blinding was unclear and not all participants were included in the final analysis. Authors' conclusions It is not possible to give definitive guidance on the test accuracy of IQCODE for the diagnosis of dementia in a primary care setting based on the single study identified. We are surprised by the lack of research using the IQCODE in primary care as this is, arguably, the most appropriate setting for targeted case finding of those with undiagnosed dementia in order to maximise opportunities to intervene and provide support for the individual and their carers. Plain language summary A structured interview for assessing change in memory and other thinking skills (the IQCODE) for assessment of possible dementia Numbers of people with dementia and other memory and thinking problems are increasing globally. Early diagnosis of dementia is recommended but there is no agreement on the best approach or how non‐memory specialists should assess patients. A potential strategy is to interview friends or family of the subject to assess for change in memory or other thinking skills. Various methods for this ""collateral"" interview are available and the most commonly used is called the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE). We searched differing databases of published research for all papers relating to the accuracy of IQCODE for detecting dementia. We found only one study that tested diagnostic accuracy of IQCODE in a primary care/general practice setting. The study was of a select population (Japanese Americans) and the results may not be applicable to patients in other countries. We also noted issues in the study methods used and the reported results. Based on this single study we are unable to give guidance on how well IQCODE may function as a dementia assessment in primary care. More research is needed in this area as many patients with memory or thinking problems will first consult their general practitioner / family doctor.","7","John Wiley & Sons, Ltd","1465-1858","*Family; *Friends; *General Practice; Asian; Cognitive Dysfunction [*diagnosis]; Dementia [*diagnosis]; Health Surveys [*standards]; Humans; Japan [ethnology]; Primary Health Care; Sensitivity and Specificity; United States","10.1002/14651858.CD010771.pub3","http://dx.doi.org/10.1002/14651858.CD010771.pub3","Dementia and Cognitive Improvement"
"CD011333.PUB3","Burton, JK; Stott, DJ; McShane, R; Noel-Storr, AH; Swann-Price, RS; Quinn, TJ","Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE) for the early detection of dementia across a variety of healthcare settings","Cochrane Database of Systematic Reviews","2021","Abstract - Background The Informant Questionnaire for Cognitive Decline in the Elderly (IQCODE) is a structured interview based on informant responses that is used to assess for possible dementia. IQCODE has been used for retrospective or contemporaneous assessment of cognitive decline. There is considerable interest in tests that may identify those at future risk of developing dementia. Assessing a population free of dementia for the prospective development of dementia is an approach often used in studies of dementia biomarkers. In theory, questionnaire‐based assessments, such as IQCODE, could be used in a similar way, assessing for dementia that is diagnosed on a later (delayed) assessment. Objectives To determine the accuracy of the informant‐based questionnaire IQCODE for the early detection of dementia across a variety of health care settings. Search methods We searched these sources on 16 January 2016: ALOIS (Cochrane Dementia and Cognitive Improvement Group), MEDLINE Ovid SP, Embase Ovid SP, PsycINFO Ovid SP, BIOSIS Previews on Thomson Reuters Web of Science, Web of Science Core Collection (includes Conference Proceedings Citation Index) on Thomson Reuters Web of Science, CINAHL EBSCOhost, and LILACS BIREME. We also searched sources specific to diagnostic test accuracy: MEDION (Universities of Maastricht and Leuven); DARE (Database of Abstracts of Reviews of Effects, in the Cochrane Library); HTA Database (Health Technology Assessment Database, in the Cochrane Library), and ARIF (Birmingham University). We checked reference lists of included studies and reviews, used searches of included studies in PubMed to track related articles, and contacted research groups conducting work on IQCODE for dementia diagnosis to try to find additional studies. We developed a sensitive search strategy; search terms were designed to cover key concepts using several different approaches run in parallel, and included terms relating to cognitive tests, cognitive screening, and dementia. We used standardised database subject headings, such as MeSH terms (in MEDLINE) and other standardised headings (controlled vocabulary) in other databases, as appropriate. Selection criteria We selected studies that included a population free from dementia at baseline, who were assessed with the IQCODE and subsequently assessed for the development of dementia over time. The implication was that at the time of testing, the individual had a cognitive problem sufficient to result in an abnormal IQCODE score (defined by the study authors), but not yet meeting dementia diagnostic criteria. Data collection and analysis We screened all titles generated by the electronic database searches, and reviewed abstracts of all potentially relevant studies. Two assessors independently checked the full papers for eligibility and extracted data. We determined quality assessment (risk of bias and applicability) using the QUADAS‐2 tool, and reported quality using the STARDdem tool. Main results From 85 papers describing IQCODE, we included three papers, representing data from 626 individuals. Of this total, 22% (N = 135/626) were excluded because of prevalent dementia. There was substantial attrition; 47% (N = 295) of the study population received reference standard assessment at first follow‐up (three to six months) and 28% (N = 174) received reference standard assessment at final follow‐up (one to three years). Prevalence of dementia ranged from 12% to 26% at first follow‐up and 16% to 35% at final follow‐up. The three studies were considered to be too heterogenous to combine, so we did not perform meta‐analyses to describe summary estimates of interest. Included patients were poststroke (two papers) and hip fracture (one paper). The IQCODE was used at three thresholds of positivity (higher than 3.0, higher than 3.12 and higher than 3.3) to predict those at risk of a future diagnosis of dementia. Using a cut‐off of 3.0, IQCODE had a sensitivity of 0.75 (95%CI 0.51 to 0.91) and a specificity of 0.46 (95%CI 0.34 to 0.59) at one year following stroke. Using a cut‐off of 3.12, the IQCODE had a sensitivity of 0.80 (95%CI 0.44 to 0.97) and specificity of 0.53 (95C%CI 0.41 to 0.65) for the clinical diagnosis of dementia at six months after hip fracture. Using a cut‐off of 3.3, the IQCODE had a sensitivity of 0.84 (95%CI 0.68 to 0.94) and a specificity of 0.87 (95%CI 0.76 to 0.94) for the clinical diagnosis of dementia at one year after stroke. In generaI, the IQCODE was sensitive for identification of those who would develop dementia, but lacked specificity. Methods for both excluding prevalent dementia at baseline and assessing for the development of dementia were varied, and had the potential to introduce bias. Authors' conclusions Included studies were heterogenous, recruited from specialist settings, and had potential biases. The studies identified did not allow us to make specific recommendations on the use of the IQCODE for the future detection of dementia in clinical practice. The included studies highlighted the challenges of delayed verification dementia research, with issues around prevalent dementia assessment, loss to follow‐up over time, and test non‐completion potentially limiting the studies. Future research should recognise these issues and have explicit protocols for dealing with them. Plain language summary Using a structured questionnaire (the IQCODE) to detect individuals who may go on to develop dementia Background Accurately identifying people with dementia is an area of public and professional concern. Dementia is often not diagnosed until late in the disease, and this may limit timely access to appropriate health and social support. There is a growing interest in tests that detect dementia at an early stage, before symptoms have become problematic or noticeable. One way to do this is to test a person and then re‐assess them over time to see if they have developed dementia. Our review focused on the accuracy of a questionnaire‐based assessment for dementia, called the IQCODE (Informant Questionnaire for Cognitive Decline in the Elderly). We described whether the initial IQCODE score can identify people who will develop dementia months or years after their first IQCODE assessment. We searched electronic databases of published research studies, looking for all studies that looked at IQCODE and a later diagnosis of dementia. We searched from the first available papers in scientific databases up to and including January 2016. Study characteristics We found three relevant studies, all of which were carried out in specific hospital settings. Two papers only included patients with acute stroke, and the other included those who had sustained a hip fracture. The papers differed in many other ways, so we we were unable to estimate a summary of their combined results. In general, a 'positive' IQCODE picked up patients who would go on to develop dementia (good sensitivity), but mislabelled a number who did not develop dementia (poor specificity). We cannot make recommendations for current practice, based on the studies we reviewed. Quality of the evidence The included studies demonstrated some of the challenges of research that follows people at risk of dementia over time. Not all the studies had a robust method of ensuring that none of the included participants had dementia at the start of the study, and that only new cases were identified. Similarly, many of the participants included at the start of the study were not available for re‐assessment, due to death or other illness. The review was performed by a team based in research centres in the UK (Glasgow, Edinburgh, Oxford). We had no external funding specific to this study, and we have no conflicts of interest that may have influenced our assessment of the research data.","7","John Wiley & Sons, Ltd","1465-1858","*Early Diagnosis; Aged; Cognition Disorders [*diagnosis]; Cohort Studies; Delivery of Health Care; Dementia [*diagnosis, epidemiology]; Health Surveys [*standards]; Hip Fractures; Humans; Reference Standards; Sensitivity and Specificity; Stroke [complications]; Time Factors","10.1002/14651858.CD011333.pub3","http://dx.doi.org/10.1002/14651858.CD011333.pub3","Dementia and Cognitive Improvement"
"CD011414.PUB3","Chan, CCH; Fage, BA; Burton, JK; Smailagic, N; Gill, SS; Herrmann, N; Nikolaou, V; Quinn, TJ; Noel-Storr, AH; Seitz, DP","Mini‐Cog for the detection of dementia within a secondary care setting","Cochrane Database of Systematic Reviews","2021","Abstract - Background The diagnosis of Alzheimer's disease dementia and other dementias relies on clinical assessment. There is a high prevalence of cognitive disorders, including undiagnosed dementia in secondary care settings. Short cognitive tests can be helpful in identifying those who require further specialist diagnostic assessment; however, there is a lack of consensus around the optimal tools to use in clinical practice. The Mini‐Cog is a short cognitive test comprising three‐item recall and a clock‐drawing test that is used in secondary care settings. Objectives The primary objective was to determine the accuracy of the Mini‐Cog for detecting dementia in a secondary care setting. The secondary objectives were to investigate the heterogeneity of test accuracy in the included studies and potential sources of heterogeneity. These potential sources of heterogeneity will include the baseline prevalence of dementia in study samples, thresholds used to determine positive test results, the type of dementia (Alzheimer's disease dementia or all causes of dementia), and aspects of study design related to study quality. Search methods We searched the following sources in September 2012, with an update to 12 March 2019: Cochrane Dementia Group Register of Diagnostic Test Accuracy Studies, MEDLINE (OvidSP), Embase (OvidSP), BIOSIS Previews (Web of Knowledge), Science Citation Index (ISI Web of Knowledge), PsycINFO (OvidSP), and LILACS (BIREME). We made no exclusions with regard to language of Mini‐Cog administration or language of publication, using translation services where necessary. Selection criteria We included cross‐sectional studies and excluded case‐control designs, due to the risk of bias. We selected those studies that included the Mini‐Cog as an index test to diagnose dementia where dementia diagnosis was confirmed with reference standard clinical assessment using standardised dementia diagnostic criteria. We only included studies in secondary care settings (including inpatient and outpatient hospital participants). Data collection and analysis We screened all titles and abstracts generated by the electronic database searches. Two review authors independently checked full papers for eligibility and extracted data. We determined quality assessment (risk of bias and applicability) using the QUADAS‐2 tool. We extracted data into two‐by‐two tables to allow calculation of accuracy metrics for individual studies, reporting the sensitivity, specificity, and 95% confidence intervals of these measures, summarising them graphically using forest plots. Main results Three studies with a total of 2560 participants fulfilled the inclusion criteria, set in neuropsychology outpatient referrals, outpatients attending a general medicine clinic, and referrals to a memory clinic. Only n = 1415 (55.3%) of participants were included in the analysis to inform evaluation of Mini‐Cog test accuracy, due to the selective use of available data by study authors. There were concerns related to high risk of bias with respect to patient selection, and unclear risk of bias and high concerns related to index test conduct and applicability. In all studies, the Mini‐Cog was retrospectively derived from historic data sets. No studies included acute general hospital inpatients. The prevalence of dementia ranged from 32.2% to 87.3%. The sensitivities of the Mini‐Cog in the individual studies were reported as 0.67 (95% confidence interval (CI) 0.63 to 0.71), 0.60 (95% CI 0.48 to 0.72), and 0.87 (95% CI 0.83 to 0.90). The specificity of the Mini‐Cog for each individual study was 0.87 (95% CI 0.81 to 0.92), 0.65 (95% CI 0.57 to 0.73), and 1.00 (95% CI 0.94 to 1.00). We did not perform meta‐analysis due to concerns related to risk of bias and heterogeneity. Authors' conclusions This review identified only a limited number of diagnostic test accuracy studies using Mini‐Cog in secondary care settings. Those identified were at high risk of bias related to patient selection and high concerns related to index test conduct and applicability. The evidence was indirect, as all studies evaluated Mini‐Cog differently from the review question, where it was anticipated that studies would conduct Mini‐Cog and independently but contemporaneously perform a reference standard assessment to diagnose dementia. The pattern of test accuracy varied across the three studies. Future research should evaluate Mini‐Cog as a test in itself, rather than derived from other neuropsychological assessments. There is also a need for evaluation of the feasibility of the Mini‐Cog for the detection of dementia to help adequately determine its role in the clinical pathway. Plain language summary How accurate is the Mini‐Cog in detecting dementia among patients in inpatient and outpatient hospital settings? Why is recognising dementia important? Dementia is a common and important condition, and many of those living with dementia have never had the condition diagnosed. Diagnosis provides opportunities for social support, advance care planning and, in specific disease types, treatment with medication. However, incorrectly diagnosing dementia when it is not present (a false‐positive result) can be distressing for the individual and their family and lead to a waste of resources in diagnostic tests. What was the aim of the review? The aim of this Cochrane Review was to find out how accurate the Mini‐Cog test is for detecting dementia among patients in inpatient and outpatient hospital settings. The researchers included three studies to answer this question. What was studied in the review? The Mini‐Cog is a short test of memory and thinking skills that tests the ability of an individual to remember three specific objects, named at the beginning of a short assessment, repeated at the time and recalled by the individual later. In addition, the individual being assessed is asked to draw a clock face at a specific time. Points are scored based on the ability to recall the three items and the completeness of the clock. The Mini‐Cog is a short test that would typically be used to identify if someone was having difficulty with memory and thinking skills who would benefit from referral to a specialist for more detailed assessment. What are the main results of the review? The review included data from three relevant studies with a total of 2560 participants. However, the study authors did not use data from many of those participants they assessed, leaving results from only 1415 participants that provide complete and useful information for addressing the review question. All three studies scored the Mini‐Cog results in the way that was recommended by the developers of the tool. There was no clear pattern in the results of what a positive result of a Mini‐Cog test meant across the three studies, making it difficult to draw summary conclusions. Using the studies with the highest and lowest Mini‐Cog results indicated that if the Mini‐Cog were to be used in secondary care in a group of 1000 people, where 640 (64%) have dementia, an estimated 510 to 557 would have a positive Mini‐Cog, of which 0 to 126 would be incorrectly classified as having dementia. Of the 443 to 490 people with a result indicating dementia is not present, 83 to 256 would be incorrectly classified as not having dementia. How reliable are the results of the studies in the review? In the included studies, the diagnosis of dementia was made by assessing all patients with a detailed clinical assessment. Detailed clinical assessment is the reference standard to which the Mini‐Cog was compared. This is likely to have been a reliable method for determining whether patients actually had dementia. However, there were some problems with how the studies were conducted in terms of the people who were included and how the Mini‐Cog was calculated, which could result in the Mini‐Cog appearing more accurate than it actually is. We decided that it was not appropriate to group the studies together to describe the average performance of the Mini‐Cog, due to the differences among them. To whom do the results of this review apply? The studies included in the review were conducted in the USA, Germany, and Brazil. Two studies included those patients referred to specialists evaluating memory and thinking skills, and one study recruited individuals attending a medical outpatient clinic. The percentage of people with a final diagnosis of dementia was between 32% and 87% (an average of 64%). What are the implications of this review? The small number of studies identified and variation in how they used the Mini‐Cog limit the evidence to make recommendations, and suggest that Mini‐Cog may not be the best test to recommend for use in inpatient and outpatient secondary care hospital settings. How up‐to‐date is this review? The review authors searched for and considered studies published up to March 2019.","7","John Wiley & Sons, Ltd","1465-1858","*Secondary Care; Aged; Aged, 80 and over; Alzheimer Disease [*diagnosis]; Bias; Cognition Disorders [*diagnosis]; Cross-Sectional Studies; Dementia [*diagnosis, epidemiology]; Diagnosis, Differential; Disease Progression; Humans; Mental Status and Dementia Tests [*standards]; Patient Selection; Prevalence; Sensitivity and Specificity","10.1002/14651858.CD011414.pub3","http://dx.doi.org/10.1002/14651858.CD011414.pub3","Dementia and Cognitive Improvement"
"CD011415.PUB3","Seitz, DP; Chan, CCH; Newton, HT; Gill, SS; Herrmann, N; Smailagic, N; Nikolaou, V; Fage, BA","Mini‐Cog for the detection of dementia within a primary care setting","Cochrane Database of Systematic Reviews","2021","Abstract - Background Alzheimer's disease and other forms of dementia are becoming increasingly common with the aging of most populations. The majority of individuals with dementia will first present for care and assessment in primary care settings. There is a need for brief dementia screening instruments that can accurately detect dementia in primary care settings. The Mini‐Cog is a brief, cognitive screening test that is frequently used to evaluate cognition in older adults in various settings. Objectives To determine the accuracy of the Mini‐Cog for detecting dementia in a primary care setting. Search methods We searched the Cochrane Dementia and Cognitive Improvement Register of Diagnostic Test Accuracy Studies, MEDLINE, Embase and four other databases, initially to September 2012. Since then, four updates to the search were performed using the same search methods, and the most recent was January 2017. We used citation tracking (using the databases' ‘related articles’ feature, where available) as an additional search method and contacted authors of eligible studies for unpublished data. Selection criteria We only included studies that evaluated the Mini‐Cog as an index test for the diagnosis of Alzheimer's disease dementia or related forms of dementia when compared to a reference standard using validated criteria for dementia. We only included studies that were conducted in primary care populations. Data collection and analysis We extracted and described information on the characteristics of the study participants and study setting. Using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) criteria we evaluated the quality of studies, and we assessed risk of bias and applicability of each study for each domain in QUADAS‐2. Two review authors independently extracted information on the true positives, true negatives, false positives, and false negatives and entered the data into Review Manager 5 (RevMan 5). We then used RevMan 5 to determine the sensitivity, specificity, and 95% confidence intervals. We summarized the sensitivity and specificity of the Mini‐Cog in the individual studies in forest plots and also plotted them in a receiver operating characteristic plot. We also created a 'Risk of bias' and applicability concerns graph to summarize information related to the quality of included studies. Main results There were a total of four studies that met our inclusion criteria, including a total of 1517 total participants. The sensitivity of the Mini‐Cog varied between 0.76 to 1.00 in studies while the specificity varied between 0.27 to 0.85. The included studies displayed significant heterogeneity in both methodologies and clinical populations, which did not allow for a meta‐analysis to be completed. Only one study (Holsinger 2012) was found to be at low risk of bias on all methodological domains. The results of this study reported that the sensitivity of the Mini‐Cog was 0.76 and the specificity was 0.73. We found the quality of all other included studies to be low due to a high risk of bias with methodological limitations primarily in their selection of participants. Authors' conclusions There is a limited number of studies evaluating the accuracy of the Mini‐Cog for the diagnosis of dementia in primary care settings. Given the small number of studies, the wide range in estimates of the accuracy of the Mini‐Cog, and methodological limitations identified in most of the studies, at the present time there is insufficient evidence to recommend that the Mini‐Cog be used as a screening test for dementia in primary care. Further studies are required to determine the accuracy of Mini‐Cog in primary care and whether this tool has sufficient diagnostic test accuracy to be useful as a screening test in this setting. Plain language summary How accurate is the mini‐cog test when used to assess dementia in general practice? Background and rationale for review In most parts of the world there are increasing numbers of older adults, and memory complaints and conditions such as Alzheimer's disease and other forms of dementia are becoming increasingly common as a result. Most individuals with memory difficulties will first seek out care or be identified in the healthcare system through their primary care health care providers, which may include family physicians or nurses. Therefore, there is a need for tools that could identify individuals who may have dementia or significant memory problems. These tools should also be able to rule out dementia in those individuals with memory complaints who do not have dementia or significant memory problems. Such tools in primary care must be relatively easy to use, quick to administer, and accurate so as to be feasible to use in primary care while at the same time not overdiagnose or underdiagnose dementia. The Mini‐Cog, a brief cognitive screening tool, has been suggested as a possible screening test for dementia in primary care as it has been reported to be accurate and relatively easy to administer in primary care settings. The Mini‐Cog consists of a memory task that involves recall of three words and an evaluation of a clock drawing task. Study characteristics We searched electronic databases for articles evaluating the Mini‐Cog and this evidence is current as of January 2017. The purpose of our review was to compare the accuracy of the Mini‐Cog for detecting dementia of any type in primary care settings when compared to in‐depth evaluation conducted by dementia specialists. We included studies that evaluated individuals with any potential severity of dementia and regardless of whether previous cognitive testing had been completed prior to the Mini‐Cog. Overall, our review identified four studies conducted in primary care settings that compared the accuracy of the Mini‐Cog to detailed assessment of dementia by dementia specialists. Quality of the evidence Of the four studies included in the review, all except one study had limitations in how the Mini‐Cog was evaluated, which may have led to an overestimation of the accuracy of the Mini‐Cog in the remaining studies. Notably, the most problematic issue in study quality related to how participants were selected to participate in research studies, which may have further contributed to an overestimation of the accuracy of the Mini‐Cog in most of the studies included in our review. Key findings The results of the highest‐quality study Holsinger 2012 found that the Mini‐Cog had a sensitivity of 76%, indicating that the Mini‐Cog failed to detect up to 24% of individuals who have dementia (e.g. false negatives). In this same study, the specificity of the Mini‐Cog was 73% indicating that up to 27% of individuals may be incorrectly identified as having dementia on the Mini‐Cog when these individuals do not actually have an underlying dementia (e.g. false positives). We conclude that at the present time there is not enough evidence to support the routine use of the Mini‐Cog as a screening test for dementia in primary care and additional studies are required before concluding that the Mini‐Cog is useful in this setting.","7","John Wiley & Sons, Ltd","1465-1858","*Primary Health Care; Aged; Alzheimer Disease [*diagnosis]; Bias; Confidence Intervals; Dementia [diagnosis]; Humans; Mental Status and Dementia Tests [*standards]; Sensitivity and Specificity","10.1002/14651858.CD011415.pub3","http://dx.doi.org/10.1002/14651858.CD011415.pub3","Dementia and Cognitive Improvement"
"CD010860.PUB3","Fage, BA; Chan, CCH; Gill, SS; Noel-Storr, AH; Herrmann, N; Smailagic, N; Nikolaou, V; Seitz, DP","Mini‐Cog for the detection of dementia within a community setting","Cochrane Database of Systematic Reviews","2021","Abstract - Background Alzheimer's disease and related forms of dementia are becoming increasingly prevalent with the aging of many populations. The diagnosis of Alzheimer's disease relies on tests to evaluate cognition and discriminate between individuals with dementia and those without dementia. The Mini‐Cog is a brief, cognitive screening test that is frequently used to evaluate cognition in older adults in various settings. Objectives The primary objective of this review was to determine the accuracy of the Mini‐Cog for detecting dementia in a community setting. Secondary objectives included investigations of the heterogeneity of test accuracy in the included studies and potential sources of heterogeneity. These potential sources of heterogeneity included the baseline prevalence of dementia in study samples, thresholds used to determine positive test results, the type of dementia (Alzheimer's disease dementia or all causes of dementia), and aspects of study design related to study quality. Overall, the goals of this review were to determine if the Mini‐Cog is a cognitive screening test that could be recommended to screen for cognitive impairment in community settings. Search methods We searched MEDLINE (OvidSP), EMBASE (OvidSP), PsycINFO (Ovid SP), Science Citation Index (Web of Science), BIOSIS previews (Web of Science), LILACS (BIREME), and the Cochrane Dementia Group's developing register of diagnostic test accuracy studies to March 2013. We used citation tracking (using the database’s ‘related articles’ feature, where available) as an additional search method and contacted authors of eligible studies for unpublished data. Selection criteria We included all cross‐sectional studies that utilized the Mini‐Cog as an index test for the diagnosis of dementia when compared to a reference standard diagnosis of dementia using standardized dementia diagnostic criteria. For the current review we only included studies that were conducted on samples from community settings, and excluded studies that were conducted in primary care or secondary care settings. We considered studies to be conducted in a community setting where participants were sampled from the general population. Data collection and analysis Information from studies meeting the inclusion criteria were extracted including information on the characteristics of participants in the studies. The quality of the studies was assessed using the QUADAS‐2 criteria and summarized using risk of bias applicability and summary graphs. We extracted information on the diagnostic test accuracy of studies including the sensitivity, specificity, and 95% confidence intervals of these measures and summarized the findings using forest plots. Study specific sensitivities and specificities were also plotted in receiver operating curve space. Main results Three studies met the inclusion criteria, with a total of 1620 participants. The sensitivities of the Mini‐Cog in the individual studies were reported as 0.99, 0.76 and 0.99. The specificity of the Mini‐Cog varied in the individual studies and was 0.93, 0.89 and 0.83. There was clinical and methodological heterogeneity between the studies which precluded a pooled meta‐analysis of the results. Methodological limitations were present in all the studies introducing potential sources of bias, specifically with respect to the methods for participant selection. Authors' conclusions There are currently few studies assessing the diagnostic test accuracy of the Mini‐Cog in community settings. The limited number of studies and the methodological limitations that are present in the current studies make it difficult to provide recommendations for or against the use of the Mini‐Cog as a cognitive screening test in community settings. Additional well‐designed studies comparing the Mini‐Cog to other brief cognitive screening tests are required in order to determine the accuracy and utility of the Mini‐Cog in community based settings. Plain language summary A brief cogntive screening test (Mini‐Cog) for the assessment of possible dementia With the aging of our populations there are increasing numbers of older adults with memory complaints and possible dementia. Identifying older adults who have dementia is important in order to help with planning their care needs and starting dementia specific treatments. In order to diagnose dementia, healthcare professionals or other service providers rely on tests of memory and other areas of cognition in combination with additional investigations. Brief memory tests, such as the Mini‐Cog, may be useful as screening tests to help identify those individuals that might benefit from further evaluation in order to determine if dementia is present. The Mini‐Cog is a brief cognitive test that involves an assessment of an older person's ability to recall three words and draw a clock. In this review, we searched medical literature databases to identify studies which evaluated how well the Mini‐Cog is able to distinguish between individuals who have dementia and those who do not have dementia when compared to in‐depth evaluation by dementia specialists. Our review focussed on those studies that were conducted in community based settings. We identified three unique randomised controlled studies that evaluated the Mini‐Cog. In these studies the accuracy of the Mini‐Cog varied and importantly there were some potential limitations within the studies which may have led to an overestimation of the accuracy of the Mini‐Cog. Based on the information that we obtained from our review, we felt that further research into the accuracy of the Mini‐Cog was required before it could be recommended for routine use for identifying dementia in community settings.","7","John Wiley & Sons, Ltd","1465-1858","*Memory, Short-Term; *Mental Status and Dementia Tests; Aged; Aged, 80 and over; Alzheimer Disease [*diagnosis]; Cognitive Dysfunction [*diagnosis]; Cross-Sectional Studies; Dementia [diagnosis]; Humans; Sensitivity and Specificity","10.1002/14651858.CD010860.pub3","http://dx.doi.org/10.1002/14651858.CD010860.pub3","Dementia and Cognitive Improvement"
"CD010775.PUB3","Davis, DHJ; Creavin, ST; Yip, JLY; Noel-Storr, AH; Brayne, C; Cullum, S","Montreal Cognitive Assessment for the detection of dementia","Cochrane Database of Systematic Reviews","2021","Abstract - Background Dementia is a progressive syndrome of global cognitive impairment with significant health and social care costs. Global prevalence is projected to increase, particularly in resource‐limited settings. Recent policy changes in Western countries to increase detection mandates a careful examination of the diagnostic accuracy of neuropsychological tests for dementia. Objectives To determine the accuracy of the Montreal Cognitive Assessment (MoCA) for the detection of dementia. Search methods We searched MEDLINE, EMBASE, BIOSIS Previews, Science Citation Index, PsycINFO and LILACS databases to August 2012. In addition, we searched specialised sources containing diagnostic studies and reviews, including MEDION (Meta‐analyses van Diagnostisch Onderzoek), DARE (Database of Abstracts of Reviews of Effects), HTA (Health Technology Assessment Database), ARIF (Aggressive Research Intelligence Facility) and C‐EBLM (International Federation of Clinical Chemistry and Laboratory Medicine Committee for Evidence‐based Laboratory Medicine) databases. We also searched ALOIS (Cochrane Dementia and Cognitive Improvement Group specialized register of diagnostic and intervention studies). We identified further relevant studies from the PubMed ‘related articles’ feature and by tracking key studies in Science Citation Index and Scopus. We also searched for relevant grey literature from the Web of Science Core Collection, including Science Citation Index and Conference Proceedings Citation Index (Thomson Reuters Web of Science), PhD theses and contacted researchers with potential relevant data. Selection criteria Cross‐sectional designs where all participants were recruited from the same sample were sought; case‐control studies were excluded due to high chance of bias. We searched for studies from memory clinics, hospital clinics, primary care and community populations. We excluded studies of early onset dementia, dementia from a secondary cause, or studies where participants were selected on the basis of a specific disease type such as Parkinson’s disease or specific settings such as nursing homes. Data collection and analysis We extracted dementia study prevalence and dichotomised test positive/test negative results with thresholds used to diagnose dementia. This allowed calculation of sensitivity and specificity if not already reported in the study. Study authors were contacted where there was insufficient information to complete the 2x2 tables. We performed quality assessment according to the QUADAS‐2 criteria. Methodological variation in selected studies precluded quantitative meta‐analysis, therefore results from individual studies were presented with a narrative synthesis. Main results Seven studies were selected: three in memory clinics, two in hospital clinics, none in primary care and two in population‐derived samples. There were 9422 participants in total, but most of studies recruited only small samples, with only one having more than 350 participants. The prevalence of dementia was 22% to 54% in the clinic‐based studies, and 5% to 10% in population samples. In the four studies that used the recommended threshold score of 26 or over indicating normal cognition, the MoCA had high sensitivity of 0.94 or more but low specificity of 0.60 or less. Authors' conclusions The overall quality and quantity of information is insufficient to make recommendations on the clinical utility of MoCA for detecting dementia in different settings. Further studies that do not recruit participants based on diagnoses already present (case‐control design) but apply diagnostic tests and reference standards prospectively are required. Methodological clarity could be improved in subsequent DTA studies of MoCA by reporting findings using recommended guidelines (e.g. STARDdem). Thresholds lower than 26 are likely to be more useful for optimal diagnostic accuracy of MoCA in dementia, but this requires confirmation in further studies. Plain language summary Montreal Cognitive Assessment (MoCA) for the detection of dementia Review question We reviewed the evidence about the accuracy of the Montreal Cognitive Assessment (MoCA) test for detecting dementia. Background Dementia is a common condition in older people, with at least 7% of people over 65 years old in the UK affected, and numbers are increasing worldwide. In this review, we wanted to discover whether using a well‐established cognitive test, MoCA, could accurately detect dementia when compared to a gold standard diagnostic test. MoCA uses a series of questions to test different aspects of mental functioning. Study characteristics The evidence we reviewed is current to August 2012. We found seven studies that matched our criteria. There were three from memory clinics (specialist clinics where people are referred for suspected dementia), two from general hospital clinics, none from primary care and two studies carried out in the general population. All studies included older people, with the youngest average age of 61 years in one study. There were a total 9422 people included in all 7 studies though only one study had more than 350 people. The proportion of people with dementia was 5% to 10% in two population‐derived studies and 22% to 54% in the five clinic‐based studies. There was a large variation in the way the different studies were carried out: therefore we chose to present the results in a narrative summary because a statistical summary (combining all the estimates into a summary sensitivity and specificity) would not have been meaningful. Key results We found that MoCA was good at detecting dementia when using a recognised cut‐off score of less than 26. In the studies that used this cut‐off, we found the test correctly detected over 94% of people with dementia in all settings. On the other hand, the test also produced a high proportion of false positives, that is people who did not have dementia but tested positive at the 'less than 26' cut‐off. In the studies we reviewed, over 40% of people without dementia would have been incorrectly diagnosed with dementia using the MoCA. Conclusion The overall quality of the studies was not good enough to make firm recommendations on using the MoCA to detect dementia in different healthcare settings. In particular, no studies looked at how useful MoCA is for diagnosing dementia in primary care settings. It is likely that a MoCA threshold lower than 26 would be more useful for optimal diagnostic accuracy in dementia, though this requires wider confirmation.","7","John Wiley & Sons, Ltd","1465-1858","*Mental Status and Dementia Tests; *Neuropsychological Tests; Aged; Alzheimer Disease [diagnosis, epidemiology]; Cognitive Dysfunction [*diagnosis]; Cross-Sectional Studies; Dementia [*diagnosis, epidemiology]; Executive Function; Humans; Memory, Short-Term; Orientation; Reference Standards; Sensitivity and Specificity","10.1002/14651858.CD010775.pub3","http://dx.doi.org/10.1002/14651858.CD010775.pub3","Dementia and Cognitive Improvement"
"CD013693.PUB2","Vonasek, B; Ness, T; Takwoingi, Y; Kay, AW; van Wyk, SS; Ouellette, L; Marais, BJ; Steingart, KR; Mandalakas, AM","Screening tests for active pulmonary tuberculosis in children","Cochrane Database of Systematic Reviews","2021","Abstract - Background Globally, children under 15 years represent approximately 12% of new tuberculosis cases, but 16% of the estimated 1.4 million deaths. This higher share of mortality highlights the urgent need to develop strategies to improve case detection in this age group and identify children without tuberculosis disease who should be considered for tuberculosis preventive treatment. One such strategy is systematic screening for tuberculosis in high‐risk groups. Objectives To estimate the sensitivity and specificity of the presence of one or more tuberculosis symptoms, or symptom combinations; chest radiography (CXR); Xpert MTB/RIF; Xpert Ultra; and combinations of these as screening tests for detecting active pulmonary childhood tuberculosis in the following groups. – Tuberculosis contacts, including household contacts, school contacts, and other close contacts of a person with infectious tuberculosis. – Children living with HIV. – Children with pneumonia. – Other risk groups (e.g. children with a history of previous tuberculosis, malnourished children). – Children in the general population in high tuberculosis burden settings. Search methods We searched six databases, including the Cochrane Central Register of Controlled Trials, MEDLINE, and Embase, on 14 February 2020 without language restrictions and contacted researchers in the field. Selection criteria Cross‐sectional and cohort studies where at least 75% of children were aged under 15 years. Studies were eligible if conducted for screening rather than diagnosing tuberculosis. Reference standards were microbiological (MRS) and composite reference standard (CRS), which may incorporate symptoms and CXR. Data collection and analysis Two review authors independently extracted data and assessed study quality using QUADAS‐2. We consolidated symptom screens across included studies into groups that used similar combinations of symptoms as follows: one or more of cough, fever, or poor weight gain and one or more of cough, fever, or decreased playfulness. For combination of symptoms, a positive screen was the presence of one or more than one symptom. We used a bivariate model to estimate pooled sensitivity and specificity with 95% confidence intervals (CIs) and performed analyses separately by reference standard. We assessed certainty of evidence using GRADE. Main results Nineteen studies assessed the following screens: one symptom (15 studies, 10,097 participants); combinations of symptoms (12 studies, 29,889 participants); CXR (10 studies, 7146 participants); and Xpert MTB/RIF (2 studies, 787 participants). Several studies assessed more than one screening test. No studies assessed Xpert Ultra. For 16 studies (84%), risk of bias for the reference standard domain was unclear owing to concern about incorporation bias. Across other quality domains, risk of bias was generally low. Symptom screen (verified by CRS) One or more of cough, fever, or poor weight gain in tuberculosis contacts  (4 studies, tuberculosis prevalence 2% to 13%): pooled sensitivity was 89% (95% CI 52% to 98%; 113 participants; low‐certainty evidence) and pooled specificity was 69% (95% CI 51% to 83%; 2582 participants; low‐certainty evidence). Of 1000 children where 50 have pulmonary tuberculosis, 339 would be screen‐positive, of whom 294 (87%) would not have pulmonary tuberculosis (false positives); 661 would be screen‐negative, of whom five (1%) would have pulmonary tuberculosis (false negatives). One or more of cough, fever, or decreased playfulness in children aged under five years, inpatient or outpatient  (3 studies, tuberculosis prevalence 3% to 13%): sensitivity ranged from 64% to 76% (106 participants; moderate‐certainty evidence) and specificity from 37% to 77% (2339 participants; low‐certainty evidence). Of 1000 children where 50 have pulmonary tuberculosis, 251 to 636 would be screen‐positive, of whom 219 to 598 (87% to 94%) would not have pulmonary tuberculosis; 364 to 749 would be screen‐negative, of whom 12 to 18 (2% to 3%) would have pulmonary tuberculosis. One or more of cough, fever, poor weight gain, or tuberculosis close contact (World Health Organization four‐symptom screen) in children living with HIV, outpatient  (2 studies, tuberculosis prevalence 3% and 8%): pooled sensitivity was 61% (95% CI 58% to 64%; 1219 screens; moderate‐certainty evidence) and pooled specificity was 94% (95% CI 86% to 98%; 201,916 screens; low‐certainty evidence). Of 1000 symptom screens where 50 of the screens are on children with pulmonary tuberculosis, 88 would be screen‐positive, of which 57 (65%) would be on children who do not have pulmonary tuberculosis; 912 would be screen‐negative, of which 19 (2%) would be on children who have pulmonary tuberculosis. CXR (verified by CRS) CXR with any abnormality in tuberculosis contacts  (8 studies, tuberculosis prevalence 2% to 25%): pooled sensitivity was 87% (95% CI 75% to 93%; 232 participants; low‐certainty evidence) and pooled specificity was 99% (95% CI 68% to 100%; 3281 participants; low‐certainty evidence). Of 1000 children, where 50 have pulmonary tuberculosis, 63 would be screen‐positive, of whom 19 (30%) would not have pulmonary tuberculosis; 937 would be screen‐negative, of whom 6 (1%) would have pulmonary tuberculosis. Xpert MTB/RIF (verified by MRS) Xpert MTB/RIF, inpatient or outpatient  (2 studies, tuberculosis prevalence 1% and 4%): sensitivity was 43% and 100% (16 participants; very low‐certainty evidence) and specificity was 99% and 100% (771 participants; moderate‐certainty evidence). Of 1000 children, where 50 have pulmonary tuberculosis, 31 to 69 would be Xpert MTB/RIF‐positive, of whom 9 to 19 (28% to 29%) would not have pulmonary tuberculosis; 931 to 969 would be Xpert MTB/RIF‐negative, of whom 0 to 28 (0% to 3%) would have tuberculosis. Studies often assessed more symptoms than those included in the index test and symptom definitions varied. These differences complicated data aggregation and may have influenced accuracy estimates. Both symptoms and CXR formed part of the CRS (incorporation bias), which may have led to overestimation of sensitivity and specificity. Authors' conclusions We found that in children who are tuberculosis contacts or living with HIV, screening tests using symptoms or CXR may be useful, but our review is limited by design issues with the index test and incorporation bias in the reference standard. For Xpert MTB/RIF, we found insufficient evidence regarding screening accuracy. Prospective evaluations of screening tests for tuberculosis in children will help clarify their use. In the meantime, screening strategies need to be pragmatic to address the persistent gaps in prevention and case detection that exist in resource‐limited settings. Plain language summary Screening tests for active pulmonary tuberculosis in children Why is improving screening for pulmonary tuberculosis in children important? Tuberculosis is one of the leading causes of death worldwide. Most children who die from tuberculosis are never diagnosed or treated. Screening may be useful to identify children with possible tuberculosis and refer them for further testing. As well, screening could be used to identify children without tuberculosis, who should be considered for preventive treatment. A false‐positive result means that children may undergo unnecessary testing and treatment and may not receive preventive treatment promptly. A false‐negative result means that children have tuberculosis, but may miss further testing to confirm the diagnosis. What is the aim of this review? To determine the accuracy of screening tests for active pulmonary tuberculosis in children in high‐risk groups, such as children with HIV and close contacts of people with tuberculosis. What was studied in this review? Screening tests were: one tuberculosis symptom; one or more of a combination of tuberculosis symptoms; the World Health Organization (WHO) four‐symptom screen (one or more of cough, fever, poor weight gain, or tuberculosis contact) in children with HIV, recommended at each healthcare visit; chest radiography (CXR); and Xpert MTB/RIF. What are the main results in this review? Nineteen studies assessed the following screening tests: one symptom (15 studies, 10,097 participants); more than one symptom (12 studies, 29,889 participants); CXR (10 studies, 7146 participants); and Xpert MTB/RIF (two studies, 787 participants). Symptom screening For every 1000 children screened, if 50 had tuberculosis according to the reference standard: One or more of cough, fever, or poor weight gain in tuberculosis contacts (composite reference standard (CRS) (4 studies) – 339 would screen positive, of whom 294 (87%) would not have tuberculosis (false positive). – 661 would screen negative, of whom 5 (1%) would have tuberculosis (false negative). One or more of cough, fever, or decreased playfulness in children under five, inpatient or outpatient (CRS) (3 studies) – 251 to 636 would screen positive, of whom 219 to 598 (87% to 94%) would not have tuberculosis (false positive). – 364 to 749 would screen negative, of whom 12 to 18 (2% to 3%) would have tuberculosis (false negative). One or more of cough, fever, poor weight gain, or tuberculosis close contact (WHO four‐symptom screen) in children with HIV, outpatient (CRS) (2 studies) – 88 would screen positive, of which 57 (65%) would not have tuberculosis (false positive). – 912 would screen negative, of which 19 (2%) would have tuberculosis (false negative). Abnormal CXR in tuberculosis contacts (CRS) (8 studies) – 63 would screen positive, of whom 19 (30%) would not have tuberculosis (false positive). – 937 would screen negative, of whom 6 (1%) would have tuberculosis (false negative). Xpert MTB/RIF in children, inpatient or outpatient microbiologic reference standard (MRS) (2 studies) – 31 to 69 would be Xpert MTB/RIF‐positive, of whom 9 to 19 (28% to 29%) would not have tuberculosis (false positive). – 931 to 969 would be Xpert MTB/RIF‐negative, of whom 0 to 28 (0% to 3%) would have tuberculosis (false negative). How reliable are the results of the studies in this review? Diagnosing tuberculosis in children is difficult. This may lead to screening tests appearing more or less accurate than they actually are. For Xpert MTB/RIF, there were few studies and children tested to be confident about results. Who do the results of this review apply to? Children at risk for pulmonary tuberculosis. Results likely do not apply to children in the general population. Studies mainly took place in countries with a high burden of tuberculosis. What are the implications of this review? In children who are tuberculosis contacts or living with HIV, screening tests using symptoms or CXR may be useful. However, symptoms and CXR formed part of the reference standard, which may falsely elevate the accuracy of the results. We urgently need better screening tests for tuberculosis in children to better identify children who should be considered for tuberculosis preventive treatment and to increase the timeliness of treatment in those with tuberculosis disease. How up‐to‐date is this review? To 14 February 2020.","6","John Wiley & Sons, Ltd","1465-1858","*Contact Tracing; Adolescent; Bias; Child; Child Behavior; Child, Preschool; Cohort Studies; Confidence Intervals; Cough [diagnosis]; Cross-Sectional Studies; False Negative Reactions; False Positive Reactions; Fever [diagnosis]; HIV Infections [epidemiology]; Humans; Mass Screening [statistics & numerical data]; Molecular Diagnostic Techniques; Radiography, Thoracic; Reference Standards; Sensitivity and Specificity; Symptom Assessment [*methods, statistics & numerical data]; Tuberculosis, Pulmonary [*diagnosis, epidemiology, prevention & control]; Weight Gain","10.1002/14651858.CD013693.pub2","http://dx.doi.org/10.1002/14651858.CD013693.pub2","Infectious Diseases"
"CD014546","Macey, R; Walsh, T; Riley, P; Glenny, A-M; Worthington, HV; O'Malley, L; Clarkson, JE; Ricketts, D","Visual or visual‐tactile examination to detect and inform the diagnosis of enamel caries","Cochrane Database of Systematic Reviews","2021","Abstract - Background The detection and diagnosis of caries at the initial (non‐cavitated) and moderate (enamel) levels of severity is fundamental to achieving and maintaining good oral health and prevention of oral diseases. An increasing array of methods of early caries detection have been proposed that could potentially support traditional methods of detection and diagnosis. Earlier identification of disease could afford patients the opportunity of less invasive treatment with less destruction of tooth tissue, reduce the need for treatment with aerosol‐generating procedures, and potentially result in a reduced cost of care to the patient and to healthcare services. Objectives To determine the diagnostic accuracy of different visual classification systems for the detection and diagnosis of non‐cavitated coronal dental caries for different purposes (detection and diagnosis) and in different populations (children or adults). Search methods Cochrane Oral Health's Information Specialist undertook a search of the following databases: MEDLINE Ovid (1946 to 30 April 2020); Embase Ovid (1980 to 30 April 2020); US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov, to 30 April 2020); and the World Health Organization International Clinical Trials Registry Platform (to 30 April 2020). We studied reference lists as well as published systematic review articles. Selection criteria We included diagnostic accuracy study designs that compared a visual classification system (index test) with a reference standard (histology, excavation, radiographs). This included cross‐sectional studies that evaluated the diagnostic accuracy of single index tests and studies that directly compared two or more index tests. Studies reporting at both the patient or tooth surface level were included. In vitro and in vivo studies were considered. Studies that explicitly recruited participants with caries into dentine or frank cavitation were excluded. We also excluded studies that artificially created carious lesions and those that used an index test during the excavation of dental caries to ascertain the optimum depth of excavation. Data collection and analysis We extracted data independently and in duplicate using a standardised data extraction and quality assessment form based on QUADAS‐2 specific to the review context. Estimates of diagnostic accuracy were determined using the bivariate hierarchical method to produce summary points of sensitivity and specificity with 95% confidence intervals (CIs) and regions, and 95% prediction regions. The comparative accuracy of different classification systems was conducted based on indirect comparisons. Potential sources of heterogeneity were pre‐specified and explored visually and more formally through meta‐regression. Main results We included 71 datasets from 67 studies (48 completed in vitro) reporting a total of 19,590 tooth sites/surfaces. The most frequently reported classification systems were the International Caries Detection and Assessment System (ICDAS) (36 studies) and Ekstrand‐Ricketts‐Kidd (ERK) (15 studies). In reporting the results, no distinction was made between detection and diagnosis. Only two studies were at low risk of bias across all four domains, and 15 studies were at low concern for applicability across all three domains. The patient selection domain had the highest proportion of high risk of bias studies (49 studies). Four studies were assessed at high risk of bias for the index test domain, nine for the reference standard domain, and seven for the flow and timing domain. Due to the high number of studies on extracted teeth concerns regarding applicability were high for the patient selection and index test domains (49 and 46 studies respectively). Studies were synthesised using a hierarchical bivariate method for meta‐analysis. There was substantial variability in the results of the individual studies: sensitivities ranged from 0.16 to 1.00 and specificities from 0 to 1.00. For all visual classification systems the estimated summary sensitivity and specificity point was 0.86 (95% CI 0.80 to 0.90) and 0.77 (95% CI 0.72 to 0.82) respectively, diagnostic odds ratio (DOR) 20.38 (95% CI 14.33 to 28.98). In a cohort of 1000 tooth surfaces with 28% prevalence of enamel caries, this would result in 40 being classified as disease free when enamel caries was truly present (false negatives), and 163 being classified as diseased in the absence of enamel caries (false positives). The addition of test type to the model did not result in any meaningful difference to the sensitivity or specificity estimates (Chi 2 (4) = 3.78, P = 0.44), nor did the addition of primary or permanent dentition (Chi 2 (2) = 0.90, P = 0.64). The variability of results could not be explained by tooth surface (occlusal or approximal), prevalence of dentinal caries in the sample, nor reference standard. Only one study intentionally included restored teeth in its sample and no studies reported the inclusion of sealants. We rated the certainty of the evidence as low, and downgraded two levels in total for risk of bias due to limitations in the design and conduct of the included studies, indirectness arising from the in vitro studies, and inconsistency of results. Authors' conclusions Whilst the confidence intervals for the summary points of the different visual classification systems indicated reasonable performance, they do not reflect the confidence that one can have in the accuracy of assessment using these systems due to the considerable unexplained heterogeneity evident across the studies. The prediction regions in which the sensitivity and specificity of a future study should lie are very broad, an important consideration when interpreting the results of this review. Should treatment be provided as a consequence of a false‐positive result then this would be non‐invasive, typically the application of fluoride varnish where it was not required, with low potential for an adverse event but healthcare resource and finance costs. Despite the robust methodology applied in this comprehensive review, the results should be interpreted with some caution due to shortcomings in the design and execution of many of the included studies. Studies to determine the diagnostic accuracy of methods to detect and diagnose caries in situ are particularly challenging. Wherever possible future studies should be carried out in a clinical setting, to provide a realistic assessment of performance within the oral cavity with the challenges of plaque, tooth staining, and restorations, and consider methods to minimise bias arising from the use of imperfect reference standards in clinical studies. Plain language summary Visual or visual‐tactile examination for the diagnosis of dental caries Why is it important to improve the detection of dental caries (tooth decay)? Dentists often aim to identify tooth decay that has already advanced to a level which needs a filling. If dentists were able to find tooth decay when it has only affected the outer layer of the tooth (enamel) then it is possible to stop the decay from spreading any further and prevent the need for fillings. It is also important to minimise the number of false‐positive results when treatment may be given when caries is absent, and improved visual detection methods may reduce such occurrences. What is the aim of this review? The aim of this Cochrane Review was to find out how accurate visual classification systems are for detecting early tooth decay as part of the dental 'check‐up' for children and adults who visit their dentist. Researchers in Cochrane included 67 studies to answer this question. What was studied in the review? Two main visual classification systems were studied in this review: the International Caries Detection and Assessment System (ICDAS) and the Ekstrand‐Ricketts‐Kidd (ERK) system. A third group of visual classifications is reported and labelled as 'Other' because the studies did not report what system was used. We studied decay on the occlusal surfaces (biting surfaces of the back teeth), the proximal surfaces (tooth surfaces that are next to each other), and smooth surfaces. What are the main results of the review? The review included 67 studies with a total of 19,590 teeth. Some studies reported on more than one type of classification system, this gave us 71 sets of data to use. The results of these studies indicate that, in theory, if the visual classification systems were to be used by a dentist for a routine dental examination in a group of 1000 tooth sites/surfaces, of whom 350 (28%) have early tooth decay: • the use of a visual classification system will indicate that an estimated 403 will have early tooth decay, and of these, 163 (40%) will not have tooth decay (false positive ‐ incorrect diagnosis); • of the 597 tooth sites/surfaces with a result indicating that tooth decay is not present, 40 (7%) will have early tooth decay (false negative ‐ incorrect diagnosis). A diagram of these results can be found at  oralhealth.cochrane.org/visual-examination-classification-systems-results-0331c . In this example, visual classification systems produce a high proportion of false‐positive results. Treatment in the absence of disease is likely to be non‐invasive such as the application of high fluoride toothpaste, or oral health advice and guidance from the dentist, but will incur financial cost to the patient or healthcare provider. We found no evidence from the data collected that the classification systems differed in their accuracy. How reliable are the results of the studies in this review? We only included studies that assessed healthy teeth or those that were thought to have early tooth decay. This is because teeth with deep tooth decay would be easier to identify. However, there were some problems with how the studies were conducted. This may result in the visual classification systems appearing more accurate than they really are, increasing the number of correct visual classification results. We judged the certainty of the evidence to be low due to how the studies selected their participants, the large number of studies that were carried out in a laboratory setting on extracted teeth, and variation in results. Who do the results of this review apply to? Studies included in the review were carried out in Brazil, Europe, Japan, and Australia. A large number of studies performed the tests on extracted teeth, while clinical studies were completed in dental hospitals, general dental practices, or schools. Studies were from the years 1988 to 2019. What are the implications of this review? We observed substantial variation in the results, which is perhaps unsurprising as the use of these classification systems involve interpretation by the user. There is considerable uncertainty in the likely performance of a future study. Further research studies should be carried out in a clinical setting. How up‐to‐date is this review? Review authors searched for and used studies published up to 30 April 2020.","6","John Wiley & Sons, Ltd","1465-1858","*Dental Enamel; *Early Diagnosis; Adult; Bias; Child; Confidence Intervals; Dental Caries [*diagnosis]; Humans; Palpation [*methods]; Physical Examination [*methods]; Sensitivity and Specificity","10.1002/14651858.CD014546","http://dx.doi.org/10.1002/14651858.CD014546","Oral Health"
"CD013021.PUB2","Nagar, H; Wietek, N; Goodall, RJ; Hughes, W; Schmidt-Hansen, M; Morrison, J","Sentinel node biopsy for diagnosis of lymph node involvement in endometrial cancer","Cochrane Database of Systematic Reviews","2021","Abstract - Background Pelvic lymphadenectomy provides prognostic information for those diagnosed with endometrial (womb) cancer and provides information that may influence decisions regarding adjuvant treatment. However, studies have not shown a therapeutic benefit, and lymphadenectomy causes significant morbidity. The technique of sentinel lymph node biopsy (SLNB), allows the first draining node from a cancer to be identified and examined histologically for involvement with cancer cells. SLNB is commonly used in other cancers, including breast and vulval cancer. Different tracers, including colloid labelled with radioactive technetium‐99, blue dyes, e.g. patent or methylene blue, and near infra‐red fluorescent dyes, e.g. indocyanine green (ICG), have been used singly or in combination for detection of sentinel lymph nodes (SLN). Objectives To assess the diagnostic accuracy of sentinel lymph node biopsy (SLNB) in the identification of pelvic lymph node involvement in women with endometrial cancer, presumed to be at an early stage prior to surgery, including consideration of the detection rate. Search methods We searched MEDLINE (1946 to July 2019), Embase (1974 to July 2019) and the relevant Cochrane trial registers. Selection criteria We included studies that evaluated the diagnostic accuracy of tracers for SLN assessment (involving the identification of a SLN plus histological examination) against a reference standard of histological examination of removed pelvic +/‐ para‐aortic lymph nodes following systematic pelvic +/‐ para‐aortic lymphadenectomy (PLND/PPALND) in women with endometrial cancer, where there were sufficient data for the construction of two‐by‐two tables. Data collection and analysis Two review authors (a combination of HN, JM, NW, RG, and WH) independently screened titles and abstracts for relevance, classified studies for inclusion/exclusion and extracted data. We assessed the methodological quality of studies using the QUADAS‐2 tool. We calculated the detection rate as the arithmetic mean of the total number of SLNs detected out of the total number of women included in the included studies with the woman as the unit of analysis, used univariate meta‐analytical methods to estimate pooled sensitivity estimates, and summarised the results using GRADE. Main results The search revealed 6259 unique records after removal of duplicates. After screening 232 studies in full text, we found 73 potentially includable records (for 52 studies), although we were only able to extract 2x2 table data for 33 studies, including 2237 women (46 records) for inclusion in the review, despite writing to trial authors for additional information. We found 11 studies that analysed results for blue dye alone, four studies for technetium‐99m alone, 12 studies that used a combination of blue dye and technetium‐99m, nine studies that used indocyanine green (ICG) and near infra‐red immunofluorescence, and one study that used a combination of ICG and technetium‐99m. Overall, the methodological reporting in most of the studies was poor, which resulted in a very large proportion of 'unclear risk of bias' ratings. Overall, the mean SLN detection rate was 86.9% (95% CI 82.9% to 90.8%; 2237 women; 33 studies; moderate‐certainty evidence). In studies that reported bilateral detection the mean rate was 65.4% (95% CI 57.8% to 73.0%) . When considered according to which tracer was used, the SLN detection rate ranged from 77.8% (95% CI 70.0% to 85.6%) for blue dye alone (559 women; 11 studies; low‐certainty evidence) to 100% for ICG and technetium‐99m (32 women; 1 study; very low‐certainty evidence). The rates of positive lymph nodes ranged from 5.2% to 34.4% with a mean of 20.1% (95% CI 17.7% to 22.3%). The pooled sensitivity of SLNB was 91.8% (95% CI 86.5% to 95.1%; total 2237 women, of whom 409 had SLN involvement; moderate‐certainty evidence). The sensitivity for of SLNB for the different tracers were: blue dye alone 95.2% (95% CI 77.2% to 99.2%; 559 women; 11 studies; low‐certainty evidence); Technetium‐99m alone 90.5% (95% CI 67.7% to 97.7%; 257 women; 4 studies; low‐certainty evidence); technetium‐99m and blue dye 91.9% (95% CI 74.4% to 97.8%; 548 women; 12 studies; low‐certainty evidence); ICG alone 92.5% (95% CI 81.8% to 97.1%; 953 women; 9 studies; moderate‐certainty evidence); ICG and blue dye 90.5% (95% CI 63.2.% to 98.1%; 215 women; 2 studies; low‐certainty evidence); and ICG and technetium‐99m 100% (95% CI 63% to 100%; 32 women; 1 study; very low‐certainty evidence). Meta‐regression analyses found that the sensitivities did not differ between the different tracers used, between studies with a majority of women with FIGO stage 1A versus 1B or above; between studies assessing the pelvic lymph node basin alone versus the pelvic and para‐aortic lymph node basin; or between studies that used subserosal alone versus subserosal and cervical injection. It should be noted that a false‐positive result cannot occur, as the histological examination of the SLN is unchanged by the results from any additional nodes removed at systematic lymphadenectomy. Authors' conclusions The diagnostic test accuracy for SLNB using either ICG alone or a combination of a dye (blue or ICG) and technetium‐99m is probably good, with high sensitivity, where a SLN could be detected. Detection rates with ICG or a combination of dye (ICG or blue) and technetium‐99m may be higher. The value of a SLNB approach in a treatment pathway, over adjuvant treatment decisions based on uterine factors and molecular profiling, requires examination in a high‐quality intervention study. Plain language summary Can tests to identify the main draining lymph nodes in women with endometrial cancer accurately diagnose if the cancer has spread to the lymph nodes? The issue  Women who have endometrial cancer (cancer arising from the lining of the womb) may have cancer cells spread to lymph nodes in the pelvis and/or the para‐aortic area (main blood vessel in the upper abdominal cavity) and need additional radiotherapy treatment, with or without chemotherapy, following initial hysterectomy. Previous studies looked at the effect of removing as many of the pelvic lymph nodes as possible (lymphadenectomy), but did not show a benefit to survival. However, lymphadenectomy often causes problems with lymphoedema (swelling of the legs) and lymphocysts (collections of lymph fluid). Sentinel lymph node (SLN) biopsy (SLNB) involves identifying the first SLN draining the tumour, using a dye or radioactive tracer, or both. The SLNs are removed and examined under a microscope to check for cancer cells. Additional treatment decisions may change depending on these findings. Why is this review important?  Several studies used dyes or traceable agents to identify SLNs in women with womb cancer. It is not clear whether all of these agents are sufficiently accurate to predict which women have cancerous spread to lymph nodes and whether it is better to inject dye to the cervix or the muscle of the womb. This review summarises the evidence and produces overall estimates of the relative accuracies of the available tests. How was the review conducted?  We included studies that tested the accuracy of tracer agent/s to identify SLN against the standard method of removing all pelvic nodes, with or without para‐aortic lymph nodes. We restricted the studies to those that examined the SLNs by taking multiple slices of the node and staining for cancer cells using antibody markers (immunohistochemistry (IHC)) before examining them under a microscope (ultrastaging).   What are the findings?  We included 33 studies (2237 women) that evaluated any techniques used to identify the SLN draining from the womb. These included: 11 for blue dye only; four for technetium‐99m (a radioactive substance) only; nine for indocyanine green (ICG) dye that fluoresces under near infra‐red light; 12 that used a combination of blue dye and technetium‐99m and one that used a combination of ICG and technetium‐99mand where tracers were injected into the cervix (neck of the womb) or directly into the muscle of the womb, or a combination. Overall, the methodological reporting was poor, which limited our ability to assess the quality of the studies. Tests have two attributes. 1. The ability of the tests to find the SLN (detection rate) varied, with the blue dye test only detecting SLN in 77.8% of women, compared with 80.9% with technetium‐99m alone, 86.3% for combined blue dye/technetium‐99m, 92.4% for ICG alone, 96.7% for ICG/blue dye and 100% for ICG/technetium‐99m. If SLN are not detected, they cannot be examined for cancer cells; therefore, these women may either need to undergo lymphadenectomy or treated based on the risk factors of the cancer within the womb. 2. If cancer cells were present in lymph nodes, did the SLNB identify a lymph node with cancer within it or did it miss a lymph node that contained cancer? This is called the sensitivity of the test and the ability of the test to avoid a false‐negative result. If a SLN was found, all tests can identify cancer in the pelvic/para‐aortic nodes with good accuracy (more than 90% of nodes with cancer will be accurately identified with any of the tests). In this setting a false‐positive result cannot occur, as the histological examination of the SLN is unchanged by the results from any additional nodes removed at systematic lymphadenectomy. What does this mean?  All of the different methods were able to identify a SLN, although techniques that used ICG, either alone or in combination with blue dyes or technetium‐99m, may be more likely to find a node. If a node was found, this was likely to be the one that contained cancer cells; if the SLN did not contain any cancer cells (negative‐SLN), the chance that other nodes contained cancer cells was less than 10%. However, these tests were limited to women who were likely to have early‐stage disease and only small volume of cancer cells within lymph nodes. Where nodes or lymphatic channels contain many cancer cells, this can block lymphatic channels, affecting lymph drainage, and may have been a reason for failure to find SLN in some of the studies. This review only looked at the accuracy of being able to identify a SLN and does not tell us whether this offers additional survival advantage to women with early‐stage womb cancer. Other types of clinical trials are needed to show us whether performing a SLNB, to guide need for additional treatment, improves survival compared to basing treatment decisions on risk factors determined from examination of the womb alone. This would be important to know, as previous studies have not demonstrated a survival advantage to removing all of the pelvic lymph nodes.","6","John Wiley & Sons, Ltd","1465-1858","Coloring Agents; Endometrial Neoplasms [*pathology]; Female; Fluorescent Antibody Technique; Humans; Indocyanine Green; Lymph Node Excision; Lymph Nodes [*pathology]; Pelvis; Radioactive Tracers; Sentinel Lymph Node Biopsy [*standards, statistics & numerical data]; Spectroscopy, Near-Infrared; Technetium","10.1002/14651858.CD013021.pub2","http://dx.doi.org/10.1002/14651858.CD013021.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD011817.PUB2","Garcia-Casal, MN; Pasricha, S-R; Martinez, RX; Lopez-Perez, L; Peña-Rosas, JP","Serum or plasma ferritin concentration as an index of iron deficiency and overload","Cochrane Database of Systematic Reviews","2021","Abstract - Background Reference standard indices of iron deficiency and iron overload are generally invasive, expensive, and can be unpleasant or occasionally risky. Ferritin is an iron storage protein and its concentration in the plasma or serum reflects iron stores; low ferritin indicates iron deficiency, while elevated ferritin reflects risk of iron overload. However, ferritin is also an acute‐phase protein and its levels are elevated in inflammation and infection. The use of ferritin as a diagnostic test of iron deficiency and overload is a common clinical practice. Objectives To determine the diagnostic accuracy of ferritin concentrations (serum or plasma) for detecting iron deficiency and risk of iron overload in primary and secondary iron‐loading syndromes. Search methods We searched the following databases (10 June 2020): DARE (Cochrane Library) Issue 2 of 4 2015, HTA (Cochrane Library) Issue 4 of 4 2016, CENTRAL (Cochrane Library) Issue 6 of 12 2020, MEDLINE (OVID) 1946 to 9 June 2020, Embase (OVID) 1947 to week 23 2020, CINAHL (Ebsco) 1982 to June 2020, Web of Science (ISI) SCI, SSCI, CPCI‐exp & CPCI‐SSH to June 2020, POPLINE 16/8/18, Open Grey (10/6/20), TRoPHI (10/6/20), Bibliomap (10/6/20), IBECS (10/6/20), SCIELO (10/6/20), Global Index Medicus (10/6/20) AIM, IMSEAR, WPRIM, IMEMR, LILACS (10/6/20), PAHO (10/6/20), WHOLIS 10/6/20, IndMED (16/8/18) and Native Health Research Database (10/6/20). We also searched two trials registers and contacted relevant organisations for unpublished studies. Selection criteria We included all study designs seeking to evaluate serum or plasma ferritin concentrations measured by any current or previously available quantitative assay as an index of iron status in individuals of any age, sex, clinical and physiological status from any country. Data collection and analysis We followed standard Cochrane methods. We designed the data extraction form to record results for ferritin concentration as the index test, and bone marrow iron content for iron deficiency and liver iron content for iron overload as the reference standards. Two other authors further extracted and validated the number of true positive, true negative, false positive, false negative cases, and extracted or derived the sensitivity, specificity, positive and negative predictive values for each threshold presented for iron deficiency and iron overload in included studies. We assessed risk of bias and applicability using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS)‐2 tool. We used GRADE assessment to enable the quality of evidence and hence strength of evidence for our conclusions. Main results Our search was conducted initially in 2014 and updated in 2017, 2018 and 2020 (10 June). We identified 21,217 records and screened 14,244 records after duplicates were removed. We assessed 316 records in full text. We excluded 190 studies (193 records) with reasons and included 108 studies (111 records) in the qualitative and quantitative analysis. There were 11 studies (12 records) that we screened from the last search update and appeared eligible for a future analysis. We decided to enter these as awaiting classification. We stratified the analysis first by participant clinical status: apparently healthy and non‐healthy populations. We then stratified by age and pregnancy status as: infants and children, adolescents, pregnant women, and adults. Iron deficiency We included 72 studies (75 records) involving 6059 participants. Apparently healthy populations Five studies screened for iron deficiency in people without apparent illness. In the general adult population, three studies reported sensitivities of 63% to 100% at the optimum cutoff for ferritin, with corresponding specificities of 92% to 98%, but the ferritin cutoffs varied between studies. One study in healthy children reported a sensitivity of 74% and a specificity of 77%. One study in pregnant women reported a sensitivity of 88% and a specificity of 100%. Overall confidence in these estimates was very low because of potential bias, indirectness, and sparse and heterogenous evidence. No studies screened for iron overload in apparently healthy people. People presenting for medical care There were 63 studies among adults presenting for medical care (5042 participants). For a sample of 1000 subjects with a 35% prevalence of iron deficiency (of the included studies in this category) and supposing a 85% specificity, there would be 315 iron‐deficient subjects correctly classified as having iron deficiency and 35 iron‐deficient subjects incorrectly classified as not having iron deficiency, leading to a 90% sensitivity. Thresholds proposed by the authors of the included studies ranged between 12 to 200 µg/L. The estimated diagnostic odds ratio was 50. Among non‐healthy adults using a fixed threshold of 30 μg/L (nine studies, 512 participants, low‐certainty evidence), the pooled estimate for sensitivity was 79% with a 95% confidence interval of (58%, 91%) and specificity of 98%, with a 95% confidence interval of (91%, 100%). The estimated diagnostic odds ratio was 140, a relatively highly informative test. Iron overload We included 36 studies (36 records) involving 1927 participants. All studies concerned non‐healthy populations. There were no studies targeting either infants, children, or pregnant women. Among all populations (one threshold for males and females; 36 studies, 1927 participants, very low‐certainty evidence): for a sample of 1000 subjects with a 42% prevalence of iron overload (of the included studies in this category) and supposing a 65% specificity, there would be 332 iron‐overloaded subjects correctly classified as having iron overload and 85 iron‐overloaded subjects incorrectly classified as not having iron overload, leading to a 80% sensitivity. The estimated diagnostic odds ratio was 8. Authors' conclusions At a threshold of 30 micrograms/L, there is low‐certainty evidence that blood ferritin concentration is reasonably sensitive and a very specific test for iron deficiency in people presenting for medical care. There is very low certainty that high concentrations of ferritin provide a sensitive test for iron overload in people where this condition is suspected. There is insufficient evidence to know whether ferritin concentration performs similarly when screening asymptomatic people for iron deficiency or overload. Plain language summary How accurate are tests to measure the level of ferritin (a protein that stores iron) in the blood at diagnosing iron deficiency and overload? Key messages ‐ Tests that measure the level of ferritin in the blood may be reasonably accurate for diagnosing iron deficiency (low iron levels) in people: ‐ seeking medical care; and ‐ whose doctors suspect iron deficiency. ‐ The accuracy of ferritin blood tests for diagnosing iron overload (high iron levels) is unclear, due to a lack of robust evidence. ‐ To strengthen the evidence, we need future studies to: ‐ investigate a wider range of populations; and ‐ identify the levels of ferritin in the blood that are the best indicators of iron deficiency and overload. Why is it important to diagnose iron deficiency and overload? Iron is a mineral found in every cell of the body. It comes from iron‐rich foods like red meat, beans and fortified cereal (to which iron has been added artificially), among others, or from supplements (iron tablets, micronutrient powders, drops or syrups). The body needs iron to make: ‐ red blood cells; and ‐ haemoglobin, a protein in blood that carries oxygen from the lungs to the rest of the body. An iron test can show if someone has too little iron (iron deficiency) or too much iron (iron overload). It is important to test iron levels because: ‐ iron deficiency can cause anaemia (low levels of red blood cells or haemoglobin), tiredness and weakness. It can be a sign that someone has a serious health problem, such as internal bleeding; while ‐ iron overload can damage the liver, heart and other organs permanently. What tests can be used to diagnose iron deficiency and overload? There are several different tests available to check the level of iron in the body. The most accurate tests involve using a needle to collect a small sample of: ‐ bone marrow fluid (to diagnose iron deficiency); or ‐ tissue from the liver (to diagnose iron overload). However, these tests are expensive and can be risky for people in poor health. A simpler test involves measuring the level of ferritin (a protein that stores iron) in the blood, to estimate the amount of iron in the body. What did we want to find out? We wanted to find out if ferritin blood tests accurately diagnose iron deficiency and iron overload. What did we do? We searched for studies that compared ferritin blood tests against: ‐ tests of iron levels in the bone marrow, to diagnose iron deficiency; and ‐ tests of iron levels in the liver, to diagnose iron overload. We compared and summarised the results of the studies and rated our confidence in the evidence, based on factors like study methods and sizes. What did we find? We included 72 studies of 6059 people investigating the ability of ferritin blood tests to diagnose: ‐ iron deficiency in people who sought medical care and whose doctor suspected iron deficiency (70 studies, 5709 people); ‐ iron deficiency in people without any sign of disease (five studies, 350 people); and ‐ iron overload suspected by a doctor (36 studies, 1927 people). Evidence suggests that ferritin blood tests may be reasonably accurate for diagnosing iron deficiency in people seeking medical care. For example, in studies where people with fewer than 30 micrograms of ferritin in one litre of blood were diagnosed with iron deficiency, ferritin blood tests correctly identified: ‐ iron deficiency in four out of five people who did have iron deficiency; and ‐ no iron deficiency in 19 out of 20 people who had normal levels of iron. The evidence was not robust enough to determine if ferritin blood tests accurately diagnose: ‐ iron deficiency in people without any sign of disease; or ‐ iron overload suspected by a doctor. What are the limitations of the evidence? The studies were: ‐ small; ‐ conducted in ways that may have introduced errors into their results; and ‐ focused on specific populations (like children, young people and pregnant women). For these reasons, we have limited confidence in the evidence. How up‐to‐date is this evidence? The evidence is up‐to‐date to June 2020.","5","John Wiley & Sons, Ltd","1465-1858","Adolescent; Adult; Aged; Anemia, Iron-Deficiency [*blood, diagnosis]; Bias; Biomarkers [blood]; Child; Child, Preschool; Female; Ferritins [*blood]; Humans; Infant; Iron Overload [*blood, diagnosis]; Male; Middle Aged; Pregnancy; Pregnant Women; Sensitivity and Specificity; Young Adult","10.1002/14651858.CD011817.pub2","http://dx.doi.org/10.1002/14651858.CD011817.pub2","Tobacco Addiction"
"CD013346.PUB2","Colli, A; Nadarevic, T; Miletic, D; Giljaca, V; Fraquelli, M; Štimac, D; Casazza, G","Abdominal ultrasound and alpha‐foetoprotein for the diagnosis of hepatocellular carcinoma in adults with chronic liver disease","Cochrane Database of Systematic Reviews","2021","Abstract - Background Hepatocellular carcinoma (HCC) occurs mostly in people with chronic liver disease and ranks sixth in terms of global instances of cancer, and fourth in terms of cancer deaths for men. Despite that abdominal ultrasound (US) is used as an initial test to exclude the presence of focal liver lesions and serum alpha‐foetoprotein (AFP) measurement may raise suspicion of HCC occurrence, further testing to confirm diagnosis as well as staging of HCC is required. Current guidelines recommend surveillance programme using US, with or without AFP, to detect HCC in high‐risk populations despite the lack of clear benefits on overall survival. Assessing the diagnostic accuracy of US and AFP may clarify whether the absence of benefit in surveillance programmes could be related to under‐diagnosis. Therefore, assessment of the accuracy of these two tests for diagnosing HCC in people with chronic liver disease, not included in surveillance programmes, is needed. Objectives Primary: the diagnostic accuracy of US and AFP, alone or in combination, for the diagnosis of HCC of any size and at any stage in adults with chronic liver disease, either in a surveillance programme or in a clinical setting. Secondary: to assess the diagnostic accuracy of abdominal US and AFP, alone or in combination, for the diagnosis of resectable HCC; to compare the diagnostic accuracy of the individual tests versus the combination of both tests; to investigate sources of heterogeneity in the results. Search methods We searched the Cochrane Hepato‐Biliary Group Controlled Trials Register, the Cochrane Hepato‐Biliary Group Diagnostic‐Test‐Accuracy Studies Register, Cochrane Library, MEDLINE, Embase, LILACS, Science Citation Index Expanded, until 5 June 2020. We applied no language or document‐type restrictions. Selection criteria Studies assessing the diagnostic accuracy of US and AFP, independently or in combination, for the diagnosis of HCC in adults with chronic liver disease, with cross‐sectional and case‐control designs, using one of the acceptable reference standards, such as pathology of the explanted liver, histology of resected or biopsied focal liver lesion, or typical characteristics on computed tomography, or magnetic resonance imaging, all with a six‐months follow‐up. Data collection and analysis We independently screened studies, extracted data, and assessed the risk of bias and applicability concerns, using the QUADAS‐2 checklist. We presented the results of sensitivity and specificity, using paired forest‐plots, and tabulated the results. We used a hierarchical meta‐analysis model where appropriate. We presented uncertainty of the accuracy estimates using 95% confidence intervals (CIs). We double‐checked all data extractions and analyses. Main results We included 373 studies. The index‐test was AFP (326 studies, 144,570 participants); US (39 studies, 18,792 participants); and a combination of AFP and US (eight studies, 5454 participants). We judged at high‐risk of bias all but one study. Most studies used different reference standards, often inappropriate to exclude the presence of the target condition, and the time‐interval between the index test and the reference standard was rarely defined. Most studies with AFP had a case‐control design. We also had major concerns for the applicability due to the characteristics of the participants. As the primary studies with AFP used different cut‐offs, we performed a meta‐analysis using the hierarchical‐summary‐receiver‐operating‐characteristic model, then we carried out two meta‐analyses including only studies reporting the most used cut‐offs: around 20 ng/mL or 200 ng/mL. AFP cut‐off 20 ng/mL : for HCC (147 studies) sensitivity 60% (95% CI 58% to 62%), specificity 84% (95% CI 82% to 86%); for resectable HCC (six studies) sensitivity 65% (95% CI 62% to 68%), specificity 80% (95% CI 59% to 91%). AFP cut‐off 200 ng/mL : for HCC (56 studies) sensitivity 36% (95% CI 31% to 41%), specificity 99% (95% CI 98% to 99%); for resectable HCC (two studies) one with sensitivity 4% (95% CI 0% to 19%), specificity 100% (95% CI 96% to 100%), and one with sensitivity 8% (95% CI 3% to 18%), specificity 100% (95% CI 97% to 100%). US : for HCC (39 studies) sensitivity 72% (95% CI 63% to 79%), specificity 94% (95% CI 91% to 96%); for resectable HCC (seven studies) sensitivity 53% (95% CI 38% to 67%), specificity 96% (95% CI 94% to 97%). Combination of AFP (cut‐off of 20 ng/mL) and US : for HCC (six studies) sensitivity 96% (95% CI 88% to 98%), specificity 85% (95% CI 73% to 93%); for resectable HCC (two studies) one with sensitivity 89% (95% CI 73% to 97%), specificity of 83% (95% CI 76% to 88%), and one with sensitivity 79% (95% CI 54% to 94%), specificity 87% (95% CI 79% to 94%). The observed heterogeneity in the results remains mostly unexplained, and only in part referable to different cut‐offs or settings (surveillance programme compared to clinical series). The sensitivity analyses, excluding studies published as abstracts, or with case‐control design, showed no variation in the results. We compared the accuracy obtained from studies with AFP (cut‐off around 20 ng/mL) and US: a direct comparison in 11 studies (6674 participants) showed a higher sensitivity of US (81%, 95% CI 66% to 90%) versus AFP (64%, 95% CI 56% to 71%) with similar specificity: US 92% (95% CI 83% to 97%) versus AFP 89% (95% CI 79% to 94%). A direct comparison of six studies (5044 participants) showed a higher sensitivity (96%, 95% CI 88% to 98%) of the combination of AFP and US versus US (76%, 95% CI 56% to 89%) with similar specificity: AFP and US 85% (95% CI 73% to 92%) versus US 93% (95% CI 80% to 98%). Authors' conclusions In the clinical pathway for the diagnosis of HCC in adults, AFP and US, singularly or in combination, have the role of triage‐tests. We found that using AFP, with 20 ng/mL as a cut‐off, about 40% of HCC occurrences would be missed, and with US alone, more than a quarter. The combination of the two tests showed the highest sensitivity and less than 5% of HCC occurrences would be missed with about 15% of false‐positive results. The uncertainty resulting from the poor study quality and the heterogeneity of included studies limit our ability to confidently draw conclusions based on our results. Plain language summary Abdominal ultrasound and alpha‐foetoprotein for the diagnosis of hepatocellular carcinoma Why is improving the diagnosis of hepatocellular carcinoma important? Hepatocellular carcinoma (HCC), i.e. cancer originating in the liver, is sixth in terms of global occurrences of cancer and fourth in terms of cancer deaths in men. This cancer occurs mostly in people with chronic liver disease regardless of the cause. Ultrasound (US), which uses ultrasound waves to show abnormalities in the liver, can detect the presence of liver lesions suspected of being HCC. Alpha‐foetoprotein (AFP), a glycoprotein, produced by the liver and measurable in the blood, is considered a tumour‐marker because high levels can be associated with the presence of HCC. These two tests (US and AFP) are used, alone or in combination, to exclude the presence of HCC in people at high risk of developing HCC. People at high risk are those who have chronic liver disease. Current guidelines recommend surveillance programmes, repeating abdominal US with or without AFP testing every six months to detect early HCC, amenable to surgical resection or other treatment. What is the aim of this review? To find out how accurate AFP, US, and a combination of AFP and US are for diagnosing HCC in people with chronic liver disease. What was studied in this review? AFP (tumour marker), that can easily be measured in the blood, using a commercial kit. Studies with AFP used various threshold values for defining the test as positive or negative. US is an equipment, available worldwide. It produces images of liver and other abdominal organs. It can detect the presence of liver lesions suspected of being HCC. A combination of AFP and US can detect or negate the presence of liver lesions suspected of being HCC. What are the main results in this review? We found 373 total studies in adults: AFP was analysed in 326 studies, 144,570 participants; US in 39 studies, 18,792 participants; and the combination of AFP and US in eight studies, 5454 participants. ‐ AFP with threshold of 20 ng/mL (147 studies): the test was positive in 60 out of 100 participants with HCC and in 16 out of 100 participants without HCC. AFP with threshold of 200 ng/mL (56 studies): the test was positive in 36 out of 100 participants with HCC and only in 1 out of 100 without HCC. ‐ US (39 studies): the test was positive in 72 out of 100 participants with HCC and in 6 out of 100 participants without HCC. ‐ The combination of AFP with threshold of 20 ng/mL and US (6 studies): one or both tests were positive in 96 out of 100 participants with HCC and in 15 out of 100 participants without HCC. Thus, the combination of the two tests is better in detecting participants with HCC. Considering that people with chronic liver disease have HCC in 5 out of 100, one can assume that among 1000 people with chronic liver disease, 50 will have HCC, and, using AFP and abdominal US in combination, one can detect 48 out of the people with HCC, and 2 people will go undetected and will not receive appropriate treatment; 950 out of 1000 will have no HCC, and 143 of them will receive a wrong diagnosis of HCC, and will undergo further unnecessary testing such as computed tomography, magnetic resonance imaging, or biopsy. How reliable are the results of the studies in this review? All but one study had issues with risk of bias, especially in participants selection and in the correct definition on presence of HCC. These problems could impair the correct estimates of the diagnostic ability of the three tests. Who do the results of this review apply to? People with chronic liver disease What are the implications of this review? Using AFP, with 20 ng/mL, as threshold, about 40% of HCC occurrences would be missed, and with US alone, more than a quarter. The sensitivity was highest when the two tests were used in combination, and less than 5% of HCC occurrences would be missed with about 15% of false‐positive results. How up‐to‐date is this review? 5 June 2020","4","John Wiley & Sons, Ltd","1465-1858","Abdomen [diagnostic imaging]; Adult; Bias; Biomarkers, Tumor [blood]; Carcinoma, Hepatocellular [blood, *diagnosis, pathology]; Case-Control Studies; Chronic Disease; Confidence Intervals; Cross-Sectional Studies; Female; Humans; Liver Diseases [*complications]; Liver Neoplasms [blood, *diagnosis, pathology]; Male; Sensitivity and Specificity; Ultrasonography [*methods]; alpha-Fetoproteins [*analysis]","10.1002/14651858.CD013346.pub2","http://dx.doi.org/10.1002/14651858.CD013346.pub2","Hepato-Biliary"
"CD013694.PUB2","Shapiro, AE; Ross, JM; Yao, M; Schiller, I; Kohli, M; Dendukuri, N; Steingart, KR; Horne, DJ","Xpert MTB/RIF and Xpert Ultra assays for screening for pulmonary tuberculosis and rifampicin resistance in adults, irrespective of signs or symptoms","Cochrane Database of Systematic Reviews","2021","Abstract - Background Tuberculosis is a leading cause of infectious disease‐related death and is one of the top 10 causes of death worldwide. The World Health Organization (WHO) recommends the use of specific rapid molecular tests, including Xpert MTB/RIF or Xpert Ultra, as initial diagnostic tests for the detection of tuberculosis and rifampicin resistance in people with signs and symptoms of tuberculosis. However, the WHO estimates that nearly one‐third of all active tuberculosis cases go undiagnosed and unreported. We were interested in whether a single test, Xpert MTB/RIF or Xpert Ultra, could be useful as a screening test to close this diagnostic gap and improve tuberculosis case detection. Objectives To estimate the accuracy of Xpert MTB/RIF and Xpert Ultra for screening for pulmonary tuberculosis in adults, irrespective of signs or symptoms of pulmonary tuberculosis in high‐risk groups and in the general population. Screening ""irrespective of signs or symptoms"" refers to screening of people who have not been assessed for the presence of tuberculosis symptoms (e.g. cough). To estimate the accuracy of Xpert MTB/RIF and Xpert Ultra for detecting rifampicin resistance in adults screened for tuberculosis, irrespective of signs and symptoms of pulmonary tuberculosis in high‐risk groups and in the general population. Search methods We searched 12 databases including the Cochrane Infectious Diseases Group Specialized Register, MEDLINE and Embase, on 19 March 2020 without language restrictions. We also reviewed reference lists of included articles and related Cochrane Reviews, and contacted researchers in the field to identify additional studies. Selection criteria Cross‐sectional and cohort studies in which adults (15 years and older) in high‐risk groups (e.g. people living with HIV, household contacts of people with tuberculosis) or in the general population were screened for pulmonary tuberculosis using Xpert MTB/RIF or Xpert Ultra. For tuberculosis detection, the reference standard was culture. For rifampicin resistance detection, the reference standards were culture‐based drug susceptibility testing and line probe assays. Data collection and analysis Two review authors independently extracted data using a standardized form and assessed risk of bias and applicability using QUADAS‐2. We used a bivariate random‐effects model to estimate pooled sensitivity and specificity with 95% credible intervals (CrIs) separately for tuberculosis detection and rifampicin resistance detection. We estimated all models using a Bayesian approach. For tuberculosis detection, we first estimated screening accuracy in distinct high‐risk groups, including people living with HIV, household contacts, people residing in prisons, and miners, and then in several high‐risk groups combined. Main results We included a total of 21 studies: 18 studies (13,114 participants) evaluated Xpert MTB/RIF as a screening test for pulmonary tuberculosis and one study (571 participants) evaluated both Xpert MTB/RIF and Xpert Ultra. Three studies (159 participants) evaluated Xpert MTB/RIF for rifampicin resistance. Fifteen studies (75%) were conducted in high tuberculosis burden and 16 (80%) in high TB/HIV‐burden countries. We judged most studies to have low risk of bias in all four QUADAS‐2 domains and low concern for applicability. Xpert MTB/RIF and Xpert Ultra as screening tests for pulmonary tuberculosis In people living with HIV (12 studies), Xpert MTB/RIF pooled sensitivity and specificity (95% CrI) were 61.8% (53.6 to 69.9) (602 participants; moderate‐certainty evidence) and 98.8% (98.0 to 99.4) (4173 participants; high‐certainty evidence). Of 1000 people where 50 have tuberculosis on culture, 40 would be Xpert MTB/RIF‐positive; of these, 9 (22%) would not have tuberculosis (false‐positives); and 960 would be Xpert MTB/RIF‐negative; of these, 19 (2%) would have tuberculosis (false‐negatives). In people living with HIV (1 study), Xpert Ultra sensitivity and specificity (95% CI) were 69% (57 to 80) (68 participants; very low‐certainty evidence) and 98% (97 to 99) (503 participants; moderate‐certainty evidence). Of 1000 people where 50 have tuberculosis on culture, 53 would be Xpert Ultra‐positive; of these, 19 (36%) would not have tuberculosis (false‐positives); and 947 would be Xpert Ultra‐negative; of these, 16 (2%) would have tuberculosis (false‐negatives). In non‐hospitalized people in high‐risk groups (5 studies), Xpert MTB/RIF pooled sensitivity and specificity were 69.4% (47.7 to 86.2) (337 participants, low‐certainty evidence) and 98.8% (97.2 to 99.5) (8619 participants, moderate‐certainty evidence). Of 1000 people where 10 have tuberculosis on culture, 19 would be Xpert MTB/RIF‐positive; of these, 12 (63%) would not have tuberculosis (false‐positives); and 981 would be Xpert MTB/RIF‐negative; of these, 3 (0%) would have tuberculosis (false‐negatives). We did not identify any studies using Xpert MTB/RIF or Xpert Ultra for screening in the general population. Xpert MTB/RIF as a screening test for rifampicin resistance Xpert MTB/RIF sensitivity was 81% and 100% (2 studies, 20 participants; very low‐certainty evidence), and specificity was 94% to 100%, (3 studies, 139 participants; moderate‐certainty evidence). Authors' conclusions Of the high‐risks groups evaluated, Xpert MTB/RIF applied as a screening test was accurate for tuberculosis in high tuberculosis burden settings. Sensitivity and specificity were similar in people living with HIV and non‐hospitalized people in high‐risk groups. In people living with HIV, Xpert Ultra sensitivity was slightly higher than that of Xpert MTB/RIF and specificity similar. As there was only one study of Xpert Ultra in this analysis, results should be interpreted with caution. There were no studies that evaluated the tests in people with diabetes mellitus and other groups considered at high‐risk for tuberculosis, or in the general population. Plain language summary How accurate are sputum Xpert tests for screening for active pulmonary tuberculosis and rifampicin resistance in adults whether or not they have tuberculosis symptoms? Why is using Xpert tests to screen for pulmonary tuberculosis important? Tuberculosis is the leading cause of infectious disease‐related death and one of the top 10 causes of death worldwide. The World Health Organization (WHO) recommends using specific rapid tests as initial tests for diagnosing tuberculosis and rifampicin resistance in people with signs and symptoms of tuberculosis. However, the WHO estimates that nearly one‐third of all active tuberculosis cases go undiagnosed and unreported. Not recognizing tuberculosis when it is present (a false negative test result) may result in illness and death and an increased risk of infecting others. An incorrect diagnosis of tuberculosis (false‐positive result) may mean that people are given antibiotics when there is no benefit to be gained. What is the aim of this review? To estimate the accuracy of Xpert MTB/RIF and Xpert Ultra as screening tests for pulmonary tuberculosis and rifampicin resistance in adults whether or not they have tuberculosis symptoms (such as cough, fever, weight loss, and night sweats). We were interested in how the tests worked in groups at high risk for tuberculosis, including people living with HIV (PLHIV), household contacts of people with tuberculosis, miners, people residing in prisons, people with diabetes, and in the general public. What was studied in this review? Xpert MTB/RIF and Xpert Ultra are rapid tests for simultaneously diagnosing tuberculosis and rifampicin resistance. We combined study results to determine: ‐ sensitivity: people with tuberculosis (rifampicin resistance) correctly diagnosed as having the condition. ‐ specificity: people without tuberculosis (rifampicin resistance) correctly identified as not having the condition. The closer sensitivity and specificity are to 100%, the better the test. What are the main results in this review? Twenty‐one studies: 18 studies (13,114 participants) evaluated Xpert MTB/RIF as a screening test for pulmonary tuberculosis and one study (571 participants) evaluated both Xpert MTB/RIF and Xpert Ultra. Three studies (159 participants) evaluated Xpert MTB/RIF for rifampicin resistance. For every 1000 people tested, if 50 had tuberculosis according to the reference standard: PLHIV ‐ Xpert MTB/RIF (12 studies): · 40 people would test positive, including 9 without tuberculosis (62% sensitivity) · 960 people would test negative, including 19 with tuberculosis (99% specificity) ‐ Xpert Ultra (1 study): · 53 people would test positive, including 19 without tuberculosis (69% sensitivity) · 947 people would test negative, including 16 with tuberculosis (98% specificity) For every 1000 people tested, if 10 had tuberculosis according to the reference standard: Other high‐risk groups combined ‐ Xpert MTB/RIF (5 studies): · 19 people would test positive, including 12 without tuberculosis (69% sensitivity) · 981 people would test negative, including 3 with tuberculosis (99% specificity) For detection of rifampicin resistance, Xpert MTB/RIF sensitivity was 81% and 100% (2 studies) and specificity was 94% to 100% (3 studies). How reliable are the results of the studies in this review? In the included studies, the reference standards for diagnosing pulmonary tuberculosis (culture) and rifampicin resistance (drug susceptibility testing) are likely to have been reliable methods for deciding whether patients really had the conditions. We were fairly confident in the results for Xpert MTB/RIF in PLHIV, and less so for other high‐risk groups. Not enough people have been studied to be confident about the results for Xpert Ultra or for detection of rifampicin resistance. Who do the results of this review apply to? Studies were mainly performed in high tuberculosis and high HIV burden settings. No studies evaluated the tests in people with diabetes mellitus or the general population. What are the implications of this review? In PLHIV, Xpert MTB/RIF as a screening test was accurate for tuberculosis in high tuberculosis burden settings. In high‐risk groups, Xpert MTB/RIF may assist in identifying tuberculosis, but the certainty of evidence is low. In PLHIV, Xpert Ultra sensitivity was slightly higher than that of Xpert MTB/RIF and specificity similar based on one study. There were few studies and few people tested for rifampicin resistance and no studies that evaluated the tests in people with diabetes or in the general population. How up‐to‐date is this review? 19 March 2020.","3","John Wiley & Sons, Ltd","1465-1858","*Drug Resistance, Bacterial; Adult; Antibiotics, Antitubercular [*pharmacology]; Bacteriological Techniques [methods]; Bayes Theorem; Bias; Cohort Studies; Cross-Sectional Studies; False Negative Reactions; False Positive Reactions; HIV Infections [complications]; Humans; Mycobacterium tuberculosis [*drug effects, isolation & purification]; Polymerase Chain Reaction [*methods]; Rifampin [*pharmacology]; Sensitivity and Specificity; Sputum [microbiology]; Tuberculosis, Pulmonary [complications, *diagnosis, drug therapy]","10.1002/14651858.CD013694.pub2","http://dx.doi.org/10.1002/14651858.CD013694.pub2","Infectious Diseases"
"CD014547","Macey, R; Walsh, T; Riley, P; Glenny, A-M; Worthington, HV; Clarkson, JE; Ricketts, D","Electrical conductance for the detection of dental caries","Cochrane Database of Systematic Reviews","2021","Abstract - Background Caries is one of the most prevalent, preventable conditions worldwide. A wide variety of management options are available at different thresholds of disease, ranging from non‐operative preventive strategies such as improved oral hygiene, reduced sugar diet, and application of topical fluoride, to minimally invasive treatments for early lesions which are limited to enamel, through to selective removal and restoration for extensive lesions. The cornerstone of caries detection is a visual and tactile dental examination, however, an increasing array of methods of caries lesion detection have been proposed that could potentially support traditional methods of detection and diagnosis. Earlier identification of disease could afford patients the opportunity of less invasive treatment with less destruction of tooth tissue, reduce the need for treatment with aerosol‐generating procedures, and potentially result in a reduced cost of care to the patient and to healthcare services. Objectives Our primary objective was to determine the diagnostic accuracy of different electrical conductance devices for the detection and diagnosis of non‐cavitated coronal dental caries in different populations (children, adolescents, and adults) and when tested against different reference standards. Search methods Cochrane Oral Health's Information Specialist undertook a search of the following databases: MEDLINE Ovid (1946 to 26 April 2019); Embase Ovid (1980 to 26 April 2019); US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov, to 26 April 2019); and the World Health Organization International Clinical Trials Registry Platform (to 26 April 2019). We studied reference lists as well as published systematic review articles. Selection criteria We included diagnostic accuracy studies that compared electrical conductance devices with a reference standard of histology or an enhanced visual examination. This included prospective studies that evaluated the diagnostic accuracy of single index tests and studies that directly compared two or more index tests. We included studies using previously extracted teeth or those that recruited participants with teeth believed to be sound or with early lesions limited to enamel. Studies that explicitly recruited participants with more advanced lesions that were obviously into dentine or frankly cavitated were excluded. Data collection and analysis Two review authors extracted data independently using a piloted study data extraction form based on the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2). Sensitivity and specificity with 95% confidence intervals (CIs) were reported for each study. This information was displayed as coupled forest plots, and plotted as summary receiver operating characteristic (SROC) plots, displaying the sensitivity‐specificity points for each study. Due to variability in thresholds we estimated diagnostic accuracy using hierarchical summary receiver operating characteristic (HSROC) methods. Main results We included seven studies reporting a total of 719 tooth sites or surfaces, with an overall prevalence of the target condition of 73% (528 tooth sites or surfaces). The included studies evaluated two index tests: the electronic caries monitor (ECM) (four studies, 475 tooth surfaces) and CarieScan Pro (three studies, 244 tooth surfaces). Six studies used histology as the reference standard, one used an enhanced visual examination. No study was considered to be at low risk of bias across all four domains or low concern for applicability or both. All studies were at high (five studies) or unclear (two studies) risk of bias for the patient selection domain. We judged two studies to be at unclear risk of bias for the index test domain, and one study to be at high risk of bias for the reference standard and flow and timing domains. We judged three studies to be at low concern for applicability for patient selection, and all seven studies to be of low concern for reference standard and flow and timing domains. Studies were synthesised using a hierarchical method for meta‐analysis. There was variability in the results of the individual studies, with sensitivities which ranged from 0.55 to 0.98 and specificities from 0 to 1.00. These extreme values of specificity may be explained by a low number of healthy tooth surfaces in the included samples. The diagnostic odds ratio (DOR) was 15.65 (95% CI 1.43 to 171.15), and indicative of the variability in the included studies. Through meta‐regression we observed no meaningful difference in accuracy according to device type or dentition. Due to the small number of studies we were unable to formally investigate other potential sources of heterogeneity. We judged the certainty of the evidence as very low, and downgraded for risk of bias due to limitations in the design and conduct of the included studies, imprecision arising from the relatively small number of surfaces studied, and inconsistency due to the variability of results. Authors' conclusions The design and conduct of studies to determine the diagnostic accuracy of methods to detect and diagnose caries in situ is particularly challenging. The evidence base to support the detection and diagnosis of caries with electrical conductance devices is sparse. Newer electrical conductance devices show promise and further research at the enamel caries threshold using a robust study design to minimise bias is warranted. In terms of applicability, any future studies should be carried out in a clinical setting to provide a realistic assessment within the oral cavity where plaque, staining, and restorations can be problematic. Plain language summary Electrical conductance for the detection of early tooth decay Why is it important to improve dental caries (tooth decay) detection? Dentists often aim to identify tooth decay that has already advanced to a level which needs a filling. If dentists were able to find tooth decay when it has only affected the outer layer of the tooth (enamel) then it is possible to stop the decay from spreading any further and prevent the need for fillings. It is also important to avoid a false‐positive result, when treatment may be provided when caries is absent. What is the aim of this review? The aim of this Cochrane Review was to find out how accurate electrical conductance devices (non‐invasive devices that send an electrical current to the surface of the tooth) are for detecting and diagnosing early tooth decay as part of the dental 'check‐up' for children and adults who visit their general dentist. Researchers in Cochrane included seven studies published between 1997 and 2018 to answer this question. What was studied in the review? There are two electrical conductance devices that were included in this review: electronic caries monitor (ECM) (four studies) and CarieScan Pro (three studies). Both place a probe on the tooth which measures the electrical conductance of that point on the tooth. We studied decay on the occlusal surfaces (biting surfaces of the back teeth), the proximal surfaces (tooth surfaces that are next to each other), and the smooth surfaces next to the tongue, cheeks, and lips. What are the main results of the review? Researchers in Cochrane included seven studies with a total of 719 tooth sites or surfaces to answer this question. Due to the small number of tooth sites or surfaces studies, the results are very imprecise. We did not find any meaningful difference in accuracy according to type of device or the teeth of children/adolescents and adults. How reliable are the results of the studies in this review? We only included studies that assessed healthy teeth or those that were thought to have early tooth decay. This is because teeth with deep tooth decay would be easier to identify. However, there were some problems with how the studies were carried out. This may result in the electrical conductance devices appearing to be more accurate than they really are, increasing the number of correct test results from the electrical conductance devices. We judged the certainty of the evidence as very low due to how the studies selected their participants, the relatively small number of surfaces studied, and the variability of results. Who do the results of this review apply to? Studies included in the review were carried out in Brazil, UK, Denmark, and Turkey. Three studies performed the tests on extracted teeth and four studies were completed in a dental hospital. What are the implications of this review? The lack of eligible studies and the variation in the results of the studies means that at present, we are very uncertain of how electrical conductance devices are in detecting and diagnosing early tooth decay. How up‐to‐date is this review? The review authors searched for and used studies published up to 26 April 2019.","3","John Wiley & Sons, Ltd","1465-1858","*Electric Conductivity; Adolescent; Adult; Child; Confidence Intervals; Dental Caries [*diagnosis]; Dental Instruments; Humans; Prospective Studies; Reference Standards; Sensitivity and Specificity","10.1002/14651858.CD014547","http://dx.doi.org/10.1002/14651858.CD014547","Oral Health"
"CD014545","Walsh, T; Macey, R; Riley, P; Glenny, A-M; Schwendicke, F; Worthington, HV; Clarkson, JE; Ricketts, D; Su, T-L; Sengupta, A","Imaging modalities to inform the detection and diagnosis of early caries","Cochrane Database of Systematic Reviews","2021","Abstract - Background The detection and diagnosis of caries at the earliest opportunity is fundamental to the preservation of tooth tissue and maintenance of oral health. Radiographs have traditionally been used to supplement the conventional visual‐tactile clinical examination. Accurate, timely detection and diagnosis of early signs of disease could afford patients the opportunity of less invasive treatment with less destruction of tooth tissue, reduce the need for treatment with aerosol‐generating procedures, and potentially result in a reduced cost of care to the patient and to healthcare services. Objectives To determine the diagnostic accuracy of different dental imaging methods to inform the detection and diagnosis of non‐cavitated enamel only coronal dental caries. Search methods Cochrane Oral Health's Information Specialist undertook a search of the following databases: MEDLINE Ovid (1946 to 31 December 2018); Embase Ovid (1980 to 31 December 2018); US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov, to 31 December 2018); and the World Health Organization International Clinical Trials Registry Platform (to 31 December 2018). We studied reference lists as well as published systematic review articles. Selection criteria We included diagnostic accuracy study designs that compared a dental imaging method with a reference standard (histology, excavation, enhanced visual examination), studies that evaluated the diagnostic accuracy of single index tests, and studies that directly compared two or more index tests. Studies reporting at both the patient or tooth surface level were included. In vitro and in vivo studies were eligible for inclusion. Studies that explicitly recruited participants with more advanced lesions that were obviously into dentine or frankly cavitated were excluded. We also excluded studies that artificially created carious lesions and those that used an index test during the excavation of dental caries to ascertain the optimum depth of excavation. Data collection and analysis Two review authors extracted data independently and in duplicate using a standardised data extraction form and quality assessment based on QUADAS‐2 specific to the clinical context. Estimates of diagnostic accuracy were determined using the bivariate hierarchical method to produce summary points of sensitivity and specificity with 95% confidence regions. Comparative accuracy of different radiograph methods was conducted based on indirect and direct comparisons between methods. Potential sources of heterogeneity were pre‐specified and explored visually and more formally through meta‐regression. Main results We included 104 datasets from 77 studies reporting a total of 15,518 tooth sites or surfaces. The most frequently reported imaging methods were analogue radiographs (55 datasets from 51 studies) and digital radiographs (42 datasets from 40 studies) followed by cone beam computed tomography (CBCT) (7 datasets from 7 studies). Only 17 studies were of an in vivo study design, carried out in a clinical setting. No studies were considered to be at low risk of bias across all four domains but 16 studies were judged to have low concern for applicability across all domains. The patient selection domain had the largest number of studies judged to be at high risk of bias (43 studies); the index test, reference standard, and flow and timing domains were judged to be at high risk of bias in 30, 12, and 7 studies respectively. Studies were synthesised using a hierarchical bivariate method for meta‐analysis. There was substantial variability in the results of the individual studies, with sensitivities that ranged from 0 to 0.96 and specificities from 0 to 1.00. For all imaging methods the estimated summary sensitivity and specificity point was 0.47 (95% confidence interval (CI) 0.40 to 0.53) and 0.88 (95% CI 0.84 to 0.92), respectively. In a cohort of 1000 tooth surfaces with a prevalence of enamel caries of 63%, this would result in 337 tooth surfaces being classified as disease free when enamel caries was truly present (false negatives), and 43 tooth surfaces being classified as diseased in the absence of enamel caries (false positives). Meta‐regression indicated that measures of accuracy differed according to the imaging method (Chi 2 (4) = 32.44, P < 0.001), with the highest sensitivity observed for CBCT, and the highest specificity observed for analogue radiographs. None of the specified potential sources of heterogeneity were able to explain the variability in results. No studies included restored teeth in their sample or reported the inclusion of sealants. We rated the certainty of the evidence as low for sensitivity and specificity and downgraded two levels in total for risk of bias due to limitations in the design and conduct of the included studies, indirectness arising from the in vitro studies, and the observed inconsistency of the results. Authors' conclusions The design and conduct of studies to determine the diagnostic accuracy of methods to detect and diagnose caries in situ are particularly challenging. Low‐certainty evidence suggests that imaging for the detection or diagnosis of early caries may have poor sensitivity but acceptable specificity, resulting in a relatively high number of false‐negative results with the potential for early disease to progress. If left untreated, the opportunity to provide professional or self‐care practices to arrest or reverse early caries lesions will be missed. The specificity of lesion detection is however relatively high, and one could argue that initiation of non‐invasive management (such as the use of topical fluoride), is probably of low risk. CBCT showed superior sensitivity to analogue or digital radiographs but has very limited applicability to the general dental practitioner. However, given the high‐radiation dose, and potential for caries‐like artefacts from existing restorations, its use cannot be justified in routine caries detection. Nonetheless, if early incidental carious lesions are detected in CBCT scans taken for other purposes, these should be reported. CBCT has the potential to be used as a reference standard in diagnostic studies of this type. Despite the robust methodology applied in this comprehensive review, the results should be interpreted with some caution due to shortcomings in the design and execution of many of the included studies. Future research should evaluate the comparative accuracy of different methods, be undertaken in a clinical setting, and focus on minimising bias arising from the use of imperfect reference standards in clinical studies. Plain language summary Dental imaging methods for the detection of early tooth decay Why is it important to improve the detection of dental caries (tooth decay)?  Dentists often aim to identify tooth decay that has already advanced to a level which needs a filling. If dentists were able to find tooth decay when it has only affected the outer layer of the tooth (enamel) then it is possible to stop the decay from spreading any further and prevent the need for fillings. It is also important to avoid a false‐positive result, when treatment may be given when caries is absent. What is the aim of this review?  This Cochrane Review aimed to find out how accurate X‐ray images and other types of dental imaging are for detecting early tooth decay as part of the dental 'check‐up' for children and adults who visit their general dentist. Researchers in Cochrane included 77 studies published between 1986 and 2018 to answer this question. What was studied in the review?  Three main types of dental imaging were studied in this review: analogue or digital radiographs (X‐rays) and three‐dimensional (3D) imaging (cone beam computed tomography (CBCT)). We studied decay on the occlusal surfaces (biting surfaces of the back teeth), the proximal surfaces (tooth surfaces that are next to each other), and smooth surfaces. What are the main results of the review?  Researchers in Cochrane included 77 studies with a total of 15,518 tooth sites or surfaces, where typically 63% of tooth sites or surfaces had enamel caries. Some of these studies reported on more than one type of imaging, on both the permanent and primary ('milk') teeth or different tooth surfaces, and this gave us 104 sets of data to use. If these methods were to be used by a dentist for a routine dental examination, out of 1000 tooth sites or surfaces seen: • the use of these methods will indicate that 336 tooth sites or surfaces will have early tooth decay, and of these, 43 (13%) will have no disease (incorrect diagnosis ‐ false positive); • of the 664 tooth sites with a result indicating that early tooth decay is absent, 337 (51%) will have early tooth decay (incorrect diagnosis ‐ false negative). This high proportion of false‐negative results means that early signs of decay will be missed. We found evidence that 3D imaging methods were better than analogue or digital radiographs at identifying early disease but that analogue radiographs were better at identifying disease‐free tooth surfaces. Please see  oralhealth.cochrane.org/imaging-modalities-inform-detection-and-diagnosis-early-caries . How reliable are the results of the studies in this review?  We only included studies that assessed healthy teeth or those that were thought to have early tooth decay. This is because teeth with deep tooth decay would be easier to identify. However, there were some problems with how the studies were conducted. This may result in these methods appearing more accurate than they are, increasing the number of correct results. We judged the certainty of the evidence to be low due to how the studies selected their participants and the large number of studies that were carried out in a laboratory setting on extracted teeth, and the variation in the results. Who do the results of this review apply to?  Studies included in the review were carried out in South America, Europe, Asia, and the US. A large number of studies examined extracted teeth, while clinical studies were completed in dental hospitals or general dental practices. What are the implications of this review?  Low‐certainty evidence suggests that imaging for the detection or diagnosis of early tooth decay may result in a relatively high proportion of false‐negative results, with the potential for early disease to become more advanced. If left untreated, the opportunity to provide professional or self‐care practices to arrest or reverse early tooth decay will be missed. How up‐to‐date is this review?  The electronic searches retrieved used studies published up to 31 December 2018.","3","John Wiley & Sons, Ltd","1465-1858","*Cone-Beam Computed Tomography [statistics & numerical data]; *Datasets as Topic; Adult; Bias; Child; Dental Caries [*diagnostic imaging]; Dentition, Permanent; False Negative Reactions; False Positive Reactions; Humans; Radiography, Dental [*methods, statistics & numerical data]; Radiography, Dental, Digital [statistics & numerical data]; Reference Standards; Sensitivity and Specificity; Tooth, Deciduous","10.1002/14651858.CD014545","http://dx.doi.org/10.1002/14651858.CD014545","Oral Health"
"CD012553.PUB2","Jahanfar, S; Ho, JJ; Jaafar, SH; Abraha, I; Noura, M; Ross, CR; Pammi, M","Ultrasound for diagnosis of birth weight discordance in twin pregnancies","Cochrane Database of Systematic Reviews","2021","Abstract - Background There is a need to standardize monitoring in obstetric research of twin pregnancies. Identification of birth weight discordance (BWD), defined as a difference in the birth weights of twins, is a well‐documented phenomenon in twin pregnancies. Ultrasound for the diagnosis of BWD informs complex decision making including whether to intervene medically (via laser photo coagulation) or deliver the twins to avoid fetal morbidities or even death. The question is, how accurate is this measurement? Objectives To determine the diagnostic accuracy (sensitivity and specificity) of ultrasound estimated fetal weight discordance (EFWD) of 20% and 25% using different estimated biometric ultrasound measurements compared with the actual BWD as the reference standard in twin pregnancies. Search methods The search for this review was performed on 15 March 2019. We searched CENTRAL, MEDLINE (Ovid), Embase (Ovid), seven other databases, conference proceedings, reference lists and contacted experts. There were no language or date restrictions applied to the electronic searches, and no methodological filters to maximize sensitivity. Selection criteria We selected cohort‐type studies with delayed verification that evaluated the accuracy of biometric measurements at ultrasound scanning of twin pregnancies that had been proposed for the diagnosis of estimated BWD, compared to BWD measurements after birth as a reference standard. In addition, we only selected studies that considered twin pregnancies and applied a reference standard for EFWD for the target condition of BWD. Data collection and analysis We screened all titles generated by electronic database searches. Two review authors independently assessed the abstracts of all potentially relevant studies. We assessed the identified full papers for eligibility, and extracted data to create 2 × 2 tables. Two review authors independently performed quality assessment using the QUADAS‐2 tool. We excluded studies that did not report data in sufficient detail to construct 2 × 2 tables, and where this information was not available from the primary investigators. We assessed the certainty of the evidence using the GRADE approach. Main results We included 39 eligible studies with a median study sample size of 140. In terms of risk of bias, there were many unclear statements regarding patient selection, index test and use of proper reference standard. Twenty‐one studies (53%) were of methodological concern due to flow and timing. In terms of applicability, most studies were of low concern. Ultrasound for diagnosis of BWD in twin pregnancies at 20% cut‐off Twenty‐two studies provided data for a BWD of 20% and the summary estimate of sensitivity was 0.51 (95% CI 0.42 to 0.60), and the summary estimate of specificity was 0.91 (95% CI 0.89 to 0.93) (8005 twin pregnancies; very low‐certainty evidence). Ultrasound for diagnosis of BWD in twin pregnancies at 25% cut‐off Eighteen studies provided data using a BWD discordance of 25%. The summary estimate of sensitivity was 0.46 (95% CI 0.26 to 0.66), and the summary estimate of specificity was 0.93 (95% CI 0.89 to 0.96) (6471 twin pregnancies; very low‐certainty evidence). Subgroup analyses were possible for both BWD of 20% and 25%. The diagnostic accuracy did not differ substantially between estimation by abdominal circumference and femur length but femur length had a trend towards higher sensitivity and specificity. Subgroup analyses were not possible by sex of twins, chorionicity or gestational age due to insufficient data. Authors' conclusions Very low‐certainty evidence suggests that EFWD identified by ultrasound has low sensitivity but good specificity in detecting BWD in twin pregnancies. There is uncertain diagnostic value of EFWD; this review suggests there is insufficient evidence to support this index as the sole measure for clinical decision making to evaluate the prognosis of twins with growth discordance. The diagnostic accuracy of other measures including amniotic fluid index and umbilical artery Doppler resistive indices in combination with ultrasound for clinical intervention requires evaluation. Future well‐designed studies could also evaluate the impact of chorionicity, sex and gestational age in the diagnostic accuracy of ultrasound for EFWD. Plain language summary Ultrasound during pregnancy for predicting differences in birth weight between twins Background Birth weight differences of more than 20% in twins is associated with poor outcomes for the mother and baby. Clinicians measure the estimated fetal weight differences by ultrasound before birth and compare it to differences in birth weight after the babies are born. In this review, we summarized data on whether the ultrasound measurements are accurate enough to predict birth weight differences in twins. Study characteristics We searched medical databases to March 2019 for studies comparing ultrasound measurements to birth weight differences and we identified 39 studies. Twenty‐two studies provided data on birth weight differences of 20% and 18 studies provided data on birth weight differences of 25%. Quality of the evidence We assessed the quality of individual studies using a tool called ""Quality Assessment of Diagnostic Accuracy Studies"" (QUADAS‐2) and the overall quality by a recommended method called GRADE to find out the reliability of the evidence. Key results We found that ultrasound estimation of fetal weight differences compared to birth weight differences was not reliable. On average, ultrasound detected birth weight differences of 20% and 25% only half the time. The quality of evidence was very low. There is insufficient evidence to support the use of ultrasound as the sole measure for detecting birth weight differences in twins, or poor outcomes. The diagnostic accuracy of other measures including amniotic fluid volume (the fluid surrounding the babies in the womb) or Doppler studies (which use sound waves to detect the movement of blood in the babies' blood vessels and the umbilical cord) in combination with ultrasound to inform clinical decisions needs to be evaluated. Future well‐designed studies could also research the impact of whether the babies share a placenta (or not), the sex of the babies, and gestational age (time from woman's last menstrual period), in the diagnostic accuracy of ultrasound for estimated birth weight differences.","3","John Wiley & Sons, Ltd","1465-1858","*Birth Weight; *Pregnancy, Twin; Bias; Cohort Studies; Female; Femur [anatomy & histology, diagnostic imaging]; Humans; Pregnancy; Reference Standards; Reproducibility of Results; Sensitivity and Specificity; Ultrasonography, Prenatal [*methods]; Waist Circumference","10.1002/14651858.CD012553.pub2","http://dx.doi.org/10.1002/14651858.CD012553.pub2","Pregnancy and Childbirth"
"CD009593.PUB5","Zifodya, JS; Kreniske, JS; Schiller, I; Kohli, M; Dendukuri, N; Schumacher, SG; Ochodo, EA; Haraka, F; Zwerling, AA; Pai, M; Steingart, KR; Horne, DJ","Xpert Ultra versus Xpert MTB/RIF for pulmonary tuberculosis and rifampicin resistance in adults with presumptive pulmonary tuberculosis","Cochrane Database of Systematic Reviews","2021","Abstract - Background Xpert MTB/RIF and Xpert MTB/RIF Ultra (Xpert Ultra) are World Health Organization (WHO)‐recommended rapid tests that simultaneously detect tuberculosis and rifampicin resistance in people with signs and symptoms of tuberculosis. This review builds on our recent extensive Cochrane Review of Xpert MTB/RIF accuracy. Objectives To compare the diagnostic accuracy of Xpert Ultra and Xpert MTB/RIF for the detection of pulmonary tuberculosis and detection of rifampicin resistance in adults with presumptive pulmonary tuberculosis. For pulmonary tuberculosis and rifampicin resistance, we also investigated potential sources of heterogeneity. We also summarized the frequency of Xpert Ultra trace‐positive results, and estimated the accuracy of Xpert Ultra after repeat testing in those with trace‐positive results. Search methods We searched the Cochrane Infectious Diseases Group Specialized Register, MEDLINE, Embase, Science Citation Index, Web of Science, LILACS, Scopus, the WHO ICTRP, the ISRCTN registry, and ProQuest to 28 January 2020 with no language restriction. Selection criteria We included diagnostic accuracy studies using respiratory specimens in adults with presumptive pulmonary tuberculosis that directly compared the index tests. For pulmonary tuberculosis detection, the reference standards were culture and a composite reference standard. For rifampicin resistance, the reference standards were culture‐based drug susceptibility testing and line probe assays. Data collection and analysis Two review authors independently extracted data using a standardized form, including data by smear and HIV status. We assessed risk of bias using QUADAS‐2 and QUADAS‐C. We performed meta‐analyses comparing pooled sensitivities and specificities, separately for pulmonary tuberculosis detection and rifampicin resistance detection, and separately by reference standard. Most analyses used a bivariate random‐effects model. For tuberculosis detection, we estimated accuracy in studies in participants who were not selected based on prior microscopy testing or history of tuberculosis. We performed subgroup analyses by smear status, HIV status, and history of tuberculosis. We summarized Xpert Ultra trace results. Main results We identified nine studies (3500 participants): seven had unselected participants (2834 participants). All compared Xpert Ultra and Xpert MTB/RIF for pulmonary tuberculosis detection; seven studies used a paired comparative accuracy design, and two studies used a randomized design. Five studies compared Xpert Ultra and Xpert MTB/RIF for rifampicin resistance detection; four studies used a paired design, and one study used a randomized design. Of the nine included studies, seven (78%) were mainly or exclusively in high tuberculosis burden countries. For pulmonary tuberculosis detection, most studies had low risk of bias in all domains. Pulmonary tuberculosis detection Xpert Ultra pooled sensitivity and specificity (95% credible interval) against culture were 90.9% (86.2 to 94.7) and 95.6% (93.0 to 97.4) (7 studies, 2834 participants; high‐certainty evidence) versus Xpert MTB/RIF pooled sensitivity and specificity of 84.7% (78.6 to 89.9) and 98.4% (97.0 to 99.3) (7 studies, 2835 participants; high‐certainty evidence). The difference in the accuracy of Xpert Ultra minus Xpert MTB/RIF was estimated at 6.3% (0.1 to 12.8) for sensitivity and −2.7% (−5.7 to −0.5) for specificity. If the point estimates for Xpert Ultra and Xpert MTB/RIF are applied to a hypothetical cohort of 1000 patients, where 10% of those presenting with symptoms have pulmonary tuberculosis, Xpert Ultra will miss 9 cases, and Xpert MTB/RIF will miss 15 cases. The number of people wrongly diagnosed with pulmonary tuberculosis would be 40 with Xpert Ultra and 14 with Xpert MTB/RIF. In smear‐negative, culture‐positive participants, pooled sensitivity was 77.5% (67.6 to 85.6) for Xpert Ultra versus 60.6% (48.4 to 71.7) for Xpert MTB/RIF; pooled specificity was 95.8% (92.9 to 97.7) for Xpert Ultra versus 98.8% (97.7 to 99.5) for Xpert MTB/RIF (6 studies). In people living with HIV, pooled sensitivity was 87.6% (75.4 to 94.1) for Xpert Ultra versus 74.9% (58.7 to 86.2) for Xpert MTB/RIF; pooled specificity was 92.8% (82.3 to 97.0) for Xpert Ultra versus 99.7% (98.6 to 100.0) for Xpert MTB/RIF (3 studies). In participants with a history of tuberculosis, pooled sensitivity was 84.2% (72.5 to 91.7) for Xpert Ultra versus 81.8% (68.7 to 90.0) for Xpert MTB/RIF; pooled specificity was 88.2% (70.5 to 96.6) for Xpert Ultra versus 97.4% (91.7 to 99.5) for Xpert MTB/RIF (4 studies). The proportion of Ultra trace‐positive results ranged from 3.0% to 30.4%. Data were insufficient to estimate the accuracy of Xpert Ultra repeat testing in individuals with initial trace‐positive results. Rifampicin resistance detection Pooled sensitivity and specificity were 94.9% (88.9 to 97.9) and 99.1% (97.7 to 99.8) (5 studies, 921 participants; high‐certainty evidence) for Xpert Ultra versus 95.3% (90.0 to 98.1) and 98.8% (97.2 to 99.6) (5 studies, 930 participants; high‐certainty evidence) for Xpert MTB/RIF. The difference in the accuracy of Xpert Ultra minus Xpert MTB/RIF was estimated at −0.3% (−6.9 to 5.7) for sensitivity and 0.3% (−1.2 to 2.0) for specificity. If the point estimates for Xpert Ultra and Xpert MTB/RIF are applied to a hypothetical cohort of 1000 patients, where 10% of those presenting with symptoms have rifampicin resistance, Xpert Ultra will miss 5 cases, and Xpert MTB/RIF will miss 5 cases. The number of people wrongly diagnosed with rifampicin resistance would be 8 with Xpert Ultra and 11 with Xpert MTB/RIF. We identified a higher number of rifampicin resistance indeterminate results with Xpert Ultra, pooled proportion 7.6% (2.4 to 21.0) compared to Xpert MTB/RIF pooled proportion 0.8% (0.2 to 2.4). The estimated difference in the pooled proportion of indeterminate rifampicin resistance results for Xpert Ultra versus Xpert MTB/RIF was 6.7% (1.4 to 20.1). Authors' conclusions Xpert Ultra has higher sensitivity and lower specificity than Xpert MTB/RIF for pulmonary tuberculosis, especially in smear‐negative participants and people living with HIV. Xpert Ultra specificity was lower than that of Xpert MTB/RIF in participants with a history of tuberculosis. The sensitivity and specificity trade‐off would be expected to vary by setting. For detection of rifampicin resistance, Xpert Ultra and Xpert MTB/RIF had similar sensitivity and specificity. Ultra trace‐positive results were common. Xpert Ultra and Xpert MTB/RIF provide accurate results and can allow rapid initiation of treatment for rifampicin‐resistant and multidrug‐resistant tuberculosis. Plain language summary Xpert Ultra compared to Xpert MTB/RIF for diagnosing pulmonary tuberculosis and rifampicin resistance in adults Why is improving the diagnosis of pulmonary tuberculosis important? Tuberculosis is one of the leading causes of death worldwide. While tuberculosis is largely curable when detected early and effectively treated, around 1.2 million people died of tuberculosis in 2019. Xpert MTB/RIF and Xpert Ultra (the newest version) are World Health Organization‐recommended rapid tests that simultaneously detect tuberculosis and rifampicin resistance in people with tuberculosis symptoms. Rifampicin is an important antituberculosis drug. Not recognizing tuberculosis when it is present (false negative) may result in severe illness and death, and an increased risk of infecting others. An incorrect diagnosis of tuberculosis (false positive) may result in anxiety, additional testing, unnecessary treatment, and medication side effects. What is the aim of this review? To determine how accurate Xpert Ultra is compared with Xpert MTB/RIF for diagnosing pulmonary tuberculosis and rifampicin resistance in adults. An extensive review of Xpert MTB/RIF accuracy was recently published as a Cochrane Review. What was studied in this review? We compared the diagnostic accuracy of Xpert Ultra and Xpert MTB/RIF with results primarily measured against culture (detection of pulmonary tuberculosis) and drug susceptibility testing and line probe assays (detection of rifampicin resistance). What are the main results in this review? Nine studies (3500 participants) compared Xpert Ultra to Xpert MTB/RIF for diagnosing pulmonary tuberculosis, and five studies (930 participants) compared Xpert Ultra to Xpert MTB/RIF for rifampicin resistance. How confident are we in the results of this review? Confident. The review included sufficient studies and participants and used optimum reference standards. In the comparison between Xpert Ultra and Xpert MTB/RIF, most studies were at low risk of bias. Who do the results of this review apply to? People considered to have pulmonary tuberculosis. What are the implications of this review? The results of these studies indicate that, in theory, for a population of 1000 people where 100 of those presenting with symptoms have pulmonary tuberculosis, Xpert Ultra will miss 9 cases, and Xpert MTB/RIF will miss 15 cases. The number of people wrongly diagnosed with pulmonary tuberculosis would be 40 with Xpert Ultra, and 14 with Xpert MTB/RIF. The results of these studies indicate that, in theory, for a population of 1000 people where 100 of those have rifampicin resistance, Xpert Ultra will miss 5 cases, and Xpert MTB/RIF will miss 5 cases. The number of people wrongly diagnosed with rifampicin resistance would be 8 with Xpert Ultra, and 11 with Xpert MTB/RIF. How up‐to‐date is this review? 28 January 2020.","2","John Wiley & Sons, Ltd","1465-1858","*Antibiotics, Antitubercular [pharmacology]; *Drug Resistance, Bacterial; *Mycobacterium tuberculosis [drug effects]; *Rifampin [pharmacology]; *Tuberculosis, Pulmonary [diagnosis, drug therapy]; Diagnostic Errors; Extensively Drug-Resistant Tuberculosis [diagnosis, drug therapy]; False Negative Reactions; False Positive Reactions; Humans; Microbial Sensitivity Tests; Sensitivity and Specificity","10.1002/14651858.CD009593.pub5","http://dx.doi.org/10.1002/14651858.CD009593.pub5","Infectious Diseases"
"CD010945.PUB2","Kokkinou, M; Beishon, LC; Smailagic, N; Noel-Storr, AH; Hyde, C; Ukoumunne, O; Worrall, RE; Hayen, A; Desai, M; Ashok, AH; Paul, EJ; Georgopoulou, A; Casoli, T; Quinn, TJ; Ritchie, CW","Plasma and cerebrospinal fluid ABeta42 for the differential diagnosis of Alzheimer's disease dementia in participants diagnosed with any dementia subtype in a specialist care setting","Cochrane Database of Systematic Reviews","2021","Abstract - Background Dementia is a syndrome that comprises many differing pathologies, including Alzheimer's disease dementia (ADD), vascular dementia (VaD) and frontotemporal dementia (FTD). People may benefit from knowing the type of dementia they live with, as this could inform prognosis and may allow for tailored treatment. Beta‐amyloid (1‐42) (ABeta42) is a protein which decreases in both the plasma and cerebrospinal fluid (CSF) of people living with ADD, when compared to people with no dementia. However, it is not clear if changes in ABeta42 are specific to ADD or if they are also seen in other types of dementia. It is possible that ABeta42 could help differentiate ADD from other dementia subtypes. Objectives To determine the accuracy of plasma and CSF ABeta42 for distinguishing ADD from other dementia subtypes in people who meet the criteria for a dementia syndrome. Search methods We searched MEDLINE, and nine other databases up to 18 February 2020. We checked reference lists of any relevant systematic reviews to identify additional studies. Selection criteria We considered cross‐sectional studies that differentiated people with ADD from other dementia subtypes. Eligible studies required measurement of participant plasma or CSF ABeta42 levels and clinical assessment for dementia subtype. Data collection and analysis Seven review authors working independently screened the titles and abstracts generated by the searches. We collected data on study characteristics and test accuracy. We used the second version of the 'Quality Assessment of Diagnostic Accuracy Studies' (QUADAS‐2) tool to assess internal and external validity of results. We extracted data into 2 x 2 tables, cross‐tabulating index test results (ABeta42) with the reference standard (diagnostic criteria for each dementia subtype). We performed meta‐analyses using bivariate, random‐effects models. We calculated pooled estimates of sensitivity, specificity, positive predictive values, positive and negative likelihood ratios, and corresponding 95% confidence intervals (CIs). In the primary analysis, we assessed accuracy of plasma or CSF ABeta42 for distinguishing ADD from other mixed dementia types (non‐ADD). We then assessed accuracy of ABeta42 for differentiating ADD from specific dementia types: VaD, FTD, dementia with Lewy bodies (DLB), alcohol‐related cognitive disorder (ARCD), Creutzfeldt‐Jakob disease (CJD) and normal pressure hydrocephalus (NPH). To determine test‐positive cases, we used the ABeta42 thresholds employed in the respective primary studies. We then performed sensitivity analyses restricted to those studies that used common thresholds for ABeta42. Main results We identified 39 studies (5000 participants) that used CSF ABeta42 levels to differentiate ADD from other subtypes of dementia. No studies of plasma ABeta42 met the inclusion criteria. No studies were rated as low risk of bias across all QUADAS‐2 domains. High risk of bias was found predominantly in the domains of patient selection (28 studies) and index test (25 studies). The pooled estimates for differentiating ADD from other dementia subtypes were as follows: ADD from non‐ADD: sensitivity 79% (95% CI 0.73 to 0.85), specificity 60% (95% CI 0.52 to 0.67), 13 studies, 1704 participants, 880 participants with ADD; ADD from VaD: sensitivity 79% (95% CI 0.75 to 0.83), specificity 69% (95% CI 0.55 to 0.81), 11 studies, 1151 participants, 941 participants with ADD; ADD from FTD: sensitivity 85% (95% CI 0.79 to 0.89), specificity 72% (95% CI 0.55 to 0.84), 17 studies, 1948 participants, 1371 participants with ADD; ADD from DLB: sensitivity 76% (95% CI 0.69 to 0.82), specificity 67% (95% CI 0.52 to 0.79), nine studies, 1929 participants, 1521 participants with ADD. Across all dementia subtypes, sensitivity was greater than specificity, and the balance of sensitivity and specificity was dependent on the threshold used to define test positivity. Authors' conclusions Our review indicates that measuring ABeta42 levels in CSF may help differentiate ADD from other dementia subtypes, but the test is imperfect and tends to misdiagnose those with non‐ADD as having ADD. We would caution against the use of CSF ABeta42 alone for dementia classification. However, ABeta42 may have value as an adjunct to a full clinical assessment, to aid dementia diagnosis. Plain language summary How accurate is the ABeta42 test for distinguishing Alzheimer's disease from other types of dementia in patients seen in a specialist clinic? Why is improving dementia diagnosis important? Dementia is a condition characterised by progressive problems with memory and thinking. Dementia can be caused be a number of different conditions (for example, by Alzheimer's disease), and the best treatments depend on the underlying cause. Levels of the protein ABeta42 in blood or spinal fluid may determine the underlying cause of dementia. This could help clinicians choose the best treatments. What is the aim of this review? The aim of this review was to find out how accurate are the levels of ABeta42 in blood or spinal fluid for determining the cause of dementia. What was studied in the review? We included studies that examined the levels of ABeta42 taken from samples of blood or spinal fluid. At present, this test is only used in specialist clinics. Levels of ABeta42 may be lower in persons with Alzheimer's dementia compared to those with other types of dementia. What are the main results of this review? We included 39 studies with a total of 5000 participants. All studies used spinal fluid tests of ABeta42. None of the included studies used a blood test of ABeta42. In theory, the results of these studies indicate that if ABeta42 were to be used in a specialist clinic in a group of 1000 people, where 520 (52%) have Alzheimer's dementia, an estimated 602 would have an ABeta42 result. This would indicate that Alzheimer's dementia is present. Of these, 192 (32%) would be incorrectly classified as having Alzheimer's disease. Of the 398 people with a result indicating that Alzheimer's disease is not present, 110 (28%) would be incorrectly classified as not having Alzheimer's disease. The included studies used different levels of ABeta42 to make the diagnosis of Alzheimer's disease, and the accuracy of the test depended on the level of ABeta42 used. How reliable are the results of the studies in this review? In most of the included studies, the diagnosis of Alzheimer's dementia was made by assessing all participants with standard diagnostic criteria.This is likely to have been a reliable method for deciding whether patients really had Alzheimer's disease. However, there were some problems with how the studies were conducted. This may result in ABeta42 appearing more accurate than it really is. To whom do the results of this review apply? The results apply to patients undergoing dementia assessment in a specialist setting. What are the implications of this review? Measuring levels of ABeta42 in spinal fluid may help distinguish Alzheimer’s disease from other types of dementia, but the test is not perfect. ABeta42 is unlikely to be used in isolation for making a diagnosis, and may have greatest value when used in addition to the other assessments and tests that are undertaken to make a diagnosis of dementia. How up‐to‐date is the review? The review authors searched for and included studies published up to February 2020.","2","John Wiley & Sons, Ltd","1465-1858","Alcoholism [complications]; Alzheimer Disease [blood, cerebrospinal fluid, *diagnosis]; Amyloid beta-Peptides [*blood, *cerebrospinal fluid]; Bias; Biomarkers [blood, cerebrospinal fluid]; Cognitive Dysfunction [blood, cerebrospinal fluid, diagnosis, etiology]; Confidence Intervals; Creutzfeldt-Jakob Syndrome [blood, cerebrospinal fluid, diagnosis]; Dementia, Vascular [blood, cerebrospinal fluid, diagnosis]; Diagnosis, Differential; Frontotemporal Dementia [blood, cerebrospinal fluid, diagnosis]; Humans; Hydrocephalus, Normal Pressure [blood, cerebrospinal fluid, diagnosis]; Lewy Body Disease [blood, cerebrospinal fluid, diagnosis]; Likelihood Functions; Peptide Fragments [*blood, *cerebrospinal fluid]; Sensitivity and Specificity","10.1002/14651858.CD010945.pub2","http://dx.doi.org/10.1002/14651858.CD010945.pub2","Dementia and Cognitive Improvement"
"CD013855","Macey, R; Walsh, T; Riley, P; Hogan, R; Glenny, A-M; Worthington, HV; Clarkson, JE; Ricketts, D","Transillumination and optical coherence tomography for the detection and diagnosis of enamel caries","Cochrane Database of Systematic Reviews","2021","Abstract - Background Caries is one of the most prevalent and preventable conditions worldwide. If identified early enough then non‐invasive techniques can be applied, and therefore this review focusses on early caries involving the enamel surface of the tooth. The cornerstone of caries detection and diagnosis is a visual and tactile dental examination, although alternative approaches are available. These include illumination‐based devices that could potentially support the dental examination. There are three categories of illumination devices that exploit various methods of application and interpretation, each primarily defined by different wavelengths, optical coherence tomography (OCT), near‐infrared (NIR), and fibre‐optic technology, which incorporates more recently developed digital fibre optics (FOTI/DIFOTI). Objectives To estimate the diagnostic test accuracy of different illumination tests for the detection and diagnosis of enamel caries in children or adults. We also planned to explore the following potential sources of heterogeneity: in vitro or in vivo studies with different reference standards; tooth surface (occlusal, proximal, smooth surface, or adjacent to a restoration); single or multiple sites of assessment on a tooth surface; and the prevalence of caries into dentine. Search methods Cochrane Oral Health's Information Specialist undertook a search of the following databases: MEDLINE Ovid (1946 to 15 February 2019); Embase Ovid (1980 to 15 February 2019); US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov, to 15 February 2019); and the World Health Organization International Clinical Trials Registry Platform (to 15 February 2019). We studied reference lists as well as published systematic review articles. Selection criteria We included diagnostic accuracy study designs that compared the use of illumination‐based devices with a reference standard (histology, enhanced visual examination with or without radiographs, or operative excavation). These included prospective studies that evaluated the diagnostic accuracy of a single index test and studies that directly compared two or more index tests. Both in vitro and in vivo studies of primary and permanent teeth were eligible for inclusion. We excluded studies that explicitly recruited participants with caries into dentine or frank cavitation. We also excluded studies that artificially created carious lesions and those that used an index test during the excavation of dental caries to ascertain the optimum depth of excavation. Data collection and analysis Two review authors extracted data independently and in duplicate using a standardised data extraction form and quality assessment based on QUADAS‐2 specific to the clinical context. Estimates of diagnostic accuracy were determined using the bivariate hierarchical method to produce summary points of sensitivity and specificity with 95% confidence regions. The comparative accuracy of different illumination devices was conducted based on indirect and direct comparisons between methods. Potential sources of heterogeneity were pre‐specified and explored visually and more formally through meta‐regression. Main results We included 24 datasets from 23 studies that evaluated 16,702 tooth surfaces. NIR was evaluated in 6 datasets (673 tooth surfaces), OCT in 10 datasets (1171 tooth surfaces), and FOTI/DIFOTI in 8 datasets (14,858 tooth surfaces). The participant selection domain had the largest number of studies judged at high risk of bias (16 studies). Conversely, for the index test, reference standard, and flow and timing domains the majority of studies were judged to be at low risk of bias (16, 12, and 16 studies respectively). Concerns regarding the applicability of the evidence were judged as high or unclear for all domains. Notably, 14 studies were judged to be of high concern for participant selection, due to selective participant recruitment, a lack of independent examiners, and the use of an in vitro study design. The summary estimate across all the included illumination devices was sensitivity 0.75 (95% confidence interval (CI) 0.62 to 0.85) and specificity 0.87 (95% CI 0.82 to 0.92), with a diagnostic odds ratio of 21.52 (95% CI 10.89 to 42.48). In a cohort of 1000 tooth surfaces with a prevalence of enamel caries of 57%, this would result in 142 tooth surfaces being classified as disease free when enamel caries was truly present (false negatives), and 56 tooth surfaces being classified as diseased in the absence of enamel caries (false positives). A formal comparison of the accuracy according to device type indicated a difference in sensitivity and/or specificity (Chi 2 (4) = 34.17, P < 0.01). Further analysis indicated a difference in the sensitivity of the different devices (Chi 2 (2) = 31.24, P < 0.01) with a higher sensitivity of 0.94 (95% CI 0.88 to 0.97) for OCT compared to NIR 0.58 (95% CI 0.46 to 0.68) and FOTI/DIFOTI 0.47 (95% CI 0.35 to 0.59), but no meaningful difference in specificity (Chi 2 (2) = 3.47, P = 0.18). In light of these results, we planned to formally assess potential sources of heterogeneity according to device type, but due to the limited number of studies for each device type we were unable to do so. For interpretation, we presented the coupled forest plots for each device type according to the potential source of heterogeneity. We rated the certainty of the evidence as low and downgraded two levels in total due to avoidable and unavoidable study limitations in the design and conduct of studies, indirectness arising from the in vitro studies, and imprecision of the estimates. Authors' conclusions Of the devices evaluated, OCT appears to show the most potential, with superior sensitivity to NIR and fibre‐optic devices. Its benefit lies as an add‐on tool to support the conventional oral examination to confirm borderline cases in cases of clinical uncertainty. OCT is not currently available to the general dental practitioner, and so further research and development are necessary. FOTI and NIR are more readily available and easy to use; however, they show limitations in their ability to detect enamel caries but may be considered successful in the identification of sound teeth. Future studies should strive to avoid research waste by ensuring that recruitment is conducted in such a way as to minimise selection bias and that studies are clearly and comprehensively reported. In terms of applicability, any future studies should be undertaken in a clinical setting that is reflective of the complexities encountered in caries assessment within the oral cavity. Plain language summary Light‐based tests for the detection and diagnosis of early tooth decay Why is it important to improve dental caries (tooth decay) detection? Dentists often aim to identify tooth decay that has already advanced to a level which needs a filling. If dentists were able to find tooth decay when it has only affected the outer layer of the tooth (enamel) then it is possible to stop the decay from spreading any further and prevent the need for fillings. It is also important to avoid a false‐positive result, when treatment may be provided when caries is absent. What is the aim of this review? This Cochrane Review aimed to find out how accurate different forms of light‐based tests are for detecting early tooth decay in patients who regularly visit their dentist. Researchers in Cochrane included 23 studies published between 1988 and 2019 to answer this question. What was studied in the review? We included three different types of light‐based devices in this review: optical coherence tomography (OCT), near‐infrared (NIR), and fibre‐optic (FOTI/DIFOTI) technology. All devices rely on shining different types of light on the tooth and can improve the dentist's ability to identify tooth decay. What are the main results of the review? The review included 23 studies with a total of 16,702 tooth surfaces. The results of these studies indicate that if the illumination devices were used by a dentist for a routine dental examination of 1000 tooth surfaces, of which 570 (57%) have early tooth decay: • an estimated 484 would be found to have tooth decay using one of the illumination detection methods, and of these 56 (12%) would not have tooth decay (false positive ‐ incorrect diagnosis); • of the 516 tooth surfaces in which a device indicated that tooth decay is not present, 142 (28%) tooth surfaces will truly have early tooth decay (false negative ‐ incorrect diagnosis). Please see  oralhealth.cochrane.org/transillumination-and-optical-coherence-tomography-detection-and-diagnosis-enamel-caries-results . In this example illumination devices produce a relatively high proportion of false‐negative results, whereby patients do not receive treatment for early tooth decay, for example, high fluoride toothpaste or oral health advice and guidance from the dentist, as they should. Of the data collected from three types of illumination devices, it seems that the OCT device is more sensitive (produces fewer false‐negative results) than NIR or fibre‐optic technology. How reliable are the results of the studies in this review? We only included studies that assessed healthy teeth or those that were thought to have early tooth decay, as teeth with deep tooth decay would be easier to identify. There were some shortcomings in how the studies were conducted, and this may have resulted in the illumination devices appearing more accurate than they really are, increasing the number of correct classifications (green rectangles in the diagram). Many studies evaluated the performance of the devices on extracted teeth, which is very different from when the devices are used inside a person's mouth, where is difficult to see clearly and where teeth may be stained or have a covering of plaque. Who do the results of this review apply to? Studies included in the review were carried out in the United States, Europe, Japan, Brazil, China, Malaysia, and Australia. Most studies were completed in dental hospitals, general dental practices, or schools. What are the implications of this review? Optical coherence tomography (OCT) shows potential as a device to detect early/enamel caries but more high‐quality research and development are required as OCT is not currently available to general dental practitioners. The analysis suggests that OCT is superior to NIR and fibre‐optic technologies. How up‐to‐date is this review? The review authors searched for and used studies published up to 15 February 2019.","1","John Wiley & Sons, Ltd","1465-1858","*Fiber Optic Technology; *Spectroscopy, Near-Infrared; *Tomography, Optical Coherence; Datasets as Topic; Dental Caries [*diagnosis]; Dental Enamel; False Negative Reactions; False Positive Reactions; Humans; Reference Standards; Selection Bias; Sensitivity and Specificity; Transillumination [*methods]","10.1002/14651858.CD013855","http://dx.doi.org/10.1002/14651858.CD013855","Oral Health"
"CD012768.PUB3","Kohli, M; Schiller, I; Dendukuri, N; Yao, M; Dheda, K; Denkinger, CM; Schumacher, SG; Steingart, KR","Xpert MTB/RIF Ultra and Xpert MTB/RIF assays for extrapulmonary tuberculosis and rifampicin resistance in adults","Cochrane Database of Systematic Reviews","2021","Abstract - Background Xpert MTB/RIF Ultra (Xpert Ultra) and Xpert MTB/RIF are World Health Organization (WHO)‐recommended rapid nucleic acid amplification tests (NAATs) widely used for simultaneous detection of  Mycobacterium tuberculosis  complex and rifampicin resistance in sputum. To extend our previous review on extrapulmonary tuberculosis (Kohli 2018), we performed this update to inform updated WHO policy (WHO Consolidated Guidelines (Module 3) 2020). Objectives To estimate diagnostic accuracy of Xpert Ultra and Xpert MTB/RIF for extrapulmonary tuberculosis and rifampicin resistance in adults with presumptive extrapulmonary tuberculosis. Search methods Cochrane Infectious Diseases Group Specialized Register, MEDLINE, Embase, Science Citation Index, Web of Science, Latin American Caribbean Health Sciences Literature, Scopus, ClinicalTrials.gov, the WHO International Clinical Trials Registry Platform, the International Standard Randomized Controlled Trial Number Registry, and ProQuest, 2 August 2019 and 28 January 2020 (Xpert Ultra studies), without language restriction. Selection criteria Cross‐sectional and cohort studies using non‐respiratory specimens. Forms of extrapulmonary tuberculosis: tuberculous meningitis and pleural, lymph node, bone or joint, genitourinary, peritoneal, pericardial, disseminated tuberculosis. Reference standards were culture and a study‐defined composite reference standard (tuberculosis detection); phenotypic drug susceptibility testing and line probe assays (rifampicin resistance detection). Data collection and analysis Two review authors independently extracted data and assessed risk of bias and applicability using QUADAS‐2. For tuberculosis detection, we performed separate analyses by specimen type and reference standard using the bivariate model to estimate pooled sensitivity and specificity with 95% credible intervals (CrIs). We applied a latent class meta‐analysis model to three forms of extrapulmonary tuberculosis. We assessed certainty of evidence using GRADE. Main results 69 studies: 67 evaluated Xpert MTB/RIF and 11 evaluated Xpert Ultra, of which nine evaluated both tests. Most studies were conducted in China, India, South Africa, and Uganda. Overall, risk of bias was low for patient selection, index test, and flow and timing domains, and low (49%) or unclear (43%) for the reference standard domain. Applicability for the patient selection domain was unclear for most studies because we were unsure of the clinical settings. Cerebrospinal fluid Xpert Ultra (6 studies) Xpert Ultra pooled sensitivity and specificity (95% CrI) against culture were 89.4% (79.1 to 95.6) (89 participants; low‐certainty evidence) and 91.2% (83.2 to 95.7) (386 participants; moderate‐certainty evidence). Of 1000 people where 100 have tuberculous meningitis, 168 would be Xpert Ultra‐positive: of these, 79 (47%) would not have tuberculosis (false‐positives) and 832 would be Xpert Ultra‐negative: of these, 11 (1%) would have tuberculosis (false‐negatives). Xpert MTB/RIF (30 studies) Xpert MTB/RIF pooled sensitivity and specificity against culture were 71.1% (62.8 to 79.1) (571 participants; moderate‐certainty evidence) and 96.9% (95.4 to 98.0) (2824 participants; high‐certainty evidence). Of 1000 people where 100 have tuberculous meningitis, 99 would be Xpert MTB/RIF‐positive: of these, 28 (28%) would not have tuberculosis; and 901 would be Xpert MTB/RIF‐negative: of these, 29 (3%) would have tuberculosis. Pleural fluid Xpert Ultra (4 studies) Xpert Ultra pooled sensitivity and specificity against culture were 75.0% (58.0 to 86.4) (158 participants; very low‐certainty evidence) and 87.0% (63.1 to 97.9) (240 participants; very low‐certainty evidence). Of 1000 people where 100 have pleural tuberculosis, 192 would be Xpert Ultra‐positive: of these, 117 (61%) would not have tuberculosis; and 808 would be Xpert Ultra‐negative: of these, 25 (3%) would have tuberculosis. Xpert MTB/RIF (25 studies) Xpert MTB/RIF pooled sensitivity and specificity against culture were 49.5% (39.8 to 59.9) (644 participants; low‐certainty evidence) and 98.9% (97.6 to 99.7) (2421 participants; high‐certainty evidence). Of 1000 people where 100 have pleural tuberculosis, 60 would be Xpert MTB/RIF‐positive: of these, 10 (17%) would not have tuberculosis; and 940 would be Xpert MTB/RIF‐negative: of these, 50 (5%) would have tuberculosis. Lymph node aspirate Xpert Ultra (1 study) Xpert Ultra sensitivity and specificity (95% confidence interval) against composite reference standard were 70% (51 to 85) (30 participants; very low‐certainty evidence) and 100% (92 to 100) (43 participants; low‐certainty evidence). Of 1000 people where 100 have lymph node tuberculosis, 70 would be Xpert Ultra‐positive and 0 (0%) would not have tuberculosis; 930 would be Xpert Ultra‐negative and 30 (3%) would have tuberculosis. Xpert MTB/RIF (4 studies) Xpert MTB/RIF pooled sensitivity and specificity against composite reference standard were 81.6% (61.9 to 93.3) (377 participants; low‐certainty evidence) and 96.4% (91.3 to 98.6) (302 participants; low‐certainty evidence). Of 1000 people where 100 have lymph node tuberculosis, 118 would be Xpert MTB/RIF‐positive and 37 (31%) would not have tuberculosis; 882 would be Xpert MTB/RIF‐negative and 19 (2%) would have tuberculosis. In lymph node aspirate, Xpert MTB/RIF pooled specificity against culture was 86.2% (78.0 to 92.3), lower than that against a composite reference standard. Using the latent class model, Xpert MTB/RIF pooled specificity was 99.5% (99.1 to 99.7), similar to that observed with a composite reference standard. Rifampicin resistance Xpert Ultra (4 studies) Xpert Ultra pooled sensitivity and specificity were 100.0% (95.1 to 100.0), (24 participants; low‐certainty evidence) and 100.0% (99.0 to 100.0) (105 participants; moderate‐certainty evidence). Of 1000 people where 100 have rifampicin resistance, 100 would be Xpert Ultra‐positive (resistant): of these, zero (0%) would not have rifampicin resistance; and 900 would be Xpert Ultra‐negative (susceptible): of these, zero (0%) would have rifampicin resistance. Xpert MTB/RIF (19 studies) Xpert MTB/RIF pooled sensitivity and specificity were 96.5% (91.9 to 98.8) (148 participants; high‐certainty evidence) and 99.1% (98.0 to 99.7) (822 participants; high‐certainty evidence). Of 1000 people where 100 have rifampicin resistance, 105 would be Xpert MTB/RIF‐positive (resistant): of these, 8 (8%) would not have rifampicin resistance; and 895 would be Xpert MTB/RIF‐negative (susceptible): of these, 3 (0.3%) would have rifampicin resistance. Authors' conclusions Xpert Ultra and Xpert MTB/RIF may be helpful in diagnosing extrapulmonary tuberculosis. Sensitivity varies across different extrapulmonary specimens: while for most specimens specificity is high, the tests rarely yield a positive result for people without tuberculosis. For tuberculous meningitis, Xpert Ultra had higher sensitivity and lower specificity than Xpert MTB/RIF against culture. Xpert Ultra and Xpert MTB/RIF had similar sensitivity and specificity for rifampicin resistance. Future research should acknowledge the concern associated with culture as a reference standard in paucibacillary specimens and consider ways to address this limitation. Plain language summary How accurate are tests (Xpert Ultra and Xpert MTB/RIF) for diagnosing tuberculosis outside the lungs (extrapulmonary tuberculosis) and rifampicin resistance? Why is using Xpert tests for extrapulmonary tuberculosis important? Tuberculosis is one of the top 10 causes of death worldwide. Tuberculosis mainly affects the lungs (pulmonary) but may occur in other parts of the body (extrapulmonary). When people receive proper and timely treatment, tuberculosis is usually curable. One problem involved in managing tuberculosis is that the bacteria become resistant to antibiotics. Not recognizing tuberculosis early may result in delayed diagnosis and treatment and increased illness and death. An incorrect tuberculosis diagnosis may result in increased anxiety and unnecessary treatment. What is the aim of this review? To update the evidence on accuracy of Xpert tests for diagnosing extrapulmonary tuberculosis and rifampicin resistance in adults. Rifampicin is an important tuberculosis drug. We included tuberculous meningitis and pleural, lymph node, bone or joint, genitourinary, peritoneal, pericardial, and disseminated tuberculosis. What was studied in this review? Xpert Ultra and Xpert MTB/RIF are rapid tests for simultaneously diagnosing tuberculosis and rifampicin resistance. We combined study results to determine: ‐ sensitivity: people with tuberculosis (rifampicin resistance) correctly diagnosed as having the condition. ‐ specificity: people without tuberculosis (rifampicin resistance) correctly identified as not having the condition. The closer sensitivity and specificity are to 100%, the better the test. We measured Xpert results against culture and a composite reference standard (neither is a perfect reference standard because extrapulmonary tuberculosis is paucibacillary (few bacteria)). What are the main results in this review? 69 studies tested lymph node, pleural, and cerebrospinal fluid, and other specimens from people with presumptive extrapulmonary tuberculosis. Studies were conducted in 28 different countries. For every 1000 people tested, if 100 had tuberculosis according to the reference standards: cerebrospinal fluid ‐ Xpert Ultra (6 studies) : · 89% sensitivity: 168 people would test positive, including 79 without tuberculosis · 91% specificity: 832 people would test negative, including 11 with tuberculosis ‐  Xpert MTB/RIF (30 studies) : · 71% sensitivity: 99 people would test positive, including 28 without tuberculosis · 97% specificity: 901 people would test negative, including 29 with tuberculosis pleural fluid ‐  Xpert Ultra (4 studies) : · 75% sensitivity: 192 people would test positive, including 117 without tuberculosis · 87% specificity: 808 people would test negative, including 25 with tuberculosis ‐  Xpert MTB/RIF (25 studies) : · 50% sensitivity: 60 people would test positive, including 10 without tuberculosis · 99% specificity: 940 would test negative, including 50 with tuberculosis lymph node fluid ‐  Xpert Ultra (1 study) : · 70% sensitivity: 70 people would test positive (all have tuberculosis) · 100% specificity: 930 people would test negative, including 30 with tuberculosis ‐ Xpert MTB/RIF (4 studies): · 82% sensitivity:118 people would test positive, including 37 without tuberculosis · 96% specificity: 882 people would test negative, including 19 with tuberculosis rifampicin resistance ‐ Xpert Ultra (4 studies) : · 100% sensitivity: 100 people would test positive (all have rifampicin resistance) · 100% specificity: 900 people would test negative (none have rifampicin resistance) ‐  MTB/RIF test (19 studies) : · 97% sensitivity: 105 people would test positive, including eight without rifampicin resistance · 99% specificity: 895 people would test negative, including three with rifampicin resistance Who do the results of this review apply to? People thought to have extrapulmonary tuberculosis. How confident are we in our results? Fairly confident for Xpert MTB/RIF in cerebrospinal fluid and less so in lymph node fluid. Less confident for Xpert Ultra, as there were few studies and few people tested. Both reference standards are imperfect, which may affect accuracy estimates. What are the implications of this review? The Xpert tests may be helpful in diagnosing extrapulmonary tuberculosis. Sensitivity varies across different extrapulmonary specimens, while for most specimens, specificity is high, the test rarely yielding a positive result for people without tuberculosis (verified by culture). For tuberculous meningitis, Xpert Ultra had higher sensitivity than Xpert MTB/RIF and lower specificity than Xpert MTB/RIF. The tests had similar accuracy for diagnosing rifampicin resistance. How up‐to‐date is this review? 28 January 2020.","1","John Wiley & Sons, Ltd","1465-1858","*Drug Resistance, Bacterial; *Nucleic Acid Amplification Techniques [methods, statistics & numerical data]; Adult; Antibiotics, Antitubercular [*therapeutic use]; Bias; False Negative Reactions; False Positive Reactions; Humans; Mycobacterium tuberculosis [*drug effects, isolation & purification]; Reagent Kits, Diagnostic; Rifampin [*therapeutic use]; Sensitivity and Specificity; Tuberculosis [cerebrospinal fluid, *diagnosis, drug therapy]; Tuberculosis, Lymph Node [cerebrospinal fluid, diagnosis, drug therapy]; Tuberculosis, Meningeal [cerebrospinal fluid, diagnosis, drug therapy]; Tuberculosis, Multidrug-Resistant [cerebrospinal fluid, diagnosis, drug therapy]; Tuberculosis, Pleural [cerebrospinal fluid, diagnosis, drug therapy]","10.1002/14651858.CD012768.pub3","http://dx.doi.org/10.1002/14651858.CD012768.pub3","Infectious Diseases"
"CD013811","Macey, R; Walsh, T; Riley, P; Glenny, A-M; Worthington, HV; Fee, PA; Clarkson, JE; Ricketts, D","Fluorescence devices for the detection of dental caries","Cochrane Database of Systematic Reviews","2020","Abstract - Background Caries is one of the most prevalent and preventable conditions worldwide. If identified early enough then non‐invasive techniques can be applied, and therefore this review focusses on early caries involving the enamel surface of the tooth. The cornerstone of caries detection is a visual and tactile dental examination, however alternative methods of detection are available, and these include fluorescence‐based devices. There are three categories of fluorescence‐based device each primarily defined by the different wavelengths they exploit; we have labelled these groups as red, blue, and green fluorescence. These devices could support the visual examination for the detection and diagnosis of caries at an early stage of decay. Objectives Our primary objectives were to estimate the diagnostic test accuracy of fluorescence‐based devices for the detection and diagnosis of enamel caries in children or adults. We planned to investigate the following potential sources of heterogeneity: tooth surface (occlusal, proximal, smooth surface or adjacent to a restoration); single point measurement devices versus imaging or surface assessment devices; and the prevalence of more severe disease in each study sample, at the level of caries into dentine. Search methods Cochrane Oral Health's Information Specialist undertook a search of the following databases: MEDLINE Ovid (1946 to 30 May 2019); Embase Ovid (1980 to 30 May 2019); US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov, to 30 May 2019); and the World Health Organization International Clinical Trials Registry Platform (to 30 May 2019). We studied reference lists as well as published systematic review articles. Selection criteria We included diagnostic accuracy study designs that compared a fluorescence‐based device with a reference standard. This included prospective studies that evaluated the diagnostic accuracy of single index tests and studies that directly compared two or more index tests. Studies that explicitly recruited participants with caries into dentine or frank cavitation were excluded. Data collection and analysis Two review authors extracted data independently using a piloted study data extraction form based on the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2). Sensitivity and specificity with 95% confidence intervals (CIs) were reported for each study. This information has been displayed as coupled forest plots and summary receiver operating characteristic (SROC) plots, displaying the sensitivity‐specificity points for each study. We estimated diagnostic accuracy using hierarchical summary receiver operating characteristic (HSROC) methods. We reported sensitivities at fixed values of specificity (median 0.78, upper quartile 0.90). Main results We included a total of 133 studies, 55 did not report data in the 2 x 2 format and could not be included in the meta‐analysis. 79 studies which provided 114 datasets and evaluated 21,283 tooth surfaces were included in the meta‐analysis. There was a high risk of bias for the participant selection domain. The index test, reference standard, and flow and timing domains all showed a high proportion of studies to be at low risk of bias. Concerns regarding the applicability of the evidence were high or unclear for all domains, the highest proportion being seen in participant selection. Selective participant recruitment, poorly defined diagnostic thresholds, and in vitro studies being non‐generalisable to the clinical scenario of a routine dental examination were the main reasons for these findings. The dominance of in vitro studies also means that the information on how the results of these devices are used to support diagnosis, as opposed to pure detection, was extremely limited. There was substantial variability in the results which could not be explained by the different devices or dentition or other sources of heterogeneity that we investigated. The diagnostic odds ratio (DOR) was 14.12 (95% CI 11.17 to 17.84). The estimated sensitivity, at a fixed median specificity of 0.78, was 0.70 (95% CI 0.64 to 0.75). In a hypothetical cohort of 1000 tooth sites or surfaces, with a prevalence of enamel caries of 57%, obtained from the included studies, the estimated sensitivity of 0.70 and specificity of 0.78 would result in 171 missed tooth sites or surfaces with enamel caries (false negatives) and 95 incorrectly classed as having early caries (false positives). We used meta‐regression to compare the accuracy of the different devices for red fluorescence (84 datasets, 14,514 tooth sites), blue fluorescence (21 datasets, 3429 tooth sites), and green fluorescence (9 datasets, 3340 tooth sites) devices. Initially, we allowed threshold, shape, and accuracy to vary according to device type by including covariates in the model. Allowing consistency of shape, removal of the covariates for accuracy had only a negligible effect (Chi 2  = 3.91, degrees of freedom (df) = 2, P = 0.14). Despite the relatively large volume of evidence we rated the certainty of the evidence as low, downgraded two levels in total, for risk of bias due to limitations in the design and conduct of the included studies, indirectness arising from the high number of in vitro studies, and inconsistency due to the substantial variability of results. Authors' conclusions There is considerable variation in the performance of these fluorescence‐based devices that could not be explained by the different wavelengths of the devices assessed, participant, or study characteristics. Blue and green fluorescence‐based devices appeared to outperform red fluorescence‐based devices but this difference was not supported by the results of a formal statistical comparison. The evidence base was considerable, but we were only able to include 79 studies out of 133 in the meta‐analysis as estimates of sensitivity or specificity values or both could not be extracted or derived. In terms of applicability, any future studies should be carried out in a clinical setting, where difficulties of caries assessment within the oral cavity include plaque, staining, and restorations. Other considerations include the potential of fluorescence devices to be used in combination with other technologies and comparative diagnostic accuracy studies. Plain language summary Fluorescence devices for the detection of dental caries Why is it important to improve dental caries (tooth decay) detection? Dentists often aim to identify tooth decay that has already advanced to a level which needs a filling. If dentists were able to find tooth decay when it has only affected the outer layer of the tooth then it is possible to stop the decay from spreading any further and prevent the need for fillings. It is also important to avoid a false‐positive result, when treatment may be provided when caries is absent. What is the aim of this review? This Cochrane Review aimed to find out how accurate fluorescence devices (non‐invasive devices that shine a light on the surface of the tooth) are for detecting and diagnosing early tooth decay as part of the dental 'check‐up' for children and adults who visit their general dentist. Researchers included 133 studies to answer this question. What was studied in the review? There are three different types of fluorescence device that use different types of light which we grouped as red, blue, and green fluorescence. Each device reflects more or less light depending on the amount of tooth decay, and this is measured by the device to give a score which indicates whether there is tooth decay and how severe the decay is. We studied decay on the occlusal surfaces (biting surfaces of the back teeth), the proximal surfaces (tooth surfaces that are next to each other), and the smooth surfaces. What are the main results of the review? The review included 133 relevant studies but 55 of these did not provide data in a format that we could use for analysis, so 79 studies with a total of 21,283 teeth were included in the analysis. Some of these studies reported on more than one type of fluorescence device, this gave us 114 sets of data. The results of these studies indicate that, in theory, if the fluorescence devices were to be used by a dentist for a routine dental examination in a group of 1000 tooth sites or surfaces, of which 574 (57%) have early tooth decay: • an estimated 494 will have a fluorescence device result indicating tooth decay, and of these, 95 (19%) will not have tooth decay (false positive ‐ incorrect diagnosis); • of the 506 tooth sites or surfaces with a result indicating that tooth decay is not present, 171 (34%) will have early tooth decay (false negative ‐ incorrect diagnosis). Please see  oralhealth.cochrane.org/fluorescence-devices-results . We found no evidence that the devices that used different types of light (red, blue, or green fluorescence) differed in their accuracy. How reliable are the results of the studies in this review? We only included studies that assessed healthy teeth or those that were thought to have early tooth decay. This is because teeth with deep tooth decay would be easier to detect. However, there were some problems with how the studies were carried out. This may have resulted in the fluorescence‐based devices appearing more accurate than they are. We judged the certainty of the evidence as low due to how the studies selected their participants, the large number of studies that were carried out in a laboratory setting on extracted teeth, and the variation in results reported. Who do the results of this review apply to? Studies included in the review were carried out in Brazil, Europe, the Middle East, Asia, North America, and Australia. A large number of studies used extracted teeth. Others were completed in dental hospitals, general dental practices, or schools. Studies were from the years 1998 and 2019. What are the implications of this review? Because of the wide variation in performance that cannot be easily explained the interpretation of results is difficult. The proportion of cases missed or incorrectly diagnosed as evidence of caries is relatively high. Important information was missing from many of the included studies. Any future studies should be carried out in a clinical setting, and look at the potential of fluorescence devices to be used alongside other devices. How up‐to‐date is this review? The review authors searched for and used studies published up to 30 May 2019.","12","John Wiley & Sons, Ltd","1465-1858","Adult; Bias; Child; Color; Dental Caries [*diagnosis]; Fluorescence; Humans; Patient Selection; Prospective Studies; Quantitative Light-Induced Fluorescence [*instrumentation]; Sensitivity and Specificity","10.1002/14651858.CD013811","http://dx.doi.org/10.1002/14651858.CD013811","Oral Health"
"CD013806","Fee, PA; Macey, R; Walsh, T; Clarkson, JE; Ricketts, D","Tests to detect and inform the diagnosis of root caries","Cochrane Database of Systematic Reviews","2020","Abstract - Background Root caries is a well‐recognised disease, with increasing prevalence as populations age and retain more of their natural teeth into later life. Like coronal caries, root caries can be associated with pain, discomfort, tooth loss, and contribute significantly to poorer oral health‐related quality of life in the elderly. Supplementing the visual‐tactile examination could prove beneficial in improving the accuracy of early detection and diagnosis. The detection of root caries lesions at an early stage in the disease continuum can inform diagnosis and lead to targeted preventive therapies and lesion arrest. Objectives To assess the diagnostic test accuracy of index tests for the detection and diagnosis of root caries in adults, used alone or in combination with other tests. Search methods Cochrane Oral Health's Information Specialist undertook a search of the following databases: MEDLINE Ovid (1946 to 31 December 2018); Embase Ovid (1980 to 31 December 2018); US National Institutes of Health Ongoing Trials Register (ClinicalTrials.gov, to 31 December 2018); and the World Health Organization International Clinical Trials Registry Platform (to 31 December 2018). We studied reference lists as well as published systematic review articles. Selection criteria We included diagnostic accuracy study designs that compared one or more index tests (laser fluorescence, radiographs, visual examination, electronic caries monitor (ECM), transillumination), either independently or in combination, with a reference standard. This included prospective studies that evaluated the diagnostic accuracy of single index tests and studies that directly compared two or more index tests. In vitro and in vivo studies were eligible for inclusion but studies that artificially created carious lesions were excluded. Data collection and analysis Two review authors extracted data independently and in duplicate using a standardised data extraction and quality assessment form based on the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2) specific to the review context. Estimates of diagnostic test accuracy were expressed as sensitivity and specificity with 95% confidence intervals (CI) for each dataset. We planned to use hierarchical models for data synthesis and explore potential sources of heterogeneity through meta‐regression. Main results Four cross‐sectional diagnostic test accuracy studies providing eight datasets with data from 4997 root surfaces were analysed. Two in vitro studies evaluated secondary root caries lesions on extracted teeth and two in vivo studies evaluated primary root caries lesions within the oral cavity. Four studies evaluated laser fluorescence and reported estimates of sensitivity ranging from 0.50 to 0.81 and specificity ranging from 0.40 to 0.80. Two studies evaluated radiographs and reported estimates of sensitivity ranging from 0.40 to 0.63 and specificity ranging from 0.31 to 0.80. One study evaluated visual examination and reported sensitivity of 0.75 (95% CI 0.48 to 0.93) and specificity of 0.38 (95% CI 0.14 to 0.68). One study evaluated the accuracy of radiograph and visual examination in combination and reported sensitivity of 0.81 (95% CI 0.54 to 0.96) and specificity of 0.54 (95% CI 0.25 to 0.81). Given the small number of studies and important differences in the clinical and methodological characteristics of the studies we were unable to pool the results. Consequently, we were unable to formally evaluate the comparative accuracy of the different tests considered in this review. Using QUADAS‐2 we judged all four studies to be at overall high risk of bias, but only two to have applicability concerns (patient selection domain). Reasons included bias in the selection process, use of post hoc (data driven) positivity thresholds, use of an imperfect reference standard, and use of extracted teeth. We downgraded the certainty of the evidence due to study limitations and serious imprecision of the results (downgraded two levels), and judged the certainty of the evidence to be very low. Authors' conclusions Visual‐tactile examination is the mainstay of root caries detection and diagnosis; however, due to the paucity of the evidence base and the very low certainty of the evidence we were unable to determine the additional benefit of adjunctive diagnostic tests for the detection and diagnosis of root caries. Plain language summary Tests to detect and inform the diagnosis of root caries Why is it important to improve root caries detection? Root caries (tooth decay on the root of a tooth) is a well‐recognised disease, that is on the increase as populations grow older and keep more of their natural teeth into later life. Like coronal caries (tooth decay on the crown of the tooth), root caries can be associated with pain, discomfort, and tooth loss, which can contribute to poorer oral health‐related quality of life in the elderly. Detecting caries earlier can mean less invasive treatment is needed, where more tooth tissue can be preserved. It could also mean less cost to the patient and to healthcare services. What is the aim of this review? The aim of this Cochrane Review was to find out whether any diagnostic tools could be used to support the general dentist to correctly identify root caries in adults. Researchers in Cochrane included four studies to answer this question. What was studied in the review? Four studies including 4997 root surfaces were included in the review. The studies took place in Switzerland and Hong Kong, and were published between 2009 and 2016. The accuracy of laser tests was examined in four studies, two studies examined radiographs (x‐rays), one study examined comprehensive visual examination, and one study examined a combined test of radiographs and visual examination. What are the main results of the review? All studies reported case finding (detection) rather than diagnosis that included the consideration of patient risk and history. Two studies evaluated the use of devices within the mouth, and two studies evaluated the use of devices on extracted teeth (in vitro studies). Due to the small number of studies and important differences in the setting of included studies we were unable to combine the results of the studies. How reliable are the results of the studies in this review? We found important study limitations in all included studies, particularly with participant enrolment which was often poorly reported. Applicability of patient selection was also of concern for two in vitro studies. For these reasons, we judged the certainty of the evidence to be very low. Who do the results of this review apply to? Studies included in the review were carried out in Hong Kong and Switzerland and aimed at the general dental practitioner conducting a clinical examination on adults attending a dental setting. What are the implications of this review? Due to the small number of studies and the very low certainty of the evidence we were unable to establish any additional benefit of diagnostic tools for the detection and diagnosis of root caries. How up‐to‐date is this review? The review authors searched for and used studies published up to 31 December 2018.","12","John Wiley & Sons, Ltd","1465-1858","Aged; Cross-Sectional Studies; Early Diagnosis; Fluorescence; Humans; Lasers; Middle Aged; Physical Examination [methods]; Radiography, Dental; Reference Standards; Root Caries [*diagnosis]; Sensitivity and Specificity; Transillumination [methods]","10.1002/14651858.CD013806","http://dx.doi.org/10.1002/14651858.CD013806","Oral Health"
"CD013787","Stegeman, I; Ochodo, EA; Guleid, F; Holtman, GA.; Yang, B; Davenport, C; Deeks, JJ; Dinnes, J; Dittrich, S; Emperador, D; Hooft, L; Spijker, R; Takwoingi, Y; Van den Bruel, A; Wang, J; Langendam, M; Verbakel, JY; Leeflang, MMG","Routine laboratory testing to determine if a patient has COVID‐19","Cochrane Database of Systematic Reviews","2020","Abstract - Background Specific diagnostic tests to detect severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2) and resulting COVID‐19 disease are not always available and take time to obtain results. Routine laboratory markers such as white blood cell count, measures of anticoagulation, C‐reactive protein (CRP) and procalcitonin, are used to assess the clinical status of a patient. These laboratory tests may be useful for the triage of people with potential COVID‐19 to prioritize them for different levels of treatment, especially in situations where time and resources are limited. Objectives To assess the diagnostic accuracy of routine laboratory testing as a triage test to determine if a person has COVID‐19. Search methods On 4 May 2020 we undertook electronic searches in the Cochrane COVID‐19 Study Register and the COVID‐19 Living Evidence Database from the University of Bern, which is updated daily with published articles from PubMed and Embase and with preprints from medRxiv and bioRxiv. In addition, we checked repositories of COVID‐19 publications. We did not apply any language restrictions. Selection criteria We included both case‐control designs and consecutive series of patients that assessed the diagnostic accuracy of routine laboratory testing as a triage test to determine if a person has COVID‐19. The reference standard could be reverse transcriptase polymerase chain reaction (RT‐PCR) alone; RT‐PCR plus clinical expertise or and imaging; repeated RT‐PCR several days apart or from different samples; WHO and other case definitions; and any other reference standard used by the study authors. Data collection and analysis Two review authors independently extracted data from each included study. They also assessed the methodological quality of the studies, using QUADAS‐2. We used the 'NLMIXED' procedure in SAS 9.4 for the hierarchical summary receiver operating characteristic (HSROC) meta‐analyses of tests for which we included four or more studies. To facilitate interpretation of results, for each meta‐analysis we estimated summary sensitivity at the points on the SROC curve that corresponded to the median and interquartile range boundaries of specificities in the included studies. Main results We included 21 studies in this review, including 14,126 COVID‐19 patients and 56,585 non‐COVID‐19 patients in total. Studies evaluated a total of 67 different laboratory tests. Although we were interested in the diagnotic accuracy of routine tests for COVID‐19, the included studies used detection of SARS‐CoV‐2 infection through RT‐PCR as reference standard. There was considerable heterogeneity between tests, threshold values and the settings in which they were applied. For some tests a positive result was defined as a decrease compared to normal vaues, for other tests a positive result was defined as an increase, and for some tests both increase and decrease may have indicated test positivity. None of the studies had either low risk of bias on all domains or low concerns for applicability for all domains. Only three of the tests evaluated had a summary sensitivity and specificity over 50%. These were: increase in interleukin‐6, increase in C‐reactive protein and lymphocyte count decrease. Blood count Eleven studies evaluated a decrease in white blood cell count, with a median specificity of 93% and a summary sensitivity of 25% (95% CI 8.0% to 27%; very low‐certainty evidence). The 15 studies that evaluated an increase in white blood cell count had a lower median specificity and a lower corresponding sensitivity. Four studies evaluated a decrease in neutrophil count. Their median specificity was 93%, corresponding to a summary sensitivity of 10% (95% CI 1.0% to 56%; low‐certainty evidence). The 11 studies that evaluated an increase in neutrophil count had a lower median specificity and a lower corresponding sensitivity. The summary sensitivity of an increase in neutrophil percentage (4 studies) was 59% (95% CI 1.0% to 100%) at median specificity (38%; very low‐certainty evidence). The summary sensitivity of an increase in monocyte count (4 studies) was 13% (95% CI 6.0% to 26%) at median specificity (73%; very low‐certainty evidence). The summary sensitivity of a decrease in lymphocyte count (13 studies) was 64% (95% CI 28% to 89%) at median specificity (53%; low‐certainty evidence). Four studies that evaluated a decrease in lymphocyte percentage showed a lower median specificity and lower corresponding sensitivity. The summary sensitivity of a decrease in platelets (4 studies) was 19% (95% CI 10% to 32%) at median specificity (88%; low‐certainty evidence). Liver function tests The summary sensitivity of an increase in alanine aminotransferase (9 studies) was 12% (95% CI 3% to 34%) at median specificity (92%; low‐certainty evidence). The summary sensitivity of an increase in aspartate aminotransferase (7 studies) was 29% (95% CI 17% to 45%) at median specificity (81%) (low‐certainty evidence). The summary sensitivity of a decrease in albumin (4 studies) was 21% (95% CI 3% to 67%) at median specificity (66%; low‐certainty evidence). The summary sensitivity of an increase in total bilirubin (4 studies) was 12% (95% CI 3.0% to 34%) at median specificity (92%; very low‐certainty evidence). Markers of inflammation The summary sensitivity of an increase in CRP (14 studies) was 66% (95% CI 55% to 75%) at median specificity (44%; very low‐certainty evidence). The summary sensitivity of an increase in procalcitonin (6 studies) was 3% (95% CI 1% to 19%) at median specificity (86%; very low‐certainty evidence). The summary sensitivity of an increase in IL‐6 (four studies) was 73% (95% CI 36% to 93%) at median specificity (58%) (very low‐certainty evidence). Other biomarkers The summary sensitivity of an increase in creatine kinase (5 studies) was 11% (95% CI 6% to 19%) at median specificity (94%) (low‐certainty evidence). The summary sensitivity of an increase in serum creatinine (four studies) was 7% (95% CI 1% to 37%) at median specificity (91%; low‐certainty evidence). The summary sensitivity of an increase in lactate dehydrogenase (4 studies) was 25% (95% CI 15% to 38%) at median specificity (72%; very low‐certainty evidence). Authors' conclusions Although these tests give an indication about the general health status of patients and some tests may be specific indicators for inflammatory processes, none of the tests we investigated are useful for accurately ruling in or ruling out COVID‐19 on their own. Studies were done in specific hospitalized populations, and future studies should consider non‐hospital settings to evaluate how these tests would perform in people with milder symptoms. Plain language summary How accurate are routine laboratory tests for diagnosis of COVID‐19? What are routine laboratory tests? Routine laboratory tests are blood tests that assess the health status of a patient. Tests include counts of different types of white blood cells (these help the body fight infection), and detection of markers (proteins) that indicate organ damage, and general inflammation. These tests are widely available and in some places they may be the only tests available for diagnosis of COVID‐19. What did we want to find out? People with suspected COVID‐19 need to know quickly whether they are infected so that they can self‐isolate, receive treatment, and inform close contacts. Currently, the standard test for COVID‐19 is usually the RT‐PCR test. In the RT‐PCR, samples from the nose and throat are sent away for testing, usually to a large, central laboratory with specialist equipment. Other tests include imaging tests, like X‐rays, which also require specialist equipment. We wanted to know whether routine laboratory tests were sufficiently accurate to diagnose COVID‐19 in people with suspected COVID‐19. We also wanted to know whether they were accurate enough to prioritize patients for different levels of treatment. What did we do? We searched for studies that assessed the accuracy of routine laboratory tests to diagnose COVID‐19 compared with RT‐PCR or other tests. Studies could be of any design and be set anywhere in the world. Studies could include participants of any age or sex, with suspected COVID‐19, or use samples from people known to have – or not to have ‐ COVID‐19. What we found We found 21 studies that looked at 67 different routine laboratory tests for COVID‐19. Most of the studies looked at how accurately these tests diagnosed infection with the virus causing COVID‐19. Four studies included both children and adults, 16 included only adults and one study only children. Seventeen studies were done in China, and one each in Iran, Italy, Taiwan and the USA. All studies took place in hospitals, except one that used samples from a database. Most studies used RT‐PCR to confirm COVID‐19 diagnosis. Accuracy of tests is most often reported using ‘sensitivity’ and ‘specificity’. Sensitivity is the proportion of people with COVID‐19 correctly detected by the test; specificity is the proportion of people without COVID‐19 who are correctly identified by the test. The nearer sensitivity and specificity are to 100%, the better the test. A test to prioritize people for treatment would require a high sensitivity of more than 80%. Where four or more studies evaluated a particular test, we pooled their results and analyzed them together. Our analyses showed that only three of the tests had both sensitivity and specificity over 50%. Two of these were markers for general inflammation (increases in interleukin‐6 and C‐reactive protein). The third was for lymphocyte count decrease. Lymphocytes are a type of white blood cell where a low count might indicate infection. How reliable are the results? Our confidence in the evidence from this review is low because the studies were different from each other, which made them difficult to compare. For example, some included very sick people, while some included people with hardly any COVID‐19 symptoms. Also, the diagnosis of COVID‐19 was confirmed in different ways: RT‐PCR was sometimes used in combination with other tests. Who do the results of this review apply to? Routine laboratory tests can be issued by most healthcare facilities. However, our results are probably not representative of most clinical situations in which these tests are being used. Most studies included very sick people with high rates of COVID‐19 virus infection of between 27% and 76%. In most primary healthcare facilities, this percentage will be lower. What does this mean? Routine laboratory tests cannot distinguish between COVID‐19 and other diseases as the cause of infection, inflammation or tissue damage. None of the tests performed well enough to be a standalone diagnostic test for COVID‐19 nor to prioritize patients for treatment. They will mainly be used to provide an overall picture about the health status of the patient. The final COVID‐19 diagnosis has to be made based on other tests. How up‐to‐date is this review? We searched all COVID‐19 studies up to 4 May 2020.","11","John Wiley & Sons, Ltd","1465-1858","Bias; Biomarkers [blood]; C-Reactive Protein [analysis]; COVID-19 Testing [*methods, standards]; COVID-19 [blood, *diagnosis, epidemiology]; Creatine Kinase [blood]; Creatinine [blood]; Diagnostic Tests, Routine [*methods, standards]; Humans; Interleukin-6 [blood]; L-Lactate Dehydrogenase [blood]; Leukocyte Count; Liver Function Tests; Lymphocyte Count; Pandemics; Platelet Count; ROC Curve; Reference Values; Reverse Transcriptase Polymerase Chain Reaction [standards]; SARS-CoV-2 [*isolation & purification]; Sensitivity and Specificity; Triage","10.1002/14651858.CD013787","http://dx.doi.org/10.1002/14651858.CD013787","Infectious Diseases"
"CD013218.PUB2","Agarwal, R; Choi, L; Johnson, S; Takwoingi, Y","Rapid diagnostic tests for   Plasmodium vivax malaria in endemic countries","Cochrane Database of Systematic Reviews","2020","Abstract - Background Plasmodium vivax  ( P vivax ) is a focus of malaria elimination. It is important because  P vivax  and  Plasmodium falciparum  infection are co‐endemic in some areas. There are asymptomatic carriers of  P vivax,  and the treatment for  P vivax  and  Plasmodium ovale  malaria differs from that used in other types of malaria. Rapid diagnostic tests (RDTs) will help distinguish  P vivax  from other malaria species to help treatment and elimination. There are RDTs available that detect  P vivax  parasitaemia through the detection of  P vivax‐ specific lactate dehydrogenase (LDH) antigens. Objectives To assess the diagnostic accuracy of RDTs for detecting  P vivax  malaria infection in people living in malaria‐endemic areas who present to ambulatory healthcare facilities with symptoms suggestive of malaria; and to identify which types and brands of commercial tests best detect  P vivax  malaria. Search methods We undertook a comprehensive search of the following databases up to 30 July 2019: Cochrane Infectious Diseases Group Specialized Register; Central Register of Controlled Trials (CENTRAL), published in the Cochrane Library; MEDLINE (PubMed); Embase (OVID); Science Citation Index Expanded (SCI‐EXPANDED) and Conference Proceedings Citation Index‐Science (CPCI‐S), both in the Web of Science. Selection criteria Studies comparing RDTs with a reference standard (microscopy or polymerase chain reaction (PCR)) in blood samples from patients attending ambulatory health facilities with symptoms suggestive of malaria in  P vivax ‐endemic areas. Data collection and analysis For each included study, two review authors independently extracted data using a pre‐piloted data extraction form. The methodological quality of the studies were assessed using a tailored Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2) tool. We grouped studies according to commercial brand of the RDT and performed meta‐analysis when appropriate. The results given by the index tests were based on the antibody affinity (referred to as the strength of the bond between an antibody and an antigen) and avidity (referred to as the strength of the overall bond between a multivalent antibody and multiple antigens). All analyses were stratified by the type of reference standard. The bivariate model was used to estimate the pooled sensitivity and specificity with 95% confidence intervals (CIs), this model was simplified when studies were few. We assessed the certainty of the evidence using the GRADE approach. Main results We included 10 studies that assessed the accuracy of six different RDT brands (CareStart Malaria Pf/Pv Combo test, Falcivax Device Rapid test, Immuno‐Rapid Malaria Pf/Pv test, SD Bioline Malaria Ag Pf/Pv test, OnSite Pf/Pv test and Test Malaria Pf/Pv rapid test) for detecting  P vivax  malaria. One study directly compared the accuracy of two RDT brands. Of the 10 studies, six used microscopy, one used PCR, two used both microscopy and PCR separately and one used microscopy corrected by PCR as the reference standard. Four of the studies were conducted in Ethiopia, two in India, and one each in Bangladesh, Brazil, Colombia and Sudan. The studies often did not report how patients were selected. In the patient selection domain, we judged the risk of bias as unclear for nine studies. We judged all studies to be of unclear applicability concern. In the index test domain, we judged most studies to be at low risk of bias, but we judged nine studies to be of unclear applicability concern. There was poor reporting on lot testing, how the RDTs were stored, and background parasitaemia density (a key variable determining diagnostic accuracy of RDTs). Only half of the included studies were judged to be at low risk of bias in the reference standard domain, Studies often did not report whether the results of the reference standard could classify the target condition or whether investigators knew the results of the RDT when interpreting the results of the reference standard. All 10 studies were judged to be at low risk of bias in the flow and timing domain. Only two brands were evaluated by more than one study. Four studies evaluated the CareStart Malaria Pf/Pv Combo test against microscopy and two studies evaluated the Falcivax Device Rapid test against microscopy. The pooled sensitivity and specificity were 99% (95% CI 94% to 100%; 251 patients, moderate‐certainty evidence) and 99% (95% CI 99% to 100%; 2147 patients, moderate‐certainty evidence) for CareStart Malaria Pf/Pv Combo test. For a prevalence of 20%, about 206 people will have a positive CareStart Malaria Pf/Pv Combo test result and the remaining 794 people will have a negative result. Of the 206 people with positive results, eight will be incorrect (false positives), and of the 794 people with a negative result, two would be incorrect (false negative). For the Falcivax Device Rapid test, the pooled sensitivity was 77% (95% CI: 53% to 91%, 89 patients, low‐certainty evidence) and the pooled specificity was 99% (95% CI: 98% to 100%, 621 patients, moderate‐certainty evidence), respectively. For a prevalence of 20%, about 162 people will have a positive Falcivax Device Rapid test result and the remaining 838 people will have a negative result. Of the 162 people with positive results, eight will be incorrect (false positives), and of the 838 people with a negative result, 46 would be incorrect (false negative). Authors' conclusions The CareStart Malaria Pf/Pv Combo test was found to be highly sensitive and specific in comparison to microscopy for detecting  P vivax  in ambulatory healthcare in endemic settings, with moderate‐certainty evidence. The number of studies included in this review was limited to 10 studies and we were able to estimate the accuracy of 2 out of 6 RDT brands included, the CareStart Malaria Pf/Pv Combo test and the Falcivax Device Rapid test. Thus, the differences in sensitivity and specificity between all the RDT brands could not be assessed. More high‐quality studies in endemic field settings are needed to assess and compare the accuracy of RDTs designed to detect  P vivax . Plain language summary Rapid tests for diagnosing malaria caused by  Plasmodium vivax  in people living in areas where malaria is very common What is the aim of the review? Malaria infection is caused mainly by two species of malaria parasite:  Plasmodium falciparum  and  Plasmodium vivax . The aim of this review was to evaluate rapid diagnostic tests (RDTs) to diagnose  P vivax  infection. Why are rapid tests for  P vivax  malaria important? For clinical management, knowing which parasite species is causing the malaria is important as the drug treatments differ. For  P vivax  infection, an additional drug is required to eliminate the infection from the liver. For public health control of malaria, we know that  P falciparum  is declining over the previous 15 years, and infections from  P vivax  have therefore increased in importance. What was studied in this review? RDTs provide results quickly and are often as a dipstick. We studied RDTs that specifically test for  P vivax  malaria. RDTs are simple to use, point‐of‐care tests. They are suitable for use in rural settings by primary healthcare workers, using drop of blood on the dipstick that causes colour change and a distinct line that indicates a positive test result. Healthcare workers in rural areas can perform RDTs for  P vivax  without needing a laboratory or special equipment. We wanted to find out which brands of RDTs were the most accurate for diagnosing  P vivax  malaria. We compared the new tests against the standard form of diagnosis with microscopy, and also more recent methods polymerase chain reaction (PCR): a molecular method to identify  P vivax  DNA in blood samples. What are the main results of the review? We included 10 studies that looked at the accuracy of six diagnostic test brands for detecting  P vivax  malaria in people with suspected malaria symptoms. The studies were conducted in Ethiopia (four studies), India (two studies) and Bangladesh, Brazil, Colombia, and Sudan (one study each). Compared with microscopy, the Care Start Malaria Pf/Pv Combo test performed well with 99% sensitivity and specificity (four studies). This means that: • for every 100 people tested who have  P vivax  malaria, one person will have a negative test result, and might not receive the right treatment soon enough; • for every 100 people tested who do not have  P vivax  malaria, one will have a positive result, and might receive unnecessary treatment. Compared with microscopy, the Falcivax Device Rapid test had a sensitivity of 77% and a specificity of 99% (two studies). This means that: • For every 100 people tested who have  P vivax  malaria, 23 people will have a negative test result; and, • for every 100 people tested who do not have  P vivax  malaria, one person will have a positive result. We are moderately confident (certain) in the accuracy results for the Care Start Malaria Pf/Pv Combo test. The results are from a small number of studies (four), so our findings may change when results from further studies become available. We are less confident in the accuracy results for the Falcivax Device Rapid test, because these came from only two studies. Our findings for this test will probably change when results from further studies become available. Our results are based on a small number of studies, so we could not reliably assess all six brands of antibody test or compare their accuracy. Most studies included in this review had limitations: it was not clear how people were selected for testing, or how the study results were assessed and checked, which could have affected the results. Some rapid antibody tests were investigated by only one study. Some studies did not report clearly how common  P  malaria was in the area where the study was done. How up‐to‐date is this review? The review authors searched for studies published up to 30 July 2019.","11","John Wiley & Sons, Ltd","1465-1858","*Endemic Diseases; *Reagent Kits, Diagnostic [statistics & numerical data]; Ambulatory Care [statistics & numerical data]; Antigens, Protozoan [blood]; Bias; False Negative Reactions; False Positive Reactions; Humans; Malaria, Vivax [blood, *diagnosis, epidemiology]; Microscopy [standards]; Plasmodium vivax [immunology]; Point-of-Care Testing [standards]; Polymerase Chain Reaction [standards]; Reference Standards; Sensitivity and Specificity; Species Specificity","10.1002/14651858.CD013218.pub2","http://dx.doi.org/10.1002/14651858.CD013218.pub2","Infectious Diseases"
"CD009185.PUB3","Shaikh, KJ; Osio, VA; Leeflang, MMG; Shaikh, N","Procalcitonin, C‐reactive protein, and erythrocyte sedimentation rate for the diagnosis of acute pyelonephritis in children","Cochrane Database of Systematic Reviews","2020","Abstract - Background In children with urinary tract infection (UTI), only those with pyelonephritis (and not cystitis) are at risk for developing long‐term renal sequelae. If non‐invasive biomarkers could accurately differentiate children with cystitis from children with pyelonephritis, treatment and follow‐up could potentially be individualized. This is an update of a review first published in 2015. Objectives The objectives of this review were to 1) determine whether procalcitonin (PCT), C‐reactive protein (CRP), erythrocyte sedimentation rate (ESR) can replace the acute DMSA scan in the diagnostic evaluation of children with UTI; 2) assess the influence of patient and study characteristics on the diagnostic accuracy of these tests, and 3) compare the performance of the three tests to each other. Search methods We searched MEDLINE, EMBASE, DARE, Web of Science, and BIOSIS Previews through to 17th December 2019 for this review. The reference lists of all included articles and relevant systematic reviews were searched to identify additional studies not found through the electronic search. Selection criteria We only considered published studies that evaluated the results of an index test (PCT, CRP, ESR) against the results of an acute‐phase  99 Tc‐dimercaptosuccinic acid (DMSA) scan (conducted within 30 days of the UTI) in children aged 0 to 18 years with a culture‐confirmed episode of UTI. The following cut‐off values were used for the primary analysis: 0.5 ng/mL for procalcitonin, 20 mg/L for CRP and 30 mm/hour for ESR. Data collection and analysis Two authors independently applied the selection criteria to all citations and independently abstracted data. We used the bivariate model to calculate pooled random‐effects pooled sensitivity and specificity values. Main results A total of 36 studies met our inclusion criteria. Twenty‐five studies provided data for the primary analysis: 12 studies (1000 children) included data on PCT, 16 studies (1895 children) included data on CRP, and eight studies (1910 children) included data on ESR (some studies had data on more than one test). The summary sensitivity estimates (95% CI) for the PCT, CRP, ESR tests at the aforementioned cut‐offs were 0.81 (0.67 to 0.90), 0.93 (0.86 to 0.96), and 0.83 (0.71 to 0.91), respectively. The summary specificity values for PCT, CRP, and ESR tests at these cut‐offs were 0.76 (0.66 to 0.84), 0.37 (0.24 to 0.53), and 0.57 (0.41 to 0.72), respectively. Authors' conclusions The ESR test does not appear to be sufficiently accurate to be helpful in differentiating children with cystitis from children with pyelonephritis. A low CRP value (< 20 mg/L) appears to be somewhat useful in ruling out pyelonephritis (decreasing the probability of pyelonephritis to < 20%), but unexplained heterogeneity in the data prevents us from making recommendations at this time. The procalcitonin test seems better suited for ruling in pyelonephritis, but the limited number of studies and the marked heterogeneity between studies prevents us from reaching definitive conclusions. Thus, at present, we do not find any compelling evidence to recommend the routine use of any of these tests in clinical practice. Plain language summary Procalcitonin, C‐reactive protein, and erythrocyte sedimentation rate for the diagnosis of acute pyelonephritis in children What is the issue? In some children with urinary tract infection (UTI), the infection is localized to the bladder (lower urinary tract). In others, bacteria ascend from the bladder to the kidney (upper urinary tract). Only children with upper urinary tract involvement are at risk for developing permanent kidney damage. If non‐invasive biomarkers could accurately differentiate children with lower urinary tract disease from children with upper urinary tract disease, treatment and follow‐up could potentially be individualized. What did we do? We examined the usefulness of three widely available blood tests (procalcitonin, C‐reactive protein, erythrocyte sedimentation rate) in differentiating upper from lower urinary tract disease. We found 34 relevant studies of which 24 provided data for our primary outcome. Twelve studies (1000 children) provided data for the procalcitonin test; 16 studies (1895 children) provided data for the C‐reactive protein test, and 8 studies (1910 children) provided data for the erythrocyte sedimentation rate test. What did we find? We found all three tests to be sensitive (summary sensitivity values ranged from 81% to 93%), but not very specific (summary specificity values ranged from 37% to 76%). Conclusions None of the tests were accurate enough to allow clinicians to confidently differentiate upper from lower urinary tract disease.","9","John Wiley & Sons, Ltd","1465-1858","*Blood Sedimentation; Acute Disease; Biomarkers [blood]; C-Reactive Protein [*analysis]; Calcitonin [*blood]; Child; Cystitis [blood, *diagnosis]; Diagnosis, Differential; Humans; Procalcitonin [*blood]; Pyelonephritis [blood, complications, *diagnosis]; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Urinary Tract Infections [blood]","10.1002/14651858.CD009185.pub3","http://dx.doi.org/10.1002/14651858.CD009185.pub3","Kidney and Transplant"
"CD013031.PUB2","Chan, KK; Joo, DA; McRae, AD; Takwoingi, Y; Premji, ZA; Lang, E; Wakai, A","Chest ultrasonography versus supine chest radiography for diagnosis of pneumothorax in trauma patients in the emergency department","Cochrane Database of Systematic Reviews","2020","Abstract - Background Chest X‐ray (CXR) is a longstanding method for the diagnosis of pneumothorax but chest ultrasonography (CUS) may be a safer, more rapid, and more accurate modality in trauma patients at the bedside that does not expose the patient to ionizing radiation. This may lead to improved and expedited management of traumatic pneumothorax and improved patient safety and clinical outcomes. Objectives To compare the diagnostic accuracy of chest ultrasonography (CUS) by frontline non‐radiologist physicians versus chest X‐ray (CXR) for diagnosis of pneumothorax in trauma patients in the emergency department (ED). To investigate the effects of potential sources of heterogeneity such as type of CUS operator (frontline non‐radiologist physicians), type of trauma (blunt vs penetrating), and type of US probe on test accuracy. Search methods We conducted a comprehensive search of the following electronic databases from database inception to 10 April 2020: Cochrane Database of Systematic Reviews, Cochrane Central Register of Controlled Trials, MEDLINE, Embase, Cumulative Index to Nursing and Allied Health Literature (CINAHL) Plus, Database of Abstracts of Reviews of Effects, Web of Science Core Collection and Clinicaltrials.gov. We handsearched reference lists of included articles and reviews retrieved via electronic searching; and we carried out forward citation searching of relevant articles in Google Scholar and looked at the ""Related articles"" on PubMed. Selection criteria We included prospective, paired comparative accuracy studies comparing CUS performed by frontline non‐radiologist physicians to supine CXR in trauma patients in the emergency department (ED) suspected of having pneumothorax, and with computed tomography (CT) of the chest or tube thoracostomy as the reference standard. Data collection and analysis Two review authors independently extracted data from each included study using a data extraction form. We included studies using patients as the unit of analysis in the main analysis and we included those using lung fields in the secondary analysis. We performed meta‐analyses by using a bivariate model to estimate and compare summary sensitivities and specificities. Main results We included 13 studies of which nine (410 traumatic pneumothorax patients out of 1271 patients) used patients as the unit of analysis; we thus included them in the primary analysis. The remaining four studies used lung field as the unit of analysis and we included them in the secondary analysis. We judged all studies to be at high or unclear risk of bias in one or more domains, with most studies (11/13, 85%) being judged at high or unclear risk of bias in the patient selection domain. There was substantial heterogeneity in the sensitivity of supine CXR amongst the included studies. In the primary analysis, the summary sensitivity and specificity of CUS were 0.91 (95% confidence interval (CI) 0.85 to 0.94) and 0.99 (95% CI 0.97 to 1.00); and the summary sensitivity and specificity of supine CXR were 0.47 (95% CI 0.31 to 0.63) and 1.00 (95% CI 0.97 to 1.00). There was a significant difference in the sensitivity of CUS compared to CXR with an absolute difference in sensitivity of 0.44 (95% CI 0.27 to 0.61; P < 0.001). In contrast, CUS and CXR had similar specificities: comparing CUS to CXR, the absolute difference in specificity was −0.007 (95% CI −0.018 to 0.005, P = 0.35). The findings imply that in a hypothetical cohort of 100 patients if 30 patients have traumatic pneumothorax (i.e. prevalence of 30%), CUS would miss 3 (95% CI 2 to 4) cases (false negatives) and overdiagnose 1 (95% CI 0 to 2) of those without pneumothorax (false positives); while CXR would miss 16 (95% CI 11 to 21) cases with 0 (95% CI 0 to 2) overdiagnosis of those who do not have pneumothorax. Authors' conclusions The diagnostic accuracy of CUS performed by frontline non‐radiologist physicians for the diagnosis of pneumothorax in ED trauma patients is superior to supine CXR, independent of the type of trauma, type of CUS operator, or type of CUS probe used. These findings suggest that CUS for the diagnosis of traumatic pneumothorax could be incorporated into trauma protocols and algorithms in future medical training programmes. In addition, CUS may beneficially change routine management of trauma Plain language summary How accurate is chest ultrasonography compared to supine chest radiography for diagnosis of traumatic pneumothorax in the emergency department? Why is improving the diagnosis of traumatic pneumothorax important? Air that collects between the lung and the chest wall is described as a pneumothorax. Pneumothorax can cause collapse of the lung, change the position of the heart and other structures in the chest, reduce the blood flow back to the heart, and cause life‐threatening shock. Physicians may perform tube thoracostomy — a procedure with risk of complications such as haemorrhage, organ injury, and infection — to evacuate the air trapped. Not recognizing a pneumothorax (false negative (FN)) can lead to heart and lung failure and death. An incorrect diagnosis of a pneumothorax (false positive (FP)) may lead to inappropriate tube thoracostomy. What is the aim of this review? To determine how accurate chest ultrasonography (CUS) is compared to chest X‐ray (CXR) in diagnosing pneumothorax in trauma patients in the emergency department (ED). Researchers included 13 studies to answer this question. What was studied in the review? We compared the diagnostic accuracy of two tests, CUS and CXR. We then compared these two tests to computed tomography (CT) or, if clinically necessary, tube thoracostomy as the reference standard. What are the main results of the review? The analysis included results from 1271 trauma patients, where 410 had traumatic pneumothorax. The results of these studies indicate that, in theory, if CUS was used on a group of 100 patients where 30 (30%) have traumatic pneumothorax, then an estimated 28 would have a CUS result positive for pneumothorax (TP) and of these one (3.6%) would be incorrectly classified as having the pneumothorax (FP); of the 72 patients with a result negative for pneumothorax, three (4.2%) would actually have a pneumothorax (FN). In theory, if CXR was used on a group of 100 patients where 30 (30%) have traumatic pneumothorax, then an estimated 14 would have a CXR result positive for pneumothorax (TP) and of these none (0%) would be incorrectly classified as having the pneumothorax (FP); of the 86 patients with a result negative for pneumothorax, 16 (18.6%) would actually have a pneumothorax (FN). How reliable are the results of the studies in this review? The numbers shown in the results are averages across all studies in the review. While CUS results were fairly consistent, CXR results were quite varied; thus, we cannot be sure that CXR will always produce the same results. In the included studies, the diagnosis of traumatic pneumothorax was confirmed by CT or tube thoracostomy. Although there were some problems with how some of the studies were conducted, their results did not differ from the more reliable studies. Who do the results of this review apply to? The results may not be representative of patients in different settings or with pneumothorax of different aetiologies. Studies included in the review were focused on diagnosing traumatic pneumothorax in the ED, conducted in three continents. The average prevalence of traumatic pneumothorax was 30% and ranged from 21% to 52%. What are the implications of this review? The studies in this review show that CUS is more accurate than CXR in diagnosing pneumothorax in ED trauma patients, which may lead to more timely treatment with tube thoracostomy, reducing pneumothorax‐related complications, and improving outcomes. The risk of missing the diagnosis with CUS is low (4.2% of those whose CUS suggests they do not have a pneumothorax) suggesting that only a few patients may not immediately receive tube thoracostomy. The risk of incorrectly diagnosing traumatic pneumothorax using CUS is low (3.6% of those whose CUS suggests they have a pneumothorax) and may result in receiving unnecessary tube thoracostomy. In comparison, the risk of missing a traumatic pneumothorax with CXR is high (18.6% of those whose CXR suggests they do not have a pneumothorax) suggesting that a large number of patients may not immediately receive tube thoracostomy. The risk of wrongly diagnosing traumatic pneumothorax using CXR is low (0% of those whose CUS suggests they have a pneumothorax). How up to date is this review? The review authors searched for and included publications from 1900 to 10 April 2020.","7","John Wiley & Sons, Ltd","1465-1858","*Supine Position; Bias; Confidence Intervals; Emergency Service, Hospital; Humans; Pneumothorax [*diagnostic imaging, etiology]; Prospective Studies; Radiography, Thoracic [*methods]; Sensitivity and Specificity; Thoracic Injuries [*complications]; Ultrasonography [*methods]; Wounds, Nonpenetrating [complications]; Wounds, Penetrating [complications]","10.1002/14651858.CD013031.pub2","http://dx.doi.org/10.1002/14651858.CD013031.pub2","Emergency and Critical Care"
"CD009833.PUB2","White, SK; Schmidt, RL; Walker, BS; Hanson, KE","(1→3)‐β‐D‐glucan testing for the detection of invasive fungal infections in immunocompromised or critically ill people","Cochrane Database of Systematic Reviews","2020","Abstract - Background Invasive fungal infections (IFIs) are life‐threatening opportunistic infections that occur in immunocompromised or critically ill people. Early detection and treatment of IFIs is essential to reduce morbidity and mortality in these populations. (1→3)‐β‐D‐glucan (BDG) is a component of the fungal cell wall that can be detected in the serum of infected individuals. The serum BDG test is a way to quickly detect these infections and initiate treatment before they become life‐threatening. Five different versions of the BDG test are commercially available: Fungitell, Glucatell, Wako, Fungitec‐G, and Dynamiker Fungus. Objectives To compare the diagnostic accuracy of commercially available tests for serum BDG to detect selected invasive fungal infections (IFIs) among immunocompromised or critically ill people. Search methods We searched MEDLINE (via Ovid) and Embase (via Ovid) up to 26 June 2019. We used SCOPUS to perform a forward and backward citation search of relevant articles. We placed no restriction on language or study design. Selection criteria We included all references published on or after 1995, which is when the first commercial BDG assays became available. We considered published, peer‐reviewed studies on the diagnostic test accuracy of BDG for diagnosis of fungal infections in immunocompromised people or people in intensive care that used the European Organization for Research and Treatment of Cancer (EORTC) criteria or equivalent as a reference standard. We considered all study designs (case‐control, prospective consecutive cohort, and retrospective cohort studies). We excluded case studies and studies with fewer than ten participants. We also excluded animal and laboratory studies. We excluded meeting abstracts because they provided insufficient information. Data collection and analysis We followed the standard procedures outlined in the  Cochrane Handbook for Diagnostic Test Accuracy Reviews . Two review authors independently screened studies, extracted data, and performed a quality assessment for each study. For each study, we created a 2 × 2 matrix and calculated sensitivity and specificity, as well as a 95% confidence interval (CI). We evaluated the quality of included studies using the Quality Assessment of Studies of Diagnostic Accuracy‐Revised (QUADAS‐2). We were unable to perform a meta‐analysis due to considerable variation between studies, with the exception of  Candida , so we have provided descriptive statistics such as receiver operating characteristics (ROCs) and forest plots by test brand to show variation in study results. Main results We included in the review 49 studies with a total of 6244 participants. About half of these studies (24/49; 49%) were conducted with people who had cancer or hematologic malignancies. Most studies (36/49; 73%) focused on the Fungitell BDG test. This was followed by Glucatell (5 studies; 10%), Wako (3 studies; 6%), Fungitec‐G (3 studies; 6%), and Dynamiker (2 studies; 4%). About three‐quarters of studies (79%) utilized either a prospective or a retrospective consecutive study design; the remainder used a case‐control design. Based on the manufacturer's recommended cut‐off levels for the Fungitell test, sensitivity ranged from 27% to 100%, and specificity from 0% to 100%. For the Glucatell assay, sensitivity ranged from 50% to 92%, and specificity ranged from 41% to 94%. Limited studies have used the Dynamiker, Wako, and Fungitec‐G assays, but individual sensitivities and specificities ranged from 50% to 88%, and from 60% to 100%, respectively. Results show considerable differences between studies, even by manufacturer, which prevented a formal meta‐analysis. Most studies (32/49; 65%) had no reported high risk of bias in any of the QUADAS‐2 domains. The QUADAS‐2 domains that had higher risk of bias included participant selection and flow and timing. Authors' conclusions We noted considerable heterogeneity between studies, and these differences precluded a formal meta‐analysis. Because of wide variation in the results, it is not possible to estimate the diagnostic accuracy of the BDG test in specific settings. Future studies estimating the accuracy of BDG tests should be linked to the way the test is used in clinical practice and should clearly describe the sampling protocol and the relationship of time of testing to time of diagnosis. Plain language summary Measurement of β‐D‐glucans to detect invasive fungal infection in immunocompromised people Why is improving the diagnosis of invasive fungal infections important?  Fungal infections occur in people who are unable to fight infection, and these infections can be life‐threatening in this group of people. Fungal infections are difficult to diagnose. Failure to recognize a fungal infection when it is present (a false‐negative test result) leads to delayed treatment and poorer outcomes. An incorrect diagnosis of infection (a false‐positive result) may result in wasted resources and unnecessary investigation and treatment. What is the aim of this review?  The aim of this review is to find out how accurate a blood test is for diagnosis of fungal infections in people who are unable to fight infection. Review authors included 49 studies to answer this question. What was studied in this review?  Five kinds of blood tests were compared. All of these tests use similar biochemical methods to detect the presence of a sugar molecule (β‐D‐glucan) that is a component of the fungal cell wall. This molecule does not normally occur in blood, so its detection indicates that fungi are present. The tests require a blood sample, which is then sent to a laboratory for analysis. Diagnosis of fungal infections is difficult, and the diagnosis is often made only after the disease has advanced. Blood tests can provide an earlier diagnosis, so they would offer an advantage over current methods. What are the main results of the review?  This review included studies of 6244 people who were at risk of getting fungal infections. Study results show that accuracy varied widely across studies. The variation was so great that it was not possible to obtain a reliable estimate of the accuracy of the various tests. How reliable are results of the studies in this review?  In the included studies, the diagnosis of invasive fungal infection was made using criteria developed by the European Organization for Research and Treatment of Cancer (EORTC)*. The EORTC criteria are considered reliable and the studies were generally well conducted, so it is likely that the reference diagnoses were accurate. Accuracy of blood tests for invasive fungal infections varied widely. Some studies found that the blood test was accurate, but others found that the blood test was not very accurate. The reason for this variation is not understood. *The EORTC criteria provide the reference diagnosis. Results of the blood test are compared to the reference diagnosis. Who do the results of this review apply to?  Most included studies were performed at academic medical centers or public hospitals in the United States, Germany, and Italy. The most common underlying conditions were cancer (47%) and admission to intensive care (33%). A majority of participants were adults. The overall prevalence of invasive fungal infection was 28%. What are the implications of this review?  Accuracy of the diagnosis varied widely across studies. It is not clear whether testing can accurately detect invasive fungal infections. Testing accurately detects disease in some studies, but in others it does not. The reasons for the variation in accuracy are not understood. How up‐to‐date is this review?  The review authors searched for and reviewed studies published up to June 2019.","7","John Wiley & Sons, Ltd","1465-1858","*Critical Illness; *Immunocompromised Host; Aspergillosis [diagnosis]; Biomarkers [blood]; Candidiasis, Invasive [diagnosis]; Case-Control Studies; Humans; Invasive Fungal Infections [*diagnosis]; Pneumocystis Infections [diagnosis]; Pneumocystis carinii; Prospective Studies; ROC Curve; Retrospective Studies; Sensitivity and Specificity; beta-Glucans [*blood]","10.1002/14651858.CD009833.pub2","http://dx.doi.org/10.1002/14651858.CD009833.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD013459.PUB2","Jullien, S; Dissanayake, HA; Chaplin, M","Rapid diagnostic tests for plague","Cochrane Database of Systematic Reviews","2020","Abstract - Background Plague is a severe disease associated with high mortality. Late diagnosis leads to advance stage of the disease with worse outcomes and higher risk of spread of the disease. A rapid diagnostic test (RDT) could help in establishing a prompt diagnosis of plague. This would improve patient care and help appropriate public health response. Objectives To determine the diagnostic accuracy of the RDT based on the antigen F1 (F1RDT) for detecting plague in people with suspected disease. Search methods We searched the CENTRAL, Embase, Science Citation Index, Google Scholar, the World Health Organization International Clinical Trials Registry Platform and ClinicalTrials.gov up to 15 May 2019, and PubMed (MEDLINE) up to 27 August 2019, regardless of language, publication status, or publication date. We handsearched the reference lists of relevant papers and contacted researchers working in the field. Selection criteria We included cross‐sectional studies that assessed the accuracy of the F1RDT for diagnosing plague, where participants were tested with both the F1RDT and at least one reference standard. The reference standards were bacterial isolation by culture, polymerase chain reaction (PCR), and paired serology (this is a four‐fold difference in F1 antibody titres between two samples from acute and convalescent phases). Data collection and analysis Two review authors independently selected studies and extracted data. We appraised the methodological quality of each selected studies and applicability by using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. When meta‐analysis was appropriate, we used the bivariate model to obtain pooled estimates of sensitivity and specificity. We stratified all analyses by the reference standard used and presented disaggregated data for forms of plague. We assessed the certainty of the evidence using GRADE. Main results We included eight manuscripts reporting seven studies. Studies were conducted in three countries in Africa among adults and children with any form of plague. All studies except one assessed the F1RDT produced at the Institut Pasteur of Madagascar (F1RDT‐IPM) and one study assessed a F1RDT produced by New Horizons (F1RDT‐NH), utilized by the US Centers for Disease Control and Prevention. We could not pool the findings from the F1RDT‐NH in meta‐analyses due to a lack of raw data and a threshold of the test for positivity different from the F1RDT‐IPM. Risk of bias was high for participant selection (retrospective studies, recruitment of participants not consecutive or random, unclear exclusion criteria), low or unclear for index test (blinding of F1RDT interpretation unknown), low for reference standards, and high or unclear for flow and timing (time of sample transportation was longer than seven days, which can lead to decreased viability of the pathogen and overgrowth of contaminating bacteria, with subsequent false‐negative results and misclassification of the target condition). F1RDT for diagnosing all forms of plague F1RDT‐IPM pooled sensitivity against culture was 100% (95% confidence interval (CI) 82 to 100; 4 studies, 1692 participants; very low certainty evidence) and pooled specificity was 70.3% (95% CI 65 to 75; 4 studies, 2004 participants; very low‐certainty evidence). The performance of F1RDT‐IPM against PCR was calculated from a single study in participants with bubonic plague (see below). There were limited data on the performance of F1RDT against paired serology. F1RDT for diagnosing pneumonic plague Performed in sputum, F1RDT‐IPM pooled sensitivity against culture was 100% (95% CI 0 to 100; 2 studies, 56 participants; very low‐certainty evidence) and pooled specificity was 71% (95% CI 59 to 80; 2 studies, 297 participants; very low‐certainty evidence). There were limited data on the performance of F1RDT against PCR or against paired serology for diagnosing pneumonic plague. F1RDT for diagnosing bubonic plague Performed in bubo aspirate, F1RDT‐IPM pooled sensitivity against culture was 100% (95% CI not calculable; 2 studies, 1454 participants; low‐certainty evidence) and pooled specificity was 67% (95% CI 65 to 70; 2 studies, 1198 participants; very low‐certainty evidence). Performed in bubo aspirate, F1RDT‐IPM pooled sensitivity against PCR for the  caf1  gene was 95% (95% CI 89 to 99; 1 study, 88 participants; very low‐certainty evidence) and pooled specificity was 93% (95% CI 84 to 98; 1 study, 61 participants; very low‐certainty evidence). There were no data providing data on both F1RDT and paired serology for diagnosing bubonic plague. Authors' conclusions Against culture, the F1RDT appeared highly sensitive for diagnosing either pneumonic or bubonic plague, and can help detect plague in remote areas to assure management and enable a public health response. False positive results mean culture or PCR confirmation may be needed. F1RDT does not replace culture, which provides additional information on resistance to antibiotics and bacterial strains. Plain language summary Rapid diagnostic tests for plague Why is improving diagnosis of plague important? Plague is a severe disease associated with high death rates. Pneumonic plague mainly affects the lungs, while bubonic plague present with painful swellings. Not recognizing plague early may result in delayed diagnosis and treatment associated with advanced illness and death, and increased disease spread. A rapid diagnostic test (RDT) could help prompt diagnosis of plague, especially in low‐resource settings. This would improve patient care and help appropriate response to avoid the disease spread. What is the aim of this review? To assess the accuracy of the F1RDT for detecting plague in people with suspected plague. What was studied in this review? F1RDT is a test that detects the F1 antigen, which is part of the outer surface of  Yersinia pestis , the bacteria causing plague. The test is simple to perform and provides a result within 15 minutes. It can be performed in the pus contained in the buboes (swellings), or in the sputum (mucous coughed up from the respiratory tract) of people with suspected pneumonic plague. We measured the results of F1RDT against culture, molecular test, or serological tests. What are the main results? Seven studies (reported in eight manuscripts) provided findings of F1RDT used in people with suspected plague in three African countries. For any form of plague and when compared to culture, F1RDT registered positive in 100% (sensitivity, which measures a test's ability to correctly identify a positive result for the disease) of people who had plague and registered negative in 70% of people who actually did not have plague (specificity, which measures a test's ability to correctly generate a negative result for people who do not have the condition that is being tested for). For pneumonic plague, sensitivity was 100% and specificity 71% compared to culture. For bubonic plague, sensitivity was 100% and specificity 67% compared to culture. Compared to a molecular test for bubonic plague, sensitivity was 95% and specificity 93%. How confident are we in the review ' s results? Overall, the reliability of the evidence was very low. Results should be interpreted with caution. There were concerns about the quality of the methodology of all included studies. Also, culture might not work well as a reference standard (comparator) when people received antibiotics before sample collection for testing. What do the results mean? In a hypothetical population of 1000 people: • with symptoms of pneumonic plague where 40 of them have the disease confirmed by culture, the utilization of F1RDT would result in: 318 people to be F1RDT‐positive, of which 278 would not have pneumonic plague (called false positives); and 682 people to be F1RDT‐negative, of which none would have pneumonic plague (called false negatives). • with symptoms of bubonic plague where 40 of them have the disease confirmed by culture, the utilization of F1RDT would result in: 357 people to be F1RDT‐positive, of which 317 would not have bubonic plague (false positives); and 643 people to be F1RDT‐negative, of which none would have bubonic plague (false negatives). • with symptoms of bubonic plague where 40 of them have the disease confirmed by molecular test, the utilization of F1RDT would result in: 105 people to be F1RDT‐positive, of which 67 would not have bubonic plague (false positives); and 895 people to be F1RDT‐negative, of which two would have bubonic plague (false negatives). Who do the review ' s results apply to? Adults and children with suspected bubonic or pneumonic plague. What are the implications of this review? F1RDT appears to be highly sensitive for pneumonic or bubonic plague. As a simple test that can be performed at a patient's bedside in remote and low‐resource areas, F1RDT can assist with plague diagnosis for early management, and appropriate preventive measures to avoid spread of the disease. The number of false positives (people with a positive F1RDT but who do not have plague) indicate that F1RDT may need to be combined with other laboratory evaluations (culture or molecular test) to confirm the diagnosis of plague. F1RDT does not replace culture, which provides additional information on resistance to antibiotics and bacterial strains. How up‐to‐date is this review? The review authors searched for studies up to 15 May 2019.","6","John Wiley & Sons, Ltd","1465-1858","Adult; Antigens, Bacterial [*analysis]; Child; Confidence Intervals; Cross-Sectional Studies; False Negative Reactions; False Positive Reactions; Humans; Plague [*diagnosis, immunology]; Sensitivity and Specificity; Time Factors; Yersinia pestis [*immunology]","10.1002/14651858.CD013459.pub2","http://dx.doi.org/10.1002/14651858.CD013459.pub2","Infectious Diseases"
"CD012824.PUB2","Iguchi, M; Noguchi, Y; Yamamoto, S; Tanaka, Y; Tsujimoto, H","Diagnostic test accuracy of jolt accentuation for headache in acute meningitis in the emergency setting","Cochrane Database of Systematic Reviews","2020","Abstract - Background Meningitis is inflammation of the meninges, the layers that protect the brain and spinal cord. Acute meningitis is an emergent disease that develops over the course of hours to several days. Delay in treatment can lead to serious outcomes. Inflammation of the meninges is assessed by analysing cerebrospinal fluid. Identifying the pathogen in cerebrospinal fluid is another way to diagnose meningitis. Cerebrospinal fluid is collected by doing a lumbar puncture, which is an invasive test, and can be avoided if a physical examination excludes the diagnosis of meningitis. However, most physical examinations, such as nuchal rigidity, Kernig's test, and Brudzinski's test, are not sufficiently sensitive to exclude meningitis completely. Jolt accentuation of headache is a new and less well‐recognised physical examination, which assesses meningeal irritation. It is judged as positive if the headache is exacerbated by rotating the head horizontally two or three times per second. A 1991 observational study initially reported high sensitivity of this examination to predict pleocytosis. Pleocytosis, an abnormally high cerebrospinal fluid sample white cell count, is an accepted indicator of nervous system infection or inflammation. Jolt accentuation of headache may therefore accurately rule out meningitis without the use of lumbar puncture. However, more recent cross‐sectional studies have reported variable diagnostic accuracy. Objectives To estimate the diagnostic accuracy of jolt accentuation of headache for detecting acute meningitis in emergency settings. Secondary objectives: to investigate the sources of heterogeneity, including study population, patient condition, and types of meningitis. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE (Ovid), and Embase (Elsevier) to 27 April 2020. We searched ClinicalTrials.gov, the World Health Organization International Clinical Trials Registry Platform, and Ichushi‐Web Version 5.0 to 28 April 2020. Selection criteria We included cross‐sectional studies that assessed the diagnostic accuracy of jolt accentuation of headache for people with suspected meningitis in emergency settings. We included participants of any age and any severity of illness. Meningitis should be diagnosed with any reference standard, such as cerebrospinal fluid pleocytosis, proof of causative agents, or autopsy. Data collection and analysis Two review authors independently collated study data. We assessed methodological quality of studies using QUADAS‐2 criteria. We used a bivariate random‐effects model to determine summary estimates of sensitivity and specificity where meta‐analysis was possible. We performed sensitivity analyses to validate the robustness of outcomes. We assessed the certainty of the evidence using the GRADE approach. Main results We included nine studies (1161 participants). Five studies included only adults. Four studies included both adults and children; however, the proportion was not reported in three of these studies. The youngest child reported in the studies was aged 13 years. There was no study including only children. The reference standard was pleocytosis in eight studies, and the combination of pleocytosis and increased protein in the cerebrospinal fluid in one study. Two studies also used smear or positive culture of cerebrospinal fluid. Risk of bias and concern about applicability was high in the participant selection domain for all included studies and the consciousness subgroup. Overall, pooled sensitivity was 65.3% (95% confidence interval (CI) 37.3 to 85.6), and pooled specificity was 70.4% (95% CI 47.7 to 86.1) (very low‐certainty evidence). We established the possibility of heterogeneity from visual inspection of forest plots. However, we were unable to conduct further analysis for study population, types of meningitis, and participants' condition, other than disturbance of consciousness (a secondary outcome). Amongst participants whose consciousness was undisturbed (8 studies, 921 participants), pooled sensitivity and specificity were 75.2% (95% CI 54.3 to 88.6) and 60.8% (95% CI 43.4 to 75.9), respectively (very low‐certainty evidence). Authors' conclusions Jolt accentuation for headache may exclude diagnoses of meningitis in emergency settings, but high‐quality evidence to support use of this test is lacking. Even where jolt accentuation of headache is negative, there is still the possibility of acute meningitis. This review identified the possibility of heterogeneity. However, factors that contribute to heterogeneity are incompletely understood, and should be considered in future research. Plain language summary Jolt accentuation of headache as a test for acute meningitis in emergency settings Why is the diagnosis of acute meningitis by physical examination important? Meningitis is inflammation of the tissue that protects the brain and spinal cord (the meninges). Acute meningitis, especially bacterial and tubercular meningitis, is potentially life‐threatening, and requires prompt diagnosis and early treatment. Diagnosis usually needs an analysis of cerebrospinal fluid collected by lumbar puncture. Lumbar puncture involves the insertion of a needle between the bones of the lumbar spine. Lumbar puncture is an invasive test that can cause headache. If physical examination can accurately exclude the possibility of acute meningitis, patients may be able to avoid having to undergo a lumbar puncture. However, traditional physical examinations of people with suspected meningitis, such as inability to flex the neck forward (nuchal rigidity), does not exclude acute meningitis. What is the aim of this review? We aimed to estimate how accurate jolt accentuation of headache is for diagnosis of acute meningitis in emergency settings. Jolt accentuation of headache is a more recent (1991) and less well‐recognised physical examination compared to other tests. Jolt accentuation involves making the headache worse by rotating the head horizontally two or three times per second. What was studied in this review? We studied jolt accentuation of headache in people who presented with potential acute meningitis in emergency settings. What are the main results of this review? We included nine studies involving 1161 participants who presented with potential acute meningitis. Five studies included only adults, and four studies included both adults and children. Due to lack of data, we could not perform separate analyses for adults and children. How confident are we in the results of the review? It appears that jolt accentuation of headache is not sensitive enough to exclude a diagnosis of acute meningitis. To whom do the review's results apply? People who present with potential acute meningitis. Most studies targeted emergency settings, therefore it is uncertain whether the test would work in primary care settings. Most studies included adults or adolescents; the youngest participant was aged 13 years. There is no evidence that this test is applicable for children. What are the implications of this review? Even where jolt accentuation of headache is negative, there is still the possibility of acute meningitis. How up‐to‐date is this review? We searched for studies published up to 27 April 2020.","6","John Wiley & Sons, Ltd","1465-1858","Acute Disease; Adolescent; Adult; Bias; Confidence Intervals; Critical Pathways; Disease Progression; Emergencies; False Negative Reactions; False Positive Reactions; Head Movements [*physiology]; Headache [cerebrospinal fluid, *etiology]; Humans; Leukocytosis [cerebrospinal fluid]; Meningitis [cerebrospinal fluid, complications, *diagnosis]; Physical Examination [*methods]; Rotation; Sensitivity and Specificity","10.1002/14651858.CD012824.pub2","http://dx.doi.org/10.1002/14651858.CD012824.pub2","Acute Respiratory Infections"
"CD012947.PUB2","Jindal, A; Ctori, I; Virgili, G; Lucenteforte, E; Lawrenson, JG","Non‐contact tests for identifying people at risk of primary angle closure glaucoma","Cochrane Database of Systematic Reviews","2020","Abstract - Background Primary angle closure glaucoma (PACG) accounts for 50% of glaucoma blindness worldwide. More than three‐quarters of individuals with PACG reside in Asia. In these populations, PACG often develops insidiously leading to chronically raised intraocular pressure and optic nerve damage, which is often asymptomatic. Non‐contact tests to identify people at risk of angle closure are relatively quick and can be carried out by appropriately trained healthcare professionals or technicians as a triage test. If the test is positive, the person will be referred for further specialist assessment. Objectives To determine the diagnostic accuracy of non‐contact tests (limbal anterior chamber depth (LACD) (van Herick test); oblique flashlight test; scanning peripheral anterior chamber depth analyser (SPAC), Scheimpflug photography; anterior segment optical coherence tomography (AS‐OCT), for identifying people with an occludable angle. Search methods We searched the following bibliographic databases 3 October 2019: CENTRAL; MEDLINE; Embase; BIOSIS; OpenGrey; ARIF and clinical trials registries. The searches were limited to remove case reports. There were no date or language restrictions in the searches. Selection criteria We included prospective and retrospective cross‐sectional, cohort and case‐control studies conducted in any setting that evaluated the accuracy of one or more index tests for identifying people with an occludable angle compared to a gonioscopic reference standard. Data collection and analysis Two review authors independently performed data extraction and quality assessment using QUADAS2 for each study. For each test, 2 x 2 tables were constructed and sensitivity and specificity were calculated. When four or more studies provided data at fixed thresholds for each test, we fitted a bivariate model using the METANDI function in STATA to calculate pooled point estimates for sensitivity and specificity. For comparisons between index tests and subgroups, we performed a likelihood ratio test comparing the model with and without the covariate. Main results We included 47 studies involving 26,151 participants and analysing data from 23,440. Most studies were conducted in Asia (36, 76.6%). Twenty‐seven studies assessed AS‐OCT (analysing 15,580 participants), 17 studies LACD (7385 participants), nine studies Scheimpflug photography (1616 participants), six studies SPAC (5239 participants) and five studies evaluated the oblique flashlight test (998 participants). Regarding study quality, 36 of the included studies (76.6%) were judged to have a high risk of bias in at least one domain.The use of a case‐control design (13 studies) or inappropriate exclusions (6 studies) raised patient selection concerns in 40.4% of studies and concerns in the index test domain in 59.6% of studies were due to lack of masking or post‐hoc determination of optimal thresholds. Among studies that did not use a case‐control design, 16 studies (20,599 participants) were conducted in a primary care/community setting and 18 studies (2590 participants) in secondary care settings, of which 15 investigated LACD. Summary estimates were calculated for commonly reported parameters and thresholds for each test; LACD ≤ 25% (16 studies, 7540 eyes): sensitivity 0.83 (95% confidence interval (CI) 0.74, 0.90), specificity 0.88 (95% CI 0.84, 0.92) (moderate‐certainty); flashlight (grade1) (5 studies, 1188 eyes): sensitivity 0.51 (95% CI 0.25, 0.76), specificity 0.92 (95% CI 0.70, 0.98) (low‐certainty); SPAC (≤ 5 and/or S or P) (4 studies, 4677 eyes): sensitivity 0.83 (95% CI 0.70, 0.91), specificity 0.78 (95% CI 0.70, 0.83) (moderate‐certainty); Scheimpflug photography (central ACD) (9 studies, 1698 eyes): sensitivity 0.92 (95% CI 0.84, 0.96), specificity 0.86 (95% CI 0.76, 0.93) (moderate‐certainty); AS‐OCT (subjective opinion of occludability) (13 studies, 9242 eyes): sensitivity 0.85 (95% CI 0.76, 0.91); specificity 0.71 (95% CI 0.62, 0.78) (moderate‐certainty). For comparisons of sensitivity and specificity between index tests we used LACD (≤ 25%) as the reference category. The flashlight test (grade 1 threshold) showed a statistically significant lower sensitivity than LACD (≤ 25%), whereas AS‐OCT (subjective judgement) had a statistically significant lower specificity. There were no statistically significant differences for the other index test comparisons. A subgroup analysis was conducted for LACD (≤ 25%), comparing community (7 studies, 14.4% prevalence) vs secondary care (7 studies, 42% prevalence) settings. We found no evidence of a statistically significant difference in test performance according to setting. Performing LACD on 1000 people at risk of angle closure with a prevalence of occludable angles of 10%, LACD would miss about 17 cases out of the 100 with occludable angles and incorrectly classify 108 out of 900 without angle closure. Authors' conclusions The finding that LACD performed as well as index tests that use sophisticated imaging technologies, confirms the potential for this test for case‐detection of occludable angles in high‐risk populations. However, methodological issues across studies may have led to our estimates of test accuracy being higher than would be expected in standard clinical practice. There is still a need for high‐quality studies to evaluate the performance of non‐invasive tests for angle assessment in both community‐based and secondary care settings. Plain language summary How accurate are screening tests in identifying those at risk of developing primary angle closure glaucoma? Why is improving the diagnosis of primary angle closure glaucoma important?  Glaucoma is a group of eye diseases that cause damage to the optic nerve at the back of the eye. If untreated, glaucoma can lead to blindness. Primary angle closure glaucoma is a type of glaucoma, where the drainage route for the fluid inside the eye (known as the angle) is narrowed or blocked, leading to raised eye pressure and loss of the field of vision. Primary angle closure glaucoma accounts for a quarter of all cases of glaucoma globally and it is more likely to lead to vision loss than the more common form, primary open angle glaucoma. A variety of non‐invasive tests are available to identify people at risk of primary angle closure glaucoma in a community or non‐specialist clinical setting. Those who test positive are referred for further specialist investigation and possible treatment. Failure to detect this condition (a false negative result) may result in an increased risk of progressive optic nerve damage and blindness. An incorrect diagnosis (a false positive result) could lead to unnecessary and costly investigation. What is the aim of this review?  The aim of this review was to find out how accurate non‐invasive screening tests are in identifying those at risk of developing primary angle closure glaucoma. What was studied in this review?   Five non‐invasive tests were studied. These range from simple tests that require either a pen torch or a widely available piece of clinical equipment known as a slit ‐lamp microscope (oblique flashlight test; limbal anterior chamber depth (LACD)) to more sophisticated imaging equipment (anterior segment optical coherence tomography (AS‐OCT), Scheimpflug photography and scanning peripheral anterior chamber depth analyser (SPAC)) that can scan and measure the dimensions of the drainage angle. What were the main results in this review?  The review included 47 relevant studies, with a total of 26,151 participants. Twenty‐seven studies assessed AS‐OCT, 17 studies assessed LACD, nine studies Scheimpflug photography, six studies SPAC and five studies evaluated the flashlight test. The overall diagnostic performance of LACD was similar to the more advanced imaging technologies, AS‐OCT, Scheimpflug photography and SPAC, however, the flashlight test showed an inferior performance. Using LACD as an example, if this test was performed on 1000 people, of whom 100 were at risk of primary angle closure, an estimated 83 would be correctly identified and 17 cases would be missed (false negatives). The test would correctly identify 792 of the 900 not at risk of angle closure glaucoma and incorrectly classify 108 (12%), who would be unnecessarily referred (false positives). How reliable were the results of the studies in this review?  Most studies were of low quality due to the way that the participants were recruited or how the tests were performed. This could have led to the tests appearing more accurate than what they really are. We can therefore not be sure that the tests will always produce the reported results. What are the implications of this review?  The studies included in this review were mostly conducted in Asia, which carries the greatest burden of primary angle closure glaucoma. The results of this review have shown that LACD, which is a quick and simple test that can be performed with a minimal amount of training, can identify people at risk of primary angle closure glaucoma, leading to early and appropriate treatment. Although this test could potentially miss approximately one in six of those at risk of the condition and lead to an over referral of 12%, the test could be useful for targeted screening in areas with a high prevalence of the condition. How up to date is this review?  Evidence in this review is current to 3 October 2019.","5","John Wiley & Sons, Ltd","1465-1858","*Diagnostic Techniques, Ophthalmological [statistics & numerical data]; Anterior Chamber [diagnostic imaging]; Bias; Case-Control Studies; Confidence Intervals; Glaucoma, Angle-Closure [*diagnosis]; Gonioscopy [standards]; Humans; Patient Selection; Photography [methods, statistics & numerical data]; Prospective Studies; Reference Standards; Retrospective Studies; Sensitivity and Specificity; Tomography, Optical Coherence; Triage [*methods]","10.1002/14651858.CD012947.pub2","http://dx.doi.org/10.1002/14651858.CD012947.pub2","Eyes and Vision"
"CD010159.PUB2","Verbeek, HHG; de Groot, JWB; Sluiter, WJ; Muller Kobold, AC; van den Heuvel, ER; Plukker, JTM; Links, TP","Calcitonin testing for detection of medullary thyroid cancer in people with thyroid nodules","Cochrane Database of Systematic Reviews","2020","Abstract - Background Thyroid nodules are very common in general medical practice, but rarely turn out to be a medullary thyroid carcinoma (MTC). Calcitonin is a sensitive tumour marker for the detection of MTC (basal calcitonin). Sometimes a stimulation test is used to improve specificity (stimulated calcitonin). Although the European Thyroid Association's guideline advocates calcitonin determination in people with thyroid nodules, the role of routine calcitonin testing in individuals with thyroid nodules is still questionable. Objectives The objective of this review was to determine the diagnostic accuracy of basal and/or stimulated calcitonin as a triage or add‐on test for detection of MTC in people with thyroid nodules. Search methods We searched CENTRAL ,  MEDLINE, Embase and Web of Science from inception to June 2018. Selection criteria We included all retrospective and prospective cohort studies in which all participants with thyroid nodules had undergone determination of basal calcitonin levels (and stimulated calcitonin, if performed). Data collection and analysis Two review authors independently scanned all retrieved records. We extracted data using a standard data extraction form. We assessed risk of bias and applicability using the QUADAS‐2 tool. Using the hierarchical summary receiver operating characteristic (HSROC) model, we estimated summary curves across different thresholds and also obtained summary estimates of sensitivity and specificity at a common threshold when possible. Main results In 16 studies, we identified 72,368 participants with nodular thyroid disease in whom routinely calcitonin testing was performed. All included studies performed the calcitonin test as a triage test. Median prevalence of MTC was 0.32%. Sensitivity in these studies ranged between 83% and 100% and specificity ranged between 94% and 100%. An important limitation in 15 of the 16 studies (94%) was the absence of adequate reference standards and follow‐up in calcitonin‐negative participants. This resulted in a high risk of bias with regard to flow and timing in the methodological quality assessment. At the median specificity of 96.6% from the included studies, the estimated sensitivity (95% confidence interval (CI)) from the summary curve was 99.7% ( 68.8% to 100%). For the median prevalence of MTC of 0.23%, the positive predictive value (PPV) for basal calcitonin testing at a threshold of 10 pg/mL was 7.7% (4.9% to 12.1%). Summary estimates of sensitivity and specificity for the threshold of 10 pg/mL of basal calcitonin testing was 100% (95% CI 99.7 to 100) and 97.2% (95% CI 95.9 to 98.6), respectively. For combined basal and stimulated calcitonin testing, sensitivity ranged between 82% and 100% with specificity between 99% and 100%. The median specificity was 99.8% with an estimated sensitivity of 98.8% (95% CI 65.8 to 100) . Authors' conclusions Both basal and combined basal and stimulated calcitonin testing have a high sensitivity and specificity. However, this may be an overestimation due to high risk of bias in the use and choice of reference standard The value of routine testing in patients with thyroid nodules remains questionable, due to the low prevalence, which results in a low PPV of basal calcitonin testing. Whether routine calcitonin testing improves prognosis in MTC patients remains unclear. Plain language summary Calcitonin testing for detection of medullary thyroid cancer in patients with thyroid nodules Review question What is the value of the calcitonin test for the diagnosis of medullary thyroid cancer in people with a thyroid nodule? Background Thyroid nodules are very common in the general population. In some people this nodule turns out to be a medullary thyroid carcinoma, which is a rare tumour of the thyroid gland. Calcitonin is one of the hormones produced by the thyroid, but in a large proportion of patients with medullary thyroid cancer the calcitonin level is increased. It can therefore be used as a sensitive tumour marker. In certain cases the production of calcitonin by the tumour can be stimulated in a stimulation test, to differentiate more accurately between calcitonin production by the tumour or other causes. However, there is no consensus if calcitonin testing should be routinely used in all people who have a thyroid nodule. We evaluated the available literature to address the accuracy of calcitonin testing in people with thyroid nodules for detection of medullary thyroid carcinoma. Study characteristics We searched for evidence in the literature until June 2018 and identified a total of 16 studies. Studies were included if a routine calcitonin test (with or without the stimulation test) was performed in all included people with thyroid nodular disease. Key results In total 72,638 people with thyroid nodular disease were enrolled in the analysed studies, of which 187 had medullary thyroid carcinoma. Our findings indicate that both basal and stimulated calcitonin testing are able to detect nearly all people with medullary thyroid carcinoma. However, because medullary thyroid carcinoma is very rare in persons with a thyroid nodule, there is large chance that calcitonin levels are false positives (i.e. the test indicates the disease, whereas in fact there is none). In practice this means that for every 10,000 persons with thyroid nodular disease, 23 persons will have medullary thyroid carcinoma. Of these, none will be missed using a basal calcitonin threshold of 10 pg/mL, while 280 people will have a false‐positive test result. This might lead to unnecessary surgery of the thyroid with the need for life‐long thyroid hormone supplementation and risk of complications. With the use of a stimulation test the chance of a false‐positive test result may be reduced, however due to lack of sufficient studies this could not be calculated. Certainty of the evidence The certainty of the evidence is importantly limited, because almost all studies did not report adequately on the outcome of people who had a negative calcitonin test. A number of patients who had medullary thyroid carcinoma were possibly not identified. The diagnostic accuracy can already be markedly affected when a small number of patients is missed because medullary thyroid carcinoma is very rare. Conclusion Based on the available literature, there is insufficient evidence for a routine calcitonin test in all people with a thyroid nodule. Further studies are needed, with also adequate reporting of the people who have a negative calcitonin test, to determine the role of the calcitonin test in people with thyroid nodules for detection of medullary thyroid carcinoma.","3","John Wiley & Sons, Ltd","1465-1858","Biomarkers, Tumor [blood]; Calcitonin [*blood]; Carcinoma, Medullary [*blood, diagnosis]; Carcinoma, Neuroendocrine [*blood, diagnosis]; Diagnosis, Differential; Humans; Randomized Controlled Trials as Topic; Thyroid Neoplasms [*blood, diagnosis]; Thyroid Nodule [blood, diagnosis]","10.1002/14651858.CD010159.pub2","http://dx.doi.org/10.1002/14651858.CD010159.pub2","Metabolic and Endocrine Disorders"
"CD009628.PUB2","Lombardi, G; Crescioli, G; Cavedo, E; Lucenteforte, E; Casazza, G; Bellatorre, AG; Lista, C; Costantino, G; Frisoni, G; Virgili, G; Filippini, G","Structural magnetic resonance imaging for the early diagnosis of dementia due to Alzheimer's disease in people with mild cognitive impairment","Cochrane Database of Systematic Reviews","2020","Abstract - Background Mild cognitive impairment (MCI) due to Alzheimer's disease is the symptomatic predementia phase of Alzheimer's disease dementia, characterised by cognitive and functional impairment not severe enough to fulfil the criteria for dementia. In clinical samples, people with amnestic MCI are at high risk of developing Alzheimer's disease dementia, with annual rates of progression from MCI to Alzheimer's disease estimated at approximately 10% to 15% compared with the base incidence rates of Alzheimer's disease dementia of 1% to 2% per year. Objectives To assess the diagnostic accuracy of structural magnetic resonance imaging (MRI) for the early diagnosis of dementia due to Alzheimer's disease in people with MCI versus the clinical follow‐up diagnosis of Alzheimer's disease dementia as a reference standard (delayed verification). To investigate sources of heterogeneity in accuracy, such as the use of qualitative visual assessment or quantitative volumetric measurements, including manual or automatic (MRI) techniques, or the length of follow‐up, and age of participants. MRI was evaluated as an add‐on test in addition to clinical diagnosis of MCI to improve early diagnosis of dementia due to Alzheimer's disease in people with MCI. Search methods On 29 January 2019 we searched Cochrane Dementia and Cognitive Improvement's Specialised Register and the databases, MEDLINE, Embase, BIOSIS Previews, Science Citation Index, PsycINFO, and LILACS. We also searched the reference lists of all eligible studies identified by the electronic searches. Selection criteria We considered cohort studies of any size that included prospectively recruited people of any age with a diagnosis of MCI. We included studies that compared the diagnostic test accuracy of baseline structural MRI versus the clinical follow‐up diagnosis of Alzheimer's disease dementia (delayed verification). We did not exclude studies on the basis of length of follow‐up. We included studies that used either qualitative visual assessment or quantitative volumetric measurements of MRI to detect atrophy in the whole brain or in specific brain regions, such as the hippocampus, medial temporal lobe, lateral ventricles, entorhinal cortex, medial temporal gyrus, lateral temporal lobe, amygdala, and cortical grey matter. Data collection and analysis Four teams of two review authors each independently reviewed titles and abstracts of articles identified by the search strategy. Two teams of two review authors each independently assessed the selected full‐text articles for eligibility, extracted data and solved disagreements by consensus. Two review authors independently assessed the quality of studies using the QUADAS‐2 tool. We used the hierarchical summary receiver operating characteristic (HSROC) model to fit summary ROC curves and to obtain overall measures of relative accuracy in subgroup analyses. We also used these models to obtain pooled estimates of sensitivity and specificity when sufficient data sets were available. Main results We included 33 studies, published from 1999 to 2019, with 3935 participants of whom 1341 (34%) progressed to Alzheimer's disease dementia and 2594 (66%) did not. Of the participants who did not progress to Alzheimer's disease dementia, 2561 (99%) remained stable MCI and 33 (1%) progressed to other types of dementia. The median proportion of women was 53% and the mean age of participants ranged from 63 to 87 years (median 73 years). The mean length of clinical follow‐up ranged from 1 to 7.6 years (median 2 years). Most studies were of poor methodological quality due to risk of bias for participant selection or the index test, or both. Most of the included studies reported data on the volume of the total hippocampus (pooled mean sensitivity 0.73 (95% confidence interval (CI) 0.64 to 0.80); pooled mean specificity 0.71 (95% CI 0.65 to 0.77); 22 studies, 2209 participants). This evidence was of low certainty due to risk of bias and inconsistency. Seven studies reported data on the atrophy of the medial temporal lobe (mean sensitivity 0.64 (95% CI 0.53 to 0.73); mean specificity 0.65 (95% CI 0.51 to 0.76); 1077 participants) and five studies on the volume of the lateral ventricles (mean sensitivity 0.57 (95% CI 0.49 to 0.65); mean specificity 0.64 (95% CI 0.59 to 0.70); 1077 participants). This evidence was of moderate certainty due to risk of bias. Four studies with 529 participants analysed the volume of the total entorhinal cortex and four studies with 424 participants analysed the volume of the whole brain. We did not estimate pooled sensitivity and specificity for the volume of these two regions because available data were sparse and heterogeneous. We could not statistically evaluate the volumes of the lateral temporal lobe, amygdala, medial temporal gyrus, or cortical grey matter assessed in small individual studies. We found no evidence of a difference between studies in the accuracy of the total hippocampal volume with regards to duration of follow‐up or age of participants, but the manual MRI technique was superior to automatic techniques in mixed (mostly indirect) comparisons. We did not assess the relative accuracy of the volumes of different brain regions measured by MRI because only indirect comparisons were available, studies were heterogeneous, and the overall accuracy of all regions was moderate. Authors' conclusions The volume of hippocampus or medial temporal lobe, the most studied brain regions, showed low sensitivity and specificity and did not qualify structural MRI as a stand‐alone add‐on test for an early diagnosis of dementia due to Alzheimer's disease in people with MCI. This is consistent with international guidelines, which recommend imaging to exclude non‐degenerative or surgical causes of cognitive impairment and not to diagnose dementia due to Alzheimer's disease. In view of the low quality of most of the included studies, the findings of this review should be interpreted with caution. Future research should not focus on a single biomarker, but rather on combinations of biomarkers to improve an early diagnosis of Alzheimer's disease dementia. Plain language summary How accurate is magnetic resonance imaging for the early diagnosis of dementia due to Alzheimer's disease in people with mild cognitive impairment? Why is improving Alzheimer ' s disease diagnosis important? Cognitive impairment is when people have problems remembering, learning, concentrating and making decisions. People with mild cognitive impairment (MCI) generally have more memory problems than other people of their age, but these problems are not severe enough to be classified as dementia. Studies have shown that people with MCI and loss of memory are more likely to develop Alzheimer's disease dementia (approximately 10% to 15% of cases per year) than people without MCI (1% to 2% per year). Currently, the only reliable way of diagnosing Alzheimer's disease dementia is to follow people with MCI and assess cognitive changes over the years. Magnetic resonance imaging (MRI) may detect changes in the brain structures that indicate the beginning of Alzheimer's disease. Early diagnosis of MCI due to Alzheimer's disease is important because people with MCI could benefit from early treatment to prevent or delay cognitive decline. What was the aim of this review? To assess the diagnostic accuracy of MRI for the early diagnosis of dementia due to Alzheimer's disease in people with MCI. What was studied in the review? The volume of several brain regions was measured with MRI. Most studies (22 studies, 2209 participants) measured the volume of the hippocampus, a region of the brain that is associated primarily with memory. What are the main results in this review? Thirty‐three studies were eligible, in which 3935 participants with MCI were included and followed up for two or three years to see if they developed Alzheimer's disease dementia. About a third of them converted to Alzheimer's disease dementia, and the others did not or developed other types of dementia. We found that MRI is not accurate enough to identify people with MCI who will develop dementia due to Alzheimer's disease. The correct prediction of Alzheimer's disease would be missed in 81 out of 300 people with MCI (false negatives) and a wrong prediction of Alzheimer's disease would be made in 203 out of 700 people with MCI (false positives). As a result, people with a false‐negative diagnosis would be falsely reassured and would not prepare themselves to cope with Alzheimer's disease, while those with a false‐positive diagnosis would suffer from the wrongly anticipated diagnosis. How reliable are the results of the studies? The included studies diagnosed Alzheimer's disease dementia by assessing all participants with standard clinical criteria after two or three years' follow‐up. We had some concerns about how the studies were conducted, since the participants were mainly selected from clinical registries and referral centres, and we also had concerns about how studies interpreted MRI. Moreover, the studies were conducted differently from each other, and they used different methods to select people with MCI and perform MRI. Who do the results of this review apply to? The results do not apply to people with MCI in the community, but only to people with MCI who attend memory clinics or referral centres. What are the implications of this review? MRI, as a single test, is not accurate for the early diagnosis of dementia due to Alzheimer's disease in people with MCI since one in three or four participants received a wrong diagnosis of Alzheimer's disease. Future research should not focus on a single test (such as MRI), but rather on combinations of tests to improve an early diagnosis of Alzheimer's disease dementia. How up to date is this review? This evidence is up to date to 29 January 2019.","3","John Wiley & Sons, Ltd","1465-1858","*Magnetic Resonance Imaging; Aged; Aged, 80 and over; Alzheimer Disease [*diagnostic imaging, pathology]; Atrophy [diagnostic imaging]; Brain [diagnostic imaging, pathology]; Cognitive Dysfunction [*complications, pathology]; Disease Progression; Entorhinal Cortex [diagnostic imaging, pathology]; Hippocampus [diagnostic imaging, pathology]; Humans; Lateral Ventricles [diagnostic imaging, pathology]; Middle Aged; Neuroimaging [methods]; Organ Size; Prospective Studies; Sensitivity and Specificity; Temporal Lobe [diagnostic imaging, pathology]","10.1002/14651858.CD009628.pub2","http://dx.doi.org/10.1002/14651858.CD009628.pub2","Dementia and Cognitive Improvement"
"CD010722.PUB2","Mattioni, A; Cenciarelli, S; Eusebi, P; Brazzelli, M; Mazzoli, T; Del Sette, M; Gandolfo, C; Marinoni, M; Finocchi, C; Saia, V; Ricci, S","Transcranial Doppler sonography for detecting stenosis or occlusion of intracranial arteries in people with acute ischaemic stroke","Cochrane Database of Systematic Reviews","2020","Abstract - Background An occlusion or stenosis of intracranial large arteries can be detected in the acute phase of ischaemic stroke in about 42% of patients. The approved therapies for acute ischaemic stroke are thrombolysis with intravenous recombinant tissue plasminogen activator (rt‐PA), and mechanical thrombectomy; both aim to recanalise an occluded intracranial artery. The reference standard for the diagnosis of intracranial stenosis and occlusion is intra‐arterial angiography (IA) and, recently, computed tomography angiography (CTA) and magnetic resonance angiography (MRA), or contrast‐enhanced MRA. Transcranial Doppler (TCD) and transcranial colour Doppler (TCCD) are useful, rapid, noninvasive tools for the assessment of intracranial large arteries pathology. Due to the current lack of consensus regarding the use of TCD and TCCD in clinical practice, we systematically reviewed the literature for studies assessing the diagnostic accuracy of these techniques compared with intra‐arterial IA, CTA, and MRA for the detection of intracranial stenosis and occlusion in people presenting with symptoms of ischaemic stroke. Objectives To assess the diagnostic accuracy of TCD and TCCD for detecting stenosis and occlusion of intracranial large arteries in people with acute ischaemic stroke. Search methods We limited our searches from January 1982 onwards as the transcranial Doppler technique was only introduced into clinical practice in the 1980s. We searched MEDLINE (Ovid) (from 1982 to 2018); Embase (Ovid) (from 1982 to 2018); Database of Abstracts of Reviews of Effects (DARE); and Health Technology Assessment Database (HTA) (from 1982 to 2018). Moreover, we perused the reference lists of all retrieved articles and of previously published relevant review articles, handsearched relevant conference proceedings, searched relevant websites, and contacted experts in the field. Selection criteria We included all studies comparing TCD or TCCD (index tests) with IA, CTA, MRA, or contrast‐enhanced MRA (reference standards) in people with acute ischaemic stroke, where all participants underwent both the index test and the reference standard within 24 hours of symptom onset. We included prospective cohort studies and randomised studies of test comparisons. We also considered retrospective studies eligible for inclusion where the original population sample was recruited prospectively but the results were analysed retrospectively. Data collection and analysis At least two review authors independently screened the titles and abstracts identified by the search strategies, applied the inclusion criteria, extracted data, assessed methodological quality (using QUADAS‐2), and investigated heterogeneity. We contacted study authors for missing data. Main results A comprehensive search of major relevant electronic databases (MEDLINE and Embase) from 1982 to 13 March 2018 yielded 13,534 articles, of which nine were deemed eligible for inclusion. The studies included a total of 493 participants. The mean age of included participants was 64.2 years (range 55.8 to 69.9 years). The proportion of men and women was similar across studies. Six studies recruited participants in Europe, one in south America, one in China, and one in Egypt. Risk of bias was high for participant selection but low for flow, timing, index and reference standard. The summary sensitivity and specificity estimates for TCD and TCCD were 95% (95% CI = 0.83 to 0.99) and 95% (95% CI = 0.90 to 0.98), respectively. Considering a prevalence of stenosis or occlusion of 42% (as reported in the literature), for every 1000 people who receive a TCD or TCCD test, stenosis or occlusion will be missed in 21 people (95% CI = 4 to 71) and 29 (95% CI = 12 to 58) will be wrongly diagnosed as harbouring an intracranial occlusion. However, there was substantial heterogeneity between studies, which was no longer evident when only occlusion of the MCA was considered, or when the analysis was limited to participants investigated within six hours. The performance of either TCD or TCCD in ruling in and ruling out a MCA occlusion was good. Limitations of this review were the small number of identified studies and the lack of data on the use of ultrasound contrast medium. Authors' conclusions This review provides evidence that TCD or TCCD, administered by professionals with adequate experience and skills, can provide useful diagnostic information for detecting stenosis or occlusion of intracranial vessels in people with acute ischaemic stroke, or guide the request for more invasive vascular neuroimaging, especially where CT or MR‐based vascular imaging are not immediately available. More studies are needed to confirm or refute the results of this review in a larger sample of stroke patients, to verify the role of contrast medium and to evaluate the clinical advantage of the use of ultrasound. Plain language summary Transcranial colour Doppler (TCD) and transcranial colour‐coded duplex (TCCD), in patients with acute ischaemic stroke for detecting intracranial vessel occlusion or stenosis What is the aim of this review? The aim of this review was to find out how accurate two imaging techniques ‐ transcranial colour Doppler (TCD) and transcranial colour‐coded duplex (TCCD) ‐ are for detecting a blockage of the arteries in the brain in the first hours after a stroke and whether they can be used to select patients who may need to receive more invasive and expensive imaging methods such as intra‐arterial angiography (IA), computed tomography angiography (CTA), and magnetic resonance angiography (MRA). There is currently no agreement on the use of TCD and TCCD in the management of people with acute stroke, and the use of TCD and TCCD varies between and within countries. Key messages TCD and TCCD may provide clinically helpful information for detecting a blockage of arteries in the brain when compared with IA, CTA, and MRA. What was studied in the review? Ischaemic stroke is the third leading cause of death and the most common cause of long‐term disability. It is usually caused by a blockage of the blood supply to one part of the brain. When stroke is caused by a blockage of a large artery due to a blood clot, the prognosis, without treatment, is often poor and can lead to severe disability. Curently, there are two effective treatment options that can be used to dissolve the blood clot: to administer a thrombolytic drug, or to physically extract the blood clot from the artery (mechanical thrombectomy). Both treatments work best within the first few hours of stroke onset. Ultrasound scans (TCD and TCCD) are a quick and simple way to detect the blockage of blood vessels in the brain. We reviewed the current literature for clinical studies assessing the accuracy of these diagnostic techniques compared with IA, CTA, and MRA for the detection of blocked blood vessels in the brain in people with symptoms of ischaemic stroke. What are the main results of the review A comprehensive search of major relevant electronic databases from 1982 to 13 March 2018 identified 13,534 articles but only nine studies met the prespecified inclusion criteria. The nine identified studies included a total of 493 stroke patients with similar proportions of men and women. The average age of included participants was 64.2 years (range 55.8 to 69.9 years). Six studies recruited participants in Europe, one in South America, one in China, and one in Egypt. The results of this review indicate that if TCD or TCCD were to be used in a group of 1000 people with symptoms of acute stroke, which in 420 (42%) of them is caused by a blockage of large arteries in the brain, then 428 would have a positive test result but 29 of these (29/428, 7%) would be wrongly identified as positives even though they would not have a blockage of large arteries. Similarly, an estimated 572 would have a negative test result indicating that their symptoms are not caused by a blockage of large arteries in the brain but 21 (21/572, 4%) of these negative cases would actually have a blockage of the large arteries, which TCD or TCCD have missed. In brief, for people with acute ischaemic stroke, TCD or TCCD can provide clinically helpful information for detecting blockage of large arteries in the brain compared with IA, CTA and MRA. Both tests studied in the review (TCD and TCCD) have shown similar accuracy. How reliable are the results of the studies in the review? The main limitation of this review is the small number of people assessed by TCD and TCCD in the individual studies. Not enough people have been studied to be really confident about these results. Further larger studies are needed to confirm or refute these results.","2","John Wiley & Sons, Ltd","1465-1858","Brain Ischemia [*diagnostic imaging]; Cerebral Arteries [diagnostic imaging]; Constriction, Pathologic [*diagnostic imaging]; Humans; Infarction, Middle Cerebral Artery; Randomized Controlled Trials as Topic; Stroke [*diagnostic imaging]; Ultrasonography, Doppler, Transcranial [*methods]","10.1002/14651858.CD010722.pub2","http://dx.doi.org/10.1002/14651858.CD010722.pub2","Stroke"
"CD011708.PUB2","Grillo‐Ardila, CF; Torres, M; Gaitán, HG","Rapid point of care test for detecting urogenital   Chlamydia trachomatis infection in nonpregnant women and men at reproductive age","Cochrane Database of Systematic Reviews","2020","Abstract - Background Chlamydia trachomatis (C trachomatis)  is one of the most frequent sexually transmitted infections and a source of deleterious effects on the reproductive health of men and women. Because this infection is likely asymptomatic and is associated with subfertility, ectopic pregnancy, and chronic pain, its presence needs to be confirmed. Technologies available for the diagnosis of  C trachomatis  infection can be classified into tests performed in a laboratory and rapid tests at the point of care (POC tests). Laboratory‐based tests include culture, nucleic acid amplification tests, enzyme immunoassays (EIA), direct fluorescent antibody, nucleic acid hybridization, and transformation tests. Rapid tests include solid‐phase EIA and solid‐phase optical immunoassay. POC tests can be performed within 30 minutes without the need for expensive or sophisticated equipment. The principal advantage of this technology is the immediate presentation of results with the subsequent possibility to start the treatment of infected patients immediately. Objectives To determine the diagnostic accuracy of rapid point‐of‐care (POC) testing for detecting urogenital  C trachomatis  infection in nonpregnant women and men of reproductive age, as verified with nucleic acid amplification tests (NAATs) as the reference standard. Search methods In November 2019 we searched CENTRAL, MEDLINE, Embase and LILACS. We also searched Web of Science, two trials registries and an abstract database. We screened reference lists of included studies for additional references. Selection criteria We included diagnostic accuracy studies of symptomatic or asymptomatic nonpregnant women and men reproductive age. Included trials should have prospectively enrolled participants without previous diagnostic testing, co‐infections or complications and consecutively or through random sampling at primary or secondary care facilities. Only studies reporting that all participants received the index test and the reference standard and presenting 2 x 2 data were eligible for inclusion. We excluded diagnostic case‐control studies. Data collection and analysis Two review authors independently screened titles and abstracts for relevance. Two review authors independently, and in duplicate, assessed eligibility, extracted data, and carried out quality assessment. We resolved differences through consensus or by involving a third review author. We assessed studies for methodological quality using QUADAS‐2 and used meta‐analysis to combine the results of studies using the bivariate approach to estimate the expected sensitivity and specificity values. We assessed the quality of the evidence using GRADE criteria and explored sources of heterogeneity. Main results We included a total of 19 studies, with 13,676 participants, that assessed the diagnostic accuracy of POC tests for  C trachomatis  infection in nonpregnant women and men of reproductive age, as verified with NAATs as the reference standard. Rapid tests were provided by the distributors in nine studies. Seven studies recruited a predominantly high risk or symptomatic population; the studies were conducted in America, Asia, Africa, Europe and Oceania, with a median prevalence of 10% (range 8% to 28%); nine different brands were assessed. The mean sensitivity for rapid tests for detecting urogenital infection was 0.48 (95% confidence interval (CI) 0.39 to 0.58; low‐quality evidence) with a mean specificity of 0.98 (95% CI 0.97 to 0.99; moderate‐quality evidence). We explored sources of heterogeneity by looking into differences in diagnostic accuracy according to the specimen (endocervical versus urine or vaginal), symptoms among participants (symptomatic versus asymptomatic), and setting (low/middle‐income versus high‐income countries). Likelihood ratio tests were not significantly different in terms of sensitivity or specificity by specimen (P = 0.27) or setting (P = 0.28); for this reason, these covariates do not appear to explain the observed variability. Included studies did not provide enough information to assess the 'presence of symptoms' covariate. We downgraded the quality of evidence because of some limitations in applicability and heterogeneity. Authors' conclusions Based on the results of this systematic review, the POC test based on antigen detection has suboptimal sensitivity but good specificity. Performance of this test translates, on average, to a 52% chance of mistakenly indicating absence of infection and a 2% chance of mistakenly pointing to the presence of this condition. Because of its deleterious consequences for reproductive health, and considering the current availability of safe and effective interventions to treat  C trachomatis  infection, the POC screening strategy should not be based on a rapid diagnostic test for antigen detection. Research in this topic should focus on different technologies. Plain language summary What are the most accurate rapid point‐of‐care tests for the detection of Chlamydia? Review question   Chlamydia trachomatis  ( C trachomatis)  is a common, sexually transmitted infection. It can cause serious health problems if not treated early, but many people with  C trachomatis  do not notice any symptoms and need to have a urine or swab test to confirm infection. This review aimed to find out how accurate new test methods, such as rapid point‐of‐care (tests at the time and place of patient care) tests are for diagnosing  C trachomatis  infection.    Background   There are about 90 million cases of  C trachomatis  infection worldwide and 3 million cases annually among young and sexually active people. Rapid point‐of‐care tests can be performed in less than 30 minutes without the need for expensive or sophisticated equipment. The main advantage of this technology is the immediate display of results so that treatment can start straight away.     Study characteristics   We searched for evidence in November 2019 and found 19 relevant studies with 13,676 participants, published between 1999 and 2016. The studies compared the accuracy of the current ‘gold standard’ test (nucleic acid amplification tests (NAAT)) with a total of nine different brands of rapid point‐of‐care tests.    Key Results   This review looked at the accuracy of rapid point‐of‐care tests for diagnosing  C trachomatis  infection in nonpregnant women and men. Our results show that there is a 42% to 62% chance of the test result incorrectly indicating no  C trachomatis  infection, and close to a 2% chance of the test incorrectly indicating a  C trachomatis  infection. This means that for every 1000 patients tested, the point‐of‐care test could fail to diagnose between 420 to 620 people, who could go on to develop serious health problems as a result of the incorrect diagnosis.    Quality of the evidence   All the included studies used reliable methods to look at the tests, so we thought that they were high quality. However, in some studies, most of the participants were at high risk of  C trachomatis  infection or already showed symptoms, so we are not sure how useful their results would be for low‐risk people not showing symptoms. Also, some studies reported very different results from others, and we could not explain the difference.     Conclusion   C trachomatis  infection is potentially very serious so we think that health providers should not rely on rapid point‐of‐care tests to diagnose  C trachomatis  infection. In future, research should investigate different technologies.","1","John Wiley & Sons, Ltd","1465-1858","*Chlamydia trachomatis; *Point‐of‐Care Systems [standards]; Adult; Chlamydia Infections [*diagnosis]; False Negative Reactions; False Positive Reactions; Female; Humans; Male; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Sexually Transmitted Diseases, Bacterial [prevention & control]","10.1002/14651858.CD011708.pub2","http://dx.doi.org/10.1002/14651858.CD011708.pub2","Sexually Transmitted Infections"
"CD013282.PUB2","Beishon, LC; Batterham, AP; Quinn, TJ; Nelson, CP; Panerai, RB; Robinson, T; Haunton, VJ","Addenbrooke’s Cognitive Examination III (ACE‐III) and mini‐ACE for the detection of dementia and mild cognitive impairment","Cochrane Database of Systematic Reviews","2019","Abstract - Background The number of new cases of dementia is projected to rise significantly over the next decade. Thus, there is a pressing need for accurate tools to detect cognitive impairment in routine clinical practice. The Addenbrooke's Cognitive Examination III (ACE‐III), and the mini‐ACE are brief, bedside cognitive screens that have previously reported good sensitivity and specificity. The quality and quantity of this evidence has not, however, been robustly investigated. Objectives To assess the diagnostic test accuracy of the ACE‐III and mini‐ACE for the detection of dementia, dementia sub‐types, and mild cognitive impairment (MCI) at published thresholds in primary, secondary, and community care settings in patients presenting with, or at high risk of, cognitive decline. Search methods We performed the search for this review on 13 February 2019. We searched MEDLINE (OvidSP), Embase (OvidSP), BIOSIS Previews (ISI Web of Knowledge), Web of Science Core Collection (ISI Web of Knowledge), PsycINFO (OvidSP), and LILACS (BIREME). We applied no language or date restrictions to the electronic searches; and to maximise sensitivity we did not use methodological filters. The search yielded 5655 records, of which 2937 remained after we removed duplicates. We identified a further four articles through PubMed 'related articles'. We found no additional records through reference list citation searching, or grey literature. Selection criteria Cross‐sectional studies investigating the accuracy of the ACE‐III or mini‐ACE in patients presenting with, or at high risk of, cognitive decline were suitable for inclusion. We excluded case‐control, delayed verification and longitudinal studies, and studies which investigated a secondary cause of dementia. We did not restrict studies by language; and we included those with pre‐specified thresholds (88 and 82 for the ACE‐III, and 21 or 25 for the mini‐ACE). Data collection and analysis We extracted information on study and participant characteristics and used information on dementia and MCI prevalence, sensitivity, specificity, and sample size to generate 2×2 tables in Review Manager 5. We assessed methodological quality of included studies using the QUADAS‐2 tool; and we assessed the quality of study reporting with the STARDdem tool. Due to significant heterogeneity in the included studies and an insufficient number of studies, we did not perform meta‐analyses. Main results This review identified seven studies (1711 participants in total) of cross‐sectional design, four examining the accuracy of the ACE‐III, and three of the mini‐ACE. Overall, the majority of studies were at low or unclear risk of bias and applicability on quality assessment. Studies were at high risk of bias for the index test (n = 4) and reference standard (n = 2). Study reporting was variable across the included studies. No studies investigated dementia sub‐types. The ACE‐III had variable sensitivity across thresholds and patient populations (range for dementia at 82 and 88: 82% to 97%, n = 2; range for MCI at 88: 75% to 77%, n = 2), but with more variability in specificity (range for dementia: 4% to 77%, n = 2; range for MCI: 89% to 92%, n = 2). Similarly, sensitivity of the mini‐ACE was variable (range for dementia at 21 and 25: 70% to 99%, n = 3; range for MCI at 21 and 25: 64% to 95%, n = 3) but with more variability specificity (range for dementia: 32% to 100%, n = 3; range for MCI: 46% to 79%, n = 3). We identified no studies in primary care populations: four studies were conducted in outpatient clinics, one study in an in‐patient setting, and in two studies the settings were unclear. Authors' conclusions There is insufficient information in terms of both quality and quantity to recommend the use of either the ACE‐III or mini‐ACE for the screening of dementia or MCI in patients presenting with, or at high risk of, cognitive decline. No studies were conducted in a primary care setting so the accuracy of the ACE‐III and mini‐ACE in this setting are not known. Lower thresholds (82 for the ACE‐III, and 21 for the mini‐ACE) provide better specificity with acceptable sensitivity and may provide better clinical utility. The ACE‐III and mini‐ACE should only be used to support the diagnosis as an adjunct to a full clinical assessment. Further research is needed to determine the utility of the ACE‐III and mini‐ACE for the detection of dementia, dementia sub‐types, and MCI. Specifically, the optimal thresholds for detection need to be determined in a variety of settings (primary care, secondary care (inpatient and outpatient), and community services), prevalences, and languages. Plain language summary How accurate are the Addenbrooke's Cognitive Examination III (ACE‐III) and mini‐ACE for the screening of dementia and mild cognitive impairment (MCI)? Why is recognising dementia important? The number of people being diagnosed with dementia is expected to increase significantly over the next 10 years. There is therefore an increasing need for tools that can assess memory and learning to aid the diagnosis of dementia and MCI. The ACE‐III and mini‐ACE are currently used in clinical practice, but the evidence for their accuracy to identify dementia has not been fully established. What was the aim of this review? The aim of this review was to find out how accurate the ACE‐III and mini‐ACE are in identifying dementia and MCI across a range of healthcare settings. The test is performed on a patient who is suspected to have dementia. What was studied in this review? The ACE‐III has 21 questions, with a total score of 100. The test is performed with the patient who presented with, or is suspected to have, dementia. The questions cover five different areas of brain function, and a higher score indicates better function. The mini‐ACE is shorter, with only five questions, and a total score of 30. The thresholds describe the score at which a diagnosis of dementia should be considered and these are usually 82 or 88/100 for the ACE‐III and 21 or 25/30 for the mini‐ACE. The ACE‐III and mini‐ACE are not used on their own to make a diagnosis of dementia, but help clinicians when used in addition to other clinical information and investigations. What are the main results of the review? This review included seven studies with a total of 1711 patients; four studies examined the ACE‐III, and three examined the mini‐ACE. We did not combine the study information statistically due to significant differences between the studies. The ability of both the ACE‐III and the mini‐ACE to identify patients with either dementia or MCI was variable (between 70% and 99% of people were correctly identified as having dementia and between 64% and 95% for MCI). However, there was more variability between the studies in the number of false positives identified by the tests (between 0% and 96% of people were incorrectly identified as having dementia and between 8% and 54% of people were incorrectly identified as having MCI). At the lower test thresholds, there were fewer false positive diagnoses of dementia (between 64% and 100% of people correctly identified as not having dementia or MCI). How reliable are the results of this review? There were some issues with the methods used by studies: the way in which patients were identified and enrolled into the studies, and the way in which the ACE‐III and mini‐ACE were carried out were not well described. The studies were small and did not study enough people to be confident about the results. These issues mean that the accuracy of the ACE‐III and mini‐ACE may have appeared better than it actually was. Who do the results of this review apply to? The average age in all the studies was over 60 years. The proportion of people with dementia was different between studies (range: 15% to 55.6%). All of the studies were conducted in a specialist setting, so we do not know if the ACE‐III or mini‐ACE could be used in general practice or the community. Four studies were in the UK, two were in China, and one in Japan. What are the implications of this review? Overall, the quality, size, and number of included studies has not allowed a definitive conclusion on whether the ACE‐III or the mini‐ACE should be used to identify dementia or MCI. These findings can only be used in a hospital setting, as none of the studies investigated community or general populations. The ACE‐III or mini‐ACE should only be used as part of a clinical assessment when making a diagnosis of dementia, and should not be relied upon alone. More research is needed to investigate the ACE‐III and mini‐ACE in different healthcare settings, languages, and cultures. How up to date is this review? The review authors searched for and included studies up to April 2019.","12","John Wiley & Sons, Ltd","1465-1858","Cognitive Dysfunction [*diagnosis]; Cross‐Sectional Studies; Dementia [*diagnosis]; Diagnosis, Differential; Humans; Mental Status and Dementia Tests [*standards]; Sensitivity and Specificity","10.1002/14651858.CD013282.pub2","http://dx.doi.org/10.1002/14651858.CD013282.pub2","Dementia and Cognitive Improvement"
"CD009977.PUB2","Rud, B; Vejborg, TS; Rappeport, ED; Reitsma, JB; Wille‐Jørgensen, P","Computed tomography for diagnosis of acute appendicitis in adults","Cochrane Database of Systematic Reviews","2019","Abstract - Background Diagnosing acute appendicitis (appendicitis) based on clinical evaluation, blood testing, and urinalysis can be difficult. Therefore, in persons with suspected appendicitis, abdominopelvic computed tomography (CT) is often used as an add‐on test following the initial evaluation to reduce remaining diagnostic uncertainty. The aim of using CT is to assist the clinician in discriminating between persons who need surgery with appendicectomy and persons who do not. Objectives Primary objective Our primary objective was to evaluate the accuracy of CT for diagnosing appendicitis in adults with suspected appendicitis. Secondary objectives Our secondary objectives were to compare the accuracy of contrast‐enhanced versus non‐contrast‐enhanced CT, to compare the accuracy of low‐dose versus standard‐dose CT, and to explore the influence of CT‐scanner generation, radiologist experience, degree of clinical suspicion of appendicitis, and aspects of methodological quality on diagnostic accuracy. Search methods We searched MEDLINE, Embase, and Science Citation Index until 16 June 2017. We also searched references lists. We did not exclude studies on the basis of language or publication status. Selection criteria We included prospective studies that compared results of CT versus outcomes of a reference standard in adults (> 14 years of age) with suspected appendicitis. We excluded studies recruiting only pregnant women; studies in persons with abdominal pain at any location and with no particular suspicion of appendicitis; studies in which all participants had undergone ultrasonography (US) before CT and the decision to perform CT depended on the US outcome; studies using a case‐control design; studies with fewer than 10 participants; and studies that did not report the numbers of true‐positives, false‐positives, false‐negatives, and true‐negatives. Two review authors independently screened and selected studies for inclusion. Data collection and analysis Two review authors independently collected the data from each study and evaluated methodological quality according to the Quality Assessment of Studies of Diagnostic Accuracy ‐ Revised (QUADAS‐2) tool. We used the bivariate random‐effects model to obtain summary estimates of sensitivity and specificity. Main results We identified 64 studies including 71 separate study populations with a total of 10,280 participants (4583 with and 5697 without acute appendicitis). Estimates of sensitivity ranged from 0.72 to 1.0 and estimates of specificity ranged from 0.5 to 1.0 across the 71 study populations. Summary sensitivity was 0.95 (95% confidence interval (CI) 0.93 to 0.96), and summary specificity was 0.94 (95% CI 0.92 to 0.95). At the median prevalence of appendicitis (0.43), the probability of having appendicitis following a positive CT result was 0.92 (95% CI 0.90 to 0.94), and the probability of having appendicitis following a negative CT result was 0.04 (95% CI 0.03 to 0.05). In subgroup analyses according to contrast enhancement, summary sensitivity was higher for CT with intravenous contrast (0.96, 95% CI 0.92 to 0.98), CT with rectal contrast (0.97, 95% CI 0.93 to 0.99), and CT with intravenous and oral contrast enhancement (0.96, 95% CI 0.93 to 0.98) than for unenhanced CT (0.91, 95% CI 0.87 to 0.93). Summary sensitivity of CT with oral contrast enhancement (0.89, 95% CI 0.81 to 0.94) and unenhanced CT was similar. Results show practically no differences in summary specificity, which varied from 0.93 (95% CI 0.90 to 0.95) to 0.95 (95% CI 0.90 to 0.98) between subgroups. Summary sensitivity for low‐dose CT (0.94, 95% 0.90 to 0.97) was similar to summary sensitivity for standard‐dose or unspecified‐dose CT (0.95, 95% 0.93 to 0.96); summary specificity did not differ between low‐dose and standard‐dose or unspecified‐dose CT. No studies had high methodological quality as evaluated by the QUADAS‐2 tool. Major methodological problems were poor reference standards and partial verification primarily due to inadequate and incomplete follow‐up in persons who did not have surgery. Authors' conclusions The sensitivity and specificity of CT for diagnosing appendicitis in adults are high. Unenhanced standard‐dose CT appears to have lower sensitivity than standard‐dose CT with intravenous, rectal, or oral and intravenous contrast enhancement. Use of different types of contrast enhancement or no enhancement does not appear to affect specificity. Differences in sensitivity and specificity between low‐dose and standard‐dose CT appear to be negligible. The results of this review should be interpreted with caution for two reasons. First, these results are based on studies of low methodological quality. Second, the comparisons between types of contrast enhancement and radiation dose may be unreliable because they are based on indirect comparisons that may be confounded by other factors. Plain language summary How accurate is computed tomography for the diagnosis of acute appendicitis in adults? Why is improving the diagnosis of appendicitis important?   The purpose of using computed tomography (CT) in persons with suspected appendicitis is to assist the clinician in differentiating between persons who need surgery with resection of the appendix (appendicectomy) and persons who do not need this procedure. What is the aim of this review?   The aim of this Cochrane Review was to find out how accurate CT of the abdomen and pelvis is for diagnosing appendicitis in adults. Researchers at Cochrane included 64 studies in the review to answer this question. What was studied in the review?   A CT‐scan can be performed in several ways. Image quality can be improved by using intravenous contrast material, and visualization of the appendix can be better when oral or rectal contrast material is used. CT can also be performed with low‐dose radiation. The radiation exposure related to CT may increase lifetime risk of cancer. This Cochrane Review studied the accuracy of the following types of CT: any type of CT, CT according to type of contrast material, and low‐dose CT. What are the main results of this review?   This review included 64 relevant studies that reported results for 71 separate study populations with a total of 10,280 participants. Overall results of these studies indicate that in theory, if CT of any type were to be used in an emergency department in a group of 1000 people, of whom 43% have appendicitis, then:  • an estimated 443 people would have a CT result indicating appendicitis, and of these, 8% would not have acute appendicitis; and  • of the 557 people with a CT result indicating that appendicitis is not present, 4% would actually have acute appendicitis.    Low‐dose CT appeared to be as accurate as standard‐dose CT for diagnosing appendicitis. CT with intravenous, rectal, or oral and intravenous contrast material appeared to be equally accurate, and more accurate than CT without use of contrast material. How reliable are the results of the studies in this review?   Among the included studies, the final diagnosis of appendicitis was based on operative findings or microscopic examination of the resected appendix. Among participants who did not have surgery, appendicitis was ruled out by following up to see whether their symptoms resolved without appendicectomy. This is likely to have been a reliable method for deciding whether patients really had appendicitis when follow‐up was careful and complete. Unfortunately, this was not so in a substantial proportion of the included studies. In general, some problems with how the studies were conducted were evident. This may have resulted in CT appearing more accurate than it really is, thereby increasing the number of correct CT results (green rectangles) in the diagram. To whom do the results of this review apply?   Studies included in the review were carried out mainly in emergency departments. Appendicitis was suspected in all participants following clinical examination and blood testing. Included studies evaluated a wide range of types of CT. Participants' average age ranged from 25 to 46 years across studies, and the percentage of women varied between 26% and 100%. The percentage of study participants with a final diagnosis of appendicitis varied between 13% and 92% across studies (average, 43%). What are the implications of this review?   CT is an accurate test that is likely to assist clinicians in treating persons with possible appendicitis. Results of this review indicate that the chance of a clinician wrongly diagnosing acute appendicitis appears to be low (8% among those whose CT results suggest they have appendicitis). The chance of missing a diagnosis of appendicitis is also low (4% among those whose CT results suggest they do not have appendicitis). How up‐to‐date is this review? The review authors searched for and included studies published up to 16 June 2017.","11","John Wiley & Sons, Ltd","1465-1858","Acute Disease; Adult; Appendicitis [*diagnostic imaging]; Humans; Randomized Controlled Trials as Topic; Tomography, X‐Ray Computed [*methods]","10.1002/14651858.CD009977.pub2","http://dx.doi.org/10.1002/14651858.CD009977.pub2","Colorectal"
"CD012267.PUB2","Ryan, A; Nevitt, SJ; Tuohy, O; Cook, P","Biomarkers for diagnosis of Wilson's disease","Cochrane Database of Systematic Reviews","2019","Abstract - Background Wilson's disease, first described by Samuel Wilson in 1912, is an autosomal recessive metabolic disorder resulting from mutations in the  ATP7B  gene. The disease develops as a consequence of copper accumulating in affected tissues. There is no gold standard for the diagnosis of Wilson's disease, which is often delayed due to the non‐specific clinical features and the need for a combination of clinical and laboratory tests for diagnosis. This delay may in turn affect clinical outcome and has implications for other family members in terms of diagnosis. The Leipzig criteria were established to help standardise diagnosis and management. However, it should be emphasised that these criteria date from 2003, and many of these have not been formally evaluated; this review examines the evidence behind biochemical testing for Wilson's disease. Objectives To determine the diagnostic accuracy of three biochemical tests at specified cut‐off levels for Wilson's disease. The index tests covered by this Cochrane Review are caeruloplasmin, 24‐hour urinary copper and hepatic copper content. These tests were evaluated in those with suspected Wilson's disease and appropriate controls (either healthy or those with chronic liver disease other than Wilson's). In the absence of a gold standard for diagnosing Wilson's disease, we have used the Leipzig criteria as a clinical reference standard. To investigate whether index tests should be performed in all individuals who have been recommended for testing for Wilson's disease, or whether these tests should be limited to subgroups of individuals. Search methods We identified studies by extensive searching of, e.g. the Cochrane Central Register of Controlled Trials (CENTRAL), PubMed, Embase, the Web of Science and clinical trial registries (29 May 2019). Date of the most recent search of the Cochrane Cystic Fibrosis and Genetic Disorders Inborn Errors of Metabolism Register: 29 May 2019. Selection criteria We included prospective and retrospective cohort studies that assessed the diagnostic accuracy of an index test using the Leipzig criteria as a clinical reference standard for the diagnosis of Wilson's disease. Data collection and analysis Two review authors independently reviewed and extracted data and assessed the methodological quality of each included study using the QUADAS‐2 tool. We had planned to undertake meta‐analyses of the sensitivity, specificity at relevant cut‐offs for each of the biochemical tests for Wilson's, however, due to differences in the methods used for each biochemical index test, it was not possible to combine the results in meta‐analyses and hence these are described narratively. Main results Eight studies, involving 5699 participants (which included 1009 diagnosed with Wilson's disease) were eligible for inclusion in the review. Three studies involved children only, one adults only and the four remaining studies involved both children and adults. Two evaluated participants with hepatic signs and six with a combination of hepatic and neurological signs and symptoms of Wilson's disease, as well as pre‐symptomatic individuals. The studies were of variable methodological quality; with high risk if bias for participant selection and the reference standard used being of greatest methodological concern. Key differences between studies include differences in assay methodology, different cut‐off values for diagnostic thresholds, different age and ethnicity groups. Concerns around study design imply that diagnostic accuracy figures may not transfer to populations outside of the relevant study. Index test: caeruloplasmin Five studies evaluated various thresholds of caeruloplasmin (4281 participants, of which 541 had WD). For caeruloplasmin a cut‐off of 0.2 g/L as in the Leipzig criteria achieved a sensitivity of 77.1% to 99%, with variable specificity of 55.9% to 82.8%. Using the cut‐off of 0.1 g/L of the Leipzig criteria seemed to lower the sensitivity overall, 65% to 78.9%, while increasing the specificity to 96.6% to 100%. Index test: hepatic copper Four studies evaluated various thresholds of hepatic copper (1150 participants, of which 367 had WD). The hepatic copper cut‐off of 4 μmol/g used in the Leipzig criteria achieved a sensitivity of 65.7% to 94.4%, with a variable specificity of 52.2% to 98.6%. Index test: 24‐hour urinary copper Three studies evaluated various thresholds of 24‐hour urinary copper (268 participants, of which 101 had WD). For 24‐hour urinary copper, a cut‐off of 0.64 to 1.6 μmol/24 hours used in the Leipzig criteria achieved a variable sensitivity of 50.0% to 80.0%, with a specificity of 75.6% to 98.3%. Authors' conclusions The cut‐offs used for caeruloplasmin, 24‐hour urinary copper and hepatic copper for diagnosing Wilson's disease are method‐dependent and require validation in the population in which such index tests are going to be used. Binary cut‐offs and use of single‐test strategies to rule Wilson's disease in or out is not supported by the evidence in this review. There is insufficient evidence to inform testing in specific subgroups, defined by age, ethnicity or clinical subgroups. Plain language summary Laboratory blood, urine tests and liver biopsy used for the diagnosis of Wilson's disease in children and adults Why is improving Wilson's disease diagnosis important? Wilson's disease is an inherited disease that leads to a build‐up of copper in affected parts of the body. Diagnosis usually occurs in children or young adults, but has been seen in adults over 60 years of age. Copper build‐up begins in the liver progressing over time to affect the brain; however, the challenge for doctors is that liver disease in Wilson's disease has non‐specific features and standard liver blood tests may be normal, even with advanced scarring of the liver or cirrhosis. Early diagnosis allows earlier treatment, however, other causes of chronic liver disease may cause false‐positive results and, depending on cut‐off values used for testing, may result in further unnecessary testing. Conversely, false‐negative results may also arise when a single‐test strategy for diagnosis is used, possibly leading to a delay in treatment. What is the aim and what was included in this review? We aimed to examine the accuracy of three commonly used diagnostic tests to correctly identify Wilson's disease. These tests are: caeruloplasmin (a protein that carries copper in blood); copper in the urine; and copper in the liver. Initial evaluation usually involves checking an individual's eyes for signs of Wilson's disease and a blood test for caeruloplasmin, as this is the most widely accessible biochemical test for Wilson's disease. However, the pathway to diagnosing Wilson's disease is highly variable. Follow‐up testing depends on results of initial testing, plus the ability to access relevant tests and the likelihood with which the doctor believes the individual has Wilson's disease. What are the main results in the review? We found eight studies (5699 participants), of whom 1009 were diagnosed with Wilson's disease. One study assessed all three biochemical tests, three assessed caeruloplasmin, one assessed 24‐hour urinary copper, two assessed hepatic copper and one assessed both urine and hepatic copper. Four studies evaluated adults and children, three evaluated children and adolescents and one evaluated adults. The clinical presentation of Wilson's disease also varied: six studies evaluated individuals with both liver and neurological symptoms of Wilson's disease in addition to individuals who had not yet developed symptoms; and two studies evaluated individuals with liver symptoms only. The ability of the three tests evaluated to detect those with Wilson's disease (termed sensitivity) was variable (50% to 94.4%); the ability to detect those without disease (termed specificity) was also variable (52.2% to 98.3%). No single test was capable of diagnosing Wilson's disease in isolation. There was also not enough evidence to determine the accuracy of the tests within different age groups or Wilson's disease subgroups (e.g. those with liver or neurological symptoms). How reliable are the results of the studies in this review? Since there is no gold standard test for diagnosing Wilson's disease, we selected a clinical and laboratory standard (the Leipzig criteria) to determine the diagnosis of the disease. Results of this review suggest that part of the variability in test sensitivity and specificity at the cut‐offs in the Leipzig criteria is likely to be influenced by the method used to undertake the diagnostic tests. However, there were some problems with how the included studies were conducted. This may result in the caeruloplasmin, urine or liver copper appearing more accurate than it is, increasing the number of positive results (sensitivity). What are the implications of this review? Limited evidence from the included studies support the use of multiple‐index testing as outlined in the Leipzig criteria. The diagnostic thresholds used in this criteria will vary with laboratory test, with the method used to conduct the laboratory test, and with the individuals in the included studies (who varied by age, ethnicity and clinical presentation of disease). These factors should therefore be taken into account when interpreting the results. High sensitivity (true‐positive rate) for each of the laboratory tests is possible at particular cut‐off values; however, when used in isolation, each laboratory test may have a false‐positive or false‐negative rate. Limitations in study design may exaggerate test accuracy. How up‐to‐date is this review? The authors searched for and used studies published up to 29 May 2019.","11","John Wiley & Sons, Ltd","1465-1858","Biomarkers [*metabolism]; Ceruloplasmin [*metabolism]; Copper [*urine]; Hepatolenticular Degeneration [*metabolism]; Humans; Liver [*metabolism, pathology]; Randomized Controlled Trials as Topic","10.1002/14651858.CD012267.pub2","http://dx.doi.org/10.1002/14651858.CD012267.pub2","Cystic Fibrosis and Genetic Disorders"
"CD011420.PUB3","Bjerrum, S; Schiller, I; Dendukuri, N; Kohli, M; Nathavitharana, RR; Zwerling, AA; Denkinger, CM; Steingart, KR; Shah, M","Lateral flow urine lipoarabinomannan assay for detecting active tuberculosis in people living with HIV","Cochrane Database of Systematic Reviews","2019","Abstract - Background The lateral flow urine lipoarabinomannan (LF‐LAM) assay Alere Determine™ TB LAM Ag is recommended by the World Health Organization (WHO) to help detect active tuberculosis in HIV‐positive people with severe HIV disease. This review update asks the question, ""does new evidence justify the use of LF‐LAM in a broader group of people?”, and is part of the WHO process for updating guidance on the use of LF‐LAM. Objectives To assess the accuracy of LF‐LAM for the diagnosis of active tuberculosis among HIV‐positive adults with signs and symptoms of tuberculosis (symptomatic participants) and among HIV‐positive adults irrespective of signs and symptoms of tuberculosis (unselected participants not assessed for tuberculosis signs and symptoms). The proposed role for LF‐LAM is as an add on to clinical judgement and with other tests to assist in diagnosing tuberculosis. Search methods We searched the Cochrane Infectious Diseases Group Specialized Register; MEDLINE, Embase, Science Citation Index, Web of Science, Latin American Caribbean Health Sciences Literature, Scopus, the WHO International Clinical Trials Registry Platform, the International Standard Randomized Controlled Trial Number Registry, and ProQuest, without language restriction to 11 May 2018. Selection criteria Randomized trials, cross‐sectional, and observational cohort studies that evaluated LF‐LAM for active tuberculosis (pulmonary and extrapulmonary) in HIV‐positive adults. We included studies that used the manufacturer's recommended threshold for test positivity, either the updated reference card with four bands (grade 1 of 4) or the corresponding prior reference card grade with five bands (grade 2 of 5). The reference standard was culture or nucleic acid amplification test from any body site (microbiological). We considered a higher quality reference standard to be one in which two or more specimen types were evaluated for tuberculosis diagnosis and a lower quality reference standard to be one in which only one specimen type was evaluated. Data collection and analysis Two review authors independently extracted data using a standardized form and REDCap electronic data capture tools. We appraised the quality of studies using the Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2) tool and performed meta‐analyses to estimate pooled sensitivity and specificity using a bivariate random‐effects model and a Bayesian approach. We analyzed studies enrolling strictly symptomatic participants separately from those enrolling unselected participants. We investigated pre‐defined sources of heterogeneity including the influence of CD4 count and clinical setting on the accuracy estimates. We assessed the certainty of the evidence using the GRADE approach. Main results We included 15 unique studies (nine new studies and six studies from the original review that met the inclusion criteria): eight studies among symptomatic adults and seven studies among unselected adults. All studies were conducted in low‐ or middle‐income countries. Risk of bias was high in the patient selection and reference standard domains, mainly because studies excluded participants unable to produce sputum and used a lower quality reference standard. Participants with tuberculosis symptoms LF‐LAM pooled sensitivity (95% credible interval (CrI) ) was 42% (31% to 55%) (moderate‐certainty evidence) and pooled specificity was 91% (85% to 95%) (very low‐certainty evidence), (8 studies, 3449 participants, 37% with tuberculosis). For a population of 1000 people where 300 have microbiologically‐confirmed tuberculosis, the utilization of LF‐LAM would result in: 189 to be LF‐LAM positive: of these, 63 (33%) would not have tuberculosis (false‐positives); and 811 to be LF‐LAM negative: of these, 174 (21%) would have tuberculosis (false‐negatives). By clinical setting, pooled sensitivity was 52% (40% to 64%) among inpatients versus 29% (17% to 47%) among outpatients; and pooled specificity was 87% (78% to 93%) among inpatients versus 96% (91% to 99%) among outpatients. Stratified by CD4 cell count, pooled sensitivity increased, and specificity decreased with lower CD4 cell count. Unselected participants not assessed for signs and symptoms of tuberculosis LF‐LAM pooled sensitivity was 35% (22% to 50%), (moderate‐certainty evidence) and pooled specificity was 95% (89% to 96%), (low‐certainty evidence), (7 studies, 3365 participants, 13% with tuberculosis). For a population of 1000 people where 100 have microbiologically‐confirmed tuberculosis, the utilization of LF‐LAM would result in: 80 to be LF‐LAM positive: of these, 45 (56%) would not have tuberculosis (false‐positives); and 920 to be LF‐LAM negative: of these, 65 (7%) would have tuberculosis (false‐negatives). By clinical setting, pooled sensitivity was 62% (41% to 83%) among inpatients versus 31% (18% to 47%) among outpatients; pooled specificity was 84% (48% to 96%) among inpatients versus 95% (87% to 99%) among outpatients. Stratified by CD4 cell count, pooled sensitivity increased, and specificity decreased with lower CD4 cell count. Authors' conclusions We found that LF‐LAM has a sensitivity of 42% to diagnose tuberculosis in HIV‐positive individuals with tuberculosis symptoms and 35% in HIV‐positive individuals not assessed for tuberculosis symptoms, consistent with findings reported previously. Regardless of how people are enrolled, sensitivity is higher in inpatients and those with lower CD4 cell, but a concomitant lower specificity. As a simple point‐of‐care test that does not depend upon sputum evaluation, LF‐LAM may assist with the diagnosis of tuberculosis, particularly when a sputum specimen cannot be produced. 17 October 2019 Up to date All studies incorporated from most recent search All studies identified during the most recent search (11 May, 2018) have been incorporated in the review, and no ongoing studies identified. Plain language summary Lateral flow urine lipoarabinomannan assay for detecting active tuberculosis in people living with HIV Why is improving the diagnosis of tuberculosis important? Tuberculosis causes more deaths in people living with HIV than any other disease. The lateral flow urine lipoarabinomannan assay (LF‐LAM, Alere Determine™ TB LAM Ag assay) is a World Health Organization‐recommended rapid test to assist in detection of active tuberculosis in HIV‐positive people with severe HIV disease. Rapid and early tuberculosis diagnosis may allow for prompt treatment and alleviate severe illness and death. An incorrect tuberculosis diagnosis may result in anxiety and unnecessary treatment. What is the aim of this review? To find out how accurate LF‐LAM is for diagnosing tuberculosis in HIV‐positive people with tuberculosis symptoms (symptomatic participants) and those not assessed for tuberculosis symptoms (unselected participants). This is an update of the 2016 Cochrane Review. What was studied in this review? LF‐LAM is a commercially available point‐of‐care test that detects lipoarabinomannan (LAM), a component of the bacterial cell walls, present in some people with active tuberculosis. The test is simple and shows results in 25 minutes. LF‐LAM results were measured against culture or molecular tests (benchmark). What are the main results of this review? Fifteen studies: eight studies evaluated LF‐LAM for tuberculosis among symptomatic participants and seven studies among unselected participants. All studies were conducted in low‐ or middle‐income countries. Tuberculosis diagnosis among symptomatic participants: LF‐LAM registered positive in 42% (sensitivity) of people who actually had tuberculosis and did not register positive in 91% of people who were actually negative (specificity). Tuberculosis diagnosis among unselected participants: LF‐LAM sensitivity was 35% and specificity 95%. How confident are we in the review’s results? Several studies excluded participants who could not produce sputum and most studies relied on a lower quality benchmark. Few studies and participants were included in some analyses and only one study was conducted outside of sub‐Saharan Africa. Results should be interpreted with caution. What do the results mean? Among symptomatic participants, in theory, for a population of 1000 people where 300 have microbiologically‐confirmed tuberculosis, the utilization of LF‐LAM would result in: 189 to be LF‐LAM positive: of these, 63 (33%) would not have tuberculosis (false‐positives); and 811 to be LF‐LAM negative: of these, 174 (21%) would have tuberculosis (false‐negatives). Among unselected participants, in theory, for a population of 1000 people where 100 have microbiologically‐confirmed tuberculosis, the utilization of LF‐LAM would result in: 80 to be LF‐LAM positive: of these, 45 (56%) would not have tuberculosis (false‐positives); and 920 to be LF‐LAM negative: of these, 65 (7%) would have tuberculosis (false‐negatives). Who do the review’s results apply to? HIV‐positive people with tuberculosis symptoms and those not assessed for tuberculosis symptoms. What are the implications of this review? LF‐LAM has sensitivity around 40% to detect tuberculosis. As the test does not require sputum collection, LF‐LAM may be the only way to diagnose tuberculosis when sputum cannot be produced. How up‐to‐date is this review? To 11 May 2018.","10","John Wiley & Sons, Ltd","1465-1858","Adult; Biomarkers [urine]; CD4 Lymphocyte Count; HIV Seropositivity [*complications]; Humans; Lipopolysaccharides [*urine]; Point-of-Care Systems; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Tuberculosis [*diagnosis]; Tuberculosis, Pulmonary [diagnosis]","10.1002/14651858.CD011420.pub3","http://dx.doi.org/10.1002/14651858.CD011420.pub3","Infectious Diseases"
"CD012777.PUB2","Van Hoving, DJ; Griesel, R; Meintjes, G; Takwoingi, Y; Maartens, G; Ochodo, EA","Abdominal ultrasound for diagnosing abdominal tuberculosis or disseminated tuberculosis with abdominal involvement in HIV‐positive individuals","Cochrane Database of Systematic Reviews","2019","Abstract - Background Accurate diagnosis of tuberculosis in people living with HIV is difficult. HIV‐positive individuals have higher rates of extrapulmonary tuberculosis and the diagnosis of tuberculosis is often limited to imaging results. Ultrasound is such an imaging test that is widely used as a diagnostic tool (including point‐of‐care) in people suspected of having abdominal tuberculosis or disseminated tuberculosis with abdominal involvement. Objectives To determine the diagnostic accuracy of abdominal ultrasound for detecting abdominal tuberculosis or disseminated tuberculosis with abdominal involvement in HIV‐positive individuals. To investigate potential sources of heterogeneity in test accuracy, including clinical setting, ultrasound training level, and type of reference standard. Search methods We searched for publications in any language up to 4 April 2019 in the following databases: MEDLINE, Embase, BIOSIS, Science Citation Index Expanded (SCI‐EXPANDED), Social Sciences Citation Index (SSCI), Conference Proceedings Citation Index‐ Science (CPCI‐S), and also ClinicalTrials.gov and the WHO International Clinical Trials Registry Platform to identify ongoing trials. Selection criteria We included cross‐sectional, cohort, and diagnostic case‐control studies (prospective and retrospective) that compared the result of the index test (abdominal ultrasound) with one of the reference standards. We only included studies that allowed for extraction of numbers of true positives (TPs), true negatives (TNs), false positives (FPs), and false negatives (FNs). Participants were HIV‐positive individuals aged 15 years and older. A higher‐quality reference standard was the bacteriological confirmation of  Mycobacterium tuberculosis  from any clinical specimen, and a lower‐quality reference standard was a clinical diagnosis of tuberculosis without microbiological confirmation. We excluded genitourinary tuberculosis. Data collection and analysis For each study, two review authors independently extracted data using a standardized form. We assessed the quality of studies using a tailored Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2) tool. We used the bivariate model to estimate pooled sensitivity and specificity. When studies were few we simplified the bivariate model to separate univariate random‐effects logistic regression models for sensitivity and specificity. We explored the influence of the type of reference standard on the accuracy estimates by conducting separate analyses for each type of reference standard. We assessed the certainty of the evidence using the GRADE approach. Main results We included 11 studies. The risks of bias and concern about applicability were often high or unclear in all domains. We included six studies in the main analyses of any abnormal finding on abdominal ultrasound; five studies reported only individual lesions. The six studies of any abnormal finding were cross‐sectional or cohort studies. Five of these (83%) were conducted in low‐ or middle‐income countries, and one in a high‐income country. The proportion of participants on antiretroviral therapy was none (1 study), fewer then 50% (4 studies), more than 50% (1 study), and not reported (5 studies). The first main analysis, studies using a higher‐quality reference standard (bacteriological confirmation), had a pooled sensitivity of 63% (95% confidence interval (CI) 43% to 79%; 5 studies, 368 participants; very low‐certainty evidence) and a pooled specificity of 68% (95% CI 42% to 87%; 5 studies, 511 participants; very low‐certainty evidence). If the results were to be applied to a hypothetical cohort of 1000 people with HIV where 200 (20%) have tuberculosis then: ‐ About 382 individuals would have an ultrasound result indicating tuberculosis; of these, 256 (67%) would be incorrectly classified as having tuberculosis (false positives). ‐ Of the 618 individuals with a result indicating that tuberculosis is not present, 74 (12%) would be incorrectly classified as not having tuberculosis (false negatives). In the second main analysis involving studies using a lower‐quality reference standard (clinical diagnosis), the pooled sensitivity was 68% (95% CI 45% to 85%; 4 studies, 195 participants; very low‐certainty evidence) and the pooled specificity was 73% (95% CI 41% to 91%; 4 studies, 202 participants; very low‐certainty evidence). Authors' conclusions In HIV‐positive individuals thought to have abdominal tuberculosis or disseminated tuberculosis with abdominal involvement, abdominal ultrasound appears to have 63% sensitivity and 68% specificity when tuberculosis was bacteriologically confirmed. These estimates are based on data that is limited, varied, and low‐certainty. The low sensitivity of abdominal ultrasound means clinicians should not use a negative test result to rule out the disease, but rather consider the result in combination with other diagnostic strategies (including clinical signs, chest x‐ray, lateral flow urine lipoarabinomannan assay (LF‐LAM), and Xpert MTB/RIF). Research incorporating the test into tuberculosis diagnostic algorithms will help in delineating more precisely its value in diagnosing abdominal tuberculosis or disseminated tuberculosis with abdominal involvement. 26 September 2019 Up to date All studies incorporated from most recent search All studies identified during the most recent search (4 Apr, 2019) have been incorporated in the review, and one ongoing study identified Plain language summary Abdominal ultrasound for diagnosing abdominal tuberculosis or disseminated tuberculosis with abdominal involvement in people with HIV Why is improving tuberculosis diagnosis in people with HIV important? Diagnosing active tuberculosis in people living with HIV is challenging. People with advanced immunosuppression have high rates of extrapulmonary tuberculosis (tuberculosis outside the lungs). What is the aim of this review? The aim of this review is to find out how accurate an ultrasound examination of the abdomen (abdominal ultrasound) is for diagnosing tuberculosis in people with HIV suspected of having tuberculosis in the abdomen or widespread tuberculosis (disseminated tuberculosis) involving the abdomen. What was studied in the review? Abdominal ultrasound can be done after other tests (e.g. the chest x‐ray did not indicate tuberculosis ) or it can be done before other tests in people suspected of having tuberculosis. This review focuses on situations where other tests are not available. What are the main results in this review? We found 11 studies, but only six were relevant for the main analyses. The six studies were divided into two groups. In the first group tuberculosis was diagnosed by identifying the organism causing tuberculosis from any specimen (microbiological confirmation). For the second group, tuberculosis was diagnosed when healthcare personnel suspected tuberculosis and started anti‐tuberculosis treatment, but without identifying the organism (clinical diagnosis). Three studies provided results for both groups. The review included five studies (a total of 879 participants) with microbiological confirmation. The results showed that if abdominal ultrasound were to be used in a group of 1000 people with HIV where 200 (20%) have tuberculosis then: ‐ About 382 individuals would have an ultrasound result indicating tuberculosis; of these, 256 (67%) would be incorrectly classified as having tuberculosis (false positives). ‐ Of the 618 individuals with a result indicating that tuberculosis is not present, 74 (12%) would be incorrectly classified as not having tuberculosis (false negatives). How reliable are the results of the studies in this review? Microbiological confirmation is likely to be a reliable method for deciding whether people really have tuberculosis; clinical diagnosis is likely to be less trustworthy. We found problems in both groups with how studies were conducted. Decreasing the number of false positive results may make abdominal ultrasound appear more accurate than it is. Numbers shown are an average across studies. As estimates from individual studies varied, we cannot be sure that abdominal ultrasound will always produce these results. Not enough people have been studied for us to be confident about the results. Who do the results of the review apply to? Studies included in the main analyses were done in Cambodia, India, South Africa, South Sudan, Spain, and Tanzania. Reasons for including people differed between the studies. Four studies used trained radiologists (specialists) or sonographers; two used doctors trained in ultrasound (non‐specialists), and two included people without any suspicion of tuberculosis. Across the studies, the percentage of people with a final diagnosis of tuberculosis ranged from 18% to 64%. What are the implications of this review? If the test is used to rule in the disease in the absence of other evidence, then, the chance of diagnosing someone with tuberculosis when they actually do not have it is high. Chances of missing a diagnosis of tuberculosis when the test is positive are lower, but a negative test alone is probably insufficient to rule out the disease. These findings should be considered when deciding whether or not to use abdominal ultrasound to test for tuberculosis involving the abdomen and how to interpret the results in the context of other clinical and diagnostic test information. How up‐to‐date is this review? The review authors searched for studies up to 4 April 2019.","9","John Wiley & Sons, Ltd","1465-1858","AIDS‐Related Opportunistic Infections [*diagnostic imaging]; HIV Infections [*complications]; Humans; Randomized Controlled Trials as Topic; Tuberculosis [*diagnostic imaging]; Ultrasonography [*methods]","10.1002/14651858.CD012777.pub2","http://dx.doi.org/10.1002/14651858.CD012777.pub2","Infectious Diseases"
"CD009551.PUB4","Cruciani, M; Mengoli, C; Barnes, R; Donnelly, JP; Loeffler, J; Jones, BL; Klingspor, L; Maertens, J; Morton, CO; White, LP","Polymerase chain reaction blood tests for the diagnosis of invasive aspergillosis in immunocompromised people","Cochrane Database of Systematic Reviews","2019","Abstract - Background This is an update of the original review published in the  Cochrane Database of Systematic Reviews  Issue 10, 2015. Invasive aspergillosis (IA) is the most common life‐threatening opportunistic invasive mould infection in immunocompromised people. Early diagnosis of IA and prompt administration of appropriate antifungal treatment are critical to the survival of people with IA. Antifungal drugs can be given as prophylaxis or empirical therapy, instigated on the basis of a diagnostic strategy (the pre‐emptive approach) or for treating established disease. Consequently, there is an urgent need for research into both new diagnostic tools and drug treatment strategies. Increasingly, newer methods such as polymerase chain reaction (PCR) to detect fungal nucleic acids are being investigated. Objectives To provide an overall summary of the diagnostic accuracy of PCR‐based tests on blood specimens for the diagnosis of IA in immunocompromised people. Search methods We searched MEDLINE (1946 to June 2015) and Embase (1980 to June 2015). We also searched LILACS, DARE, Health Technology Assessment, Web of Science and Scopus to June 2015. We checked the reference lists of all the studies identified by the above methods and contacted relevant authors and researchers in the field. For this review update we updated electronic searches of the Cochrane Central Register of Controlled Trials (CENTRAL; 2018, Issue 3) in the Cochrane Library; MEDLINE via Ovid (June 2015 to March week 2 2018); and Embase via Ovid (June 2015 to 2018 week 12). Selection criteria We included studies that: i) compared the results of blood PCR tests with the reference standard published by the European Organisation for Research and Treatment of Cancer/Mycoses Study Group (EORTC/MSG); ii) reported data on false‐positive, true‐positive, false‐negative and true‐negative results of the diagnostic tests under investigation separately; and iii) evaluated the test(s) prospectively in cohorts of people from a relevant clinical population, defined as a group of individuals at high risk for invasive aspergillosis. Case‐control and retrospective studies were excluded from the analysis. Data collection and analysis Authors independently assessed quality and extracted data. For PCR assays, we evaluated the requirement for either one or two consecutive samples to be positive for diagnostic accuracy. We investigated heterogeneity by subgroup analyses. We plotted estimates of sensitivity and specificity from each study in receiver operating characteristics (ROC) space and constructed forest plots for visual examination of variation in test accuracy. We performed meta‐analyses using the bivariate model to produce summary estimates of sensitivity and specificity. Main results We included 29 primary studies (18 from the original review and 11 from this update), corresponding to 34 data sets, published between 2000 and 2018 in the meta‐analyses, with a mean prevalence of proven or probable IA of 16.3 (median prevalence 11.1% , range 2.5% to 57.1%). Most patients had received chemotherapy for haematological malignancy or had undergone hematopoietic stem cell transplantation. Several PCR techniques were used among the included studies. The sensitivity and specificity of PCR for the diagnosis of IA varied according to the interpretative criteria used to define a test as positive. The summary estimates of sensitivity and specificity were 79.2% (95% confidence interval (CI) 71.0 to 85.5) and 79.6% (95% CI 69.9 to 86.6) for a single positive test result, and 59.6% (95% CI 40.7 to 76.0) and 95.1% (95% CI 87.0 to 98.2) for two consecutive positive test results. Authors' conclusions PCR shows moderate diagnostic accuracy when used as screening tests for IA in high‐risk patient groups. Importantly the sensitivity of the test confers a high negative predictive value (NPV) such that a negative test allows the diagnosis to be excluded. Consecutive positives show good specificity in diagnosis of IA and could be used to trigger radiological and other investigations or for pre‐emptive therapy in the absence of specific radiological signs when the clinical suspicion of infection is high. When a single PCR positive test is used as the diagnostic criterion for IA in a population of 100 people with a disease prevalence of 16.3% (overall mean prevalence), three people with IA would be missed (sensitivity 79.2%, 20.8% false negatives), and 17 people would be unnecessarily treated or referred for further tests (specificity of 79.6%, 21.4% false positives). If we use the two positive test requirement in a population with the same disease prevalence, it would mean that nine IA people would be missed (sensitivity 59.6%, 40.4% false negatives) and four people would be unnecessarily treated or referred for further tests (specificity of 95.1%, 4.9% false positives). Like galactomannan, PCR has good NPV for excluding disease, but the low prevalence of disease limits the ability to rule in a diagnosis. As these biomarkers detect different markers of disease, combining them is likely to prove more useful. Plain language summary A new, non‐invasive diagnostic blood test — polymerase chain reaction — for people at risk of an invasive mould infection (aspergillosis) Review question   We reviewed the evidence about the accuracy of polymerase chain reaction (PCR) tests for diagnosing invasive aspergillosis (IA) among people with defective immune systems from medical treatment such as chemotherapy or following organ or bone marrow transplant. Background   IA is a fungal disease caused by the widespread mould  Aspergillus,  with  Aspergillus fumigatus  being the most common species .  Most people breathe in  Aspergillus  spores every day without becoming ill. However people with weakened immune systems or lung diseases are at a higher risk of developing respiratory problems of the lungs and sinuses due to  Aspergillus,  ranging from allergic complications to IA, which is the most common life‐threatening, invasive fungal infection of people whose immune systems are compromised. Without antifungal treatment, most people with IA will die as a direct result of IA, so early diagnosis and prompt administration of appropriate antifungal treatment are both critical to the survival of these people. The ideal specimen for diagnosing IA would be lung tissue but obtaining this carries a significant risk to the patient so there is a clear need for new, non‐invasive methods such as PCR to demonstrate the fungus’s presence in blood by detecting its nucleic acids. Study characteristics   We conducted our most recent search for studies in March 2018 and combined with an earlier search selected 29 clinical studies reporting the evaluation of PCR tests prospectively in cohorts of people at high risk of IA. Study funding sources   None of the companies involved in the diagnosis of invasive fungal diseases funded any of the studies included in the review. Quality of the evidence   Most studies were at low risk of bias and low concern regarding applicability. However, differences in the reference standard may have contributed to differences we found in the distribution of cases as being classified as IA or not. Key results   Several PCR techniques were used in the studies. Pooling the data from the studies showed that sensitivity and specificity of PCR for the diagnosis of IA varied (from 59% to 79.2% and from 79% to 95.2%, respectively) depending on the interpretative criteria used to define a test as positive. When used as a diagnostic criterion for IA in a population of 100 people with a disease prevalence of 16.3% (overall mean prevalence), a single PCR positive test would have missed three people with the disease, and falsely classified 17 people as having the disease, who would be treated unnecessarily or referred for further tests. A requirement of two positive tests as a diagnostic criterion in a population with the same disease prevalence would miss nine people with the disease and falsely classify four people as having the disease. These numbers should be interpreted with caution because the reference standard is based on the degree of certainty of diagnosis and is rarely proven so cannot provide consistent assessment of cases as being IA or not. Overall, PCR shows moderate diagnostic accuracy when used as a screening test for IA in high‐risk patient groups. Importantly, when the rate of sensitivity is low, the sensitivity of the tests means that a negative result allows the diagnosis to be excluded with confidence except when the patient is receiving certain antifungal drugs. With the low prevalence of the disease, a high negative predictive value such that a negative test allows the diagnosis to be excluded.","9","John Wiley & Sons, Ltd","1465-1858","*Immunocompromised Host; *Opportunistic Infections [blood, diagnosis]; Aspergillosis [*blood, *diagnosis]; Case-Control Studies; Humans; Polymerase Chain Reaction [*methods]; Predictive Value of Tests; Sensitivity and Specificity","10.1002/14651858.CD009551.pub4","http://dx.doi.org/10.1002/14651858.CD009551.pub4","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD011871.PUB2","Yang, B; de Vries, SG; Ahmed, A; Visser, BJ; Nagel, IM; Spijker, R; Grobusch, MP; Hartskeerl, RA; Goris, MGA; Leeflang, MMG","Nucleic acid and antigen detection tests for leptospirosis","Cochrane Database of Systematic Reviews","2019","Abstract - Background Early diagnosis of leptospirosis may contribute to the effectiveness of antimicrobial therapy and early outbreak recognition. Nucleic acid and antigen detection tests have the potential for early diagnosis of leptospirosis. With this systematic review, we assessed the sensitivity and specificity of nucleic acid and antigen detection tests. Objectives To determine the diagnostic test accuracy of nucleic acid and antigen detection tests for the diagnosis of human symptomatic leptospirosis. Search methods We searched electronic databases including MEDLINE, Embase, the Cochrane Library, and regional databases from inception to 6 July 2018. We did not apply restrictions to language or time of publication. Selection criteria We included diagnostic cross‐sectional studies and case‐control studies of tests that made use of nucleic acid and antigen detection methods in people suspected of systemic leptospirosis. As reference standards, we considered the microscopic agglutination test alone (which detects antibodies against leptospirosis) or in a composite reference standard with culturing or other serological tests. Studies were excluded when the controls were healthy individuals or when there were insufficient data to calculate sensitivity and specificity. Data collection and analysis At least two review authors independently extracted data from each study. We used the revised Quality Assessment of Diagnostic Accuracy Studies tool (QUADAS‐2) to assess risk of bias. We calculated study‐specific values for sensitivity and specificity with 95% confidence intervals (CI) and pooled the results in a meta‐analysis when appropriate. We used the bivariate model for index tests with one positivity threshold, and we used the hierarchical summary receiver operating characteristic model for index tests with multiple positivity thresholds. As possible sources of heterogeneity, we explored: timing of index test, disease prevalence, blood sample type, primers or target genes, and the real‐time polymerase chain reaction (PCR) visualisation method. These were added as covariates to the meta‐regression models. Main results We included 41 studies evaluating nine index tests (conventional PCR (in short: PCR), real‐time PCR, nested PCR, PCR performed twice, loop‐mediated isothermal amplification, enzyme‐linked immunosorbent assay (ELISA), dot‐ELISA, immunochromatography‐based lateral flow assay, and dipstick assay) with 5981 participants (1834 with and 4147 without leptospirosis). Methodological quality criteria were often not reported, and the risk of bias of the reference standard was generally considered high. The applicability of findings was limited by the frequent use of frozen samples. We conducted meta‐analyses for the PCR and the real‐time PCR on blood products. The pooled sensitivity of the PCR was 70% (95% CI 37% to 90%) and the pooled specificity was 95% (95% CI 75% to 99%). When studies with a high risk of bias in the reference standard domain were excluded, the pooled sensitivity was 87% (95% CI 44% to 98%) and the pooled specificity was 97% (95% CI 60% to 100%). For the real‐time PCR, we estimated a summary receiver operating characteristic curve. To illustrate, a point on the curve with 85% specificity had a sensitivity of 49% (95% CI 30% to 68%). Likewise, at 90% specificity, sensitivity was 40% (95% CI 24% to 59%) and at 95% specificity, sensitivity was 29% (95% CI 15% to 49%). The median specificity of real‐time PCR on blood products was 92%. We did not formally compare the diagnostic test accuracy of PCR and real‐time PCR, as direct comparison studies were lacking. Three of 15 studies analysing PCR on blood products reported the timing of sample collection in the studies included in the meta‐analyses (range 1 to 7 days postonset of symptoms), and nine out of 16 studies analysing real‐time PCR on blood products (range 1 to 19 days postonset of symptoms). In PCR studies, specificity was lower in settings with high leptospirosis prevalence. Other investigations of heterogeneity did not identify statistically significant associations. Two studies suggested that PCR and real‐time PCR may be more sensitive on blood samples collected early in the disease stage. Results of other index tests were described narratively. Authors' conclusions The validity of review findings are limited and should be interpreted with caution. There is a substantial between‐study variability in the accuracy of PCR and real‐time PCR, as well as a substantial variability in the prevalence of leptospirosis. Consequently, the position of PCR and real‐time PCR in the clinical pathway depends on regional considerations such as disease prevalence, factors that are likely to influence accuracy, and downstream consequences of test results. There is insufficient evidence to conclude which of the nucleic acid and antigen detection tests is the most accurate. There is preliminary evidence that PCR and real‐time PCR are more sensitive on blood samples collected early in the disease stage, but this needs to be confirmed in future studies. Plain language summary How accurate are nucleic and antigen detection tests in diagnosing leptospirosis? What was studied in this review? Leptospirosis is an infectious disease, caused by bacteria called  Leptospira  that can be found in soil, freshwater, or in the infected urine of certain animals. It is mainly a problem in humid, tropical countries in Southeast Asia, and Central and South America, but it can also occur in temperate regions. Leptospirosis causes fever and headache, and in some cases kidney, lung, or heart problems. Often, the symptoms are not unique for the disease, which makes it difficult to diagnose, and is therefore frequently missed. Laboratory tests confirm diagnosis. These tests are based on demonstration of the presence of  Leptospira , its DNA, or antibodies against  Leptospira . Nucleic acid and antigen detection tests, such as conventional polymerase chain reaction (PCR) and real‐time PCR, identify the bacterium or its DNA directly in blood or urine. Nucleic acid and antigen detection tests may detect  Leptospira  better in the early days of an infection, so that people can be treated earlier with antibiotics – resulting in better outcomes – and can provide useful information in outbreak situations. In outbreak situations, nucleic acid and antigen detection tests could serve as early warning systems. What was the aim of this review? The aim was to assess how well nucleic acid and antigen tests perform in detecting leptospirosis. In other words, to assess how many mistakes these tests make by either missing people with leptospirosis or misidentifying people without leptospirosis (healthy people or people with another disease). What were the main results in this review? The review included information from 41 studies with 5981 participants. We identified nine nucleic acid and antigen detection tests, of which PCR and real‐time PCR were most often investigated. An important finding was that the accuracy of both PCR and real‐time PCR varied strongly between studies. We presented average accuracies for both tests, but there was great uncertainty around these averages. PCR often correctly identified people without leptospirosis (averaging 95 in 100 people), but frequently missed people with leptospirosis (averaging 30 in 100 people). The accuracy of the real‐time PCR depended on the cut‐off value for a positive test result. At a cut‐off value where real‐time PCR often correctly identified people without leptospirosis (averaging 95 in 100 people), it also frequently missed people with leptospirosis (averaging 71 in 100 people). If a person tests positive or negative for PCR or real‐time PCR, the chance of the person actually having the disease depends on whether the suspicion of leptospirosis in that person was already high before taking the test. So, when interpreting the results of any of these tests, one must consider the strength of suspicion of leptospirosis in an individual, and how often leptospirosis occurs in the setting in which the test will be used. It was uncertain whether PCR or real‐time PCR performed better in detecting leptospirosis, since studies directly comparing these two tests were lacking. The results of other nucleic and antigen detection tests are described in the main text of the review. How reliable were the results of the studies in this review? Not all studies were conducted according to the highest scientific standards. This means that the results of some studies may have been overestimated or underestimated. Furthermore, the tests used to verify whether a person truly had leptospirosis or not (called the reference standard) may not accurately distinguish people with or without leptospirosis. For these reasons, more high‐quality studies are needed to confirm the reliability of these results. Who do the results of this review apply to? The results may apply to people who may have leptospirosis. However, the performance of the PCR and real‐time PCR vary considerably among studies and it is yet unclear what causes this difference in performances. It is probable that the test performs better or worse depending on how prevalent leptospirosis is in the region, and depending on the time between the onset of symptoms and time of testing. Therefore, it is difficult to generalise the results of this review to all settings. How up‐to‐date is this review? The review authors searched for and used studies published up to 6 July 2018.","8","John Wiley & Sons, Ltd","1465-1858","Antibodies, Bacterial [*immunology]; Enzyme‐Linked Immunosorbent Assay [methods]; Humans; Leptospira [*immunology]; Leptospirosis [blood, *diagnosis]; Nucleic Acids [*blood]; Polymerase Chain Reaction [*methods]; ROC Curve; Sensitivity and Specificity","10.1002/14651858.CD011871.pub2","http://dx.doi.org/10.1002/14651858.CD011871.pub2","Hepato-Biliary"
"CD012546.PUB2","Manzotti, C; Casazza, G; Stimac, T; Nikolova, D; Gluud, C","Total serum bile acids or serum bile acid profile, or both, for the diagnosis of intrahepatic cholestasis of pregnancy","Cochrane Database of Systematic Reviews","2019","Abstract - Background Intrahepatic cholestasis of pregnancy is a pregnancy‐specific liver disorder, possibly associated with an increased risk of severe fetal adverse events. Total serum bile acids (TSBA) concentration, alone or in combination with serum aminotransferases, have been the most often used biomarkers for the diagnosis of intrahepatic cholestasis of pregnancy in clinical practice. Serum bile acid profile, composed of primary or secondary, conjugated or non‐conjugated bile acids, may provide more specific disease information. Objectives To assess and compare, independently or in combination, the diagnostic accuracy of total serum bile acids or serum bile acids profile, or both, for the diagnosis of intrahepatic cholestasis of pregnancy in pregnant women, presenting with pruritus. To define the optimal cut‐off values for components of serum bile acid profile; to investigate possible sources of heterogeneity. Search methods We searched the Cochrane Hepato‐Biliary Group Controlled Trials Register, the Cochrane Hepato‐Biliary Group Diagnostic Test Accuracy Studies Register, the Cochrane Library, MEDLINE Ovid, Embase Ovid, Science Citation Index Expanded, Conference Proceedings Citation Index – Science, BIOSIS, CINAHL, two Chinese databases (CKNI, VIP), Latin American and Caribbean Health Sciences Literature (LILACS), Scientific Electronic Library Online (SciELO), Evidence Search: Health and Social Care by the National Institute for Health and Care Excellence (NICE), the World Health Organization (WHO) Reproductive Health Library (RHL), and the Turning Research into Practice database (TRIP). The most recent date of search was 6 May 2019. We identified additional references by handsearching the references of articles, meta‐analyses, and evidence‐based guidelines retrieved from the computerised databases, on‐line trial registries, and grey literature through OpenSIGLE, National Technical Information Service (NTIS), ProQuest Dissertations & Thesis Database, and Index to Theses in Great Britain and Ireland. Selection criteria Prospective or retrospective diagnostic case‐control or cross‐sectional studies, irrespective of publication date, format, and language, which evaluated the diagnostic accuracy of total serum bile acids (TSBA) or components of serum bile acid profile for the diagnosis of intrahepatic cholestasis of pregnancy in pregnant women of any age or ethnicity, in any clinical setting, symptomatic for pruritus. Data collection and analysis We selected studies by reading titles, abstracts, or full texts, and assessing their fulfilment of our inclusion criteria. We emailed primary authors to request missing data or individual participant data. Having extracted data from each included study, we built the two‐by‐two tables for each primary study and for all the index tests considered. We estimated sensitivity and specificity with their 95% confidence intervals (CI). We presented data in coupled forest plots, showing sensitivities and specificities of each study, and we plotted the studies in the Receiver Operating Characteristic (ROC) space. We performed meta‐analyses adopting the hierarchical summary ROC model (HSROC) or the bivariate model to meta‐analyse the data. We made indirect comparisons of the considered index tests by adding the index tests as covariates to the bivariate or HSROC models. We performed heterogeneity analysis and sensitivity analysis on studies assessing TSBA accuracy. We used Review Manager 5 (RevMan 5) and SAS statistical software, release 9.4 (SAS Institute Inc., Cary, NC, USA), to perform all statistical analyses. We used QUADAS‐2 domains to assess the risk of bias of the included studies. Main results Our search yielded 5073 references, but at the end of our selection process, only 16 studies fulfilled the review inclusion criteria. Nine of these provided individual participant data. We analysed only data concerning TSBA, cholic acid (CA), glycocholic acid (GCA), chenodeoxycholic acid (CDCA), and CA/CDCA because the remaining planned index tests were assessed in few studies. Only one study had low risk of bias in all four QUADAS‐2 domains. The most biased domains were the patient sampling and the reference standard domains. When considering all studies with a cut‐off of 10 μmol/L, TSBA overall sensitivity ranged from 0.72 to 0.98 and specificity ranged from 0.81 to 0.97. After a sensitivity analysis excluding case‐control studies, TSBA sensitivity ranged from 0.48 to 0.66 and specificity from 0.52 to 0.99. After a sensitivity analysis excluding studies in which TSBA was part of the reference standard, TSBA sensitivity ranged from 0.49 to 0.65 and specificity from 0.53 to 0.99. We found the estimates of the overall accuracy for some serum bile acid components (CA, GCA, CDCA, and CA/CDCA) to be imprecise, with the CI for sensitivity and specificity very wide or impossible to calculate. Indirect comparisons between serum bile acid profile components and TSBA were not statistically significant. None of the heterogeneity analysis performed was statistically significant, except for the timing of assessment of TSBA (onset of symptoms, peak value among multiple assessments, delivery) but without clinically relevant results. We could not analyse the diagnostic accuracy of combinations of index tests because none of the included studies carried them out, and because of the small number of included studies. Authors' conclusions The overall high risk of bias, the existing concern regarding applicability of the results in clinical practice, and the great heterogeneity of the results in the included studies prevents us from making recommendations and reaching definitive conclusions at the present time. Thus, we do not find any compelling evidence to recommend or refute the routine use of any of these tests in clinical practice. So far, the diagnostic accuracy of TSBA for intrahepatic cholestasis of pregnancy might have been overestimated. There were too few studies to permit a precise estimate of the accuracy of serum bile acid profile components. Further primary clinical research is mandatory. We need both further phase II and phase III diagnostic studies. Plain language summary Diagnostic accuracy of total serum bile acids or individual bile acids for intrahepatic cholestasis of pregnancy in woman claiming pruritus Review question   To assess and compare the diagnostic accuracy of total serum bile acids (TSBA) and some components of serum bile acid profile for the diagnosis of intrahepatic cholestasis of pregnancy in woman with onset of pruritus during pregnancy. Background   'Diagnostic accuracy' means how well a test correctly identifies or rules out disease and informs subsequent decisions about treatment. Intrahepatic cholestasis of pregnancy is a pregnancy‐specific liver disorder, in which bile (a digestive fluid) builds up in the liver, impairing the liver (intrahepatic) function. Intrahepatic cholestasis of pregnancy is possibly associated with an increased risk of premature delivery and fetal death, which seems to occur most often during the last weeks of pregnancy. This is why most clinicians choose to induce early delivery of the baby. In clinical practice, presence of severe pruritus (itchiness) during late pregnancy and 'otherwise unexplained' abnormalities in serum liver tests, seems enough to support the diagnosis of intrahepatic cholestasis of pregnancy. However, excluding all other possible underlying diseases is not always easy; hence confirmation of the intrahepatic cholestasis of pregnancy diagnosis may be possible only after delivery, when spontaneous disappearance of pruritus and improvement of liver tests on blood exams usually occur. Total serum bile acids (TSBA) are the most used biomarkers for intrahepatic cholestasis of pregnancy in clinical practice. Some components of the serum bile acid profile might provide more specific information than total serum bile acids when diagnosing the disease, defining its severity and monitoring its response to treatment. Study characteristics   This review considered all evidence provided by studies that assess the diagnostic accuracy of total serum bile acids (TSBA) and any component of serum bile acid profile for intrahepatic cholestasis of pregnancy in woman claiming onset of pruritus during pregnancy. We assessed all available reports from a wide, systematic search of databases of medical literature, irrespective of design, publication status, language, and study design. We finally included 16 studies, most of them assessing the accuracy (sensitivity and specificity) of TSBA with a cut‐off of 10 μmol/L. Most studies had a case‐control design, and these studies could have overestimated the diagnostic accuracy. Key results   When considering the studies with a cut‐off of 10 μmol/L for TSBA serum concentration, TSBA overall sensitivity (the ability to correctly identify women with the disease) ranged from 72% to 98% and specificity (the ability to correctly identify women without the disease) ranged from 81% to 97%. However, after performing two different analyses excluding studies with probably less reliable results, the diagnostic accuracy seemed lower. We calculated the overall accuracy also of some components of serum bile acid profile, but the small number of studies and the high variability of the results led to very imprecise data. Quality of the evidence   Only one of the 16 included studies was performed and reported well (low risk of bias). The remaining 15 studies had problems with study design or reporting (high risk of bias). Only five studies seemed to show low concern regarding applicability of the results in clinical practice. Conclusions   The overall high risk of bias, the existing concern regarding applicability of the results in clinical practice, and the poor uniformity of our results in the included studies prevents us from making recommendations and reaching definitive conclusions at present. Thus, we do not find any compelling evidence to recommend or refute the routine use of any of these tests in clinical practice. So far, the diagnostic accuracy of TSBA for intrahepatic cholestasis of pregnancy might have been overestimated. There were too few studies to permit a precise estimate of the accuracy of serum bile acid profile components. Further primary clinical research is mandatory. We need both further phase II and phase III diagnostic studies.","7","John Wiley & Sons, Ltd","1465-1858","Bile Acids and Salts [*blood]; Biomarkers [blood]; Cholestasis, Intrahepatic [*blood, *diagnosis]; Diagnostic Tests, Routine; Female; Humans; Pregnancy; Pregnancy Complications [*blood, *diagnosis]; Randomized Controlled Trials as Topic","10.1002/14651858.CD012546.pub2","http://dx.doi.org/10.1002/14651858.CD012546.pub2","Hepato-Biliary"
"CD012806.PUB2","Dinnes, J; Ferrante di Ruffano, L; Takwoingi, Y; Cheung, ST; Nathan, P; Matin, RN; Chuchu, N; Chan, SA; Durack, A; Bayliss, SE; Gulati, A; Patel, L; Davenport, C; Godfrey, K; Subesinghe, M; Traill, Z; Deeks, JJ; Williams, HC","Ultrasound, CT, MRI, or PET‐CT for staging and re‐staging of adults with cutaneous melanoma","Cochrane Database of Systematic Reviews","2019","Abstract - Background Melanoma is one of the most aggressive forms of skin cancer, with the potential to metastasise to other parts of the body via the lymphatic system and the bloodstream. Melanoma accounts for a small percentage of skin cancer cases but is responsible for the majority of skin cancer deaths. Various imaging tests can be used with the aim of detecting metastatic spread of disease following a primary diagnosis of melanoma (primary staging) or on clinical suspicion of disease recurrence (re‐staging). Accurate staging is crucial to ensuring that patients are directed to the most appropriate and effective treatment at different points on the clinical pathway. Establishing the comparative accuracy of ultrasound, computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET)‐CT imaging for detection of nodal or distant metastases, or both, is critical to understanding if, how, and where on the pathway these tests might be used. Objectives Primary objectives We estimated accuracy separately according to the point in the clinical pathway at which imaging tests were used. Our objectives were: • to determine the diagnostic accuracy of ultrasound or PET‐CT for detection of  nodal metastases  before sentinel lymph node biopsy in adults with confirmed cutaneous invasive melanoma; and • to determine the diagnostic accuracy of ultrasound, CT, MRI, or PET‐CT for whole body imaging in adults with cutaneous invasive melanoma: ○ for detection of  any metastasis in adults with a primary diagnosis  of melanoma (i.e. primary staging at presentation); and ○ for detection of  any metastasis in adults undergoing staging of recurrence  of melanoma (i.e. re‐staging prompted by findings on routine follow‐up). We undertook separate analyses according to whether accuracy data were reported per patient or per lesion. Secondary objectives We sought to determine the diagnostic accuracy of ultrasound, CT, MRI, or PET‐CT for whole body imaging (detection of any metastasis) in mixed or not clearly described populations of adults with cutaneous invasive melanoma. For study participants undergoing primary staging or re‐staging (for possible recurrence), and for mixed or unclear populations, our objectives were: • to determine the diagnostic accuracy of ultrasound, CT, MRI, or PET‐CT for detection of nodal metastases; • to determine the diagnostic accuracy of ultrasound, CT, MRI, or PET‐CT for detection of distant metastases; and • to determine the diagnostic accuracy of ultrasound, CT, MRI, or PET‐CT for detection of distant metastases according to metastatic site. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists as well as published systematic review articles. Selection criteria We included studies of any design that evaluated ultrasound (with or without the use of fine needle aspiration cytology (FNAC)), CT, MRI, or PET‐CT for staging of cutaneous melanoma in adults, compared with a reference standard of histological confirmation or imaging with clinical follow‐up of at least three months' duration. We excluded studies reporting multiple applications of the same test in more than 10% of study participants. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS‐2)). We estimated accuracy using the bivariate hierarchical method to produce summary sensitivities and specificities with 95% confidence and prediction regions. We undertook analysis of studies allowing direct and indirect comparison between tests. We examined heterogeneity between studies by visually inspecting the forest plots of sensitivity and specificity and summary receiver operating characteristic (ROC) plots. Numbers of identified studies were insufficient to allow formal investigation of potential sources of heterogeneity. Main results We included a total of 39 publications reporting on 5204 study participants; 34 studies reporting data per patient included 4980 study participants with 1265 cases of metastatic disease, and seven studies reporting data per lesion included 417 study participants with 1846 potentially metastatic lesions, 1061 of which were confirmed metastases. The risk of bias was low or unclear for all domains apart from participant flow. Concerns regarding applicability of the evidence were high or unclear for almost all domains. Participant selection from mixed or not clearly defined populations and poorly described application and interpretation of index tests were particularly problematic. The accuracy of imaging for detection of regional nodal metastases before sentinel lymph node biopsy (SLNB) was evaluated in 18 studies. In 11 studies (2614 participants; 542 cases), the summary sensitivity of ultrasound alone was 35.4% (95% confidence interval (CI) 17.0% to 59.4%) and specificity was 93.9% (95% CI 86.1% to 97.5%). Combining pre‐SLNB ultrasound with FNAC revealed summary sensitivity of 18.0% (95% CI 3.58% to 56.5%) and specificity of 99.8% (95% CI 99.1% to 99.9%) (1164 participants; 259 cases). Four studies demonstrated lower sensitivity (10.2%, 95% CI 4.31% to 22.3%) and specificity (96.5%,95% CI 87.1% to 99.1%) for PET‐CT before SLNB (170 participants, 49 cases). When these data are translated to a hypothetical cohort of 1000 people eligible for SLNB, 237 of whom have nodal metastases (median prevalence), the combination of ultrasound with FNAC potentially allows 43 people with nodal metastases to be triaged directly to adjuvant therapy rather than having SLNB first, at a cost of two people with false positive results (who are incorrectly managed). Those with a false negative ultrasound will be identified on subsequent SLNB. Limited test accuracy data were available for whole body imaging via PET‐CT for primary staging or re‐staging for disease recurrence, and none evaluated MRI. Twenty‐four studies evaluated whole body imaging. Six of these studies explored primary staging following a confirmed diagnosis of melanoma (492 participants), three evaluated re‐staging of disease following some clinical indication of recurrence (589 participants), and 15 included mixed or not clearly described population groups comprising participants at a number of different points on the clinical pathway and at varying stages of disease (1265 participants). Results for whole body imaging could not be translated to a hypothetical cohort of people due to paucity of data. Most of the studies (6/9) of primary disease or re‐staging of disease considered PET‐CT, two in comparison to CT alone, and three studies examined the use of ultrasound. No eligible evaluations of MRI in these groups were identified. All studies used histological reference standards combined with follow‐up, and two included FNAC for some participants. Observed accuracy for detection of any metastases for PET‐CT was higher for re‐staging of disease (summary sensitivity from two studies: 92.6%, 95% CI 85.3% to 96.4%; specificity: 89.7%, 95% CI 78.8% to 95.3%; 153 participants; 95 cases) compared to primary staging (sensitivities from individual studies ranged from 30% to 47% and specificities from 73% to 88%), and was more sensitive than CT alone in both population groups, but participant numbers were very small. No conclusions can be drawn regarding routine imaging of the brain via MRI or CT. Authors' conclusions Review authors found a disappointing lack of evidence on the accuracy of imaging in people with a diagnosis of melanoma at different points on the clinical pathway. Studies were small and often reported data according to the number of lesions rather than the number of study participants. Imaging with ultrasound combined with FNAC before SLNB may identify around one‐fifth of those with nodal disease, but confidence intervals are wide and further work is needed to establish cost‐effectiveness. Much of the evidence for whole body imaging for primary staging or re‐staging of disease is focused on PET‐CT, and comparative data with CT or MRI are lacking. Future studies should go beyond diagnostic accuracy and consider the effects of different imaging tests on disease management. The increasing availability of adjuvant therapies for people with melanoma at high risk of disease spread at presentation will have a considerable impact on imaging services, yet evidence for the relative diagnostic accuracy of available tests is limited. Plain language summary How good are ultrasound, CT, MRI, and PET‐CT for identifying spread of disease in the body among people with melanoma? What is the aim of the review? We wanted to find out which imaging tests are better for identifying spread of disease among people with a first diagnosis of melanoma (primary staging) and among people with possible recurrence of melanoma (re‐staging). We looked at the evidence for ultrasound, CT, MRI, and PET‐CT and included 39 studies to answer these questions. Why are imaging tests for melanoma important? Melanoma is one of the most aggressive forms of skin cancer, with potential for metastases (cancer cells) to spread to the lymph nodes and other organs of the body. To make sure that people with melanoma receive the most appropriate and effective treatment, it is important to identify whether the disease has spread and to which parts of the body it has spread. This is called 'staging of disease'. Staging is done to find out if a melanoma has spread to regional lymph nodes or to lymph nodes close to the original melanoma, and to determine if the melanoma has spread to lymph nodes in other parts of the body or to organs of the body such as the liver or the brain (distant metastases). Imaging tests are tools that can be used to help find out how much the disease has spread. Several new treatments are now available for reducing the risk of spread of melanoma and for treating melanoma when it has spread. What was studied in the review? The review includes four imaging tests that create images of the body in different ways. Ultrasound uses high‐frequency sound waves to create images, CT scans use ionising radiation in the form of X‐rays (a very low dose of radiation), and MRI uses large magnets and non‐ionising radiation in the form of radio waves (which are not harmful) to generate images of the body. PET‐CT requires injection of a weakly radioactive substance (FDG). The PET part of the scan identifies areas of the body that take up a lot of FDG (indicating possibly cancerous cells), and the CT part of the scan helps to improve image quality and to more accurately pinpoint areas using more FDG. Ultrasound can also be performed along with a fairly simple procedure called 'fine needle aspiration cytology' (FNAC), by which a very fine needle is used to take a small sample of cells from a lymph node that looks suspicious on ultrasound. A microscope is then used to identify whether or not the cells are malignant. Imaging can be used at different time points after diagnosis of melanoma. Healthcare providers can use imaging to look at the regional lymph nodes closest to the melanoma before a type of surgery called sentinel lymph node biopsy is performed. Sentinel lymph node biopsy takes out the lymph nodes that are most likely to have metastases inside them so they can be tested in a laboratory. Imaging can also be used after sentinel lymph node biopsy or in people with higher‐risk melanoma to look for any spread of disease. Imaging can be used in people who were treated for melanoma at an earlier point and who might be having a recurrence of their disease. What are the main results of the review? Ultrasound of regional lymph nodes before sentinel lymph node biopsy We found 11 relevant studies including 2614 people. Three of these studies compared ultrasound on its own to ultrasound combined with FNAC. Results suggest that the combined procedure correctly identifies around one‐fifth of people with metastases in the lymph nodes with very few false positive results (people with incorrect diagnosis of metastasis). These results can be illustrated by imagining a group of 1000 people with melanoma who are going to have sentinel lymph node biopsy, of whom 237 (24%) have metastases in the lymph nodes. The combination of ultrasound with FNAC potentially allows 43 people with lymph node metastases to be identified and avoid a sentinel lymph node biopsy, at a cost of two people with false positive results who might go on to have the wrong treatment. Those with metastases in the lymph nodes that are missed on ultrasound (false negatives) will be identified on subsequent SLNB. Whole body imaging (detection of any metastases) We found 24 studies, but only nine were clear about the point in the time course of disease that imaging was carried out. Six studies including 492 people looked at imaging for primary staging following a confirmed diagnosis of melanoma, and three studies in 589 people evaluated re‐staging of disease in people with possible recurrence of disease. Most of the studies (6/9) considered PET‐CT, two in comparison to CT alone, and three studies examined the use of ultrasound. We did not find any suitable studies of MRI in these groups. Overall results suggest that PET‐CT is better for correctly identifying people with metastatic spread of disease who might be having a recurrence of disease (re‐staging) than people who have a new diagnosis of melanoma (primary staging). PET‐CT also seems to be better than CT for identifying spread of disease in both groups of people, but studies were very small and results might not be reliable. How reliable are the results of the studies included in this review? In most of our studies, a reliable diagnosis of spread of disease (or reference standard) was made by performing biopsy and by following up with people over time using clinical assessment and imaging. There was often a lack of detail on how patients were followed up and which tests were used. Lots of studies did not include people at clearly defined time points in the disease process, making it difficult to assess the relevance of their results. Reporting of application and interpretation of tests was poor. To whom do the results of this review apply? Thirty‐three studies were done in Europe (85%), and the rest in North America (n = 4), Asia (n = 1), or Oceania (n = 1). The average age of people in the studies was between 50 and 67 years, and around half were men. Studies mostly included people with melanoma on any part of the body, but two included only people with melanoma on the head or neck. Studies often included people at different stages of disease, and we were not able to look at the accuracy of tests for people at any particular disease stage. Studies were small, and their results might not match what happens in real life. What are the implications of this review? Reviewers found some evidence to support the use of imaging with ultrasound combined with FNAC before sentinel lymph node biopsy, but further work is needed to establish cost‐effectiveness. Limited evidence is available for whole body imaging for primary staging or re‐staging of disease. Available evidence is focused on PET‐CT; there are few comparisons with CT and no comparisons with MRI. Future research needs to look at more than test accuracy and must consider the effects of different imaging tests on treatment decisions for patients. How up‐to‐date is this review? The reviewers searched for and included studies published up to August 2016.* *In these studies, biopsy and clinical or imaging follow‐up were the reference standards (methods of establishing the final diagnosis).","7","John Wiley & Sons, Ltd","1465-1858","*Neoplasm Metastasis; *Neoplasm Staging; Adult; Diagnosis, Computer-Assisted [methods]; Humans; Magnetic Resonance Imaging; Melanoma [*diagnostic imaging]; Melanoma, Cutaneous Malignant; Neoplasm Recurrence, Local [diagnostic imaging]; Positron Emission Tomography Computed Tomography; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging]; Tomography, X-Ray Computed; Ultrasonography","10.1002/14651858.CD012806.pub2","http://dx.doi.org/10.1002/14651858.CD012806.pub2","Skin"
"CD012399.PUB2","de Heer, K; Gerritsen, MG; Visser, CE; Leeflang, MMG","Galactomannan detection in broncho‐alveolar lavage fluid for invasive aspergillosis in immunocompromised patients","Cochrane Database of Systematic Reviews","2019","Abstract - Background Invasive aspergillosis (IA) is a life‐threatening opportunistic mycosis that occurs in some people with a compromised immune system. The serum galactomannan enzyme‐linked immunosorbent assay (ELISA) rapidly gained widespread acceptance as part of the diagnostic work‐up of a patient suspected of IA. Due to its non‐invasive nature, it can be used as a routine screening test. The ELISA can also be performed on bronchoalveolar lavage (BAL), allowing sampling of the immediate vicinity of the infection. The invasive nature of acquiring BAL, however, changes the role of the galactomannan test significantly, for example by precluding its use as a routine screening test. Objectives To assess the diagnostic accuracy of galactomannan detection in BAL for the diagnosis of IA in people who are immunocompromised, at different cut‐off values for test positivity, in accordance with the  Cochrane Diagnostic Test Accuracy Handbook . Search methods We searched three bibliographic databases including MEDLINE on 9 September 2016 for aspergillosis and galactomannan as text words and subject headings where appropriate. We checked reference lists of included studies for additional studies. Selection criteria We included cohort studies that examined the accuracy of BAL galactomannan for the diagnosis of IA in immunocompromised patients if they used the European Organization for Research and Treatment of Cancer/Invasive Fungal Infections Cooperative Group and the National Institute of Allergy and Infectious Diseases Mycoses Study Group (EORTC/MSG) classification as reference standard. Data collection and analysis Two review authors assessed study quality and extracted data. Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2) was used for quality assessment. Main results We included 17 studies in our review. All studies except one had a high risk of bias in two or more domains. The diagnostic performance of an optical density index (ODI) of 0.5 as cut‐off value was reported in 12 studies (with 1123 patients). The estimated sensitivity was 0.88 (95% confidence interval (CI) 0.75 to 1.00) and specificity 0.81 (95% CI 0.71 to 0.91). The performance of an ODI of 1.0 as cut‐off value could be determined in 11 studies (with 648 patients). The sensitivity was 0.78 (95% CI 0.61 to 0.95) and specificity 0.93 (95% CI 0.87 to 0.98). At a cut‐off ODI of 1.5 or higher, the heterogeneity in specificity decreased significantly and was invariably >90%. Authors' conclusions The optimal cut‐off value depends on the local incidence and clinical pathway. At a prevalence of 12% a hypothetical population of 1000 patients will consist of 120 patients with IA. At a cut‐off value of 0.5 14 patients with IA will be missed and there will be 167 patients incorrectly diagnosed with IA. If we use the test at a cut‐off value of 1.0, we will miss 26 patients with IA. And there will be 62 patients incorrectly diagnosed with invasive aspergillosis. The populations and results were very heterogeneous. Therefore, interpretation and extrapolation of these results has to be performed with caution. A test result of 1.5 ODI or higher appears a strong indicator of IA. Plain language summary Measuring galactomannan in lung‐washing fluid to diagnose invasive aspergillosis in patients with an impaired immune system Why is improving the accuracy of the diagnosis of invasive fungal disease important? Some people have a weakened immune system; they are said to be 'immunocompromised'. People can be immunocompromised when they have leukaemia, or HIV/AIDS, and when using some forms of medication. Immunocompromised means that the immune system is less able to fight infections. This can result in an infection that does not occur under normal conditions. This is called an opportunistic infection. An example is aspergillosis, an infection caused by a fungus. As the fungal infection is not stopped by the immune system, it spreads through the body. The fungus 'invades' the body ‐ hence we call this invasive aspergillosis. This may be lethal. Early diagnosis allows timely treatment and stops the fungus from further invading the body. What diagnostic test was studied in the review? We wanted to find out how good 'galactomannan detection' is for diagnosing an infection of invasive aspergillosis. Galactomannan detection is a diagnostic procedure. It works by passing an instrument through the mouth or nose into the airways. Fluid is flushed into a part of the lung. This fluid washes fungi and other microbes from the lungs and is collected and analysed. This process is called bronchoalveolar lavage (lung‐washing), shortened to BAL. One of the things that may be in the collected BAL fluid is galactomannan, a component of the fungal cell wall. The result of the BAL galactomannan test is part of international criteria used to differentiate between patients who are likely to have invasive aspergillosis and patients who are less likely to have invasive aspergillosis. If multiple criteria are fulfilled (e.g. imaging tests and clinical signs and symptoms), then the patients will be treated with antifungal medication. What is the aim of this review? The aim of this review was to determine the error rate of galactomannan testing on BAL. To this end, we searched for studies on this subject. The galactomannan test results in an 'optical density index' (ODI), not a yes or no answer. The studies differed with respect to the cut‐off value above which a result was considered positive. Most considered a result above an ODI of 0.5 or 1.0 as positive. What are the main results of this review? Imagine a group of 1000 people with an impaired immune system where 120 (12%) have invasive aspergillosis. The results of this review indicate that in theory, if the BAL galactomannan test were to be used using an ODI of 0.5 or higher to decide which results are positive: ‐ an estimated 273 will have a test result indicating that they probably have invasive aspergillosis, and of these 167 will be incorrectly classified as having invasive aspergillosis; and ‐ of the 727 people with a result indicating that invasive aspergillosis is not likely to be present, 14 will be incorrectly classified as not having invasive aspergillosis. If a higher cut‐off value is used to determine test positivity, then more patients will have a false‐negative result and fewer patients will have a false‐positive result. How reliable are the results of the studies in this review? In the included studies, the diagnosis of invasive aspergillosis was made by assessing all patients with international criteria (known as the reference standard). This is likely to have been a reliable method for deciding whether patients really had invasive aspergillosis. The results of the various studies differed and had small numbers of patients with invasive aspergillosis. Therefore, the numbers are an average across the studies in the review and we cannot be sure that the test will always produce these results. Who do the results of this review apply to? The results are mainly applicable to adults who undergo intensive chemotherapy or a stem cell transplant for a form of cancer of the blood cells. How up‐to‐date is this review? The review authors searched for and used studies published up to 9 September 2016.","5","John Wiley & Sons, Ltd","1465-1858","*Immunocompromised Host; Aspergillosis [*diagnosis, immunology]; Biomarkers [blood]; Bronchoalveolar Lavage Fluid [*microbiology]; Galactose [analogs & derivatives]; Humans; Invasive Fungal Infections; Mannans [*blood]; Randomized Controlled Trials as Topic; Sensitivity and Specificity","10.1002/14651858.CD012399.pub2","http://dx.doi.org/10.1002/14651858.CD012399.pub2","Airways"
"CD012245.PUB2","Heazell, AEP; Hayes, DJL; Whitworth, M; Takwoingi, Y; Bayliss, SE; Davenport, C","Biochemical tests of placental function versus ultrasound assessment of fetal size for stillbirth and small‐for‐gestational‐age infants","Cochrane Database of Systematic Reviews","2019","Abstract - Background Stillbirth affects 2.6 million pregnancies worldwide each year. Whilst the majority of cases occur in low‐ and middle‐income countries, stillbirth remains an important clinical issue for high‐income countries (HICs) ‐ with both the UK and the USA reporting rates above the mean for HICs. In HICs, the most frequently reported association with stillbirth is placental dysfunction. Placental dysfunction may be evident clinically as fetal growth restriction (FGR) and small‐for‐dates infants. It can be caused by placental abruption or hypertensive disorders of pregnancy and many other disorders and factors Placental abnormalities are noted in 11% to 65% of stillbirths. Identification of FGR is difficult in utero. Small‐for‐gestational age (SGA), as assessed after birth, is the most commonly used surrogate measure for this outcome. The degree of SGA is associated with the likelihood of FGR; 30% of infants with a birthweight < 10th centile are thought to be FGR, while 70% of infants with a birthweight < 3rd centile are thought to be FGR. Critically, SGA is the most significant antenatal risk factor for a stillborn infant. Correct identification of SGA infants is associated with a reduction in the perinatal mortality rate. However, currently used tests, such as measurement of symphysis‐fundal height, have a low reported sensitivity and specificity for the identification of SGA infants. Objectives The primary objective was to assess and compare the diagnostic accuracy of ultrasound assessment of fetal growth by estimated fetal weight (EFW) and placental biomarkers alone and in any combination used after 24 weeks of pregnancy in the identification of placental dysfunction as evidenced by either stillbirth, or birth of a SGA infant. Secondary objectives were to investigate the effect of clinical and methodological factors on test performance. Search methods We developed full search strategies with no language or date restrictions. The following sources were searched: MEDLINE, MEDLINE In Process and Embase via Ovid, Cochrane (Wiley) CENTRAL, Science Citation Index (Web of Science), CINAHL (EBSCO) with search strategies adapted for each database as required; ISRCTN Registry, UK Clinical Trials Gateway, WHO International Clinical Trials Portal and ClinicalTrials.gov for ongoing studies; specialist abstract and conference proceeding resources (British Library’s ZETOC and Web of Science Conference Proceedings Citation Index). Search last conducted in Ocober 2016. Selection criteria We included studies of pregnant women of any age with a gestation of at least 24 weeks if relevant outcomes of pregnancy (live birth/stillbirth; SGA infant) were assessed. Studies were included irrespective of whether pregnant women were deemed to be low or high risk for complications or were of mixed populations (low and high risk). Pregnancies complicated by fetal abnormalities and multi‐fetal pregnancies were excluded as they have a higher risk of stillbirth from non‐placental causes. With regard to biochemical tests, we included assays performed using any technique and at any threshold used to determine test positivity. Data collection and analysis We extracted the numbers of true positive, false positive, false negative, and true negative test results from each study. We assessed risk of bias and applicability using the QUADAS‐2 tool. Meta‐analyses were performed using the hierarchical summary ROC model to estimate and compare test accuracy. Main results We included 91 studies that evaluated seven tests — blood tests for human placental lactogen (hPL), oestriol, placental growth factor (PlGF) and uric acid, ultrasound EFW and placental grading and urinary oestriol — in a total of 175,426 pregnant women, in which 15,471 pregnancies ended in the birth of a small baby and 740 pregnancies which ended in stillbirth. The quality of included studies was variable with most domains at low risk of bias although 59% of studies were deemed to be of unclear risk of bias for the reference standard domain. Fifty‐three per cent of studies were of high concern for applicability due to inclusion of only high‐ or low‐risk women. Using all available data for SGA (86 studies; 159,490 pregnancies involving 15,471 SGA infants), there was evidence of a difference in accuracy (P < 0.0001) between the seven tests for detecting pregnancies that are SGA at birth. Ultrasound EFW was the most accurate test for detecting SGA at birth with a diagnostic odds ratio (DOR) of 21.3 (95% CI 13.1 to 34.6); hPL was the most accurate biochemical test with a DOR of 4.78 (95% CI 3.21 to 7.13). In a hypothetical cohort of 1000 pregnant women, at the median specificity of 0.88 and median prevalence of 19%, EFW, hPL, oestriol, urinary oestriol, uric acid, PlGF and placental grading will miss 50 (95% CI 32 to 68), 116 (97 to 133), 124 (108 to 137), 127 (95 to 152), 139 (118 to 154), 144 (118 to 161), and 144 (122 to 161) SGA infants, respectively. For the detection of pregnancies ending in stillbirth (21 studies; 100,687 pregnancies involving 740 stillbirths), in an indirect comparison of the four biochemical tests, PlGF was the most accurate test with a DOR of 49.2 (95% CI 12.7 to 191). In a hypothetical cohort of 1000 pregnant women, at the median specificity of 0.78 and median prevalence of 1.7%, PlGF, hPL, urinary oestriol and uric acid will miss 2 (95% CI 0 to 4), 4 (2 to 8), 6 (6 to 7) and 8 (3 to 13) stillbirths, respectively. No studies assessed the accuracy of ultrasound EFW for detection of pregnancy ending in stillbirth. Authors' conclusions Biochemical markers of placental dysfunction used alone have insufficient accuracy to identify pregnancies ending in SGA or stillbirth. Studies combining U and placental biomarkers are needed to determine whether this approach improves diagnostic accuracy over the use of ultrasound estimation of fetal size or biochemical markers of placental dysfunction used alone. Many of the studies included in this review were carried out between 1974 and 2016. Studies of placental substances were mostly carried out before 1991 and after 2013; earlier studies may not reflect developments in test technology. Plain language summary Blood tests in late pregnancy to identify small babies and those at risk of stillbirth Background Placental dysfunction describes when the placenta does not meet the demands of the growing baby; it may result in a baby that is smaller than expected or is stillborn. Currently, it is not easy to detect placental dysfunction before birth; ultrasound scans are most often used to identify small babies. However, tests can measure substances made by the placenta in mothers’ blood and urine which may detect a placenta that is not functioning well. We aimed to find the best test to identify placental dysfunction. What we did We searched for studies in October 2016 and identified and total of 24,059 studies ‐ with 91 of those studies providing us with information that we could include in this review. We looked at ultrasound scanning and six different tests of placental substances, including proteins and hormones. These studies involved 175,426 women in total of which 15,471 pregnancies ended in the birth of a small baby and 740 pregnancies which ended in stillbirth. What we found Of the 91 included studies, 86 had information on small babies, of which 18 also looked at stillbirth; another five studies only looked at stillbirth. The most accurate test for detecting a small baby was ultrasound scan to estimate a baby’s weight. Of the substances measured in mother’s blood, human placental lactogen (hPL), a hormone produced by the placenta during pregnancy, was the most accurate. There was only one study which looked at both ultrasound scanning and measurement of a placental substance. Placental growth factor (PlGF) was the most accurate test of a placental substance to identify a baby that would be stillborn; there were no studies of ultrasound scanning to detect a baby that would be stillborn. Tests of placental substances were better at identifying a baby at risk of stillbirth than detecting a small baby. Other important information to consider Many of the studies included in this review were carried out between 1974 and 2016. Studies of placental substances were mostly carried out before 1991 and after 2013; earlier studies may not reflect developments in test technology. More studies are needed to find out whether a combination of ultrasound scans and mother’s blood tests could improve identification of pregnancies which end in the birth of a small baby or in a stillborn baby. No studies were identified for this review that looked at the accuracy of ultrasound and blood tests used together.","5","John Wiley & Sons, Ltd","1465-1858","*Infant, Small for Gestational Age; *Pregnancy Outcome; *Ultrasonography, Prenatal; Female; Fetal Development [*physiology]; Fetal Growth Retardation; Humans; Infant, Newborn; Perinatal Mortality; Placenta [*diagnostic imaging]; Pregnancy; Stillbirth","10.1002/14651858.CD012245.pub2","http://dx.doi.org/10.1002/14651858.CD012245.pub2","Pregnancy and Childbirth"
"CD011811.PUB2","Molano Franco, D; Arevalo‐Rodriguez, I; Roqué i Figuls, M; Montero Oleas, NG; Nuvials, X; Zamora, J","Plasma interleukin‐6 concentration for the diagnosis of sepsis in critically ill adults","Cochrane Database of Systematic Reviews","2019","Abstract - Background The definition of sepsis has evolved over time, along with the clinical and scientific knowledge behind it. For years, sepsis was defined as a systemic inflammatory response syndrome (SIRS) in the presence of a documented or suspected infection. At present, sepsis is defined as a life‐threatening organ dysfunction resulting from a dysregulated host response to infection. Even though sepsis is one of the leading causes of mortality in critically ill patients, and the World Health Organization (WHO) recognizes it as a healthcare priority, it still lacks an accurate diagnostic test. Determining the accuracy of interleukin‐6 (IL‐6) concentrations in plasma, which is proposed as a new biomarker for the diagnosis of sepsis, might be helpful to provide adequate and timely management of critically ill patients, and thus reduce the morbidity and mortality associated with this condition. Objectives To determine the diagnostic accuracy of plasma interleukin‐6 (IL‐6) concentration for the diagnosis of bacterial sepsis in critically ill adults. Search methods We searched CENTRAL, MEDLINE, Embase, LILACS, and Web of Science on 25 January 2019. We screened references in the included studies to identify additional studies. We did not apply any language restriction to the electronic searches. Selection criteria We included diagnostic accuracy studies enrolling critically ill adults aged 18 years or older under suspicion of sepsis during their hospitalization, where IL‐6 concentrations were evaluated by serological measurement. Data collection and analysis Two review authors independently screened the references to identify relevant studies and extracted data. We assessed the methodological quality of studies using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. We estimated a summary receiver operating characteristic (SROC) curve by fitting a hierarchical summary ROC (HSROC) non‐linear mixed model. We explored sources of heterogeneity using the HSROC model parameters. We conducted all analyses in the SAS statistical software package and R software. Main results We included 23 studies (n = 4192) assessing the accuracy of IL‐6 for the diagnosis of sepsis in critically ill adults. Twenty studies that were available as conference proceedings only are awaiting classification. The included participants were heterogeneous in terms of their distribution of age, gender, main diagnosis, setting, country, positivity threshold, sepsis criteria, year of publication, and origin of infection, among other factors. Prevalence of sepsis greatly varied across studies, ranging from 12% to 78%. We considered all studies to be at high risk of bias due to issues related to the index test domain in QUADAS‐2. The SROC curve showed a great dispersion in individual studies accuracy estimates (21 studies, 3650 adult patients), therefore the considerable heterogeneity in the collected data prevented us from calculating formal accuracy estimates. Using a fixed prevalence of sepsis of 50% and a fixed specificity of 74%, we found a sensitivity of 66% (95% confidence interval 60 to 72). If we test a cohort 1000 adult patients under suspicion of sepsis with IL‐6, we will find that 330 patients would receive appropriate and timely antibiotic therapy, while 130 patients would be wrongly considered to have sepsis. In addition, 370 out of 1000 patients would avoid unnecessary antibiotic therapy, and 170 patients would have been undiagnosed of sepsis. This numerical approach should be interpreted with caution due to the limitations described above. Authors' conclusions Our evidence assessment of plasma interleukin‐6 concentrations for the diagnosis of sepsis in critically ill adults reveals several limitations. High heterogeneity of collected evidence regarding the main diagnosis, setting, country, positivity threshold, sepsis criteria, year of publication, and the origin of infection, among other factors, along with the potential number of misclassifications, remain significant constraints for its implementation. The 20 conference proceedings assessed as studies awaiting classification may alter the conclusions of the review once they are fully published and evaluated. Further studies about the accuracy of interleukin‐6 for the diagnosis of sepsis in adults that apply rigorous methodology for conducting diagnostic test accuracy studies are needed. The conclusions of the review will likely change once the 20 studies pending publication are fully published and included. Plain language summary Levels of interleukin‐6 in identifying severely ill adult patients with sepsis Review question We evaluated the evidence on the ability of interleukin‐6 (IL‐6) levels in plasma to identify adult patients with sepsis. Interleukin‐6 is a cytokine (a broad and loose category of small proteins) secreted by immune cells that mediates a wide range of biological activities. Background Sepsis is a potentially life‐threatening response by the immune system to an infection that can result in tissue damage, organ failure, and even death, and should be considered as a medical emergency. About 288 septic cases by 100,000 person‐years occur in hospital settings, and 17% of those patients could die. Early identification of patients having sepsis is the first step for immediate medical management, which is essential to avoid further complications and death. Treatment consists mainly of the use of antibiotics (a drug that inhibits the growth of dangerous micro‐organisms). Several tools have been proposed for sepsis diagnosis, as well as the physical examination of blood cultures (the assessment of blood samples to identify micro‐organisms causing the infection). Interleukin‐6 is a molecule that helps in the communication of cells during the body's response to an infection. It has been suggested that the measurement of levels of IL‐6 in the plasma from blood samples during the onset of sepsis can be helpful in identifying sepsis patients early and initiating adequate treatment. Study characteristics We performed a thorough literature search for studies reporting the use of IL‐6 levels for detection of sepsis up to January 2019. We found 23 studies enrolling 4192 severely ill adults. Key results Our assessment of the evidence reveals the complexity of the research topic, represented in the high variability of information reported by the studies. We found the characteristics of assessed patients to vary considerably between studies in terms of age, gender, setting, initial diagnosis, indicative value for sepsis, and source of infection, among other factors. This variability in the collected data prevented a formal numerical synthesis of the findings. Using the available data to perform an approximated estimation of the consequences, we found that 700 out of 1000 patients under suspicion of sepsis might be correctly classified, but 130 out of 1000 patients would be wrongly considered as having sepsis, while 170 out of 1000 patients might be incorrectly considered as not having sepsis. These errors would result in a serious increase in the risk of further morbidity and death due to delays of adequate treatment. This information should be interpreted with caution due to limitations in the collected data. Quality of the evidence We judged the included studies to have important limitations in their validity, hence they are at high risk of providing distorted results (i.e. to be at high risk of bias).","4","John Wiley & Sons, Ltd","1465-1858","Biomarkers [blood]; Critical Illness; Diagnosis, Differential; Humans; Interleukin‐6 [*blood]; Sepsis [*diagnosis]","10.1002/14651858.CD011811.pub2","http://dx.doi.org/10.1002/14651858.CD011811.pub2","Emergency and Critical Care"
"CD012663.PUB2","Drost, FJH; Osses, DF; Nieboer, D; Steyerberg, EW; Bangma, CH; Roobol, MJ; Schoots, IG","Prostate MRI, with or without MRI‐targeted biopsy, and systematic biopsy for detecting prostate cancer","Cochrane Database of Systematic Reviews","2019","Abstract - Background Multiparametric magnetic resonance imaging (MRI), with or without MRI‐targeted biopsy, is an alternative test to systematic transrectal ultrasonography‐guided biopsy in men suspected of having prostate cancer. At present, evidence on which test to use is insufficient to inform detailed evidence‐based decision‐making. Objectives To determine the diagnostic accuracy of the index tests MRI only, MRI‐targeted biopsy, the MRI pathway (MRI with or without MRI‐targeted biopsy) and systematic biopsy as compared to template‐guided biopsy as the reference standard in detecting clinically significant prostate cancer as the target condition, defined as International Society of Urological Pathology (ISUP) grade 2 or higher. Secondary target conditions were the detection of grade 1 and grade 3 or higher‐grade prostate cancer, and a potential change in the number of biopsy procedures. Search methods We performed a comprehensive systematic literature search up to 31 July 2018. We searched CENTRAL, MEDLINE, Embase, eight other databases and one trials register. Selection criteria We considered for inclusion any cross‐sectional study if it investigated one or more index tests verified by the reference standard, or if it investigated the agreement between the MRI pathway and systematic biopsy, both performed in the same men. We included only studies on men who were biopsy naïve or who previously had a negative biopsy (or a mix of both). Studies involving MRI had to report on both MRI‐positive and MRI‐negative men. All studies had to report on the primary target condition. Data collection and analysis Two reviewers independently extracted data and assessed the risk of bias using the QUADAS‐2 tool. To estimate test accuracy, we calculated sensitivity and specificity using the bivariate model. To estimate agreement between the MRI pathway and systematic biopsy, we synthesised detection ratios by performing random‐effects meta‐analyses. To estimate the proportions of participants with prostate cancer detected by only one of the index tests, we used random‐effects multinomial or binary logistic regression models. For the main comparisions, we assessed the certainty of evidence using GRADE. Main results The test accuracy analyses included 18 studies overall. MRI compared to template‐guided biopsy:  Based on a pooled sensitivity of 0.91 (95% confidence interval (CI): 0.83 to 0.95; 12 studies; low certainty of evidence) and a pooled specificity of 0.37 (95% CI: 0.29 to 0.46; 12 studies; low certainty of evidence) using a baseline prevalence of 30%, MRI may result in 273 (95% CI: 249 to 285) true positives, 441 false positives (95% CI: 378 to 497), 259 true negatives (95% CI: 203 to 322) and 27 (95% CI: 15 to 51) false negatives per 1000 men. We downgraded the certainty of evidence for study limitations and inconsistency. MRI‐targeted biopsy compared to template‐guided biopsy:  Based on a pooled sensitivity of 0.80 (95% CI: 0.69 to 0.87; 8 studies; low certainty of evidence) and a pooled specificity of 0.94 (95% CI: 0.90 to 0.97; 8 studies; low certainty of evidence) using a baseline prevalence of 30%, MRI‐targeted biopsy may result in 240 (95% CI: 207 to 261) true positives, 42 (95% CI: 21 to 70) false positives, 658 (95% CI: 630 to 679) true negatives and 60 (95% CI: 39 to 93) false negatives per 1000 men. We downgraded the certainty of evidence for study limitations and inconsistency. The MRI pathway compared to template‐guided biopsy:  Based on a pooled sensitivity of 0.72 (95% CI: 0.60 to 0.82; 8 studies; low certainty of evidence) and a pooled specificity of 0.96 (95% CI: 0.94 to 0.98; 8 studies; low certainty of evidence) using a baseline prevalence of 30%, the MRI pathway may result in 216 (95% CI: 180 to 246) true positives, 28 (95% CI: 14 to 42) false positives, 672 (95% CI: 658 to 686) true negatives and 84 (95% CI: 54 to 120) false negatives per 1000 men. We downgraded the certainty of evidence for study limitations, inconsistency and imprecision. Systemic biopsy compared to template‐guided biopsy:  Based on a pooled sensitivity of 0.63 (95% CI: 0.19 to 0.93; 4 studies; low certainty of evidence) and a pooled specificity of 1.00 (95% CI: 0.91 to 1.00; 4 studies; low certainty of evidence) using a baseline prevalence of 30%, systematic biopsy may result in 189 (95% CI: 57 to 279) true positives, 0 (95% CI: 0 to 63) false positives, 700 (95% CI: 637 to 700) true negatives and 111 (95% CI: 21 to 243) false negatives per 1000 men. We downgraded the certainty of evidence for study limitations and inconsistency. Agreement analyses:  In a mixed population of both biopsy‐naïve and prior‐negative biopsy men comparing the MRI pathway to systematic biopsy, we found a pooled detection ratio of 1.12 (95% CI: 1.02 to 1.23; 25 studies). We found pooled detection ratios of 1.44 (95% CI 1.19 to 1.75; 10 studies) in prior‐negative biopsy men and 1.05 (95% CI: 0.95 to 1.16; 20 studies) in biopsy‐naïve men. Authors' conclusions Among the diagnostic strategies considered, the MRI pathway has the most favourable diagnostic accuracy in clinically significant prostate cancer detection. Compared to systematic biopsy, it increases the number of significant cancer detected while reducing the number of insignificant cancer diagnosed. The certainty in our findings was reduced by study limitations, specifically issues surrounding selection bias, as well as inconsistency. Based on these findings, further improvement of prostate cancer diagnostic pathways should be pursued. Plain language summary Is prostate MRI, with or without MRI‐targeted biopsy, better than systematic biopsy for detecting prostate cancer in men? Background Many prostate cancers are slow growing and may not have any harmful effects during a man's lifetime. Meanwhile, clinically significant cancers can cause problems such as blockage of the urinary tract, painful bone lesions and death. The prostate‐specific antigen (PSA) test followed by tissue samples of the prostate with ultrasound guidance is often used to detect these cancers early. More recently, magnetic resonance imaging (MRI) has also been used to help make the diagnosis. What is the aim of this review? The aim of this review was to compare MRI alone, MRI together with a biopsy, and a pathway that uses MRI to help decide whether to do a biopsy or not (hereinafter named ‘the MRI pathway’) with the standard ultrasound guided biopsy (hereinafter called ‘systematic biopsy’) in reference to template‐guided biopsy. What are the main results? We examined evidence up to July 2018. The review included 43 studies, mainly from Western countries, of men aged 61 to 73 years. In a population of 1000 men at risk for prostate cancer, where 300 men actually have clinically significant prostate cancer, MRI will correctly identify 273 men as having clinically significant prostate cancer but miss the remaining 27 men; for the 700 men that do not have clinically significant prostate cancer, MRI will correctly identify 259 as not having prostate cancer but will misclassify 441 men as having clinically significant prostate cancer. In the same population, MRI‐targeted biopsy will correctly identify 240 of 300 men as having clinically significant prostate cancer but miss the remaining 60 men; for the 700 men that do not have clinically significant prostate cancer, MRI will correctly identify 658 as not having prostate cancer but misclassify 42 men as having clinically significant prostate cancer. The MRI pathway will correctly identify 216 of 300 men as having clinically significant prostate cancer but miss the remaining 84 men; for the 700 men that do not have clinically significant prostate cancer, MRI pathway will correctly identify 672 as not having prostate cancer but will misclassify 28 men as having clinically significant prostate cancer. Systematic biopsies will correctly identify 189 of 300 men as having clinically significant prostate cancer but miss the remaining 111 men; for the 700 men that do not have clinically significant prostate cancer, systematic biopsies may correctly identify all 700 as not having prostate cancer and will not misclassify any men as having clinically significant prostate cancer. When comparing the MRI pathway to systematic biopsy in a mixed group of men who may or may not have had a prior biopsy, we found that MRI pathway is 12% more likely to make the correct diagnosis. In men without a prior biopsy, the MRI pathway is 5% more likely to make the correct diagnosis, whereas in men who have had a negative biospy, it is 44% more likely to make the correct diagnosis. How reliable is the evidence? We rated the quality of evidence for the main findings of this review as low. Additional high‐quality research is likely to change these findings. What are the implications of this review? The findings of this Cochrane review suggest that the MRI pathway is better than systematic biopsies in making a correct diagnosis of clinically significant prostate cancer. However, the MRI pathway still misses some men with clinically significant prostate cancer. Therefore, further research in this area is important.","4","John Wiley & Sons, Ltd","1465-1858","Biopsy [*methods]; Humans; Magnetic Resonance Imaging [*methods]; Male; Prostate [*pathology]; Prostatic Neoplasms [*diagnosis, pathology]","10.1002/14651858.CD012663.pub2","http://dx.doi.org/10.1002/14651858.CD012663.pub2","Urology"
"CD011427.PUB2","Zhelev, Z; Walker, G; Henschke, N; Fridhandler, J; Yip, S","Prehospital stroke scales as screening tools for early identification of stroke and transient ischemic attack","Cochrane Database of Systematic Reviews","2019","Abstract - Background Rapid and accurate detection of stroke by paramedics or other emergency clinicians at the time of first contact is crucial for timely initiation of appropriate treatment. Several stroke recognition scales have been developed to support the initial triage. However, their accuracy remains uncertain and there is no agreement which of the scales perform better. Objectives To systematically identify and review the evidence pertaining to the test accuracy of validated stroke recognition scales, as used in a prehospital or emergency room (ER) setting to screen people suspected of having stroke. Search methods We searched CENTRAL, MEDLINE (Ovid), Embase (Ovid) and the Science Citation Index to 30 January 2018. We handsearched the reference lists of all included studies and other relevant publications and contacted experts in the field to identify additional studies or unpublished data. Selection criteria We included studies evaluating the accuracy of stroke recognition scales used in a prehospital or ER setting to identify stroke and transient Ischemic attack (TIA) in people suspected of stroke. The scales had to be applied to actual people and the results compared to a final diagnosis of stroke or TIA. We excluded studies that applied scales to patient records; enrolled only screen‐positive participants and without complete 2 × 2 data. Data collection and analysis Two review authors independently conducted a two‐stage screening of all publications identified by the searches, extracted data and assessed the methodologic quality of the included studies using a tailored version of QUADAS‐2. A third review author acted as an arbiter. We recalculated study‐level sensitivity and specificity with 95% confidence intervals (CI), and presented them in forest plots and in the receiver operating characteristics (ROC) space. When a sufficient number of studies reported the accuracy of the test in the same setting (prehospital or ER) and the level of heterogeneity was relatively low, we pooled the results using the bivariate random‐effects model. We plotted the results in the summary ROC (SROC) space presenting an estimate point (mean sensitivity and specificity) with 95% CI and prediction regions. Because of the small number of studies, we did not conduct meta‐regression to investigate between‐study heterogeneity and the relative accuracy of the scales. Instead, we summarized the results in tables and diagrams, and presented our findings narratively. Main results We selected 23 studies for inclusion (22 journal articles and one conference abstract). We evaluated the following scales: Cincinnati Prehospital Stroke Scale (CPSS; 11 studies), Recognition of Stroke in the Emergency Room (ROSIER; eight studies), Face Arm Speech Time (FAST; five studies), Los Angeles Prehospital Stroke Scale (LAPSS; five studies), Melbourne Ambulance Stroke Scale (MASS; three studies), Ontario Prehospital Stroke Screening Tool (OPSST; one study), Medic Prehospital Assessment for Code Stroke (MedPACS; one study) and PreHospital Ambulance Stroke Test (PreHAST; one study). Nine studies compared the accuracy of two or more scales. We considered 12 studies at high risk of bias and one with applicability concerns in the patient selection domain; 14 at unclear risk of bias and one with applicability concerns in the reference standard domain; and the risk of bias in the flow and timing domain was high in one study and unclear in another 16. We pooled the results from five studies evaluating ROSIER in the ER and five studies evaluating LAPSS in a prehospital setting. The studies included in the meta‐analysis of ROSIER were of relatively good methodologic quality and produced a summary sensitivity of 0.88 (95% CI 0.84 to 0.91), with the prediction interval ranging from approximately 0.75 to 0.95. This means that the test will miss on average 12% of people with stroke/TIA which, depending on the circumstances, could range from 5% to 25%. We could not obtain a reliable summary estimate of specificity due to extreme heterogeneity in study‐level results. The summary sensitivity of LAPSS was 0.83 (95% CI 0.75 to 0.89) and summary specificity 0.93 (95% CI 0.88 to 0.96). However, we were uncertain in the validity of these results as four of the studies were at high and one at uncertain risk of bias. We did not report summary estimates for the rest of the scales, as the number of studies per test per setting was small, the risk of bias was high or uncertain, the results were highly heterogenous, or a combination of these. Studies comparing two or more scales in the same participants reported that ROSIER and FAST had similar accuracy when used in the ER. In the field, CPSS was more sensitive than MedPACS and LAPSS, but had similar sensitivity to that of MASS; and MASS was more sensitive than LAPSS. In contrast, MASS, ROSIER and MedPACS were more specific than CPSS; and the difference in the specificities of MASS and LAPSS was not statistically significant. Authors' conclusions In the field, CPSS had consistently the highest sensitivity and, therefore, should be preferred to other scales. Further evidence is needed to determine its absolute accuracy and whether alternatives scales, such as MASS and ROSIER, which might have comparable sensitivity but higher specificity, should be used instead, to achieve better overall accuracy. In the ER, ROSIER should be the test of choice, as it was evaluated in more studies than FAST and showed consistently high sensitivity. In a cohort of 100 people of whom 62 have stroke/TIA, the test will miss on average seven people with stroke/TIA (ranging from three to 16). We were unable to obtain an estimate of its summary specificity. Because of the small number of studies per test per setting, high risk of bias, substantial differences in study characteristics and large between‐study heterogeneity, these findings should be treated as provisional hypotheses that need further verification in better‐designed studies. Plain language summary Accuracy of prehospital stroke scales to identify people with stroke or transient ischemic attack (TIA) Background Stroke is a life‐threatening medical condition in which brain tissue is damaged. This could be caused by a clot blocking the blood supply to part of the brain or bleeding in the brain. If symptoms resolve within 24 hours without lasting consequences, the condition is called TIA (mini stroke). Effective treatment depends on early identification of stroke and any delays may result in brain damage or death. Emergency medical services are the first point of contact for people experiencing symptoms suggestive of stroke. Medical responders could identify people with stroke more accurately if they use checklists called stroke recognition scales. Such scales include symptoms and other readily‐available information. A positive result on the scale indicates high risk of stroke and the need of urgent specialist assessment. The scales do not differentiate between stroke and TIA; this is done in hospital by a neurologist or stroke physician. Our objective was to review the research evidence on how accurately stroke recognition scales can detect stroke or TIA when used by paramedics or other prehospital clinicians, who are the first point of contact for people suspected of stroke. Study characteristics The evidence is current to 30 January 2018. We included studies assessing the accuracy of stroke recognition scales when applied to adults suspected of stroke out of hospital. We included 23 studies evaluating the following scales: Cincinnati Prehospital Stroke Scale (CPSS; 11 studies), Recognition of Stroke in the Emergency Room (ROSIER; eight studies), Face Arm Speech Time (FAST; five studies), Los Angeles Prehospital Stroke Scale (LAPSS; five studies), Melbourne Ambulance Stroke Scale (MASS; three studies), Ontario Prehospital Stroke Screening Tool (OPSST; one study), Medic Prehospital Assessment for Code Stroke (MedPACS; one study) and PreHospital Ambulance Stroke Test (PreHAST; one study). Nine studies compared two or more scales in the same people. The results from five studies were combined to estimate the accuracy of ROSIER in the emergency room (ER) and five studies to estimate the accuracy of LAPSS when used by ambulance clinicians. Quality of the evidence Many of the studies were of poor or unclear quality and we could not be sure that their results were valid. Key results of the accuracy of the evaluated prehospital stroke scales Studies differed considerably in terms of included participants and other characteristics. As a consequence, studies evaluating the same scale reported variable results. We combined five studies evaluating ROSIER in the ER and obtained average sensitivity of 88% (88 out of 100 people with stroke/TIA will test positive on ROSIER). We were unable to obtain an estimate of specificity (how many people without stroke/TIA will test negative). We also combined the results for LAPSS, but the included studies were of poor quality and the results may not be valid. The rest of the scales were evaluated in a smaller number of studies or the results were too variable to be combined statistically. A small number of studies compared two or more scales when applied to the same participants. Such studies are more likely to produce valid results as the scales are used in the same circumstances. They reported that in the ER, ROSIER and FAST had similar accuracy, but ROSIER was evaluated in more studies. When used by ambulance staff, CPSS identified more people with stroke/TIA in all studies, but also more people without stroke/TIA tested positive. Conclusion Current evidence suggests that CPSS should be used by ambulance clinicians in the field. Further research is needed to estimate the proportion of wrong results and whether alternatives scales, such as MASS and ROSIER, which might have comparable sensitivity but higher specificity, should be used instead to achieve better overall accuracy. In the ER, ROSIER should be the test of choice. In a group of 100 people of whom 62 have stroke/TIA, the test will miss on average seven people with stroke/TIA (ranging from three to 16). Because of the small number of studies evaluating the tests in a specific setting, poor quality, substantial differences in study characteristics and variability in results, these findings should be treated with caution and need further verification in better‐designed studies.","4","John Wiley & Sons, Ltd","1465-1858","Humans; Ischemic Attack, Transient [*diagnosis]; Mass Screening; Randomized Controlled Trials as Topic; Severity of Illness Index; Stroke [*diagnosis]","10.1002/14651858.CD011427.pub2","http://dx.doi.org/10.1002/14651858.CD011427.pub2","Stroke"
"CD009786.PUB3","van de Vrie, R; Rutten, MJ; Asseler, JD; Leeflang, MMG; Kenter, GG; Mol, BWJ; Buist, M","Laparoscopy for diagnosing resectability of disease in women with advanced ovarian cancer","Cochrane Database of Systematic Reviews","2019","Abstract - Background This is an update of a Cochrane Review that was originally published in 2014, Issue 2.    The presence of residual disease after primary debulking surgery is a highly significant prognostic factor in women with advanced ovarian cancer. In up to 60% of women, residual tumour of > 1 cm is left behind after primary debulking surgery (defined as suboptimal debulking). These women might have benefited from neoadjuvant chemotherapy (NACT) prior to interval debulking surgery instead of primary debulking surgery followed by chemotherapy. It is therefore important to select accurately those women who would best be treated with primary debulking surgery followed by chemotherapy from those who would benefit from NACT prior to surgery. Objectives To determine if performing a laparoscopy, in addition to conventional diagnostic work‐up, in women suspected of advanced ovarian cancer is accurate in predicting the resectability of disease. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL; 2018, Issue 6) in the Cochrane Library; MEDLINE via Ovid, Embase via Ovid, MEDION and Science Citation Index and Conference Proceedings Citation Index (ISI Web of Science) to July 2018. We also checked references of identified primary studies and review articles. Selection criteria We included studies that evaluated the diagnostic accuracy of laparoscopy to determine the resectability of disease in women who are suspected of advanced ovarian cancer and planned to receive primary debulking surgery. Data collection and analysis Pairs of review authors independently assessed the quality of included studies using QUADAS‐2 and extracted data on study and participant characteristics, index test, target condition and reference standard. We extracted data for two‐by‐two tables and summarised these graphically .  We calculated sensitivity and specificity and negative predictive values. Main results We included 18 studies, reporting on 14 cohorts of women (including 1563 participants), of which one was a randomised controlled trial (RCT). Laparoscopic assessment suggested that disease was suitable for optimal debulking surgery (no macroscopic residual disease or residual disease < 1 cm (negative predictive values)) in 54% to 96% of women who had macroscopic complete debulking surgery (no visible disease at end of laparotomy) and in 69% to 100% of women who had optimal debulking surgery (residual tumour < 1 cm at end of laparotomy). Only two studies avoided partial verification bias by operating on all women independent of laparoscopic findings, and provided data to calculate sensitivity and specificity. These two studies had no false positive laparoscopies (i.e. no women had a laparoscopy indicating unresectable disease and then went on to have optimal debulking surgery (no disease > 1 cm remaining)). Due to the large heterogeneity pooling of the data was not possible for meta‐analysis. Authors' conclusions Laparoscopy may be a useful tool to identify those women who have unresectable disease, as no women were inappropriately unexplored. However, some women had suboptimal primary debulking surgery, despite laparoscopy predicting optimal debulking and data are at high risk of verification bias as only two studies performed the reference standard (debulking laparotomy) in test (laparoscopy)‐positive women. Using a prediction model does not increase the sensitivity and will result in more unnecessarily explored women, due to a lower specificity. Plain language summary Laparoscopy in diagnosing extensiveness of ovarian cancer Why is improving the diagnosis of extensiveness of ovarian cancer important?    Ovarian cancer is a disease with a high‐mortality (death) rate. Many women (75%) are diagnosed when their disease is already at an advanced stage and 140,000 women die of this disease each year worldwide. Treatment consist of debulking surgery (removal of as much of the tumour as possible during an operation called a laparotomy ‐ normally through a long vertical cut on the abdomen) and six cycles of chemotherapy. The order in which these two treatments are given depends on the extensiveness of disease (how widespread) and on the general health of the patient. The goal of debulking surgery is to remove all visible tumour or at least to leave no residual tumour deposit bigger than 1 cm in diameter. When the diagnostic evaluation suggests that the goal of debulking surgery could not be achieved, initial treatment may be three cycles of chemotherapy to first shrink the tumour, followed by debulking surgery and then further chemotherapy to complete the course of six cycles of chemotherapy.    To diagnose the extensiveness of disease by physical examination, ultrasonography, abdominal computed tomography (CT scan), and measurement of serum tumour(blood) markers are performed. An incorrect diagnosis could result in women having unsuccessful primary debulking surgery. What is the aim of this review?   The aim of this review was to investigate if laparoscopy (keyhole surgery to look inside the abdominal cavity) is accurate in predicting whether a women can be successfully operated to remove of all visible tumour or at least to leave no tumour deposits larger than 1 cm. If so, this could help to avoid operating on those women who would be better treated with chemotherapy first. What are the main results in this review?   The review included a total of 18 relevant studies, 11 of which were added for this update, and looked at 14 groups of women. In total 1563 women underwent a laparoscopy to evaluate the extensiveness of disease in the abdomen. Two studies concluded that laparoscopy was good at identifying those women in whom optimal debulking surgery was not feasible (with tumour deposits > 1 cm left after surgery) (low false positive rate for laparoscopy) and in all women the diagnosis was correct. However, even after a laparoscopy had suggested that optimal debulking surgery was feasible, some women had suboptimal primary debulking surgery where tumour deposits of > 1 cm were left). For every 100 women referred for primary debulking surgery after laparoscopy, between four and 46 will be left with visible residual tumour. How reliable are the results of the studies in this review?   A limitation of this review is that only two studies performed diagnostic laparoscopy and then went on to attempt debulking laparotomy in all women. The other studies only performed a laparotomy when laparoscopy suggested that debulking to < 1 cm tumour residue was feasible. The correct diagnosis at laparoscopy is thereby not confirmed when > 1 cm tumour residue was predicted, this is called verification bias. Who do the results of this review apply to?   Some studies used for this review also included women who underwent debulking surgery after chemotherapy or for recurrence. But mainly women were only included who were planned for primary debulking surgery. Therefore, the results presented in this review are applicable for all women who are scheduled for primary debulking surgery. What are the implications of this review?   The studies in this review suggest that laparoscopy can accurately diagnose the extensiveness of disease. When performed after standard diagnostic work‐up less women had unsuccessful debulking surgery and therefore resulting in less morbidity, Yet, there will still be women undergoing a laparotomy resulting in residual tumour of > 1 cm after surgery. How up to date is this review?   The review authors searched for and used studies published from inception of databases until July 2018.","3","John Wiley & Sons, Ltd","1465-1858","*Laparoscopy; Chemotherapy, Adjuvant; Female; Humans; Laparoscopes; Neoplasm, Residual; Ovarian Neoplasms [drug therapy, *pathology, *surgery]; Randomized Controlled Trials as Topic; Tumor Burden; Validation Studies as Topic","10.1002/14651858.CD009786.pub3","http://dx.doi.org/10.1002/14651858.CD009786.pub3","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD011121.PUB2","Hendry, K; Green, C; McShane, R; Noel‐Storr, AH; Stott, DJ; Anwer, S; Sutton, AJ; Burton, JK; Quinn, TJ","AD‐8 for detection of dementia across a variety of healthcare settings","Cochrane Database of Systematic Reviews","2019","Abstract - Background Dementia assessment often involves initial screening, using a brief tool, followed by more detailed assessment where required. The AD‐8 is a short questionnaire, completed by a suitable 'informant' who knows the person well. AD‐8 is designed to assess change in functional performance secondary to cognitive change. Objectives To determine the diagnostic accuracy of the informant‐based AD‐8 questionnaire, in detection of all‐cause (undifferentiated) dementia in adults. Where data were available, we described the following: the diagnostic accuracy of the AD‐8 at various predefined threshold scores; the diagnostic accuracy of the AD‐8 for each healthcare setting and the effects of heterogeneity on the reported diagnostic accuracy of the AD‐8. Search methods We searched the following sources on 27 May 2014, with an update to 7 June 2018: ALOIS (Cochrane Dementia and Cognitive Improvement Group), MEDLINE (Ovid SP), Embase (Ovid SP), PsycINFO (Ovid SP), BIOSIS Previews (Thomson Reuters Web of Science), Web of Science Core Collection (includes Conference Proceedings Citation Index) (Thomson Reuters Web of Science), CINAHL (EBSCOhost) and LILACS (BIREME). We checked reference lists of relevant studies and reviews, used searches of known relevant studies in PubMed to track related articles, and contacted research groups conducting work on the AD‐8 to try to find additional studies. We developed a sensitive search strategy and used standardised database subject headings as appropriate. Foreign language publications were translated. Selection criteria We selected those studies which included the AD‐8 to assess for the presence of dementia and where dementia diagnosis was confirmed with clinical assessment. We only included those studies where the AD‐8 was used as an informant assessment. We made no exclusions in relation to healthcare setting, language of AD‐8 or the AD‐8 score used to define a 'test positive' case. Data collection and analysis We screened all titles generated by electronic database searches, and reviewed abstracts of potentially relevant studies. Two independent assessors checked full papers for eligibility and extracted data. We extracted data into two‐by‐two tables to allow calculation of accuracy metrics for individual studies. We then created summary estimates of sensitivity, specificity and likelihood ratios using the bivariate approach and plotting results in receiver operating characteristic (ROC) space. We determined quality assessment (risk of bias and applicability) using the QUADAS‐2 tool. Main results From 36 papers describing AD‐8 test accuracy, we included 10 papers. We utilised data from nine papers with 4045 individuals, 1107 of whom (27%) had a clinical diagnosis of dementia. Pooled analysis of seven studies, using an AD‐8 informant cut‐off score of two, indicated that sensitivity was 0.92 (95% confidence interval (CI) 0.86 to 0.96); specificity was 0.64 (95% CI 0.39 to 0.82); the positive likelihood ratio was 2.53 (95% CI 1.38 to 4.64); and the negative likelihood ratio was 0.12 (95% CI 0.07 to 0.21). Pooled analysis of five studies, using an AD‐8 informant cut‐off score of three, indicated that sensitivity was 0.91 (95% CI 0.80 to 0.96); specificity was 0.76 (95% CI 0.57 to 0.89); the positive likelihood ratio was 3.86 (95% CI 2.03 to 7.34); and the negative likelihood ratio was 0.12 (95% CI 0.06 to 0.24). Four studies were conducted in community settings; four were in secondary care (one in the acute hospital); and one study was in primary care. The AD‐8 has a higher relative sensitivity (1.11, 95% CI 1.02 to 1.21), but lower relative specificity (0.51, 95% CI 0.23 to 1.09) in secondary care compared to community care settings. There was heterogeneity across the included studies. Dementia prevalence rate varied from 12% to 90% of included participants. The tool was also used in various different languages. Among all the included studies there was evidence of risk of bias. Issues included the selection of participants, conduct of index test, and flow of assessment procedures. Authors' conclusions The high sensitivity of the AD‐8 suggests it can be used to identify adults who may benefit from further specialist assessment and diagnosis, but is not a diagnostic test in itself. This pattern of high sensitivity and lower specificity is often suited to a screening test. Test accuracy varies by setting, however data in primary care and acute hospital settings are limited. This review identified significant heterogeneity and risk of bias, which may affect the validity of its summary findings. Plain language summary How accurate is the AD‐8 informant questionnaire for diagnosing dementia in all healthcare settings? Why is recognising dementia important? Many people are living with dementia but have never had the condition diagnosed. Not recognising dementia when it is present (a false negative result) may deny people access to social support, medications, and financial assistance. It also prevents the individual and their family from planning for the future. However, incorrectly diagnosing dementia when it is not present (a false positive result) can cause distress or fear and lead to additional investigations which can waste resources. What is the aim of the review? The aim of this Cochrane Review was to find out how accurate the AD‐8 informant questionnaire is for detecting dementia in all healthcare settings. The researchers included 10 studies to answer this question, nine of which reported numerical information that could be used. What was studied in the review? The AD‐8 questionnaire includes eight ‘yes or no’ questions, to be answered by someone who knows the person under investigation; for example, a relative, carer or close friend (sometimes described as an informant). The questions ask about whether the informant has noticed a change in the individual’s memory and thinking abilities over the past years. A point is given for every item where they think the person’s abilities have changed. Higher scores occur when more changes are noted by the informant. The AD‐8 would not usually be used to make a final diagnosis of dementia, but it may help identify those who require further assessment. What are the main results of the review? The review included data from nine relevant studies, with a total of 4045 participants. Seven of the studies used a score of two or more to indicate dementia. A score of two or more is the cut‐off recommended for the AD‐8. The results of these studies indicate that, in theory, if the AD‐8 were to be used to diagnose dementia in a group of 1000 people across all healthcare settings, of whom 280 (28%) have dementia, an estimated 517 would have an AD‐8 result indicating dementia is present and of these 259 (50%) will not have dementia. Of the 483 people with a result indicating that dementia is not present, 22 (5%) would be incorrectly classified as not having dementia It is possible that the AD‐8 may work differently in different settings, for example in hospital or General practice. In secondary care, the AD‐8 produces more false positive results, but fewer false negative results, than when it is used in community settings. We could not directly compare the performance of the AD‐8 in primary care as there was only one study from this setting. How reliable are the results of the studies in this review? In the included studies, the diagnosis of dementia was made by assessing all patients with a detailed clinical assessment. (In these studies, detailed clinical assessment was the gold standard we compared the AD‐8 to.) This is likely to have been a reliable method for deciding whether patients really had dementia. However, there were some problems with how the studies were conducted. This may have resulted in the AD‐8 appearing more accurate than it really is. The numbers described are an average across studies in the review. However, as estimates from individual studies varied we cannot be sure that the AD‐8 will always produce these results. Who do the results of this review apply to? Studies included in the review were conducted in Brazil, China, Japan, Singapore, Taiwan, the UK and USA. Studies included those attending primary and secondary care services, and populations of older adults living in the community. Five studies used the English‐language version of the AD‐8. The percentage of people with a final diagnosis of dementia was between 12% and 90% (an average of 38%). What are the implications of this review? The studies included in this review suggest the AD‐8 can identify adults who may have a diagnosis of dementia, who would benefit from specialist assessment and diagnosis. If the AD‐8 was used alone to diagnose dementia, the chance of wrongly diagnosing someone with dementia when they do not actually have it is high (50% of those whose AD‐8 score suggests they have dementia). This makes the AD‐8 an unsuitable single diagnostic test as it would potentially create anxiety and distress. The chance of missing a diagnosis of dementia is much lower (5% of those whose AD‐8 score suggests they do not have dementia when they actually have it). This group will miss out on opportunities to plan their future care and would not be eligible to be assessed for treatment with medicines. These findings should be considered when deciding whether or not to use the AD‐8 to test for dementia. How up‐to‐date is this review? The review authors searched for and used studies published up to June 2018.","3","John Wiley & Sons, Ltd","1465-1858","*Patient Health Questionnaire; *Proxy; Aged; Dementia [*diagnosis]; Humans; Sensitivity and Specificity","10.1002/14651858.CD011121.pub2","http://dx.doi.org/10.1002/14651858.CD011121.pub2","Dementia and Cognitive Improvement"
"CD012126.PUB2","Brown, JVE; Meader, N; Cleminson, J; McGuire, W","C‐reactive protein for diagnosing late‐onset infection in newborn infants","Cochrane Database of Systematic Reviews","2019","Abstract - Background Late‐onset infection is the most common serious complication associated with hospital care for newborn infants. Because confirming the diagnosis by microbiological culture typically takes 24 to 48 hours, the serum level of the inflammatory marker C‐reactive protein (CRP) measured as part of the initial investigation is used as an adjunctive rapid test to guide management in infants with suspected late‐onset infection. Objectives To determine the diagnostic accuracy of serum CRP measurement in detecting late‐onset infection in newborn infants. Search methods We searched electronic databases (MEDLINE, Embase, and Science Citation Index to September 2017) ,  conference proceedings, previous reviews, and the reference lists of retrieved articles. Selection criteria We included cohort and cross‐sectional studies evaluating the diagnostic accuracy of serum CRP levels for the detection of late‐onset infection (occurring more than 72 hours after birth) in newborn infants. Data collection and analysis Two review authors independently assessed eligibility for inclusion, evaluated the methodological quality of included studies, and extracted data to estimate diagnostic accuracy using hierarchical summary receiver operating characteristic (SROC) models. We assessed heterogeneity by examining variability of study estimates and overlap of the 95% confidence interval (CI) in forest plots of sensitivity and specificity. Main results The search identified 20 studies (1615 infants). Most were small, single‐centre, prospective cohort studies conducted in neonatal units in high‐ or middle‐income countries since the late 1990s. Risk of bias in the included studies was generally low with independent assessment of index and reference tests. Most studies used a prespecified serum CRP threshold level as the definition of a 'positive' index test (typical cut‐off level between 5 mg/L and 10 mg/L) and the culture of a pathogenic micro‐organism from blood as the reference standard. At median specificity (0.74), sensitivity was 0.62 (95% CI 0.50 to 0.73). Heterogeneity was evident in the forest plots but it was not possible to conduct subgroup or meta‐regression analyses by gestational ages, types of infection, or types of infecting micro‐organism. Covariates for whether studies used a predefined threshold or not, and whether studies used a standard threshold of between 5 mg/L and 10 mg/L, were not statistically significant. Authors' conclusions The serum CRP level at initial evaluation of an infant with suspected late‐onset infection is unlikely to be considered sufficiently accurate to aid early diagnosis or select infants to undergo further investigation or treatment with antimicrobial therapy or other interventions. Plain language summary C‐reactive protein for diagnosing infection in newborn infants Review question We reviewed studies that assessed whether measuring the blood level of C‐reactive protein (CRP) helped to make an earlier diagnosis of serious infections in newborn infants. Background Newborn infants, especially sick or preterm infants, are at risk of developing severe infections (such as bloodstream infections) during their stay on neonatal units. Infections are often difficult to diagnose early with certainty, and quick tests such as measuring the blood level of a protein that responds to infection (called CRP) are sometimes used to help make an earlier diagnosis. We aimed to assess the evidence for the accuracy of this test. Study characteristics We found 20 studies that assessed the accuracy of measuring the blood level of CRP to diagnose infections in newborn infants. These studies were similar enough to justify a combined analysis of their findings. Key results The combined analysis indicated that a positive CRP test correctly identified infants with infection about six times out of 10. Conclusion Measuring the blood level of CRP is not sufficiently accurate to help early diagnosis of infection in newborn infants.","1","John Wiley & Sons, Ltd","1465-1858","Bacterial Infections [*diagnosis]; Biomarkers [blood]; Cross Infection [*diagnosis, microbiology]; C‐Reactive Protein [*analysis]; False Negative Reactions; False Positive Reactions; Humans; Infant, Newborn; Infant, Premature; Mycoses [*diagnosis, microbiology]; Prospective Studies; Reference Standards; Sensitivity and Specificity","10.1002/14651858.CD012126.pub2","http://dx.doi.org/10.1002/14651858.CD012126.pub2","Neonatal"
"CD012669.PUB2","Stengel, D; Leisterer, J; Ferrada, P; Ekkernkamp, A; Mutze, S; Hoenning, A","Point‐of‐care ultrasonography for diagnosing thoracoabdominal injuries in patients with blunt trauma","Cochrane Database of Systematic Reviews","2018","Abstract - Background Point‐of‐care sonography (POCS) has emerged as the screening modality of choice for suspected body trauma in many emergency departments worldwide. Its best known application is FAST (focused abdominal sonography for trauma). The technology is almost ubiquitously available, can be performed during resuscitation, and does not expose patients or staff to radiation. While many authors have stressed the high specificity of POCS, its sensitivity varied markedly across studies. This review aimed to compile the current best evidence about the diagnostic accuracy of POCS imaging protocols in the setting of blunt thoracoabdominal trauma. Objectives To determine the diagnostic accuracy of POCS for detecting and excluding free fluid, organ injuries, vascular lesions, and other injuries (e.g. pneumothorax) compared to a diagnostic reference standard (i.e. computed tomography (CT), magnetic resonance imaging (MRI), thoracoscopy or thoracotomy, laparoscopy or laparotomy, autopsy, or any combination of these) in patients with blunt trauma. Search methods We searched Ovid MEDLINE (1946 to July 2017) and Ovid Embase (1974 to July 2017), as well as PubMed (1947 to July 2017), employing a prospectively defined literature and data retrieval strategy. We also screened the Cochrane Library, Google Scholar, and BIOSIS for potentially relevant citations, and scanned the reference lists of full‐text papers for articles missed by the electronic search. We performed a top‐up search on 6 December 2018, and identified eight new studies which may be incorporated into the first update of this review. Selection criteria We assessed studies for eligibility using predefined inclusion and exclusion criteria. We included either prospective or retrospective diagnostic cohort studies that enrolled patients of any age and gender who sustained any type of blunt injury in a civilian scenario. Eligible studies had to provide sufficient information to construct a 2 x 2 table of diagnostic accuracy to allow for calculating sensitivity, specificity, and other indices of diagnostic test accuracy. Data collection and analysis Two review authors independently screened titles, abstracts, and full texts of reports using a prespecified data extraction form. Methodological quality of individual studies was rated by the QUADAS‐2 instrument (the revised and updated version of the original Quality Assessment of Diagnostic Accuracy Studies list of items). We calculated sensitivity and specificity with 95% confidence intervals (CI), tabulated the pairs of sensitivity and specificity with CI, and depicted these estimates by coupled forest plots using Review Manager 5 (RevMan 5). For pooling summary estimates of sensitivity and specificity, and investigating heterogeneity across studies, we fitted a bivariate model using Stata 14.0. Main results We included 34 studies with 8635 participants in this review. Summary estimates of sensitivity and specificity were 0.74 (95% CI 0.65 to 0.81) and 0.96 (95% CI 0.94 to 0.98). Pooled positive and negative likelihood ratios were estimated at 18.5 (95% CI 10.8 to 40.5) and 0.27 (95% CI 0.19 to 0.37), respectively. There was substantial heterogeneity across studies, and the reported accuracy of POCS strongly depended on the population and affected body area. In children, pooled sensitivity of POCS was 0.63 (95% CI 0.46 to 0.77), as compared to 0.78 (95% CI 0.69 to 0.84) in an adult or mixed population. Associated specificity in children was 0.91 (95% CI 0.81 to 0.96) and in an adult or mixed population 0.97 (95% CI 0.96 to 0.99). For abdominal trauma, POCS had a sensitivity of 0.68 (95% CI 0.59 to 0.75) and a specificity of 0.95 (95% CI 0.92 to 0.97). For chest injuries, sensitivity and specificity were calculated at 0.96 (95% CI 0.88 to 0.99) and 0.99 (95% CI 0.97 to 1.00). If we consider the results of all 34 included studies in a virtual population of 1000 patients, based on the observed median prevalence (pretest probability) of thoracoabdominal trauma of 28%, POCS would miss 73 patients with injuries and falsely suggest the presence of injuries in another 29 patients. Furthermore, in a virtual population of 1000 children, based on the observed median prevalence (pretest probability) of thoracoabdominal trauma of 31%, POCS would miss 118 children with injuries and falsely suggest the presence of injuries in another 62 children. Authors' conclusions In patients with suspected blunt thoracoabdominal trauma, positive POCS findings are helpful for guiding treatment decisions. However, with regard to abdominal trauma, a negative POCS exam does not rule out injuries and must be verified by a reference test such as CT. This is of particular importance in paediatric trauma, where the sensitivity of POCS is poor. Based on a small number of studies in a mixed population, POCS may have a higher sensitivity in chest injuries. This warrants larger, confirmatory trials to affirm the accuracy of POCS for diagnosing thoracic trauma. Plain language summary How accurate is bedside ultrasound for the diagnosis of injuries to the abdomen or chest in patients with blunt injuries? Background and aims People who sustain a road traffic crash or fall from a height are at risk for blunt body trauma (i.e. non‐penetrating trauma) and multiple injuries. Medical professionals caring for these patients in hospital need to know if vital organs or vessels are damaged, and whether there is any major bleeding that requires immediate intervention. Point‐of‐care sonography (POCS), a form of ultrasound, is a non‐invasive, radiation‐free, portable imaging technique that can be used at the patient's bedside. It is frequently used to help diagnose injuries in the emergency department. We reviewed the best scientific evidence about the accuracy of POCS, that is its ability to identify or exclude injuries correctly, compared to other diagnostic tests. We considered computed tomography, laparotomy, and autopsy to be good comparative tests against which to measure the accuracy of POCS. Study characteristics We searched for studies from the year in which the first paper about using ultrasound to diagnose trauma patients was published until 15 July 2017. We considered 2296 records and included 34 relevant studies that involved 8635 participants in this review. All 34 studies were published between 1992 and 2017, with the number of participants in each study ranging from 51 to 3181. Ten studies included only children, two studies only adults, and the remaining 22 studies included both children and adults. Quality of the evidence In many studies, important information about the selection of participants and choice of the diagnostic tests against which to compare POCS was not reported. We therefore rated the methodological quality of the available evidence mostly as unclear. Key results Point‐of‐care sonography had a sensitivity (i.e. the ability to detect a person with the disease) of 74% and a specificity (i.e. the ability to exclude a person without the disease) of 96%. Sensitivity and specificity varied considerably across studies, which was due in part to variation in study, participant, and injury characteristics. In children, both the sensitivity and specificity of POCS were lower than in an adult or mixed population, meaning that POCS was less able to identify or rule out an injury. Based on our results, we would expect that amongst 1000 patients of a mixed‐age population with suspected blunt trauma to the abdomen or chest, POCS would miss 73 patients with injuries, and would falsely suggest the presence of injuries in 29 patients who were unaffected. This result emphasises the need for additional imaging in trauma patients for whom POCS shows no injuries (i.e. a negative result), to check whether they are really injury‐free.","12","John Wiley & Sons, Ltd","1465-1858","*Point‐of‐Care Systems; Abdominal Injuries [*diagnostic imaging]; Adult; Age Factors; Child; Female; Focused Assessment with Sonography for Trauma [*methods]; Humans; Male; Reference Standards; Sensitivity and Specificity; Thoracic Injuries [*diagnostic imaging]; Wounds, Nonpenetrating [*diagnostic imaging]","10.1002/14651858.CD012669.pub2","http://dx.doi.org/10.1002/14651858.CD012669.pub2","Injuries"
"CD013194","Dinnes, J; Deeks, JJ; Grainge, MJ; Chuchu, N; Ferrante di Ruffano, L; Matin, RN; Thomson, DR; Wong, KY; Aldridge, RB; Abbott, R; Fawzy, M; Bayliss, SE; Takwoingi, Y; Davenport, C; Godfrey, K; Walter, FM; Williams, HC","Visual inspection for diagnosing cutaneous melanoma in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Melanoma has one of the fastest rising incidence rates of any cancer. It accounts for a small percentage of skin cancer cases but is responsible for the majority of skin cancer deaths. History‐taking and visual inspection of a suspicious lesion by a clinician is usually the first in a series of ‘tests’ to diagnose skin cancer. Establishing the accuracy of visual inspection alone is critical to understating the potential contribution of additional tests to assist in the diagnosis of melanoma. Objectives To determine the diagnostic accuracy of visual inspection for the detection of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in adults with limited prior testing and in those referred for further evaluation of a suspicious lesion. Studies were separated according to whether the diagnosis was recorded face‐to‐face (in‐person) or based on remote (image‐based) assessment. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: CENTRAL; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Test accuracy studies of any design that evaluated visual inspection in adults with lesions suspicious for melanoma, compared with a reference standard of either histological confirmation or clinical follow‐up. We excluded studies reporting data for ‘clinical diagnosis’ where dermoscopy may or may not have been used. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities per algorithm and threshold using the bivariate hierarchical model. We investigated the impact of: in‐person test interpretation; use of a purposely developed algorithm to assist diagnosis; and observer expertise. Main results We included 49 publications reporting on a total of 51 study cohorts with 34,351 lesions (including 2499 cases), providing 134 datasets for visual inspection. Across almost all study quality domains, the majority of study reports provided insufficient information to allow us to judge the risk of bias, while in three of four domains that we assessed we scored concerns regarding applicability of study findings as 'high'. Selective participant recruitment, lack of detail regarding the threshold for deciding on a positive test result, and lack of detail on observer expertise were particularly problematic. Attempts to analyse studies by degree of prior testing were hampered by a lack of relevant information and by the restricted inclusion of lesions selected for biopsy or excision. Accuracy was generally much higher for in‐person diagnosis compared to image‐based evaluations (relative diagnostic odds ratio of 8.54, 95% CI 2.89 to 25.3, P < 0.001). Meta‐analysis of in‐person evaluations that could be clearly placed on the clinical pathway showed a general trade‐off between sensitivity and specificity, with the highest sensitivity (92.4%, 95% CI 26.2% to 99.8%) and lowest specificity (79.7%, 95% CI 73.7% to 84.7%) observed in participants with limited prior testing (n = 3 datasets). Summary sensitivities were lower for those referred for specialist assessment but with much higher specificities (e.g. sensitivity 76.7%, 95% CI 61.7% to 87.1%) and specificity 95.7%, 95% CI 89.7% to 98.3%) for lesions selected for excision, n = 8 datasets) .  These differences may be related to differences in the spectrum of included lesions, differences in the definition of a positive test result, or to variations in observer expertise. We did not find clear evidence that accuracy is improved by the use of any algorithm to assist diagnosis in all settings. Attempts to examine the effect of observer expertise in melanoma diagnosis were hindered due to poor reporting. Authors' conclusions Visual inspection is a fundamental component of the assessment of a suspicious skin lesion; however, the evidence suggests that melanomas will be missed if visual inspection is used on its own. The evidence to support its accuracy in the range of settings in which it is used is flawed and very poorly reported. Although published algorithms do not appear to improve accuracy, there is insufficient evidence to suggest that the ‘no algorithm’ approach should be preferred in all settings. Despite the volume of research evaluating visual inspection, further prospective evaluation of the potential added value of using established algorithms according to the prior testing or diagnostic difficulty of lesions may be warranted. Plain language summary How accurate is visual inspection of skin lesions with the naked eye for diagnosis of melanoma in adults? What is the aim of the review? Melanoma is one of the most dangerous forms of skin cancer. The aim of this Cochrane Review was to find out how accurate checking suspicious skin lesions (lumps, bumps, wounds, scratches or grazes) with the naked eye (visual inspection) can be to diagnose melanoma (diagnostic accuracy). The Review also investigated whether diagnostic accuracy was different depending on whether the clinician was face to face with the patient (in‐person visual inspection), or looked at an image of the lesion (image‐based visual inspection). Cochrane researchers included 19 studies to answer this question. Why is it important to know the diagnostic accuracy of visual examination of skin lesions suspected to be melanomas? Not recognising a melanoma when it is present (a false‐negative test result) delays surgery to remove it (excision), risking cancer spreading to other organs in the body and possibly death. Diagnosing a skin lesion (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) as a melanoma when it is not (a false‐positive result) may result in unnecessary surgery, further investigations, and patient anxiety. Visual inspection of suspicious skin lesions by a clinician using the naked eye is usually the first of a series of ‘tests’ to diagnose melanoma. Knowing the diagnostic accuracy of visual inspection alone is important to decide whether additional tests, such as a biopsy (removing a part of the lesion for examination under a microscope) are needed to improve accuracy to an acceptable level. What did the review study? Researchers wanted to find out the diagnostic accuracy of in‐person compared with image‐based visual inspection of suspicious skin lesions. Researchers also wanted to find out whether diagnostic accuracy was improved if doctors used a 'visual inspection checklist' or depending on how experienced in visual inspection they were (level of clinical expertise). They considered the diagnostic accuracy of the first visual inspection of a lesion, for example, by a general practitioner (GP), and of lesions that had been referred for further evaluation, for example, by a dermatologist (doctor specialising in skin problems). What are the main results of the review? Only 19 studies (17 in‐person studies and 2 image‐based studies) were clear whether the test was the first visual inspection of a lesion or was a visual inspection following referral (for example, when patients are referred by a GP to skin specialists for visual inspection). First in‐person visual inspection (3 studies) The results of three studies of 1339 suspicious skin lesions suggest that in a group of 1000 lesions, of which 90 (9%) actually are melanoma: ‐ An estimated 268 will have a visual inspection result indicating melanoma is present. Of these, 185 will not be melanoma and will result in an unnecessary biopsy (false‐positive results). ‐ An estimated 732 will have a visual inspection result indicating that melanoma is not present. Of these, seven will actually have melanoma and would not be sent for biopsy (false‐negative results). Two further studies restricted to 4228 suspicious skin lesions that were all selected to be excised found similar results. In‐person visual inspection after referral, all lesions selected to be excised (8 studies) The results of eight studies of 5331 suspicious skin lesions suggest that in a group of 1000 lesions, of which 90 (9%) actually are melanoma: ‐ An estimated 108 will have a visual inspection result indicating melanoma is present, and of these, 39 will not be melanoma and will result in an unnecessary biopsy (false‐positive results). ‐ Of the 892 lesions with a visual inspection result indicating that melanoma is not present, 21 will actually be melanoma and would not be sent for biopsy (false‐negative results). Overall, the number of false‐positive results (diagnosing a skin lesion as a melanoma when it is not) was observed to be higher and the number of false‐negative results (not recognising a melanoma when it is present) lower for first visual inspections of suspicious skin lesions compared to visual inspection following referral. Visual inspection of images of suspicious skin lesions (2 studies) Accuracy was much lower for visual inspection of images of lesions compared to visual inspection in person. Value of visual inspection checklists There was no evidence that use of a visual inspection checklist or the level of clinical expertise changed diagnostic accuracy. How reliable are the results of the studies of this review? The majority of included studies diagnosed melanoma by lesion biopsy and confirmed that melanoma was not present by biopsy or by follow‐up over time to make sure the skin lesion remained negative for melanoma. In these studies, biopsy, clinical follow‐up, or specialist clinician diagnosis were the reference standards (means of establishing final diagnoses). Biopsy or follow‐up are likely to have been reliable methods for deciding whether patients really had melanoma. In a few studies, experts diagnosed the absence of melanoma (expert diagnosis), which is less likely to have been a reliable method for deciding whether patients really had melanoma. There was lots of variation in the results of the studies in this review and the studies did not always describe fully the methods they used, which made it difficult to assess their reliability. Who do the results of this review apply to? Thirteen studies were undertaken in Europe (68%), with the remainder undertaken in Asia (n = 1), Oceania (n = 4), and North America (n = 1). Mean age ranged from 30 to 73.6 years (reported in 10 studies). The percentage of individuals with melanoma ranged between 4% and 20% in first visualised lesions and between 1% and 50% in studies of referred lesions. In the majority of studies, the lesions were unlikely to be representative of the range of those seen in practice, for example, only including skin lesions of a certain size or with a specific appearance. In addition, variation in the expertise of clinicians performing visual inspection and in the definition used to decide whether or not melanoma was present across studies makes it unclear as to how visual inspection should be carried out and by whom in order to achieve the accuracy observed in studies. What are the implications of this review? Error rates from visual inspection are too high for it to be relied upon alone. Although not evaluated in this review, other technologies need to be used to ensure accurate diagnosis of skin cancer. There is considerable variation and uncertainty about the diagnostic accuracy of visual inspection alone for the diagnosis of melanoma. There is no evidence to suggest that visual inspection checklists reliably improve the diagnostic accuracy of visual inspection, so recommendations cannot be made about when they should be used. Despite the existence of numerous research studies, further, well‐reported studies assessing the diagnostic accuracy of visual inspection with and without visual inspection checklists and by clinicians with different levels of expertise are needed. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016.","12","John Wiley & Sons, Ltd","1465-1858","Adult; Aged; Algorithms; Diagnostic Errors; Humans; Melanoma [*diagnosis, diagnostic imaging]; Melanoma, Cutaneous Malignant; Middle Aged; Physical Examination [*methods]; Sensitivity and Specificity; Skin Neoplasms [*diagnosis, diagnostic imaging]","10.1002/14651858.CD013194","http://dx.doi.org/10.1002/14651858.CD013194","Skin"
"CD013193","Chuchu, N; Dinnes, J; Takwoingi, Y; Matin, RN; Bayliss, SE; Davenport, C; Moreau, JF; Bassett, O; Godfrey, K; O'Sullivan, C; Walter, FM; Motley, R; Deeks, JJ; Williams, HC","Teledermatology for diagnosing skin cancer in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and squamous cell carcinoma (SCC) are high‐risk skin cancers which have the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised with potential to infiltrate and damage surrounding tissue. Anxiety around missing early curable cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Teledermatology provides a way for generalist clinicians to access the opinion of a specialist dermatologist for skin lesions that they consider to be suspicious without referring the patients through the normal referral pathway. Teledermatology consultations can be 'store‐and‐forward' with electronic digital images of a lesion sent to a dermatologist for review at a later time, or can be live and interactive consultations using videoconferencing to connect the patient, referrer and dermatologist in real time. Objectives To determine the diagnostic accuracy of teledermatology for the detection of any skin cancer (melanoma, BCC or cutaneous squamous cell carcinoma (cSCC)) in adults, and to compare its accuracy with that of in‐person diagnosis. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials, MEDLINE, Embase, CINAHL, CPCI, Zetoc, Science Citation Index, US National Institutes of Health Ongoing Trials Register, NIHR Clinical Research Network Portfolio Database and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Studies evaluating skin cancer diagnosis for teledermatology alone, or in comparison with face‐to‐face diagnosis by a specialist clinician, compared with a reference standard of histological confirmation or clinical follow‐up and expert opinion. We also included studies evaluating the referral accuracy of teledermatology compared with a reference standard of face‐to‐face diagnosis by a specialist clinician. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where there were information related to the target condition of any skin cancer missing. Data permitting, we estimated summary sensitivities and specificities using the bivariate hierarchical model. Due to the scarcity of data, we undertook no covariate investigations for this review. For illustrative purposes, we plotted estimates of sensitivity and specificity on coupled forest plots for diagnostic threshold and target condition under consideration. Main results The review included 22 studies reporting diagnostic accuracy data for 4057 lesions and 879 malignant cases (16 studies) and referral accuracy data for reported data for 1449 lesions and 270 'positive' cases as determined by the reference standard face‐to‐face decision (six studies). Methodological quality was variable with poor reporting hindering assessment. The overall risk of bias was high or unclear for participant selection, reference standard, and participant flow and timing in at least half of all studies; the majority were at low risk of bias for the index test. The applicability of study findings were of high or unclear concern for most studies in all domains assessed due to the recruitment of participants from secondary care settings or specialist clinics rather than from primary or community‐based settings in which teledermatology is more likely to be used and due to the acquisition of lesion images by dermatologists or in specialist imaging units rather than by primary care clinicians. Seven studies provided data for the primary target condition of any skin cancer (1588 lesions and 638 malignancies). For the correct diagnosis of lesions as malignant using photographic images, summary sensitivity was 94.9% (95% confidence interval (CI) 90.1% to 97.4%) and summary specificity was 84.3% (95% CI 48.5% to 96.8%) (from four studies). Individual study estimates using dermoscopic images or a combination of photographic and dermoscopic images generally suggested similarly high sensitivities with highly variable specificities. Limited comparative data suggested similar diagnostic accuracy between teledermatology assessment and in‐person diagnosis by a dermatologist; however, data were too scarce to draw firm conclusions. For the detection of invasive melanoma or atypical intraepidermal melanocytic variants both sensitivities and specificities were more variable. Sensitivities ranged from 59% (95% CI 42% to 74%) to 100% (95% CI 48% to 100%) and specificities from 30% (95% CI 22% to 40%) to 100% (95% CI 93% to 100%), with reported diagnostic thresholds including the correct diagnosis of melanoma, classification of lesions as 'atypical' or 'typical, and the decision to refer or to excise a lesion. Referral accuracy data comparing teledermatology against a face‐to‐face reference standard suggested good agreement for lesions considered to require some positive action by face‐to‐face assessment (sensitivities of over 90%). For lesions considered of less concern when assessed face‐to‐face (e.g. for lesions not recommended for excision or referral), agreement was more variable with teledermatology specificities ranging from 57% (95% CI 39% to 73%) to 100% (95% CI 86% to 100%), suggesting that remote assessment is more likely recommend excision, referral or follow‐up compared to in‐person decisions. Authors' conclusions Studies were generally small and heterogeneous and methodological quality was difficult to judge due to poor reporting. Bearing in mind concerns regarding the applicability of study participants and of lesion image acquisition in specialist settings, our results suggest that teledermatology can correctly identify the majority of malignant lesions. Using a more widely defined threshold to identify 'possibly' malignant cases or lesions that should be considered for excision is likely to appropriately triage those lesions requiring face‐to‐face assessment by a specialist. Despite the increasing use of teledermatology on an international level, the evidence base to support its ability to accurately diagnose lesions and to triage lesions from primary to secondary care is lacking and further prospective and pragmatic evaluation is needed. Plain language summary What is the diagnostic accuracy of teledermatology for the diagnosis of skin cancer in adults? Why is improving the diagnosis of skin cancer important? There are different types of skin cancer. Melanoma is one of the most dangerous forms and it is important to identify it early so that it can be removed. If it is not recognised when first brought to the attention of doctors (also known as a false‐negative test result) treatment can be delayed resulting in the melanoma spreading to other organs in the body and possibly causing early death. Cutaneous squamous cell carcinoma (cSCC) and basal cell carcinoma (BCC) are usually localised skin cancers, although cSCC can spread to other parts of the body and BCC can cause disfigurement if not recognised early. Calling something a skin cancer when it is not really a skin cancer (a false‐positive result) may result in unnecessary surgery and other investigations that can cause stress and worry to the patient. Making the correct diagnosis is important. Mistaking one skin cancer for another can lead to the wrong treatment being used or lead to a delay in effective treatment. What is the aim of the review? The aim of this Cochrane Review was to find out whether teledermatology is accurate enough to identify which people with skin lesions need to be referred to see a specialist dermatologist (a doctor concerned with disease of the skin) and who can be safely reassured that their lesion (damage or change of the skin) is not malignant. We included 22 studies to answer this question. What was studied in the review? Teledermatology means sending pictures of skin lesions or rashes to a specialist for advice on diagnosis or management. It is a way for primary care doctors (general practitioners (GPs)) to get an opinion from a specialist dermatologist without having to refer patients through the normal referral pathway. Teledermatology can involve sending photographs or magnified images of a skin lesion taken with a special camera (dermatoscope) to a skin specialist to look at or it might involve immediate discussion about a skin lesion between a GP and a skin specialist using videoconferencing. What are the main results of the review? The review included 22 studies, 16 studies comparing teledermatology diagnoses to the final lesion diagnoses (diagnostic accuracy) for 4057 lesions and 879 malignant cases and five studies comparing teledermatology decisions to the decisions that would be made with the patient present (referral accuracy) for 1449 lesions and 270 'positive' cases. The studies were very different from each other in terms of the types of people with suspicious skin cancer lesions included and the type of teledermatology used. A single reliable estimate of the accuracy of teledermatology could not be made. For the correct diagnosis of a lesion to be a skin cancer, data suggested that less than 7% of malignant skin lesions were missed by teledermatology. Study results were too variable to tell us how many people would be referred unnecessarily for a specialist dermatology appointment following a teledermatology consultation. Without access to teledermatology services however, most of the lesions included in these studies would likely be referred to a dermatologist. How reliable are the results of the studies of this review? In the included studies, the final diagnosis of skin cancer was made by lesion biopsy (taking a small sample of the lesion so it could be examined under a microscope) and the absence of skin cancer was confirmed by biopsy or by follow‐up over time to make sure the skin lesion remained negative for melanoma. This is likely to have been a reliable method for deciding whether people really had skin cancer. In a few studies, a diagnosis of no skin cancer was made by a skin specialist rather than biopsy. This is less likely to have been a reliable method for deciding whether people really had skin cancer*. Poor reporting of what was done in the study made it difficult for us to say how reliable the study results are. Selecting some patients from specialist clinics instead of primary care along with different ways of doing teledermatology were common problems. Who do the results of this review apply to? Studies were conducted in: Europe (64%), North America (18%), South America (9%) or Oceania (9%). The average age of people who were studied was 52 years; however, several studies included at least some people under the age of 16 years. The percentage of people with skin cancer ranged between 2% and 88% with an average of 30%, which is much higher than would be observed in a primary care setting in the UK. What are the implications of this review? Teledermatology is likely to be a good way of helping GPs to decide which skin lesions need to be seen by a skin specialist. Our review suggests that using magnified images, in addition to photographs of the lesion, improves accuracy. More research is needed to establish the best way of providing teledermatology services. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies, biopsy, clinical follow‐up or specialist clinician diagnosis were the reference comparisons.","12","John Wiley & Sons, Ltd","1465-1858","Adult; Carcinoma, Basal Cell [*diagnostic imaging]; Carcinoma, Squamous Cell [*diagnostic imaging]; Data Accuracy; Dermatology [*methods]; Diagnostic Errors [statistics & numerical data]; Humans; Melanoma [*diagnostic imaging]; Melanoma, Cutaneous Malignant; Photography; Physical Examination [methods]; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging]; Telemedicine [*methods]","10.1002/14651858.CD013193","http://dx.doi.org/10.1002/14651858.CD013193","Skin"
"CD013189","Ferrante di Ruffano, L; Dinnes, J; Deeks, JJ; Chuchu, N; Bayliss, SE; Davenport, C; Takwoingi, Y; Godfrey, K; O'Sullivan, C; Matin, RN; Tehrani, H; Williams, HC","Optical coherence tomography for diagnosing skin cancer in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and squamous cell carcinoma (SCC) are high‐risk skin cancers, which have the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised, with potential to infiltrate and damage surrounding tissue. Anxiety around missing early cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Optical coherence tomography (OCT) is a microscopic imaging technique, which magnifies the surface of a skin lesion using near‐infrared light. Used in conjunction with clinical or dermoscopic examination of suspected skin cancer, or both, OCT may offer additional diagnostic information compared to other technologies. Objectives To determine the diagnostic accuracy of OCT for the detection of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants, basal cell carcinoma (BCC), or cutaneous squamous cell carcinoma (cSCC) in adults. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria We included studies of any design evaluating OCT in adults with lesions suspicious for invasive melanoma and atypical intraepidermal melanocytic variants, BCC or cSCC, compared with a reference standard of histological confirmation or clinical follow‐up. Data collection and analysis Two review authors independently extracted data using a standardised data extraction and quality assessment form (based on QUADAS‐2). Our unit of analysis was lesions. Where possible, we estimated summary sensitivities and specificities using the bivariate hierarchical model. Main results We included five studies with 529 cutaneous lesions (282 malignant lesions) providing nine datasets for OCT, two for visual inspection alone, and two for visual inspection plus dermoscopy. Studies were of moderate to unclear quality, using data‐driven thresholds for test positivity and giving poor accounts of reference standard interpretation and blinding. Studies may not have been representative of populations eligible for OCT in practice, for example due to high disease prevalence in study populations, and may not have reflected how OCT is used in practice, for example by using previously acquired OCT images. It was not possible to make summary statements regarding accuracy of detection of melanoma or of cSCC because of the paucity of studies, small sample sizes, and for melanoma differences in the OCT technologies used (high‐definition versus conventional resolution OCT), and differences in the degree of testing performed prior to OCT (i.e. visual inspection alone or visual inspection plus dermoscopy). Pooled data from two studies using conventional swept‐source OCT alongside visual inspection and dermoscopy for the detection of BCC estimated the sensitivity of OCT as 95% (95% confidence interval (CI) 91% to 97%) and specificity of 77% (95% CI 69% to 83%). When applied to a hypothetical population of 1000 lesions at the mean observed BCC prevalence of 60%, OCT would miss 31 BCCs (91 fewer than would be missed by visual inspection alone and 53 fewer than would be missed by visual inspection plus dermoscopy), and OCT would lead to 93 false‐positive results for BCC (a reduction in unnecessary excisions of 159 compared to using visual inspection alone and of 87 compared to visual inspection plus dermoscopy). Authors' conclusions Insufficient data are available on the use of OCT for the detection of melanoma or cSCC. Initial data suggest conventional OCT may have a role for the diagnosis of BCC in clinically challenging lesions, with our meta‐analysis showing a higher sensitivity and higher specificity when compared to visual inspection plus dermoscopy. However, the small number of studies and varying methodological quality means implications to guide practice cannot currently be drawn. Appropriately designed prospective comparative studies are required, given the paucity of data comparing OCT with dermoscopy and other similar diagnostic aids such as reflectance confocal microscopy. Plain language summary What is the diagnostic accuracy of optical coherence tomography (OCT), an imaging test, for the detection of skin cancer in adults? Why is improving the diagnosis of skin cancer important? There are several different types of skin cancer. Melanoma is one of the most dangerous forms, and it is important that it is recognised early so that it can be removed. If it is not recognised (also known as a false‐negative test result), treatment can be delayed, and this risks the melanoma spreading to other organs in the body, which may lead to eventual death. Cutaneous squamous cell carcinoma (cSCC) and basal cell carcinoma (BCC) are usually localised (i.e. limited to a certain part of the body) skin cancers, although cSCC can spread to other parts of the body and BCC can cause disfigurement if not recognised early. Diagnosing a skin cancer when it is not actually present (a false‐positive result) may result in unnecessary surgery and other investigations and can cause stress and anxiety to the patient. Making the correct diagnosis is important, and mistaking one skin cancer for another can lead to the wrong treatment being used or lead to a delay in effective treatment. What is the aim of the review? The aim of this Cochrane Review was to find out how accurate optical coherence tomography (OCT) is for diagnosing skin cancer. Researchers in Cochrane included five studies to answer this question. Two studies were concerned with the diagnosis of melanoma and three with the diagnosis of BCC. What was studied in the review? A number of tools are available to skin cancer specialists which allow a more detailed examination of the skin compared to examination by the naked eye alone. Currently, a dermoscope is used by most skin cancer specialists, which magnifies the skin lesion (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) using a bright light source. OCT magnifies the surface of a skin lesion to the level of that seen using a microscope using near‐infrared light. It is quick to perform but is more expensive compared to dermoscopy and requires specialist training. Review authors examined how useful OCT is to help diagnose skin cancers when used after visual inspection or visual inspection plus dermoscopy. What are the main results of the review? The review included five studies: two studies with 97 participants with 133 skin lesions suspected of being melanoma, and three studies with 305 participants with 396 lesions suspected of being BCC of which one (50 lesions) also analysed cSCCs (nine lesions). The studies investigating the accuracy of OCT for diagnosing melanoma were small and too different from each other to allow a reliable estimate of the accuracy of OCT for melanoma to be made. Similarly, only one small, low‐quality study investigated the accuracy of OCT for diagnosing cSCC. For identifying BCC, two studies showed the effects of skin specialists using OCT after visual inspection alone, or visual inspection with dermoscopic examination. These two studies indicated that in theory, if OCT were to be used in a group of 1000 people with skin lesions that were particularly difficult to diagnose, of whom 600 (60%) actually had BCC, then: ‐ an estimated 662 people would have an OCT result confirming that a BCC was present and of these 93 (14%) would not actually have had a BCC (false‐positive result); ‐ of the 338 people with an OCT result indicating that no BCC was present, 31 (9%) would actually have a BCC (false‐negative result). Compared to making a diagnosis of BCC using visual inspection plus dermoscopy, the addition of OCT in this group would reduce the number of false‐positive results by 87 (thus reducing unnecessary surgical procedures) and would miss 53 fewer BCCs. How reliable are the results of the studies of this review? In all included studies, the diagnosis of skin cancer was made by lesion biopsy (OCT/dermoscopy positive) (a biopsy involves taking a sample of body cells and examining them under a microscope), and the absence of skin cancer was confirmed by biopsy (OCT/dermoscopy negative)*. This is likely to have been a reliable method for deciding whether people really had skin cancer. However, the small number of studies included in this review, and variability between them, reduced the reliability of findings. Included studies also had important limitations, in particular study participants were from more restricted groups than would be eligible for an OCT scan in practice (e.g. all studies included people with skin lesions that had already been selected for surgical removal), while the way in which OCT was used may not reflect real‐life situations. Who do the results of this review apply to? Studies were conducted in Europe and the US only. Average age (reported in only two studies) was 46 years for melanoma and 63 years for BCC. The percentage of people with a final diagnosis of melanoma was 23% and 27% (in two studies), ranged from 58% to 61% for BCC (three studies), and was 18% for cSCC (one study). For the diagnosis of BCC, the results apply to people with 'pink' and non‐pigmented skin lesions that the clinician considers particularly difficult to diagnose by the naked eye alone. What are the implications of this review? Not enough research has been done on using OCT in detecting skin cancers. The results of this review suggest that OCT might help to diagnose BCC when it is difficult to distinguish it from benign skin lesions, but it is not yet clear whether it can adequately distinguish between BCC, cSCC, and melanoma skin cancers. More studies are needed comparing OCT to dermoscopy and to other microscopic techniques (such as reflectance confocal microscopy) in well‐described groups of people with suspicious skin lesions. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies biopsy or clinical follow‐up were the standard comparisons.","12","John Wiley & Sons, Ltd","1465-1858","*Tomography, Optical Coherence; Adult; Carcinoma, Basal Cell [*diagnostic imaging]; Carcinoma, Squamous Cell [diagnostic imaging]; Humans; Melanoma [*diagnostic imaging]; Melanoma, Cutaneous Malignant; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging]","10.1002/14651858.CD013189","http://dx.doi.org/10.1002/14651858.CD013189","Skin"
"CD013187","Ferrante di Ruffano, L; Dinnes, J; Chuchu, N; Bayliss, SE; Takwoingi, Y; Davenport, C; Matin, RN; O'Sullivan, C; Roskell, D; Deeks, JJ; Williams, HC","Exfoliative cytology for diagnosing basal cell carcinoma and other skin cancers in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early accurate detection of all skin cancer types is essential to guide appropriate management, reduce morbidity and improve survival. Basal cell carcinoma (BCC) is usually localised to the skin but has potential to infiltrate and damage surrounding tissue, while cutaneous squamous cell carcinoma (cSCC) and melanoma have a much higher potential to metastasise and ultimately lead to death. Exfoliative cytology is a non‐invasive test that uses the Tzanck smear technique to identify disease by examining the structure of cells obtained from scraped samples. This simple procedure is a less invasive diagnostic test than a skin biopsy, and for BCC it has the potential to provide an immediate diagnosis that avoids an additional clinic visit to receive skin biopsy results. This may benefit patients scheduled for either Mohs micrographic surgery or non‐surgical treatments such as radiotherapy. A cytology scrape can never give the same information as a skin biopsy, however, so it is important to better understand in which skin cancer situations it may be helpful. Objectives To determine the diagnostic accuracy of exfoliative cytology for detecting basal cell carcinoma (BCC) in adults, and to compare its accuracy with that of standard diagnostic practice (visual inspection with or without dermoscopy). Secondary objectives were: to determine the diagnostic accuracy of exfoliative cytology for detecting cSCC, invasive melanoma and atypical intraepidermal melanocytic variants, and any other skin cancer; and for each of these secondary conditions to compare the accuracy of exfoliative cytology with visual inspection with or without dermoscopy in direct test comparisons; and to determine the effect of observer experience. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We also studied the reference lists of published systematic review articles. Selection criteria Studies evaluating exfoliative cytology in adults with lesions suspicious for BCC, cSCC or melanoma, compared with a reference standard of histological confirmation. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). Where possible we estimated summary sensitivities and specificities using the bivariate hierarchical model. Main results We synthesised the results of nine studies contributing a total of 1655 lesions to our analysis, including 1120 BCCs (14 datasets), 41 cSCCs (amongst 401 lesions in 2 datasets), and 10 melanomas (amongst 200 lesions in 1 dataset). Three of these datasets (one each for BCC, melanoma and any malignant condition) were derived from one study that also performed a direct comparison with dermoscopy. Studies were of moderate to poor quality, providing inadequate descriptions of participant selection, thresholds used to make cytological and histological diagnoses, and blinding. Reporting of participants' prior referral pathways was particularly poor, as were descriptions of the cytodiagnostic criteria used to make diagnoses. No studies evaluated the use of exfoliative cytology as a primary diagnostic test for detecting BCC or other skin cancers in lesions suspicious for skin cancer. Pooled data from seven studies using standard cytomorphological criteria (but various stain methods) to detect BCC in participants with a high clinical suspicion of BCC estimated the sensitivity and specificity of exfoliative cytology as 97.5% (95% CI 94.5% to 98.9%) and 90.1% (95% CI 81.1% to 95.1%). respectively. When applied to a hypothetical population of 1000 clinically suspected BCC lesions with a median observed BCC prevalence of 86%, exfoliative cytology would miss 21 BCCs and would lead to 14 false positive diagnoses of BCC. No false positive cases were histologically confirmed to be melanoma. Insufficient data are available to make summary statements regarding the accuracy of exfoliative cytology to detect melanoma or cSCC, or its accuracy compared to dermoscopy. Authors' conclusions The utility of exfoliative cytology for the primary diagnosis of skin cancer is unknown, as all included studies focused on the use of this technique for confirming strongly suspected clinical diagnoses. For the confirmation of BCC in lesions with a high clinical suspicion, there is evidence of high sensitivity and specificity. Since decisions to treat low‐risk BCCs are unlikely in practice to require diagnostic confirmation given that clinical suspicion is already high, exfoliative cytology might be most useful for cases of BCC where the treatments being contemplated require a tissue diagnosis (e.g. radiotherapy). The small number of included studies, poor reporting and varying methodological quality prevent us from drawing strong conclusions to guide clinical practice. Despite insufficient data on the use of cytology for cSCC or melanoma, it is unlikely that cytology would be useful in these scenarios since preservation of the architecture of the whole lesion that would be available from a biopsy provides crucial diagnostic information. Given the paucity of good quality data, appropriately designed prospective comparative studies may be required to evaluate both the diagnostic value of exfoliative cytology by comparison to dermoscopy, and its confirmatory value in adequately reported populations with a high probability of BCC scheduled for further treatment requiring a tissue diagnosis. Plain language summary How accurate is exfoliative cytology ('skin scrape' cytology) for diagnosing basal cell carcinoma and other skin cancers in adults? Why is improving the diagnosis of skin cancer important? There are a number of different types of skin cancer. The most common is basal cell carcinoma (BCC). BCC is a localised cancer that can grow and destroy the skin around it. They rarely spread into the body like other cancers can. Very small or superficial low‐risk BCCs can generally be treated with treatments such as creams rather than surgery, while it is better to surgically remove BCCs that are more likely to grow and spread. Radiotherapy (a treatment where radiation is used to kill cancer cells) can also be used if BCCs are very large or cannot be removed by surgery. Cutaneous squamous cell carcinoma (cSCC) is also usually a localised skin cancer. In a small proportion of cases it can spread to other parts of the body, so the best treatment is to remove it using surgery. Melanoma is one of the most dangerous forms of skin cancer as it has a higher potential to spread to other parts of the body, and so it is vital to recognise it and remove it early. If people with BCC do not receive the correct diagnosis (known as a false negative test result), their treatment can be delayed, making the surgical procedure more complicated. Diagnosing BCC when it is actually something else (a false positive result) may result in unnecessary treatment, surgery or other investigations and can cause the patient stress and anxiety. If BCC is incorrectly diagnosed in an individual who actually has an cSCC or melanoma, effective treatment can be delayed and this might lead to a greater chance that the cSCC or melanoma spreads to other organs in the body, which can be very serious. What is the aim of the review? The aim of this Cochrane Review was to find out how accurate a technique called 'exfoliative cytology' is for diagnosing skin cancer. Researchers in Cochrane found nine studies to answer this question. Nine studies were concerned with the diagnosis of BCC, two with the diagnosis of cSCC and one with the diagnosis of melanoma. What was studied in the review? Exfoliative cytology means scraping the surface of a possible skin cancer with a knife and then spreading a small layer of the scrape onto a glass slide so that the cells in the scrape can be stained and looked at under a microscope. It is less invasive than skin biopsy and quick to perform, with results available immediately. This could save patients an additional clinic visit to receive skin biopsy results. What are the main results of the review? The review examined nine studies with a total of 1655 lesions (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) that were given these final diagnoses*: 1120 BCCs, 41 cSCCs and 10 melanomas. For identifying BCC, seven studies show the effect of using exfoliative cytology to confirm BCC in lesions that doctors already suspected were BCCs. In a group of 1000 such lesions, of which 860 (86%) actually do have BCC, then: ‐ an estimated 853 people will have an exfoliative cytology result confirming that a BCC is present. Of these 14 (1.6%) will not actually have a BCC (false positive result); ‐ of the 147 people with an exfoliative cytology result indicating that no BCC is present, 21 (14%) will in fact actually have a BCC (false negative result). One study compared the accuracy of exfoliative cytology to using a hand‐held microscope (dermoscopy) for making a diagnosis of BCC but used a different method of removing cells and included patients with a higher risk of melanoma than found in the other eight studies. There was not enough evidence to determine the accuracy of exfoliative cytology for diagnosing cSCC or melanoma. How reliable are the results of the studies of this review? The small number of studies included in this review, poor description of how patients were selected to be included in the study, and limited information on how the test results were used to make diagnoses, reduces the reliability of our results. The studies did not explain how patients had been referred to have the exfoliative cytology test. Most important of all, the test was only used in people in whom doctors had already diagnosed a BCC just by looking at the skin lesion. In other words, the test was being used to confirm a doctor's diagnosis. Most studies did not include enough people with skin lesions that are similar in appearance to a BCC to be sure that this test correctly identifies a BCC. This may cause exfoliative cytology to appear more accurate than it would be in actual practice. Who do the results of this review apply to? Studies were conducted in the UK, across Europe and in Australia. Study authors rarely described patient characteristics, such as age and location of the lesion. The percentage of people included in the studies with a final diagnosis of BCC ranged from 18% to 90% (nine studies). For cSCC it was 4% and 18% (two studies), and for melanoma it was 5% (one study). It was not possible to tell from the studies how clinicians had decided that study participants had lesions that could be a skin cancer. What are the implications of this review? No research has been done using exfoliative cytology to diagnose a skin cancer when a patient is first seen by a doctor. The results of this review suggest that exfoliative cytology can help to confirm BCC in patients with skin lesions that a doctor already suspects of being a BCC. This test could be useful for patients with BCCs that need non‐surgical treatments, such as radiotherapy, where a tissue diagnosis is needed before the treatment can be given. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies, biopsy was the reference standard (means of establishing the final diagnosis).","12","John Wiley & Sons, Ltd","1465-1858","Adult; Azure Stains; Carcinoma, Basal Cell [diagnosis, *pathology]; Carcinoma, Squamous Cell [diagnosis, *pathology]; Coloring Agents; Cytodiagnosis [*methods]; Dermoscopy; Humans; Melanoma [*pathology]; Melanoma, Cutaneous Malignant; Papanicolaou Test; Sensitivity and Specificity; Skin Neoplasms [diagnosis, *pathology]","10.1002/14651858.CD013187","http://dx.doi.org/10.1002/14651858.CD013187","Skin"
"CD013188","Dinnes, J; Bamber, J; Chuchu, N; Bayliss, SE; Takwoingi, Y; Davenport, C; Godfrey, K; O'Sullivan, C; Matin, RN; Deeks, JJ; Williams, HC","High‐frequency ultrasound for diagnosing skin cancer in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early, accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and squamous cell carcinoma (SCC) are high‐risk skin cancers with the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised, with potential to infiltrate and damage surrounding tissue. Anxiety around missing early curable cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Ultrasound is a non‐invasive imaging technique that relies on the measurement of sound wave reflections from the tissues of the body. At lower frequencies, the deeper structures of the body such as the internal organs can be visualised, while high‐frequency ultrasound (HFUS) with transducer frequencies of 20 MHz or more has a much lower depth of tissue penetration but produces a higher resolution image of tissues and structures closer to the skin surface. Used in conjunction with clinical and/or dermoscopic examination of suspected skin cancer, HFUS may offer additional diagnostic information compared to other technologies. Objectives To assess the diagnostic accuracy of HFUS to assist in the diagnosis of a) cutaneous invasive melanoma and atypical intraepidermal melanocytic variants, b) cutaneous squamous cell carcinoma (cSCC), and c) basal cell carcinoma (BCC) in adults. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists as well as published systematic review articles. Selection criteria Studies evaluating HFUS (20 MHz or more) in adults with lesions suspicious for melanoma, cSCC or BCC versus a reference standard of histological confirmation or clinical follow‐up. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). Due to scarcity of data and the poor quality of studies, we did not undertake a meta‐analysis for this review. For illustrative purposes, we plot estimates of sensitivity and specificity on coupled forest plots. Main results We included six studies, providing 29 datasets: 20 for diagnosis of melanoma (1125 lesions and 242 melanomas) and 9 for diagnosis of BCC (993 lesions and 119 BCCs). We did not identify any data relating to the diagnosis of cSCC. Studies were generally poorly reported, limiting judgements of methodological quality. Half the studies did not set out to establish test accuracy, and all should be considered preliminary evaluations of the potential usefulness of HFUS. There were particularly high concerns for applicability of findings due to selective study populations and data‐driven thresholds for test positivity. Studies reporting qualitative assessments of HFUS images excluded up to 22% of lesions (including some melanomas) due to lack of visualisation in the test. Derived sensitivities for qualitative HFUS characteristics were at least 83% (95% CI 75% to 90%) for the detection of melanoma; the combination of three features (lesions appearing hypoechoic, homogenous and well defined) demonstrating 100% sensitivity in two studies (lower limits of the 95% CIs were 94% and 82%), with variable corresponding specificities of 33% (95% CI 20% to 48%) and 73% (95% CI 57% to 85%), respectively. Quantitative measurement of HFUS outputs in two studies enabled decision thresholds to be set to achieve 100% sensitivity; specificities were 93% (95% CI 77% to 99%) and 65% (95% CI 51% to 76%). It was not possible to make summary statements regarding HFUS accuracy for the diagnosis of BCC due to highly variable sensitivities and specificities. Authors' conclusions Insufficient data are available on the potential value of HFUS in the diagnosis of melanoma or BCC. Given the between‐study heterogeneity, unclear to low methodological quality and limited volume of evidence, we cannot draw any implications for practice. The main value of the preliminary studies included may be in providing guidance on the possible components of new diagnostic rules for diagnosis of melanoma or BCC using HFUS that will require future evaluation. A prospective evaluation of HFUS added to visual inspection and dermoscopy alone in a standard healthcare setting, with a clearly defined and representative population of participants, would be required for a full and proper evaluation of accuracy. Plain language summary How accurate is high‐frequency ultrasound for diagnosing skin cancer in adults? Why is improving the diagnosis of skin cancer important? There are several types of skin cancer. Melanoma is one of the most dangerous forms, so it is important to detect it early and remove it as soon as possible. Failure to recognise melanoma for what it is (known as a false negative test result) can delay treatment, risking the spread of melanoma to other organs in the body and possibly premature death. Other skin cancers, like cutaneous squamous cell carcinoma and basal cell carcinoma, are more localised. However, cutaneous squamous cell carcinoma can spread to other parts of the body, and basal cell carcinoma can cause disfigurement if left untreated. Diagnosing a harmless lesion (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) as skin cancer (a false positive result) may result in unnecessary surgery and other tests that can cause stress and anxiety to the patient. Mistaking one skin cancer for another can lead to the wrong treatment or delays in effective treatment. Thus, the correct diagnosis is important. What is the aim of the review? We wanted to find out whether high‐frequency ultrasound can help doctors diagnose skin cancer. We found six studies to try and answer this question. Five studies investigated the diagnosis of melanoma and three, basal cell carcinoma. What was studied in the review? A number of tools allow skin cancer specialists to examine the skin in more detail than by the naked eye alone. Most skin cancer specialists currently use a dermatoscope, which magnifies the skin lesion using a natural light. Ultrasound is another non‐invasive technique that measures sound wave reflections from body tissues. High‐frequency ultrasound can produce a good‐quality image of structures closer to the skin surface. When used alongside a doctor's examination and dermoscopy, high‐frequency ultrasound may help doctors make a more accurate diagnosis. What are the main results of the review? The review included six studies: five with 1125 skin lesions suspected of being melanoma, and three with 993 lesions suspected of being basal cell carcinoma. We did not find any studies on the diagnosis of cutaneous squamous cell carcinoma. The included studies were small and too different from each other to allow reliable estimates of accuracy to be made for identifying melanoma or basal cell carcinoma. Half were not actually designed to establish test accuracy and all can be considered preliminary experiments on the potential value of high‐frequency ultrasound. The main value of the studies may be in helping researchers to identify the best ways of interpreting high‐frequency ultrasound for the diagnosis of melanoma or basal cell carcinoma for evaluation in future research studies. How reliable are the results? Study results are not very reliable when considered collectively. The small number and variability between studies reduces reliability, while all had important limitations. In particular, those taking part in the studies and the way in which the tests were used may not reflect real life situations. In all studies the final diagnosis was confirmed by biopsy. This is likely to have been a reliable method for deciding whether patients really had skin cancer*. Who do the results of this review apply to? Studies all took place in Europe, and only one reported participants' average age (55.3 years). The percentage of people with a final diagnosis of melanoma ranged from 14% to 58%, while 8% to 49% had basal cell carcinoma. It was not possible to tell whether doctors suspected skin cancer based on clinical examination alone or both clinical and dermoscopic examination. What are the implications of this review? At present, there is not enough good research to draw a conclusion on using high‐frequency ultrasound for diagnosing skin cancers. The results of this review suggest that high‐frequency ultrasound has potential to separate melanoma or basal cell carcinoma from some harmless types of lesions, but it is still unclear whether it can adequately distinguish these skin cancers from the full range of skin conditions that patients show their doctors in everyday practice. There is a need for more studies investigating high‐frequency ultrasound alongside dermoscopy or other microscopic techniques (such as reflectance confocal microscopy) in people with suspicious skin lesions. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies biopsy was the reference standard (means of establishing the final diagnosis).","12","John Wiley & Sons, Ltd","1465-1858","Adult; Carcinoma, Basal Cell [*diagnostic imaging]; Carcinoma, Squamous Cell [*diagnostic imaging]; Humans; Melanoma [*diagnostic imaging]; Melanoma, Cutaneous Malignant; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging]; Ultrasonography [*methods]","10.1002/14651858.CD013188","http://dx.doi.org/10.1002/14651858.CD013188","Skin"
"CD013186","Ferrante di Ruffano, L; Takwoingi, Y; Dinnes, J; Chuchu, N; Bayliss, SE; Davenport, C; Matin, RN; Godfrey, K; O'Sullivan, C; Gulati, A; Chan, SA; Durack, A; O'Connell, S; Gardiner, MD; Bamber, J; Deeks, JJ; Williams, HC","Computer‐assisted diagnosis techniques (dermoscopy and spectroscopy‐based) for diagnosing skin cancer in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and cutaneous squamous cell carcinoma (cSCC) are high‐risk skin cancers which have the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised with potential to infiltrate and damage surrounding tissue. Anxiety around missing early curable cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Computer‐assisted diagnosis (CAD) systems use artificial intelligence to analyse lesion data and arrive at a diagnosis of skin cancer. When used in unreferred settings ('primary care'), CAD may assist general practitioners (GPs) or other clinicians to more appropriately triage high‐risk lesions to secondary care. Used alongside clinical and dermoscopic suspicion of malignancy, CAD may reduce unnecessary excisions without missing melanoma cases. Objectives To determine the accuracy of CAD systems for diagnosing cutaneous invasive melanoma and atypical intraepidermal melanocytic variants, BCC or cSCC in adults, and to compare its accuracy with that of dermoscopy. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials (CENTRAL); MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Studies of any design that evaluated CAD alone, or in comparison with dermoscopy, in adults with lesions suspicious for melanoma or BCC or cSCC, and compared with a reference standard of either histological confirmation or clinical follow‐up. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities separately by type of CAD system, using the bivariate hierarchical model. We compared CAD with dermoscopy using (a) all available CAD data (indirect comparisons), and (b) studies providing paired data for both tests (direct comparisons). We tested the contribution of human decision‐making to the accuracy of CAD diagnoses in a sensitivity analysis by removing studies that gave CAD results to clinicians to guide diagnostic decision‐making. Main results We included 42 studies, 24 evaluating digital dermoscopy‐based CAD systems (Derm–CAD) in 23 study cohorts with 9602 lesions (1220 melanomas, at least 83 BCCs, 9 cSCCs), providing 32 datasets for Derm–CAD and seven for dermoscopy. Eighteen studies evaluated spectroscopy‐based CAD (Spectro–CAD) in 16 study cohorts with 6336 lesions (934 melanomas, 163 BCC, 49 cSCCs), providing 32 datasets for Spectro–CAD and six for dermoscopy. These consisted of 15 studies using multispectral imaging (MSI), two studies using electrical impedance spectroscopy (EIS) and one study using diffuse‐reflectance spectroscopy. Studies were incompletely reported and at unclear to high risk of bias across all domains. Included studies inadequately address the review question, due to an abundance of low‐quality studies, poor reporting, and recruitment of highly selected groups of participants. Across all CAD systems, we found considerable variation in the hardware and software technologies used, the types of classification algorithm employed, methods used to train the algorithms, and which lesion morphological features were extracted and analysed across all CAD systems, and even between studies evaluating CAD systems. Meta–analysis found CAD systems had high sensitivity for correct identification of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in highly selected populations, but with low and very variable specificity, particularly for Spectro–CAD systems. Pooled data from 22 studies estimated the sensitivity of Derm–CAD for the detection of melanoma as 90.1% (95% confidence interval (CI) 84.0% to 94.0%) and specificity as 74.3% (95% CI 63.6% to 82.7%). Pooled data from eight studies estimated the sensitivity of multispectral imaging CAD (MSI–CAD) as 92.9% (95% CI 83.7% to 97.1%) and specificity as 43.6% (95% CI 24.8% to 64.5%). When applied to a hypothetical population of 1000 lesions at the mean observed melanoma prevalence of 20%, Derm–CAD would miss 20 melanomas and would lead to 206 false‐positive results for melanoma. MSI–CAD would miss 14 melanomas and would lead to 451 false diagnoses for melanoma. Preliminary findings suggest CAD systems are at least as sensitive as assessment of dermoscopic images for the diagnosis of invasive melanoma and atypical intraepidermal melanocytic variants. We are unable to make summary statements about the use of CAD in unreferred populations, or its accuracy in detecting keratinocyte cancers, or its use in any setting as a diagnostic aid, because of the paucity of studies. Authors' conclusions In highly selected patient populations all CAD types demonstrate high sensitivity, and could prove useful as a back‐up for specialist diagnosis to assist in minimising the risk of missing melanomas. However, the evidence base is currently too poor to understand whether CAD system outputs translate to different clinical decision–making in practice. Insufficient data are available on the use of CAD in community settings, or for the detection of keratinocyte cancers. The evidence base for individual systems is too limited to draw conclusions on which might be preferred for practice. Prospective comparative studies are required that evaluate the use of already evaluated CAD systems as diagnostic aids, by comparison to face–to–face dermoscopy, and in participant populations that are representative of those in which the test would be used in practice. Plain language summary What is the diagnostic accuracy of computer‐assisted diagnosis techniques for the detection of skin cancer in adults? Why is improving the diagnosis of skin cancer important? There are a number of different types of skin cancer, including melanoma, squamous cell carcinoma (SCC) and basal cell carcinoma (BCC). Melanoma is one of the most dangerous forms. If it is not recognised early treatment can be delayed and this risks the melanoma spreading to other organs in the body and may eventually lead to death. Cutaneous squamous cell carcinoma (cSCC) and BCC are considered less dangerous, as they are localised (less likely to spread to other parts of the body compared to melanoma). However, cSCC can spread to other parts of the body and BCC can cause disfigurement if not recognised early. Diagnosing a skin cancer when it is not actually present (a false‐positive result) might result in unnecessary surgery and other investigations and can cause stress and anxiety to the patient. Missing a diagnosis of skin cancer may result in the wrong treatment being used or lead to a delay in effective treatment. What is the aim of the review? The aim of this Cochrane Review was to find out how accurate computer–assisted diagnosis (CAD) is for diagnosing melanoma, BCC or cSCC. The review also compared the accuracy of two different types of CAD, and compared the accuracy of CAD with diagnosis by a doctor using a handheld illuminated microscope (a dermatoscope or ‘dermoscopy’). We included 42 studies to answer these questions. What was studied in the review? A number of tools are available to skin cancer specialists which allow a more detailed examination of the skin compared to examination by the naked eye alone. Currently a dermatoscope which magnifies the skin lesion (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) using a bright light source is used by most skin cancer specialists. CAD tests are computer systems that analyse information about skin lesions obtained from a dermatoscope or other techniques that use light to describe the features of a skin lesion (spectroscopy) to produce a result indicating whether skin cancer is likely to be present. We included CAD systems that get their information from dermoscopic images of lesions (Derm–CAD), or that use data from spectroscopy. Most of the spectroscopy studies used data from multispectral imaging (MSI–CAD) and are the main focus here. When a skin cancer specialist finds a lesion is suspicious using visual examination with or without additional dermoscopy, results from CAD systems can be used alone to make a diagnosis of skin cancer (CAD–based diagnosis), or can be used by doctors in addition to their visual inspection examination of a skin lesion to help them reach a diagnosis (CAD–aided diagnosis). Researchers examined how useful CAD systems are to help diagnose skin cancers in addition to visual inspection and dermoscopy. What are the main results of the review? The review included 42 studies looking at CAD systems for the diagnosis of melanoma. There was not enough evidence to determine the accuracy of CAD systems for the diagnosis of BCC (3 studies) or cSCC (1 study). Derm‐CAD results for diagnosis of melanoma The main results for Derm‐CAD are based on 22 studies including 8992 lesions. Applied to a group of 1000 skin lesions, of which 200 (20%) are given a final diagnosis* of melanoma, the results suggest that: ‐ An estimated 386 people will have a Derm–CAD result suggesting that a melanoma is present, and of these 206 (53%) will not actually have a melanoma (false‐positive result) ‐ Of the 614 people with a Derm–CAD result indicating that no melanoma is present, 20 (3%) will in fact actually have a melanoma (false‐negative result) There was no evidence to suggest that dermoscopy or Derm–CAD was different in its ability to detect or rule out melanoma. MSI‐CAD results for diagnosis of melanoma The main results for MSI–CAD are based on eight studies including 2401 lesions. In a group of 1000 people, of whom 200 (20%) actually do have melanoma*, then: ‐ An estimated 637 people will have an MSI–CAD result suggesting that a melanoma is present, and of these 451 (71%) will not actually have a melanoma (false‐positive result) ‐ Of the 363 people with an MSI–CAD result indicating that no melanoma is present, 14 (4%) will in fact have a melanoma (false‐negative result) MSI–CAD detects more melanomas, but possibly produces more false‐positive results (an increase in unnecessary surgery). How reliable are the results of the studies of this review? Incomplete reporting of studies made it difficult for us to judge how reliable they were. Many studies had important limitations. Some studies only included particular types of skin lesions or excluded lesions that were considered difficult to diagnose. Importantly, most of the studies only included skin lesions with a biopsy result, which means that only a sample of lesions that would be seen by a doctor in practice were included. These characteristics may result in CAD systems appearing more or less accurate than they actually are. Who do the results of this review apply to? Studies were largely conducted in Europe (29, 69%) and North America (8, 19%). Mean age (reported in 6/42 studies) ranged from 32 to 49 years for melanoma. The percentage of people with a final diagnosis of melanoma ranged from 1% to 52%. It was not always possible to tell whether suspicion of skin cancer in study participants was based on clinical examination alone, or both clinical and dermoscopic examinations. Almost all studies were done in people with skin lesions who were seen at specialist clinics rather than by doctors in primary care. What are the implications of this review? CAD systems appear to be accurate for identification of melanomas in skin lesions that have already been selected for excision on the basis of clinical examination (visual inspection and dermoscopy). It is possible that some CAD systems identify more melanomas than doctors using dermoscopy images. However, CAD systems also produced far more false ‐ positive diagnoses than dermoscopy, and could lead to considerable increases in unnecessary surgery. The performance of CAD systems for detecting BCC and cSCC skin cancers is unclear. More studies are needed to evaluate the use of CAD by doctors for the diagnosis of skin cancer in comparison to face‐to‐face diagnosis using dermoscopy, both in primary care and in specialist skin cancer clinics. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies, biopsy, clinical follow up, or specialist clinician diagnosis were the reference standards (means of establishing the final diagnosis).","12","John Wiley & Sons, Ltd","1465-1858","*Electric Impedance; Adult; Algorithms; Carcinoma, Basal Cell [*diagnosis, diagnostic imaging, pathology]; Carcinoma, Squamous Cell [*diagnosis, diagnostic imaging, pathology]; Clinical Decision‐Making; Dermoscopy [*methods, standards]; Diagnosis, Computer‐Assisted [*methods, standards]; False Positive Reactions; Humans; Melanoma [*diagnosis, diagnostic imaging, pathology]; Sensitivity and Specificity; Skin Neoplasms [*diagnosis, diagnostic imaging, pathology]","10.1002/14651858.CD013186","http://dx.doi.org/10.1002/14651858.CD013186","Skin"
"CD013191","Dinnes, J; Deeks, JJ; Chuchu, N; Saleh, D; Bayliss, SE; Takwoingi, Y; Davenport, C; Patel, L; Matin, RN; O'Sullivan, C; Patalay, R; Williams, HC","Reflectance confocal microscopy for diagnosing keratinocyte skin cancers in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early accurate detection of all skin cancer types is important to guide appropriate management and improve morbidity and survival. Basal cell carcinoma (BCC) is usually a localised skin cancer but with potential to infiltrate and damage surrounding tissue, whereas cutaneous squamous cell carcinoma (cSCC) and melanoma are higher risk skin cancers with the potential to metastasise and ultimately lead to death. When used in conjunction with clinical or dermoscopic suspicion of malignancy, or both, reflectance confocal microscopy (RCM) may help to identify cancers eligible for non‐surgical treatment without the need for a diagnostic biopsy, particularly in people with suspected BCC. Any potential benefit must be balanced against the risk of any misdiagnoses. Objectives To determine the diagnostic accuracy of RCM for the detection of BCC, cSCC, or any skin cancer in adults with any suspicious lesion and lesions that are difficult to diagnose (equivocal); and to compare its accuracy with that of usual practice (visual inspection or dermoscopy, or both). Search methods We undertook a comprehensive search of the following databases from inception to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Studies of any design that evaluated the accuracy of RCM alone, or RCM in comparison to visual inspection or dermoscopy, or both, in adults with lesions suspicious for skin cancer compared with a reference standard of either histological confirmation or clinical follow‐up, or both. Data collection and analysis Two review authors independently extracted data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities using the bivariate hierarchical model. For computation of likely numbers of true‐positive, false‐positive, false‐negative, and true‐negative findings in the 'Summary of findings' tables, we applied summary sensitivity and specificity estimates to lower quartile, median and upper quartiles of the prevalence observed in the study groups. We also investigated the impact of observer experience. Main results The review included 10 studies reporting on 11 study cohorts. All 11 cohorts reported data for the detection of BCC, including 2037 lesions (464 with BCC); and four cohorts reported data for the detection of cSCC, including 834 lesions (71 with cSCC). Only one study also reported data for the detection of BCC or cSCC using dermoscopy, limiting comparisons between RCM and dermoscopy. Studies were at high or unclear risk of bias across almost all methodological quality domains, and were of high or unclear concern regarding applicability of the evidence. Selective participant recruitment, unclear blinding of the reference test, and exclusions due to image quality or technical difficulties were observed. It was unclear whether studies were representative of populations eligible for testing with RCM, and test interpretation was often undertaken using images, remotely from the participant and the interpreter blinded to clinical information that would normally be available in practice. Meta‐analysis found RCM to be more sensitive but less specific for the detection of BCC in studies of participants with equivocal lesions (sensitivity 94%, 95% confidence interval (CI) 79% to 98%; specificity 85%, 95% CI 72% to 92%; 3 studies) compared to studies that included any suspicious lesion (sensitivity 76%, 95% CI 45% to 92%; specificity 95%, 95% CI 66% to 99%; 4 studies), although CIs were wide. At the median prevalence of disease of 12.5% observed in studies including any suspicious lesion, applying these results to a hypothetical population of 1000 lesions results in 30 BCCs missed with 44 false‐positive results (lesions misdiagnosed as BCCs). At the median prevalence of disease of 15% observed in studies of equivocal lesions, nine BCCs would be missed with 128 false‐positive results in a population of 1000 lesions. Across both sets of studies, up to 15% of these false‐positive lesions were observed to be melanomas mistaken for BCCs. There was some suggestion of higher sensitivities in studies with more experienced observers. Summary sensitivity and specificity could not be estimated for the detection of cSCC due to paucity of data. Authors' conclusions There is insufficient evidence for the use of RCM for the diagnosis of BCC or cSCC in either population group. A possible role for RCM in clinical practice is as a tool to avoid diagnostic biopsies in lesions with a relatively high clinical suspicion of BCC. The potential for, and consequences of, misclassification of other skin cancers such as melanoma as BCCs requires further research. Importantly, data are lacking that compare RCM to standard clinical practice (with or without dermoscopy). Plain language summary What is the diagnostic accuracy of reflectance confocal microscopy for the detection of basal or squamous cell carcinoma of the skin in adults? What is the aim of the review? The aim of this Cochrane Review was to find out how accurate reflectance confocal microscopy (RCM) is on its own or compared to inspection of a skin lesion with the naked eye alone or using a hand‐held microscope called dermoscopy for diagnosing two common forms of keratinocyte skin cancer: basal cell carcinoma (BCC) or cutaneous squamous cell carcinoma (cSCC) in adults. Review authors in Cochrane included 10 studies to answer this question. Why is improving the diagnosis of BCC or cSCC important? There are a number of different types of skin cancer. BCC and cSCC are usually localised skin cancers. Making the correct diagnosis is important because mistaking one skin cancer for another can lead to the wrong treatment being used or lead to a delay in effective treatment. A missed diagnosis of BCC (known as a false‐negative result) can result in the missed BCC growing and causing disfigurement. A missed diagnosis of cSCC is more serious as it could spread to other parts of the body. Diagnosing a skin cancer when it is not actually present (a false‐positive result) may result in unnecessary biopsy or treatment and can cause discomfort and worry to patients. What was studied in the review? Microscopic techniques are used by skin cancer specialists to provide a more detailed, magnified examination of suspicious skin lesions than can be achieved using the naked eye alone. Currently, dermoscopy is used by doctors as part of the examination of suspicious skin lesions. RCM is a new microscopic technique to increase the magnification. It is a hand‐held device or static unit using infrared light that can visualise deeper layers of the skin when compared with dermoscopy. Both techniques are painless procedures, but RCM is more expensive, time consuming, and requires additional specialised training. Dermoscopy can be used by general practitioners (GP) whereas RCM is likely to only be used by hospital specialists for people who have been referred with a skin lesion that is suspected to be a skin cancer. We wanted to see if RCM should be used instead of, or as well as, inspection of a skin lesion with the naked eye alone or using dermoscopy to diagnose BCC or cSCC. The accuracy of the test was looked at when used on people with any suspicious skin lesion and also in people with skin lesions that were tricky to diagnose. What are the main results of the review? We found 10 studies that included information on 11 groups of people with lesions suspicious for skin cancer. The main results were based on seven of the 11 sets of data: four in any lesion suspicious for skin cancer and three in particularly difficult to diagnose skin lesions. For the comparison of RCM versus dermoscopy, we found four sets of data that included 912 suspicious skin lesions. The results suggested that in a group of 1000 people with any suspicious lesion, of whom 125 (12.5%) really do have BCC: ‐ an estimated 139 people will have an RCM result indicating BCC is present; ‐ of these, 44 (32%) people will not have BCC (false‐positive results) including one person with a melanoma mistaken for a BCC; ‐ of the 861 people with an RCM result indicating that BCC is not present, 30 (3%) will actually have BCC. The review also included three sets of data on people that had 668 particularly difficult to diagnose skin lesions, one comparing RCM to dermoscopy. The results suggested that if RCM was to be used by skin specialists in a group of 1000 people, of whom 150 (15%) really do have BCC: ‐ an estimated 269 people will have an RCM result indicating BCC is present; ‐ of these, 128 (48%) people will not have a BCC (known as a false‐positive result), including as many as 19 people with melanomas mistaken for BCCs; ‐ of the 732 people with an RCM result indicating that BCC is not present, nine (1%) will actually have BCC. There was not enough evidence to determine the accuracy of RCM for the detection of cSCC in either population group. How reliable are the results of this review? There was a lot of variation in the results of the studies in this review. Poor reporting of study conduct made assessment of the reliability of studies difficult. It was unclear whether studies were representative of populations eligible for testing with RCM, and test interpretation was often undertaken using images, remotely from the patient and the interpreter blinded to clinical information that would normally be available in practice. Only one study compared the accuracy of dermoscopy and RCM. Most studies were conducted by specialist research teams with high levels of training and experience with RCM, meaning that RCM may appear better than it would be when used in everyday practice. Most studies reported diagnosis based on observers' subjective views, which might not be the same for people using the technique in everyday practice. In nine studies, the diagnosis of skin cancer was made by a skin biopsy or by following up those people over time to make sure they remained negative for skin cancer*. This is likely to have been a reliable method for deciding whether patients really had skin cancer. In one study, the absence of skin cancer was made by experts looking at the skin, a method that may be less reliable for deciding whether patients really had skin cancer. Who do the results of this review apply to? Five studies were carried out in Europe (61%), and the rest in Asia, Oceania, North America, or more than one continent. The average ages of people who took part ranged from 41 to 65 years. The percentage of people with BCC in these studies ranged from 6% to 83% (a middle value of 12% for any suspicious lesion and 15% for difficult to diagnose skin lesions). For studies of RCM used for cSCC, the percentage of people with cSCC ranged between 4% and 13%. In many studies it was not clear what tests people taking part had received before RCM. What are the implications of this review? There was not enough good evidence to support the use of RCM for the diagnosis of BCC or cSCC outside of research studies. There was a lot of variation and uncertainty in results and in the ways studies were carried out, reducing the reliability of findings. Using RCM might avoid the need for a diagnostic biopsy in people who see a doctor with a high suspicion of a BCC lesion, but more research is needed to confirm this. Such research should compare RCM to dermoscopy in well‐described groups of people with suspicious skin lesions and they must say whether other skin cancers end up being missed or being wrongly classified as BCC. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies, biopsy or clinical follow‐up were the reference standards (means of establishing final diagnoses).","12","John Wiley & Sons, Ltd","1465-1858","Adult; Carcinoma, Basal Cell [*diagnostic imaging]; Carcinoma, Squamous Cell [*diagnostic imaging]; Dermoscopy; False Positive Reactions; Humans; Microscopy, Confocal [*methods]; Physical Examination [methods]; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging]","10.1002/14651858.CD013191","http://dx.doi.org/10.1002/14651858.CD013191","Skin"
"CD013192","Chuchu, N; Takwoingi, Y; Dinnes, J; Matin, RN; Bassett, O; Moreau, JF; Bayliss, SE; Davenport, C; Godfrey, K; O'Connell, S; Jain, A; Walter, FM; Deeks, JJ; Williams, HC","Smartphone applications for triaging adults with skin lesions that are suspicious for melanoma","Cochrane Database of Systematic Reviews","2018","Abstract - Background Melanoma accounts for a small proportion of all skin cancer cases but is responsible for most skin cancer‐related deaths. Early detection and treatment can improve survival. Smartphone applications are readily accessible and potentially offer an instant risk assessment of the likelihood of malignancy so that the right people seek further medical attention from a clinician for more detailed assessment of the lesion. There is, however, a risk that melanomas will be missed and treatment delayed if the application reassures the user that their lesion is low risk. Objectives To assess the diagnostic accuracy of smartphone applications to rule out cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in adults with concerns about suspicious skin lesions. Search methods We undertook a comprehensive search of the following databases from inception to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Studies of any design evaluating smartphone applications intended for use by individuals in a community setting who have lesions that might be suspicious for melanoma or atypical intraepidermal melanocytic variants versus a reference standard of histological confirmation or clinical follow‐up and expert opinion. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). Due to scarcity of data and poor quality of studies, we did not perform a meta‐analysis for this review. For illustrative purposes, we plotted estimates of sensitivity and specificity on coupled forest plots for each application under consideration. Main results This review reports on two cohorts of lesions published in two studies. Both studies were at high risk of bias from selective participant recruitment and high rates of non‐evaluable images. Concerns about applicability of findings were high due to inclusion only of lesions already selected for excision in a dermatology clinic setting, and image acquisition by clinicians rather than by smartphone app users. We report data for five mobile phone applications and 332 suspicious skin lesions with 86 melanomas across the two studies. Across the four artificial intelligence‐based applications that classified lesion images (photographs) as melanomas (one application) or as high risk or 'problematic' lesions (three applications) using a pre‐programmed algorithm, sensitivities ranged from 7% (95% CI 2% to 16%) to 73% (95% CI 52% to 88%) and specificities from 37% (95% CI 29% to 46%) to 94% (95% CI 87% to 97%). The single application using store‐and‐forward review of lesion images by a dermatologist had a sensitivity of 98% (95% CI 90% to 100%) and specificity of 30% (95% CI 22% to 40%). The number of test failures (lesion images analysed by the applications but classed as 'unevaluable' and excluded by the study authors) ranged from 3 to 31 (or 2% to 18% of lesions analysed). The store‐and‐forward application had one of the highest rates of test failure (15%). At least one melanoma was classed as unevaluable in three of the four application evaluations. Authors' conclusions Smartphone applications using artificial intelligence‐based analysis have not yet demonstrated sufficient promise in terms of accuracy, and they are associated with a high likelihood of missing melanomas. Applications based on store‐and‐forward images could have a potential role in the timely presentation of people with potentially malignant lesions by facilitating active self‐management health practices and early engagement of those with suspicious skin lesions; however, they may incur a significant increase in resource and workload. Given the paucity of evidence and low methodological quality of existing studies, it is not possible to draw any implications for practice. Nevertheless, this is a rapidly advancing field, and new and better applications with robust reporting of studies could change these conclusions substantially. Plain language summary How accurate are smartphone applications ('apps') for detecting melanoma in adults? What is the aim of the review? We wanted to find out how well smartphone applications can help the general public understand whether their skin lesions might be melanoma. Why is improving the diagnosis of malignant melanoma skin cancer important? Melanoma is one of the most dangerous forms of skin cancer. Not recognising a melanoma (a false negative test result) could delay seeking appropriate advice and surgery to remove it. This increases the risk of the cancer spreading to other organs in the body and possibly causing death. Diagnosing a skin lesion as a melanoma when it is not present (a false positive result) may cause anxiety and lead to unnecessary surgery and further investigations. What was studied in the review? Specialised applications ('apps') that provide advice on skin lesions or moles that might cause people concern are widely available for smartphones. Some apps allow people to photograph any skin lesion they might be worried about and then receive guidance on whether to get medical advice. Apps may automatically classify lesions as high or low risk, while others can act as store‐and‐forward devices where images are sent to an experienced professional, such as a dermatologist, who then makes a risk assessment based on the photo. Cochrane researchers found two studies, evaluating five apps that used automated analysis of images and one that used a store‐and‐forward approach, to evaluate suspicious skin lesions. What are the main results of the review? The review included two studies with 332 lesions, including 86 melanomas, analysed by at least one smartphone application. Both studies used photographs of moles or skin lesions that were about to be removed because doctors had already decided they could be melanomas. The photographs were taken by doctors instead of people taking pictures of their lesions with their own smartphones. For these reasons, we are not able to make a reliable estimate about how well the apps actually work. Four apps that produce an immediate (automated) assessment of a skin lesion or mole that has been photographed by the smartphone missed between 7 and 55 melanomas. One app that sends the photograph of a mole or skin lesion to a dermatologist for assessment missed only one melanoma. Another 6 melanomas examined by the dermatologist via the application were not classified as high risk; instead the dermatologist was not able to classify the lesion as either 'atypical' (possibly a melanoma) or 'typical' (definitely not a melanoma). How reliable are the results of the studies of this review? The small number and poor quality of included studies reduces the reliability of findings. The people included were not typical of those who would use the applications in real life. The final diagnosis of melanoma was made by histology, which is likely to have been a reliable method for deciding whether patients really had melanoma*. However, the studies excluded between 2% and 18% of images because the applications failed to produce a recommendation. Who do the results of this review apply to? Studies took place in the USA and Germany. They did not report key patient information such as age and gender. The percentage of people with a final diagnosis of melanoma was 18% and 35%, much higher than that observed in community settings. The definition of eligible patients was narrow in comparison to likely users of the applications. The photographs used were taken by doctors rather than by smartphone users, which seriously impacts the applicability of results. What are the implications of this review? Current smartphone applications using automated analysis are observed to have a high chance of missing melanomas (false negatives). Store‐and‐forward image applications could have a potential role in the timely identification of people with potentially malignant lesions by facilitating early engagement of those with suspicious skin lesions, but they have resource and workload implications. The development of applications to help identify people who might have melanoma is a fast‐moving field. The emergence of new applications, higher quality and better reported studies could change the conclusions of this review substantially. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies biopsy was the reference standard (means of establishing final diagnoses).","12","John Wiley & Sons, Ltd","1465-1858","*Mobile Applications; *Smartphone; Adult; Algorithms; Diagnostic Errors [statistics & numerical data]; Early Detection of Cancer [instrumentation, *methods]; Humans; Melanoma [*diagnostic imaging]; Melanoma, Cutaneous Malignant; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging]; Triage [*methods]","10.1002/14651858.CD013192","http://dx.doi.org/10.1002/14651858.CD013192","Skin"
"CD013190","Dinnes, J; Deeks, JJ; Saleh, D; Chuchu, N; Bayliss, SE; Patel, L; Davenport, C; Takwoingi, Y; Godfrey, K; Matin, RN; Patalay, R; Williams, HC","Reflectance confocal microscopy for diagnosing cutaneous melanoma in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Melanoma has one of the fastest rising incidence rates of any cancer. It accounts for a small percentage of skin cancer cases but is responsible for the majority of skin cancer deaths. Early detection and treatment is key to improving survival; however, anxiety around missing early cases needs to be balanced against appropriate levels of referral and excision of benign lesions. Used in conjunction with clinical or dermoscopic suspicion of malignancy, or both, reflectance confocal microscopy (RCM) may reduce unnecessary excisions without missing melanoma cases. Objectives To determine the diagnostic accuracy of reflectance confocal microscopy for the detection of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in adults with any lesion suspicious for melanoma and lesions that are difficult to diagnose, and to compare its accuracy with that of dermoscopy. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; and seven other databases. We studied reference lists and published systematic review articles. Selection criteria Studies of any design that evaluated RCM alone, or RCM in comparison to dermoscopy, in adults with lesions suspicious for melanoma or atypical intraepidermal melanocytic variants, compared with a reference standard of either histological confirmation or clinical follow‐up. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities per algorithm and threshold using the bivariate hierarchical model. To compare RCM with dermoscopy, we grouped studies by population (defined by difficulty of lesion diagnosis) and combined data using hierarchical summary receiver operating characteristic (SROC) methods. Analysis of studies allowing direct comparison between tests was undertaken. To facilitate interpretation of results, we computed values of specificity at the point on the SROC curve with 90% sensitivity as this value lies within the estimates for the majority of analyses. We investigated the impact of using a purposely developed RCM algorithm and in‐person test interpretation. Main results The search identified 18 publications reporting on 19 study cohorts with 2838 lesions (including 658 with melanoma), which provided 67 datasets for RCM and seven for dermoscopy. Studies were generally at high or unclear risk of bias across almost all domains and of high or unclear concern regarding applicability of the evidence. Selective participant recruitment, lack of blinding of the reference test to the RCM result, and differential verification were particularly problematic. Studies may not be representative of populations eligible for RCM, and test interpretation was often undertaken remotely from the patient and blinded to clinical information. Meta‐analysis found RCM to be more accurate than dermoscopy in studies of participants with any lesion suspicious for melanoma and in participants with lesions that were more difficult to diagnose (equivocal lesion populations). Assuming a fixed sensitivity of 90% for both tests, specificities were 82% for RCM and 42% for dermoscopy for any lesion suspicious for melanoma (9 RCM datasets; 1452 lesions and 370 melanomas). For a hypothetical population of 1000 lesions at the median observed melanoma prevalence of 30%, this equated to a reduction in unnecessary excisions with RCM of 280 compared to dermoscopy, with 30 melanomas missed by both tests. For studies in equivocal lesions, specificities of 86% would be observed for RCM and 49% for dermoscopy (7 RCM datasets; 1177 lesions and 180 melanomas). At the median observed melanoma prevalence of 20%, this reduced unnecessary excisions by 296 with RCM compared with dermoscopy, with 20 melanomas missed by both tests. Across all populations, algorithms and thresholds assessed, the sensitivity and specificity of the Pellacani RCM score at a threshold of three or greater were estimated at 92% (95% confidence interval (CI) 87 to 95) for RCM and 72% (95% CI 62 to 81) for dermoscopy. Authors' conclusions RCM may have a potential role in clinical practice, particularly for the assessment of lesions that are difficult to diagnose using visual inspection and dermoscopy alone, where the evidence suggests that RCM may be both more sensitive and specific in comparison to dermoscopy. Given the paucity of data to allow comparison with dermoscopy, the results presented require further confirmation in prospective studies comparing RCM with dermoscopy in a real‐world setting in a representative population. Plain language summary What is the diagnostic accuracy of the imaging test reflectance confocal microscopy (RCM) for the detection of melanoma in adults? What was the aim of the review? The aim of this Cochrane Review was to find out how accurate reflectance confocal microscopy (RCM) was on its own and used in addition to dermoscopy compared to dermoscopy alone for diagnosing melanoma. Review authors in Cochrane included 18 publications to answer this question. Why is improving the diagnosis of melanoma important? Melanoma is one of the most dangerous forms of skin cancer. Not recognising a melanoma when it is present (called a false negative test result) delays surgery to remove it, risking cancer spreading to other parts in the body and possibly death. Diagnosing a skin lesion as a melanoma when it is not present (called a false positive result) may result in unnecessary surgery, further investigations, and patient anxiety. What did the review study? Microscopic techniques are used by skin cancer specialists to allow a more detailed, magnified examination of suspicious skin lesions than can be achieved using the naked eye alone. Currently, dermoscopy (a handheld device using natural light) can be used as part of the clinical examination of suspicious skin lesions. RCM is a new microscopic technique (a handheld device or static unit using infrared light) that can visualise deeper layers of the skin compared to dermoscopy. Both techniques are painless procedures, but RCM is more expensive, time consuming, and requires additional training. Dermoscopy can be used by general practitioners whereas RCM is likely to only be used by secondary care specialists in people who have been referred with a lesion suspicious for skin cancer. We sought to find out whether RCM should be used instead of, or in addition to, dermoscopy, to diagnose melanoma in any suspicious skin lesion or only in particularly difficult to diagnose skin lesions. What were the main results of the review? The review included 18 publications reporting data for 19 groups of participants with lesions suspected of melanoma. The main results were based on 16 of the 19 datasets (sets of information and results). The review included nine datasets with 1452 lesions in people with any suspicious skin lesion, three of which compared RCM to dermoscopy. The results suggested that in 1000 lesions, of which 300 (30%) actually are melanoma: ‐ an estimated 396 would have an RCM result indicating melanoma was present, and of these, 126 (32%) would not be melanoma (false positive results);  ‐ in the same group of 1000 lesions, dermoscopy would produce 406 false positive results, meaning RCM would avoid unnecessary surgery in 280 lesions compared to dermoscopy;  ‐ of the 604 lesions with an RCM result indicating that melanoma was not present (and 324 lesions with a dermoscopy result indicating that melanoma was not present), 30 would actually be melanoma (false negative results). This equated to a false negative rate of 5% for RCM and 9% for dermoscopy. The review also included seven datasets with 1177 lesions in people with particularly difficult to diagnose skin lesions, three of which compared RCM to dermoscopy. The results suggested that if skin specialists used RCM in a group of 1000 lesions, of which 200 (20%) were actually melanoma: ‐ an estimated 292 would have an RCM result indicating melanoma was present, and of these, 112 (38%) would not be melanoma (false positive results);  ‐ in the same group of 1000 lesions, dermoscopy would produce 408 false positive results, meaning RCM would avoid unnecessary surgery in 296 lesions compared to dermoscopy;  ‐ of the 708 lesions with an RCM result indicating that melanoma was not present (and 412 lesions with a dermoscopy result indicating that melanoma was not present), 20 would actually have melanoma (false negative results). This equates to a false negative rate of 3% for RCM and 5% for dermoscopy. How reliable were the results of the studies of this review? In all included studies, the diagnosis of melanoma was made by lesion biopsy (RCM/dermoscopy positive) (a biopsy involves taking a sample of body cells and examining them under a microscope), and the absence of melanoma was confirmed by biopsy or by follow‐up over time to make sure the skin lesion remained negative for melanoma (RCM/dermoscopy negative)*. This is likely to have been a reliable method for deciding whether people really had melanoma. Only a small number of studies compared the accuracy of dermoscopy and RCM. Most were conducted by specialist research teams with high levels of experience with RCM. Therefore, RCM may have appeared more accurate than it actually was. Participants in the nine studies of any suspicious lesion may have had very obvious disease compared to that seen in practice leading to a lower number of false positive results than would actually occur. It is not possible to recommend a definition of a positive RCM test that will reliably produce the results presented here due to differences between studies. Who do the results of this review apply to? Eleven studies were undertaken in Europe (61%), with the remainder undertaken in Oceania, North America, or more than one continent. Mean age ranged from 39 to 54.7 years. The percentage of people with melanoma ranged between 1.9% and 41.5% (a median (midpoint reading) of 19% for difficult to diagnose skin lesions and 32% for any suspicious lesion). The majority of studies only included people with certain types of skin lesion. In many studies, it was not clear what tests participants had received before RCM. What are the implications of this review? RCM appears to be an accurate test for identifying melanoma, and it may reduce the number of people receiving unnecessary surgery by up to three‐quarters compared to dermoscopy. There is considerable variation and uncertainty in results and in study conduct, reducing the reliability of findings. Use of RCM may be of most benefit in people with particularly difficult to diagnose lesions rather than people with any lesion suspicious for melanoma. Further research comparing RCM and dermoscopy in well described groups of people with difficult to diagnose skin lesions is needed. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies, biopsy or clinical follow‐up were the reference standards (means of establishing final diagnoses).","12","John Wiley & Sons, Ltd","1465-1858","*Dermoscopy; Adult; Biopsy; Humans; Melanoma [*diagnostic imaging, pathology]; Melanoma, Cutaneous Malignant; Microscopy, Confocal [*methods]; Sensitivity and Specificity; Skin Neoplasms [*diagnostic imaging, pathology]; Skin [pathology]","10.1002/14651858.CD013190","http://dx.doi.org/10.1002/14651858.CD013190","Skin"
"CD011902.PUB2","Dinnes, J; Deeks, JJ; Chuchu, N; Ferrante di Ruffano, L; Matin, RN; Thomson, DR; Wong, KY; Aldridge, RB; Abbott, R; Fawzy, M; Bayliss, SE; Grainge, MJ; Takwoingi, Y; Davenport, C; Godfrey, K; Walter, FM; Williams, HC","Dermoscopy, with and without visual inspection, for diagnosing melanoma in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Melanoma has one of the fastest rising incidence rates of any cancer. It accounts for a small percentage of skin cancer cases but is responsible for the majority of skin cancer deaths. Although history‐taking and visual inspection of a suspicious lesion by a clinician are usually the first in a series of ‘tests’ to diagnose skin cancer, dermoscopy has become an important tool to assist diagnosis by specialist clinicians and is increasingly used in primary care settings. Dermoscopy is a magnification technique using visible light that allows more detailed examination of the skin compared to examination by the naked eye alone. Establishing the additive value of dermoscopy over and above visual inspection alone across a range of observers and settings is critical to understanding its contribution for the diagnosis of melanoma and to future understanding of the potential role of the growing number of other high‐resolution image analysis techniques. Objectives To determine the diagnostic accuracy of dermoscopy alone, or when added to visual inspection of a skin lesion, for the detection of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in adults. We separated studies according to whether the diagnosis was recorded face‐to‐face (in‐person), or based on remote (image‐based), assessment. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: CENTRAL; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Studies of any design that evaluated dermoscopy in adults with lesions suspicious for melanoma, compared with a reference standard of either histological confirmation or clinical follow‐up. Data on the accuracy of visual inspection, to allow comparisons of tests, was included only if reported in the included studies of dermoscopy. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated accuracy using hierarchical summary receiver operating characteristic (SROC),methods. Analysis of studies allowing direct comparison between tests was undertaken. To facilitate interpretation of results, we computed values of sensitivity at the point on the SROC curve with 80% fixed specificity and values of specificity with 80% fixed sensitivity. We investigated the impact of in‐person test interpretation; use of a purposely developed algorithm to assist diagnosis; observer expertise; and dermoscopy training. Main results We included a total of 104 study publications reporting on 103 study cohorts with 42,788 lesions (including 5700 cases), providing 354 datasets for dermoscopy. The risk of bias was mainly low for the index test and reference standard domains and mainly high or unclear for participant selection and participant flow. Concerns regarding the applicability of study findings were largely scored as ‘high’ concern in three of four domains assessed. Selective participant recruitment, lack of reproducibility of diagnostic thresholds and lack of detail on observer expertise were particularly problematic. The accuracy of dermoscopy for the detection of invasive melanoma or atypical intraepidermal melanocytic variants was reported in 86 datasets; 26 for evaluations conducted in person (dermoscopy added to visual inspection), and 60 for image‐based evaluations (diagnosis based on interpretation of dermoscopic images). Analyses of studies by prior testing revealed no obvious effect on accuracy; analyses were hampered by the lack of studies in primary care, lack of relevant information and the restricted inclusion of lesions selected for biopsy or excision. Accuracy was higher for in‐person diagnosis compared to image‐based evaluations (relative diagnostic odds ratio (RDOR) 4.6, 95% confidence interval (CI) 2.4 to 9.0; P < 0.001). We compared accuracy for (a), in‐person evaluations of dermoscopy (26 evaluations; 23,169 lesions and 1664 melanomas),versus visual inspection alone (13 evaluations; 6740 lesions and 459 melanomas), and for (b), image‐based evaluations of dermoscopy (60 evaluations; 13,475 lesions and 2851 melanomas),versus image‐based visual inspection (11 evaluations; 1740 lesions and 305 melanomas). For both comparisons, meta‐analysis found dermoscopy to be more accurate than visual inspection alone, with RDORs of (a), 4.7 (95% CI 3.0 to 7.5; P < 0.001), and (b), 5.6 (95% CI 3.7 to 8.5; P < 0.001). For a), the predicted difference in sensitivity at a fixed specificity of 80% was 16% (95% CI 8% to 23%; 92% for dermoscopy + visual inspection versus 76% for visual inspection), and predicted difference in specificity at a fixed sensitivity of 80% was 20% (95% CI 7% to 33%; 95% for dermoscopy + visual inspection versus 75% for visual inspection). For b) the predicted differences in sensitivity was 34% (95% CI 24% to 46%; 81% for dermoscopy versus 47% for visual inspection), at a fixed specificity of 80%, and predicted difference in specificity was 40% (95% CI 27% to 57%; 82% for dermoscopy versus 42% for visual inspection), at a fixed sensitivity of 80%. Using the median prevalence of disease in each set of studies ((a), 12% for in‐person and (b), 24% for image‐based), for a hypothetical population of 1000 lesions, an increase in sensitivity of (a), 16% (in‐person), and (b), 34% (image‐based), from using dermoscopy at a fixed specificity of 80% equates to a reduction in the number of melanomas missed of (a), 19 and (b), 81 with (a), 176 and (b), 152 false positive results. An increase in specificity of (a), 20% (in‐person), and (b), 40% (image‐based), at a fixed sensitivity of 80% equates to a reduction in the number of unnecessary excisions from using dermoscopy of (a), 176 and (b), 304 with (a), 24 and (b), 48 melanomas missed. The use of a named or published algorithm to assist dermoscopy interpretation (as opposed to no reported algorithm or reported use of pattern analysis), had no significant impact on accuracy either for in‐person (RDOR 1.4, 95% CI 0.34 to 5.6; P = 0.17), or image‐based (RDOR 1.4, 95% CI 0.60 to 3.3; P = 0.22), evaluations. This result was supported by subgroup analysis according to algorithm used. We observed higher accuracy for observers reported as having high experience and for those classed as ‘expert consultants’ in comparison to those considered to have less experience in dermoscopy, particularly for image‐based evaluations. Evidence for the effect of dermoscopy training on test accuracy was very limited but suggested associated improvements in sensitivity. Authors' conclusions Despite the observed limitations in the evidence base, dermoscopy is a valuable tool to support the visual inspection of a suspicious skin lesion for the detection of melanoma and atypical intraepidermal melanocytic variants, particularly in referred populations and in the hands of experienced users. Data to support its use in primary care are limited, however, it may assist in triaging suspicious lesions for urgent referral when employed by suitably trained clinicians. Formal algorithms may be of most use for dermoscopy training purposes and for less expert observers, however reliable data comparing approaches using dermoscopy in person are lacking. Plain language summary How accurate is dermoscopy compared to visual inspection of the skin for diagnosing skin cancer (melanoma) in adults? What is the aim of the review? The aim of this Cochrane Review was to find out the accuracy of dermoscopy for the diagnosis of melanoma in comparison to visual inspection of the skin with the naked eye. The Review also investigated whether diagnostic accuracy using dermoscopy on a patient in person differed to the accuracy of diagnosis using dermoscopic images of the skin. Researchers in Cochrane included 104 studies to answer this question. Why is improving the diagnosis of melanoma important? Melanoma is one of the most dangerous forms of skin cancer. Not recognising a melanoma when it is present (a false‐negative test result), delays surgery to remove it, risking cancer spreading to other organs in the body, and possibly death. Diagnosing a skin lesion (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) as a melanoma when it is not (a false‐positive result), may result in unnecessary surgery, further investigations and patient anxiety. Visual inspection of suspicious skin lesions by a clinician using the naked eye is usually the first of a series of ‘tests’ to diagnose melanoma. Magnification techniques can be used by skin cancer specialists to allow a more detailed examination of suspicious skin lesions than can be achieved using the naked eye alone. What was studied in the review? A dermatoscope is a handheld device using visible light (such as from incandescent or LED bulbs), that can be used as part of the clinical examination of suspicious skin lesions. Dermoscopy has become an important tool to assist diagnosis by specialist clinicians and is also increasingly used in primary care settings. Knowing the diagnostic accuracy of dermoscopy added to visual inspection alone is important to understanding who it should be used by and in which healthcare settings. Researchers sought to find out the diagnostic accuracy of dermoscopy of suspicious skin lesions on a patient in person and using dermoscopic images compared to visual inspection alone. Researchers also sought to find out whether diagnostic accuracy was improved by use of a dermoscopy checklist or by an increase in level of clinical expertise. What are the main results of the review? The review included 104 studies reporting data for people with lesions suspected of melanoma. The main results for the diagnosis of melanoma (including very early melanomas), are based on 86 of the studies, 26 of which provide information on the accuracy of dermoscopy added to in‐person visual inspection of a skin lesion and 60 provide information based on examination of dermoscopic images without the patient being present. The 26 in‐person studies provide the most relevant data for the use of dermoscopy in practice and their results are summarised here. A total of 23,169 suspicious skin lesions were included in the 26 studies and 13 of them also provided information on the accuracy of visual inspection of a lesion without the use of dermoscopy. The results suggest that dermoscopy is more accurate than visual inspection on its own, both for identifying melanoma correctly and excluding things that are not melanoma. The studies used different ways of deciding whether a skin lesion was a melanoma or not, which means that we cannot be exactly sure about how much better dermoscopy is compared to visual inspection alone. Instead we can give an illustrative example of the expected effect of the increase in accuracy using a group of 1000 lesions, of which 120 (12%), are melanoma. In order to see how much better dermoscopy is in identifying melanoma correctly when compared to just looking at the skin, we have to assume that both lead to the same number of lesions being falsely diagnosed as melanoma (we assumed that 176 of the 880 lesions without melanoma would have an incorrect diagnosis of melanoma). In this fixed situation, adding dermoscopy to visual inspection would correctly identify an extra 19 melanomas (110 compared with 91), that would have been missed by just looking at the skin alone. In other words, more melanomas would be correctly identified. In order to see how much better dermoscopy is in deciding if a skin lesion is not a melanoma when compared to just looking at the skin, we have to assume that both lead to the same number of melanomas being correctly diagnosed (in this case we assumed that 96 out of the 120 melanomas would be correctly diagnosed). In this situation, adding in dermoscopy to visual inspection would reduce the number of lesions being wrongly diagnosed as being melanoma by 176 (a reduction from 220 in the visual inspection group to 44 lesions in the dermoscopy group). In other words, more lesions that were not melanoma would be correctly identified and fewer people would end up being sent for surgery. Value of visual inspection checklists and effect of observer expertise There was no evidence that use of a checklist to help dermoscopy interpretation changed diagnostic accuracy. Accuracy was better (with fewer missed melanomas and fewer people having unnecessary surgery), when the diagnosis was made by people with more clinical expertise and training. How reliable are the results of the studies of this review? In the majority of included studies, the diagnosis of melanoma was made by lesion biopsy and the absence of melanoma was confirmed by biopsy or by follow‐up over time to make sure the skin lesion remained negative for melanoma, both of which are likely to have been a reliable method for deciding whether patients really had melanoma*. In a few studies, the absence of melanoma was made by expert diagnosis, which is unlikely to have been a reliable method for deciding whether patients really had melanoma. Poor reporting of study conduct made assessment of the reliability of studies difficult. Selective participant recruitment and lack of detail regarding the threshold for deciding on a positive test result were particularly problematic. Who do the results of this review apply to? Sixty‐six studies were undertaken in Europe (77%), with the remainder undertaken in North America (6 studies), Asia (4), Oceania (4), or were multicentre (7). Mean age ranged from 30 to 58 years (reported in 26 studies). The percentage of individuals with melanoma ranged between 1% and 41% for dermoscopy in‐person studies (median 12%), and between 3% and 61% in studies using dermoscopy images (median 24%). Almost all of the studies were carried out in referral settings rather than in primary care. In the majority of studies the lesions were unlikely to be representative of the range of those seen in practice, for example only including skin lesions of a certain size or with a specific appearance. In addition variation in the expertise of clinicians performing visual inspection and the definition used for a positive dermoscopy test result across studies makes it unclear as to how dermoscopy should be carried out and by people with different levels of clinical expertise in order to achieve the accuracy observed in studies. What are the implications of this review? When used by specialists, dermoscopy is better at diagnosing melanoma compared to inspection of a suspicious skin lesion using the naked eye alone. Dermoscopy is more accurate when interpreted with the patient present rather than using dermoscopy images. Dermoscopy might help general practitioners to correctly identify people with suspicious lesions who need to be seen by a specialist. Checklists to help interpret dermoscopy might improve the accuracy of people with less expertise and training. Further, well‐reported studies assessing the diagnostic accuracy of dermoscopy when used in primary care and to identify the best way of delivering dermoscopy training are needed. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies biopsy, clinical follow‐up or specialist clinician diagnosis were the reference standards (means of establishing the final diagnosis).","12","John Wiley & Sons, Ltd","1465-1858","*Dermoscopy; Adult; Algorithms; Biopsy; Humans; Melanoma [*diagnosis, diagnostic imaging, pathology]; Melanoma, Cutaneous Malignant; Physical Examination [*methods]; Sensitivity and Specificity; Skin Neoplasms [*diagnosis, diagnostic imaging, pathology]; Skin [pathology]","10.1002/14651858.CD011902.pub2","http://dx.doi.org/10.1002/14651858.CD011902.pub2","Skin"
"CD011901.PUB2","Dinnes, J; Deeks, JJ; Chuchu, N; Matin, RN; Wong, KY; Aldridge, RB; Durack, A; Gulati, A; Chan, SA; Johnston, L; Bayliss, SE; Leonardi‐Bee, J; Takwoingi, Y; Davenport, C; O'Sullivan, C; Tehrani, H; Williams, HC","Visual inspection and dermoscopy, alone or in combination, for diagnosing keratinocyte skin cancers in adults","Cochrane Database of Systematic Reviews","2018","Abstract - Background Early accurate detection of all skin cancer types is important to guide appropriate management, to reduce morbidity and to improve survival. Basal cell carcinoma (BCC) is almost always a localised skin cancer with potential to infiltrate and damage surrounding tissue, whereas a minority of cutaneous squamous cell carcinomas (cSCCs) and invasive melanomas are higher‐risk skin cancers with the potential to metastasise and cause death. Dermoscopy has become an important tool to assist specialist clinicians in the diagnosis of melanoma, and is increasingly used in primary‐care settings. Dermoscopy is a precision‐built handheld illuminated magnifier that allows more detailed examination of the skin down to the level of the superficial dermis. Establishing the value of dermoscopy over and above visual inspection for the diagnosis of BCC or cSCC in primary‐ and secondary‐care settings is critical to understanding its potential contribution to appropriate skin cancer triage, including referral of higher‐risk cancers to secondary care, the identification of low‐risk skin cancers that might be treated in primary care and to provide reassurance to those with benign skin lesions who can be safely discharged. Objectives To determine the diagnostic accuracy of visual inspection and dermoscopy, alone or in combination, for the detection of (a) BCC and (b) cSCC, in adults. We separated studies according to whether the diagnosis was recorded face‐to‐face (in person) or based on remote (image‐based) assessment. Search methods We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria Studies of any design that evaluated visual inspection or dermoscopy or both in adults with lesions suspicious for skin cancer, compared with a reference standard of either histological confirmation or clinical follow‐up. Data collection and analysis Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS‐2). We contacted authors of included studies where information related to the target condition or diagnostic thresholds were missing. We estimated accuracy using hierarchical summary ROC methods. We undertook analysis of studies allowing direct comparison between tests. To facilitate interpretation of results, we computed values of sensitivity at the point on the SROC curve with 80% fixed specificity and values of specificity with 80% fixed sensitivity. We investigated the impact of in‐person test interpretation; use of a purposely‐developed algorithm to assist diagnosis; and observer expertise. Main results We included 24 publications reporting on 24 study cohorts, providing 27 visual inspection datasets (8805 lesions; 2579 malignancies) and 33 dermoscopy datasets (6855 lesions; 1444 malignancies). The risk of bias was mainly low for the index test (for dermoscopy evaluations) and reference standard domains, particularly for in‐person evaluations, and high or unclear for participant selection, application of the index test for visual inspection and for participant flow and timing. We scored concerns about the applicability of study findings as of ‘high’ or 'unclear' concern for almost all studies across all domains assessed. Selective participant recruitment, lack of reproducibility of diagnostic thresholds and lack of detail on observer expertise were particularly problematic. The detection of BCC was reported in 28 datasets; 15 on an in‐person basis and 13 image‐based. Analysis of studies by prior testing of participants and according to observer expertise was not possible due to lack of data. Studies were primarily conducted in participants referred for specialist assessment of lesions with available histological classification. We found no clear differences in accuracy between dermoscopy studies undertaken in person and those which evaluated images. The lack of effect observed may be due to other sources of heterogeneity, including variations in the types of skin lesion studied, in dermatoscopes used, or in the use of algorithms and varying thresholds for deciding on a positive test result. Meta‐analysis found in‐person evaluations of dermoscopy (7 evaluations; 4683 lesions and 363 BCCs) to be more accurate than visual inspection alone for the detection of BCC (8 evaluations; 7017 lesions and 1586 BCCs), with a relative diagnostic odds ratio (RDOR) of 8.2 (95% confidence interval (CI) 3.5 to 19.3; P < 0.001). This corresponds to predicted differences in sensitivity of 14% (93% versus 79%) at a fixed specificity of 80% and predicted differences in specificity of 22% (99% versus 77%) at a fixed sensitivity of 80%. We observed very similar results for the image‐based evaluations. When applied to a hypothetical population of 1000 lesions, of which 170 are BCC (based on median BCC prevalence across studies), an increased sensitivity of 14% from dermoscopy would lead to 24 fewer BCCs missed, assuming 166 false positive results from both tests. A 22% increase in specificity from dermoscopy with sensitivity fixed at 80% would result in 183 fewer unnecessary excisions, assuming 34 BCCs missed for both tests. There was not enough evidence to assess the use of algorithms or structured checklists for either visual inspection or dermoscopy. Insufficient data were available to draw conclusions on the accuracy of either test for the detection of cSCCs. Authors' conclusions Dermoscopy may be a valuable tool for the diagnosis of BCC as an adjunct to visual inspection of a suspicious skin lesion following a thorough history‐taking including assessment of risk factors for keratinocyte cancer. The evidence primarily comes from secondary‐care (referred) populations and populations with pigmented lesions or mixed lesion types. There is no clear evidence supporting the use of currently‐available formal algorithms to assist dermoscopy diagnosis. Plain language summary Does dermoscopy improve the accuracy of diagnosing basal cell or squamous cell skin cancer (BCC or cSCC) compared to using the naked eye alone? What is the aim of the review? We wanted to find out whether using a handheld illuminated microscope (dermatoscope or ‘dermoscopy’) is any better at diagnosing basal cell carcinoma (BCC) or cutaneous squamous cell carcinoma (cSCC) compared to just looking at the skin with the naked eye. We included 24 studies to answer this question. Why is improving diagnosis of BCC or cSCC important? There are a number of different types of skin cancer. BCC and cSCC are less serious than melanoma skin cancer, because they usually grow more slowly and BCC does not spread to other organs in the body. Making the correct diagnosis of BCC or cSCC is still important, because their treatment may differ. A missed BCC (known as a false negative result) can result in disfigurement and the need for more major surgery. A missed cSCC can spread to other parts of the body. Diagnosing BCC or cSCC when they are not actually present (a false positive result) may mean unnecessary treatment, e.g. surgical removal which may result in a disfiguring scar, and worry to patients if the lesion (a mole or area of skin with an unusual appearance in comparison with the surrounding skin) is benign (not a cancer), or may result in wrong treatment, e.g. a non‐surgical therapy, being used if the lesion is misdiagnosed. What was studied in the review? A dermatoscope is a handheld magnifier that includes a light source. Dermoscopy is often used by skin specialists to help diagnose skin cancer. It is also being used more by community doctors. As well as seeing whether dermoscopy added anything to visual inspection alone overall, we also wanted to find out whether dermoscopy accuracy was different when used in a face‐to‐face consultation or when used on images of skin lesions sent to specialists. We also tried to find out whether the accuracy of dermoscopy was improved by use of a checklist, or if it was better when used by a skin specialist compared to a non‐specialist. What are the main results of the review? The review included 24 studies reporting information for people with lesions suspected of skin cancer. Diagnosis of BCC with the patient present We found 11 relevant studies. Eight studies (including 7017 suspicious skin lesions) investigated the accuracy of visual inspection on its own and seven studies (with 4683 suspicious skin lesions) investigated the accuracy of dermoscopy added to visual inspection (four of which reported data for both visual inspection on its own and for dermoscopy added to visual inspection). The results suggest that dermoscopy is more accurate than visual inspection on its own, both for identifying BCC correctly and for excluding things that are not BCCs. The results can be illustrated using a group of 1000 lesions, of which 170 (17%) are BCC. In order to see how much better dermoscopy is in identifying BCC correctly when compared to just looking at the skin, we have to assume that both lead to the same number of lesions being falsely diagnosed as BCC (we assumed that 166 of the 830 lesions without BCC would have an incorrect diagnosis of BCC). In this fixed situation, adding dermoscopy to visual inspection would correctly identify an extra 24 BCCs (158 compared with 134) that would have been missed by just looking at the skin alone. In other words, more BCC cancers would be correctly identified. In order to see how much better dermoscopy is in deciding if a skin lesion is  not  a BCC when compared to just looking at the skin, we have to assume that both lead to the same number of BCCs being correctly diagnosed (in this case we assumed that 136 out of the 170 BCCs would be correctly diagnosed). In this situation, adding in dermoscopy to visual inspection would reduce the number of lesions being wrongly diagnosed as being BCC by 183 (a reduction from 191 in the visual inspection group to eight people in the dermoscopy group). In other words, more lesions that were not BCC would be correctly identified, and fewer people would end up being sent for surgery. Image‐based diagnosis of BCC Eleven studies concerning BCC diagnosis using either clinical photographs or magnified images from a dermatoscope were included. Four studies, (including 853 suspicious skin lesions) used visual inspection of photographs and nine studies (including 2271 suspicious lesions) used dermoscopic images (two studies reported data for diagnosis using both photographs and using dermoscopic images). Results were very similar to the in‐person studies. Value of checklists and observer expertise There was no evidence that use of a checklist to help visual inspection or dermoscopy interpretation improved diagnostic accuracy. There was not enough evidence to examine the effect of clinical expertise and training. Diagnosis of cSCC There was not enough evidence to reliably comment on the accuracy of either test for the detection of cSCCs. How reliable are the results of the studies of this review? Most of our studies made a reliable final diagnosis by lesion biopsy and by following people up over time to make sure the skin lesion remained negative for skin cancer. Some studies used expert diagnosis to confirm the absence of skin cancer, which is less reliable*. Poor reporting of what was done in the studies made it difficult for us to judge how reliable they were. Some studies excluded certain types of skin lesion and some did not describe how a positive test result to trigger referral to a specialist or treatment was defined. Who do the results of this review apply to? Eleven studies were done in Europe (46%), and the rest in North America (n = 3), Asia (n = 5), Oceania (n = 2), or multiple countries (n = 3). People included in the studies were on average between 30 and 74 years old. The percentage of people with BCC ranged between 1% and 61% for in‐person studies and between 2% and 63% in studies using images. Almost all studies were done with people referred from primary care to specialist skin clinics. Over half of studies considered the ability of dermoscopy and visual inspection to diagnose any skin cancer, including melanoma and BCC, while 10 (42%) focused on just BCC. Variation in the expertise of doctors doing the examinations and differences in the definitions used to decide when a test was positive make it unclear how dermoscopy should be carried out and what level of training is needed in order to achieve the accuracy observed in studies. What are the implications of this review? When used by specialists, dermoscopy may be a useful tool to help diagnose BCC correctly when compared with visual inspection alone. It is not clear whether dermoscopy should be used by general practitioners to correctly identify people with suspicious lesions who need to be seen by a specialist. Checklists to help interpret dermoscopy do not seem to help improve accuracy for BCC. Further research is needed, to see if dermoscopy is useful in primary care. How up‐to‐date is this review? The review authors searched for and used studies published up to August 2016. *In these studies biopsy, clinical follow‐up or specialist clinician diagnosis were the reference standards (means of establishing the final diagnosis).","12","John Wiley & Sons, Ltd","1465-1858","*Dermoscopy; Adult; Aged; Algorithms; Carcinoma, Basal Cell [*diagnosis, diagnostic imaging]; Carcinoma, Squamous Cell [*diagnosis, diagnostic imaging]; Humans; Keratinocytes; Middle Aged; Photography; Physical Examination [*methods]; Sensitivity and Specificity; Skin Neoplasms [*diagnosis, diagnostic imaging]","10.1002/14651858.CD011901.pub2","http://dx.doi.org/10.1002/14651858.CD011901.pub2","Skin"
"CD012567.PUB2","Roze, JF; Hoogendam, JP; van de Wetering, FT; Spijker, R; Verleye, L; Vlayen, J; Veldhuis, WB; Scholten, RJPM; Zweemer, RP","Positron emission tomography (PET) and magnetic resonance imaging (MRI) for assessing tumour resectability in advanced epithelial ovarian/fallopian tube/primary peritoneal cancer","Cochrane Database of Systematic Reviews","2018","Abstract - Background Ovarian cancer is the leading cause of death from gynaecological cancer in developed countries. Surgery and chemotherapy are considered its mainstay of treatment and the completeness of surgery is a major prognostic factor for survival in these women. Currently, computed tomography (CT) is used to preoperatively assess tumour resectability. If considered feasible, women will be scheduled for primary debulking surgery (i.e. surgical efforts to remove the bulk of tumour with the aim of leaving no visible (macroscopic) tumour). If primary debulking is not considered feasible (i.e. the tumour load is too extensive), women will receive neoadjuvant chemotherapy to reduce tumour load and subsequently undergo (interval) surgery. However, CT is imperfect in assessing tumour resectability, so additional imaging modalities can be considered to optimise treatment selection. Objectives To assess the diagnostic accuracy of fluorodeoxyglucose‐18 (FDG) PET/CT, conventional and diffusion‐weighted (DW) MRI as replacement or add‐on to abdominal CT, for assessing tumour resectability at primary debulking surgery in women with stage III to IV epithelial ovarian/fallopian tube/primary peritoneal cancer. Search methods We searched MEDLINE and Embase (OVID) for potential eligible studies (1946 to 23 February 2017). Additionally, ClinicalTrials.gov, WHO‐ICTRP and the reference list of all relevant studies were searched. Selection criteria Diagnostic accuracy studies addressing the accuracy of preoperative FDG‐PET/CT, conventional or DW‐MRI on assessing tumour resectability in women with advanced stage (III to IV) epithelial ovarian/fallopian tube/primary peritoneal cancer who are scheduled to undergo primary debulking surgery. Data collection and analysis Two authors independently screened titles and abstracts for relevance and inclusion, extracted data and performed methodological quality assessment using QUADAS‐2. The limited number of studies did not permit meta‐analyses. Main results Five studies (544 participants) were included in the analysis. All studies performed the index test as replacement of abdominal CT. Two studies (366 participants) addressed the accuracy of FDG‐PET/CT for assessing incomplete debulking with residual disease of any size (> 0 cm) with sensitivities of 1.0 (95% CI 0.54 to 1.0) and 0.66 (95% CI 0.60 to 0.73) and specificities of 1.0 (95% CI 0.80 to 1.0) and 0.88 (95% CI 0.80 to 0.93), respectively (low‐ and moderate‐certainty evidence). Three studies (178 participants) investigated MRI for different target conditions, of which two investigated DW‐MRI and one conventional MRI. The first study showed that DW‐MRI determines incomplete debulking with residual disease of any size with a sensitivity of 0.94 (95% CI 0.83 to 0.99) and a specificity of 0.98 (95% CI 0.88 to 1.00) (low‐ and moderate‐certainty evidence). For abdominal CT, the sensitivity for assessing incomplete debulking was 0.66 (95% CI 0.52 to 0.78) and the specificity 0.77 (95% CI 0.63 to 0.87) (low‐ and low‐certainty evidence). The second study reported a sensitivity of DW‐MRI of 0.75 (95% CI 0.35 to 0.97) and a specificity of 0.96 (95% CI 0.80 to 1.00) (very low‐certainty evidence) for assessing incomplete debulking with residual disease > 1 cm. In the last study, the sensitivity for assessing incomplete debulking with residual disease of > 2 cm on conventional MRI was 0.91 (95% CI 0.59 to 1.00) and the specificity 0.97 (95% CI 0.87 to 1.00) (very low‐certainty evidence). Overall, the certainty of evidence was very low to moderate (according to GRADE), mainly due to small sample sizes and imprecision. Authors' conclusions Studies suggested a high specificity and moderate sensitivity for FDG‐PET/CT and MRI to assess macroscopic incomplete debulking. However, the certainty of the evidence was insufficient to advise routine addition of FDG‐PET/CT or MRI to clinical practice.. In a research setting, adding an alternative imaging method could be considered for women identified as suitable for primary debulking by abdominal CT, in an attempt to filter out false‐negatives (i.e. debulking, feasible based on abdominal CT, unfeasible at actual surgery). Plain language summary How accurate are the imaging techniques PET and MRI to determine the feasibility of primary debulking surgery for ovarian cancer? Why is it important to determine the feasibility of ovarian tumour resection?   Ovarian cancer is a disease with a high mortality that affects 239,000 women each year across the world. By the time it is symptomatic and detected, cancer cells have spread throughout the abdomen in most women. Treatment consists of surgery to remove as much visible tumour as possible (also called debulking surgery) and chemotherapy. Randomised controlled trials have shown that in women where all visible cancer cannot be removed with surgery, giving chemotherapy first to shrink the tumour is an alternative treatment strategy. This can improve the number of women having successful removal of all visible tumour, known as macroscopic debulking. Therefore, it is important to determine beforehand if all visible tumour deposits can be removed by surgery, followed by chemotherapy, or if chemotherapy is needed first to reduce tumour size before surgery is performed. Imaging with abdominal computed tomography (abdominal CT) is currently used to determine whether primary debulking surgery is feasible. However, it cannot determine the outcome correctly in all women. Other imaging techniques that can be used are positron emission tomography (PET) and magnetic resonance imaging (MRI). PET visualises glucose uptake by cells and allows detection of distant metastases and is frequently performed parallel with abdominal CT (FDG‐PET/CT). MRI provides good soft tissue contrast to detect small lesions. These additional imaging techniques may improve treatment selection. What is the aim of this review? To investigate the accuracy of PET and MRI in women with advanced stage ovarian cancer to determine the feasibility of primary debulking surgery. What are the main results of the review? We identified two studies (with 366 participants) addressing the accuracy of FDG‐PET/CT and three studies (with 178 participants) investigating the accuracy of MRI. In a hypothetical group of 1000 women, of whom 620 would have residual tumour after surgery (prevalence 62%), 211 women would incorrectly be considered suitable for surgery according to FDG‐PET/CT and 37 women according to MRI. However, the quality and quantity of these studies were insufficient for these imaging techniques to be used routinely in clinical practice. Therefore, the authors concluded that more research is needed before such a recommendation can be made.","10","John Wiley & Sons, Ltd","1465-1858","*Diffusion Magnetic Resonance Imaging; *Fluorodeoxyglucose F18; *Positron Emission Tomography Computed Tomography; *Radiopharmaceuticals; Fallopian Tube Neoplasms [*diagnostic imaging, pathology, surgery]; Feasibility Studies; Female; Humans; Neoplasm, Residual [diagnostic imaging]; Ovarian Neoplasms [*diagnostic imaging, pathology, surgery]; Peritoneal Neoplasms [*diagnostic imaging, pathology, surgery]; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Tomography, X‐Ray Computed","10.1002/14651858.CD012567.pub2","http://dx.doi.org/10.1002/14651858.CD012567.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD012233.PUB2","Wennmacker, SZ; Lamberts, MP; Di Martino, M; Drenth, JPH; Gurusamy, KS; van Laarhoven, CJHM","Transabdominal ultrasound and endoscopic ultrasound for diagnosis of gallbladder polyps","Cochrane Database of Systematic Reviews","2018","Abstract - Background Approximately 0.6% to 4% of cholecystectomies are performed because of gallbladder polyps. The decision to perform cholecystectomy is based on presence of gallbladder polyp(s) on transabdominal ultrasound (TAUS) or endoscopic ultrasound (EUS), or both. These polyps are currently considered for surgery if they grow more than 1 cm. However, non‐neoplastic polyps (pseudo polyps) do not need surgery, even when they are larger than 1 cm. True polyps are neoplastic, either benign (adenomas) or (pre)malignant (dysplastic polyps/carcinomas). True polyps need surgery, especially if they are premalignant or malignant. There has been no systematic review and meta‐analysis on the accuracy of TAUS and EUS in the diagnosis of gallbladder polyps, true gallbladder polyps, and (pre)malignant polyps. Objectives To summarise and compare the accuracy of transabdominal ultrasound (TAUS) and endoscopic ultrasound (EUS) for the detection of gallbladder polyps, for differentiating between true and pseudo gallbladder polyps, and for differentiating between dysplastic polyps/carcinomas and adenomas/pseudo polyps of the gallbladder in adults. Search methods We searched the Cochrane Library, MEDLINE, Embase, Science Citation Index Expanded, and trial registrations (last date of search 09 July 2018). We had no restrictions regarding language, publication status, or prospective or retrospective nature of the studies. Selection criteria Studies reporting on the diagnostic accuracy data (true positive, false positive, false negative and true negative) of the index test (TAUS or EUS or both) for detection of gallbladder polyps, differentiation between true and pseudo polyps, or differentiation between dysplastic polyps/carcinomas and adenomas/pseudo polyps. We only accepted histopathology after cholecystectomy as the reference standard, except for studies on diagnosis of gallbladder polyp. For the latter studies, we also accepted repeated imaging up to six months by TAUS or EUS as the reference standard. Data collection and analysis Two authors independently screened abstracts, selected studies for inclusion, and collected data from each study. The quality of the studies was evaluated using the QUADAS‐2 tool. The bivariate random‐effects model was used to obtain summary estimates of sensitivity and specificity, to compare diagnostic performance of the index tests, and to assess heterogeneity. Main results A total of 16 studies were included. All studies reported on TAUS and EUS as separate tests and not as a combination of tests. All studies were at high or unclear risk of bias, ten studies had high applicability concerns in participant selection (because of inappropriate participant exclusions) or reference standards (because of lack of follow‐up for non‐operated polyps), and three studies had unclear applicability concerns in participant selection (because of high prevalence of gallbladder polyps) or index tests (because of lack of details on ultrasound equipment and performance). A meta‐analysis directly comparing results of TAUS and EUS in the same population could not be performed because only limited studies executed both tests in the same participants. Therefore, the results below were obtained only from indirect test comparisons. There was significant heterogeneity amongst all comparisons (target conditions) on TAUS and amongst studies on EUS for differentiating true and pseudo polyps. Detection of gallbladder polyps:  Six studies (16,260 participants) used TAUS. We found no studies on EUS. The summary sensitivity and specificity of TAUS for the detection of gallbladder polyps was 0.84 (95% CI 0.59 to 0.95) and 0.96 (95% CI 0.92 to 0.98), respectively. In a cohort of 1000 people, with a 6.4% prevalence of gallbladder polyps, this would result in 37 overdiagnosed and seven missed gallbladder polyps. Differentiation between true polyp and pseudo gallbladder polyp:  Six studies (1078 participants) used TAUS; the summary sensitivity was 0.68 (95% CI 0.44 to 0.85) and the summary specificity was 0.79 (95% CI 0.57 to 0.91). Three studies (209 participants) used EUS; the summary sensitivity was 0.85 (95% CI 0.46 to 0.97) and the summary specificity was 0.90 (95% CI 0.78 to 0.96). In a cohort of 1000 participants with gallbladder polyps, with 10% having true polyps, this would result in 189 overdiagnosed and 32 missed true polyps by TAUS, and 90 overdiagnosed and 15 missed true polyps by EUS. There was no evidence of a difference between the diagnostic accuracy of TAUS and EUS (relative sensitivity 1.06, P = 0.70, relative specificity 1.15, P = 0.12). Differentiation between dysplastic polyps/carcinomas and adenomas/pseudo polyps of the gallbladder:  Four studies (1,009 participants) used TAUS; the summary sensitivity was 0.79 (95% CI 0.62 to 0.90) and the summary specificity was 0.89 (95% CI 0.68 to 0.97). Three studies (351 participants) used EUS; the summary sensitivity was 0.86 (95% CI 0.76 to 0.92) and the summary specificity was 0.92 (95% CI 0.85 to 0.95). In a cohort of 1000 participants with gallbladder polyps, with 5% having a dysplastic polyp/carcinoma, this would result in 105 overdiagnosed and 11 missed dysplastic polyps/carcinomas by TAUS and 76 overdiagnosed and seven missed dysplastic polyps/carcinomas by EUS. There was no evidence of a difference between the diagnostic accuracy of TAUS and EUS (log likelihood test P = 0.74). Authors' conclusions Although TAUS seems quite good at discriminating between gallbladder polyps and no polyps, it is less accurate in detecting whether the polyp is a true or pseudo polyp and dysplastic polyp/carcinoma or adenoma/pseudo polyp. In practice, this would lead to both unnecessary surgeries for pseudo polyps and missed cases of true polyps, dysplastic polyps, and carcinomas. There was insufficient evidence that EUS is better compared to TAUS in differentiating between true and pseudo polyps and between dysplastic polyps/carcinomas and adenomas/pseudo polyps. The conclusions are based on heterogeneous studies with unclear criteria for diagnosis of the target conditions and studies at high or unclear risk of bias. Therefore, results should be interpreted with caution. Further studies of high methodological quality, with clearly stated criteria for diagnosis of gallbladder polyps, true polyps, and dysplastic polyps/carcinomas are needed to accurately determine diagnostic accuracy of EUS and TAUS. Plain language summary Transabdominal ultrasound and endoscopic ultrasound for detection of gallbladder polyps and differentiating between polyp types Background   The gallbladder is an organ situated close to the liver. It stores bile, produced by the liver, before it is released to the small bowel for digestion. Abnormal growths inside the gallbladder, called 'gallbladder polyps', can develop. Most polyps (90%) are harmless; these are called pseudo polyps. The remaining are true polyps and can be cancerous, have cancer‐like parts (precancerous dysplastic polyps), or be benign, but they can potentially turn into cancer. Dysplastic polyps and cancerous polyps should be treated. Most people also treat benign polyps because of their potential to become cancerous. Treatment is done by removal of the gallbladder with the polyp within (cholecystectomy). To decide which patients should undergo surgery, it is important to (1) be certain that a gallbladder polyp is present, (2) know whether it is a true or pseudo polyp, and (3) whether a polyp is (pre)cancerous. Transabdominal ultrasound (TAUS), which uses ultrasound waves to differentiate between tissues, and endoscopic ultrasound (EUS), ultrasound attached to an endoscope introduced into the small intestine through the mouth and stomach, are the two tests currently used to detect gallbladder polyps and identify the type of gallbladder polyps. We performed a thorough search for studies that reported the accuracy (ability) of TAUS and EUS for the detection of gallbladder polyps and for differentiating between true and pseudo polyps, and between (pre)cancerous and benign polyps. Study characteristics   A total of 16 studies were included. All studies reported on TAUS and EUS as separate tests and did not use a combination of TAUS and EUS. Six studies (16,260 participants) used TAUS for diagnosis of gallbladder polyps. No studies on the diagnosis of gallbladder polyps by EUS were found. Six studies (1,078 participants) used TAUS and three studies (209 participants) used EUS for differentiating between true and pseudo polyps. Four studies (1,009 participants) used TAUS and three studies (351 participants) used EUS for differentiating between (pre)cancerous and benign polyps. Key results   In a general population of 1000 people (in which 6.4% have a gallbladder polyp), TAUS will overdiagnose 37 people without a polyp as having a polyp, and in 7 people with a polyp, the polyp will be missed. In a population of 1000 people with a gallbladder polyp, of which 10% have a true polyp, 189 people with a pseudo polyp will be indicated as having a true polyp by TAUS, and 90 people by EUS. These people may be treated, which is not necessary. In 32 people, the true polyp will be misclassified as a pseudo polyp by TAUS and in 15 people by EUS. These people would not be treated, while they may need treatment. In a population of 1000 people with a gallbladder polyp, of which 5% have a (pre)cancerous polyp, 105 people with a benign polyp will be indicated as having a (pre)cancerous polyp by TAUS, and 75 people by EUS. These people may be overtreated for a (precursor of) cancer, which is not there. In 11 people, the (pre)cancerous polyp will be misclassified as a benign polyp by TAUS, and in 7 people by EUS. These participants may not receive proper treatment for their (precursor of) cancer. TAUS will correctly diagnose 956 out of 1000 people regarding the presence or absence of gallbladder polyps. For differentiating between polyp types, fewer people will be correctly diagnosed by TAUS, leading to unnecessary treatment for pseudo polyps and neglect of (pre)cancerous polyps. There was insufficient evidence that EUS is better than TAUS in differentiating between true and pseudo polyps and between (pre)cancerous and benign polyps. Quality of evidence   All studies were either at high or unclear risk of bias and 13 studies had either high or unclear applicability concerns. This may undermine the validity of the studies. Future research   Further studies of high methodological quality and with clearly reported criteria for diagnosis of gallbladder polyps, true polyps, and (pre)cancerous polyps are necessary.","8","John Wiley & Sons, Ltd","1465-1858","Adenoma [diagnostic imaging]; Adult; Carcinoma [diagnostic imaging]; Diagnosis, Differential; Endosonography; Gallbladder Diseases [*diagnostic imaging, pathology]; Gallbladder Neoplasms [diagnostic imaging]; Humans; Polyps [*diagnostic imaging, pathology]; Sensitivity and Specificity; Ultrasonography [*methods]","10.1002/14651858.CD012233.pub2","http://dx.doi.org/10.1002/14651858.CD012233.pub2","Hepato-Biliary"
"CD009044.PUB2","Randall, M; Egberts, KJ; Samtani, A; Scholten, RJPM; Hooft, L; Livingstone, N; Sterling‐Levis, K; Woolfenden, S; Williams, K","Diagnostic tests for autism spectrum disorder (ASD) in preschool children","Cochrane Database of Systematic Reviews","2018","Abstract - Background Autism spectrum disorder (ASD) is a behaviourally diagnosed condition. It is defined by impairments in social communication or the presence of restricted or repetitive behaviours, or both. Diagnosis is made according to existing classification systems. In recent years, especially following publication of the  Diagnostic and Statistical Manual of Mental Disorders ‐ Fifth Edition  (DSM‐5; APA 2013), children are given the diagnosis of ASD, rather than subclassifications of the spectrum such as autistic disorder, Asperger syndrome, or pervasive developmental disorder ‐ not otherwise specified. Tests to diagnose ASD have been developed using parent or carer interview, child observation, or a combination of both. Objectives Primary objectives 1. To identify which diagnostic tools, including updated versions, most accurately diagnose ASD in preschool children when compared with multi‐disciplinary team clinical judgement. 2. To identify how the best of the interview tools compare with CARS, then how CARS compares with ADOS. a. Which ASD diagnostic tool ‐ among ADOS, ADI‐R, CARS, DISCO, GARS, and 3di ‐ has the best diagnostic test accuracy? b. Is the diagnostic test accuracy of any one test sufficient for that test to be suitable as a sole assessment tool for preschool children? c. Is there any combination of tests that, if offered in sequence, would provide suitable diagnostic test accuracy and enhance test efficiency? d. If data are available, does the combination of an interview tool with a structured observation test have better diagnostic test accuracy (i.e. fewer false‐positives and fewer false‐negatives) than either test alone? As only one interview tool was identified, we modified the first three aims to a single aim (Differences between protocol and review): This Review evaluated diagnostic tests in terms of sensitivity and specificity. Specificity is the most important factor for diagnosis; however, both sensitivity and specificity are of interest in this Review because there is an inherent trade‐off between these two factors. Secondary objectives 1. To determine whether any diagnostic test has greater diagnostic test accuracy for age‐specific subgroups within the preschool age range. Search methods In July 2016, we searched CENTRAL, MEDLINE, Embase, PsycINFO, 10 other databases, and the reference lists of all included publications. Selection criteria Publications had to:   1. report diagnostic test accuracy for any of the following six included diagnostic tools: Autism Diagnostic Interview ‐ Revised (ADI‐R), Gilliam Autism Rating Scale (GARS), Diagnostic Interview for Social and Communication Disorder (DISCO), Developmental, Dimensional, and Diagnostic Interview (3di), Autism Diagnostic Observation Schedule ‐ Generic (ADOS), and Childhood Autism Rating Scale (CARS);   2. include children of preschool age (under six years of age) suspected of having an ASD; and   3. have a multi‐disciplinary assessment, or similar, as the reference standard. Eligible studies included cohort, cross‐sectional, randomised test accuracy, and case‐control studies. The target condition was ASD. Data collection and analysis Two review authors independently assessed all studies for inclusion and extracted data using standardised forms. A third review author settled disagreements. We assessed methodological quality using the QUADAS‐2 instrument (Quality Assessment of Studies of Diagnostic Accuracy ‐ Revised). We conducted separate univariate random‐effects logistical regressions for sensitivity and specificity for CARS and ADI‐R. We conducted meta‐analyses of pairs of sensitivity and specificity using bivariate random‐effects methods for ADOS. Main results In this Review, we included 21 sets of analyses reporting different tools or cohorts of children from 13 publications, many with high risk of bias or potential conflicts of interest or a combination of both. Overall, the prevalence of ASD for children in the included analyses was 74%. For versions and modules of ADOS, there were 12 analyses with 1625 children. Sensitivity of ADOS ranged from 0.76 to 0.98, and specificity ranged from 0.20 to 1.00. The summary sensitivity was 0.94 (95% confidence interval (CI) 0.89 to 0.97), and the summary specificity was 0.80 (95% CI 0.68 to 0.88). For CARS, there were four analyses with 641 children. Sensitivity of CARS ranged from 0.66 to 0.89, and specificity ranged from 0.21 to 1.00. The summary sensitivity for CARS was 0.80 (95% CI 0.61 to 0.91), and the summary specificity was 0.88 (95% CI 0.64 to 0.96). For ADI‐R, there were five analyses with 634 children. Sensitivity for ADI‐R ranged from 0.19 to 0.75, and specificity ranged from 0.63 to 1.00. The summary sensitivity for the ADI‐R was 0.52 (95% CI 0.32 to 0.71), and the summary specificity was 0.84 (95% CI 0.61 to 0.95). Studies that compared tests were few and too small to allow clear conclusions. In two studies that included analyses for both ADI‐R and ADOS, tests scored similarly for sensitivity, but ADOS scored higher for specificity. In two studies that included analyses for ADI‐R, ADOS, and CARS, ADOS had the highest sensitivity and CARS the highest specificity. In one study that explored individual and additive sensitivity and specificity of ADOS and ADI‐R, combining the two tests did not increase the sensitivity nor the specificity of ADOS used alone. Performance for all tests was lower when we excluded studies at high risk of bias. Authors' conclusions We observed substantial variation in sensitivity and specificity of all tests, which was likely attributable to methodological differences and variations in the clinical characteristics of populations recruited. When we compared summary statistics for ADOS, CARS, and ADI‐R, we found that ADOS was most sensitive. All tools performed similarly for specificity. In lower prevalence populations, the risk of falsely identifying children who do not have ASD would be higher. Now available are new versions of tools that require diagnostic test accuracy assessment, ideally in clinically relevant situations, with methods at low risk of bias and in children of varying abilities. Plain language summary How accurate are diagnostic tools for autism spectrum disorder in preschool children? Review question How accurate are tools for diagnosing autism spectrum disorder (ASD) in preschool children? Why is accurate ASD diagnosis important? Not diagnosing ASD in children when it is present (false‐negative result) means children with ASD may miss receiving early intervention and families may miss receiving timely support and education. An incorrect diagnosis of ASD (false‐positive result) may cause family stress, lead to unnecessary investigations and treatments, and place greater strain on already limited service resources. What is the aim of this Review? To find out which of the commonly used tools is most accurate for diagnosing ASD in preschool children. Cochrane researchers reviewed 13 published articles to answer this question. What was studied in the Review? Six tests were reviewed: Four gathered information about children’s behaviours from interviews with parents or carers (Autism Diagnostic Interview‐Revised (ADI‐R), Gilliam Autism Rating Scale (GARS), Diagnostic Interview for Social and Communication Disorder (DISCO), and Developmental, Dimensional, and Diagnostic Interview (3di)); one required that a trained professional observe a child’s behaviour on specific tasks (Autism Diagnostic Observation Schedule (ADOS)); and one combined observation of the child with interview of parents or carers (Childhood Autism Rating Scale (CARS)). What are the main results of the Review? The Review included 21 relevant sets of analyses conducted on a total of 2900 children. Results were available for only three tools: ADOS (Modules 1 and 2), CARS, and ADI‐R. If instruments were applied to 1000 children, 740 of whom had ASD, then 696, 592, and 385 children would be correctly identified by ADOS, CARS, and ADI‐R, respectively, whereas 52, 31, and 42 children without ASD would be incorrectly classified as having ASD. Of 260 children without ASD, 208, 229, and 218 would be correctly classified by ADOS, CARS, and ADI‐R, respectively, whereas 44, 148, and 355 children with ASD would be incorrectly classified as not having ASD. See  Figure 1 . One publication looked at using ADI‐R together with ADOS and found that use of both tools together was no more accurate than use of ADOS alone. How reliable are the results of analyses in this Review? Using a variety of best‐estimate clinical approaches led to diagnosis in children. This method is commonly used in research but does not always replicate the multi‐disciplinary assessment recommended for clinical diagnosis. Problems with how some studies were conducted and the presence of conflicts of interest in some publications may result in ADOS, CARS, and ADI‐R appearing more accurate than they really are. Also, if these tools are used in populations with a lower prevalence of ASD, a higher proportion of children who do not have ASD are likely to receive an ASD diagnosis. The numbers shown above represent average values across analyses. However, as individual estimates varied, we cannot be sure that ADOS will always produce these results. Numbers of children included in studies conducted to date, including studies comparing the accuracy of different tools, are insufficient to evoke confidence in these results. Who do results of the Review apply to? Studies included were carried out in Australia, Canada, India, the Netherlands, United Kingdom, and United States. Studies included children younger than six years of age, or children with a mean age less than six years, with language difficulties, developmental delay, intellectual disability, or a mental health problem, presenting to a clinical service or enrolling in a research study. What are the implications of this Review? Current findings suggest that ADOS is best for not missing children who have ASD and is similar to CARS and ADI‐R in not falsely diagnosing ASD in a child who does not have ASD. ADOS has acceptable accuracy in populations with a high prevalence of ASD. However, overdiagnosis is likely if the tool is used in populations with a lower prevalence of ASD. This finding supports current recommended practice for ASD diagnostic tools to be used as part of a multi‐disciplinary assessment, rather than as stand‐alone diagnostic instruments. How up‐to‐date is this Review? This Review was up‐to‐date as of July 2016.","7","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.CD009044.pub2","http://dx.doi.org/10.1002/14651858.CD009044.pub2","Developmental, Psychosocial and Learning Problems"
"CD008874.PUB2","Roth, D; Pace, NL; Lee, A; Hovhannisyan, K; Warenits, AM; Arrich, J; Herkner, H","Airway physical examination tests for detection of difficult airway management in apparently normal adult patients","Cochrane Database of Systematic Reviews","2018","Abstract - Background The unanticipated difficult airway is a potentially life‐threatening event during anaesthesia or acute conditions. An unsuccessfully managed upper airway is associated with serious morbidity and mortality. Several bedside screening tests are used in clinical practice to identify those at high risk of difficult airway. Their accuracy and benefit however, remains unclear. Objectives The objective of this review was to characterize and compare the diagnostic accuracy of the Mallampati classification and other commonly used airway examination tests for assessing the physical status of the airway in adult patients with no apparent anatomical airway abnormalities. We performed this individually for each of the four descriptors of the difficult airway: difficult face mask ventilation, difficult laryngoscopy, difficult tracheal intubation, and failed intubation. Search methods We searched major electronic databases including CENTRAL, MEDLINE, Embase, ISI Web of Science, CINAHL, as well as regional, subject specific, and dissertation and theses databases from inception to 16 December 2016, without language restrictions. In addition, we searched the Science Citation Index and checked the references of all the relevant studies. We also handsearched selected journals, conference proceedings, and relevant guidelines. We updated this search in March 2018, but we have not yet incorporated these results. Selection criteria We considered full‐text diagnostic test accuracy studies of any individual index test, or a combination of tests, against a reference standard. Participants were adults without obvious airway abnormalities, who were having laryngoscopy performed with a standard laryngoscope and the trachea intubated with a standard tracheal tube. Index tests included the Mallampati test, modified Mallampati test, Wilson risk score, thyromental distance, sternomental distance, mouth opening test, upper lip bite test, or any combination of these. The target condition was difficult airway, with one of the following reference standards: difficult face mask ventilation, difficult laryngoscopy, difficult tracheal intubation, and failed intubation. Data collection and analysis We performed screening and selection of the studies, data extraction and assessment of methodological quality (using QUADAS‐2) independently and in duplicate. We designed a Microsoft Access database for data collection and used Review Manager 5 and R for data analysis. For each index test and each reference standard, we assessed sensitivity and specificity. We produced forest plots and summary receiver operating characteristic (ROC) plots to summarize the data. Where possible, we performed meta‐analyses to calculate pooled estimates and compare test accuracy indirectly using bivariate models. We investigated heterogeneity and performed sensitivity analyses. Main results We included 133 (127 cohort type and 6 case‐control) studies involving 844,206 participants. We evaluated a total of seven different prespecified index tests in the 133 studies, as well as 69 non‐prespecified, and 32 combinations. For the prespecified index tests, we found six studies for the Mallampati test, 105 for the modified Mallampati test, six for the Wilson risk score, 52 for thyromental distance, 18 for sternomental distance, 34 for the mouth opening test, and 30 for the upper lip bite test. Difficult face mask ventilation was the reference standard in seven studies, difficult laryngoscopy in 92 studies, difficult tracheal intubation in 50 studies, and failed intubation in two studies. Across all studies, we judged the risk of bias to be variable for the different domains; we mostly observed low risk of bias for patient selection, flow and timing, and unclear risk of bias for reference standard and index test. Applicability concerns were generally low for all domains. For difficult laryngoscopy, the summary sensitivity ranged from 0.22 (95% confidence interval (CI) 0.13 to 0.33; mouth opening test) to 0.67 (95% CI 0.45 to 0.83; upper lip bite test) and the summary specificity ranged from 0.80 (95% CI 0.74 to 0.85; modified Mallampati test) to 0.95 (95% CI 0.88 to 0.98; Wilson risk score). The upper lip bite test for diagnosing difficult laryngoscopy provided the highest sensitivity compared to the other tests (P < 0.001). For difficult tracheal intubation, summary sensitivity ranged from 0.24 (95% CI 0.12 to 0.43; thyromental distance) to 0.51 (95% CI 0.40 to 0.61; modified Mallampati test) and the summary specificity ranged from 0.87 (95% CI 0.82 to 0.91; modified Mallampati test) to 0.93 (0.87 to 0.96; mouth opening test). The modified Mallampati test had the highest sensitivity for diagnosing difficult tracheal intubation compared to the other tests (P < 0.001). For difficult face mask ventilation, we could only estimate summary sensitivity (0.17, 95% CI 0.06 to 0.39) and specificity (0.90, 95% CI 0.81 to 0.95) for the modified Mallampati test. Authors' conclusions Bedside airway examination tests, for assessing the physical status of the airway in adults with no apparent anatomical airway abnormalities, are designed as screening tests. Screening tests are expected to have high sensitivities. We found that all investigated index tests had relatively low sensitivities with high variability. In contrast, specificities were consistently and markedly higher than sensitivities across all tests. The standard bedside airway examination tests should be interpreted with caution, as they do not appear to be good screening tests. Among the tests we examined, the upper lip bite test showed the most favourable diagnostic test accuracy properties. Given the paucity of available data, future research is needed to develop tests with high sensitivities to make them useful, and to consider their use for screening difficult face mask ventilation and failed intubation. The 27 studies in 'Studies awaiting classification' may alter the conclusions of the review, once we have assessed them. Plain language summary Bedside examination tests to detect beforehand adults who are likely to be difficult to intubate Review question We looked for the most suitable and accurate rapid screening test in adults with no obvious airway abnormalities, to identify those who are likely to be difficult to intubate (i.e. insertion of a tube into the windpipe). Background Intubation ensures a patient’s airway is clear while they are heavily sedated, unconscious or anaesthetized, so their breathing can be controlled by machine (ventilation), and appropriate levels of oxygen can be given during surgery, following major trauma, during critical illness, or following cardiac arrest. Having an airway that is difficult to intubate is a potentially life‐threatening situation. Tube insertion is preceded by laryngoscopy (insertion of mini‐camera to view route of tube insertion), requires advanced skills, and is generally uneventful. Intubation is difficult in approximately 10% of patients, who require special equipment and precautions. Several physical features are associated with difficult airways and failed intubation, so warning of potentially difficult airways would be helpful. Several quick bedside tests are in routine clinical use to identify those at high risk for difficult airways, but how accurate these are remains unclear. Population We included studies of adults aged 16 years or older without obvious airway abnormalities who were to receive standard intubation. Test under investigation We assessed the seven most common bedside tests, routinely used to detect difficult airways. These take only a few seconds to complete and require no special equipment. The index tests (diagnostic tests of interest) included: ‐ the Mallampati test (original or modified; asking a sitting patient to open his mouth and to protrude the tongue as much as possible so that visibility can be determined); ‐ Wilson risk score (including patient's weight, head and neck movement, jaw movement, receding chin, buck teeth); ‐ thyromental distance (length between the chin and the upper edge of Adam's apple); ‐ sternomental distance (length between the chin and the notch between the collar bones); ‐ mouth opening test; ‐ upper lip bite test; ‐ or any combination of these tests. Search date The evidence is current to 16 December 2016. (We searched for new studies in March 2018, but we have not yet included them in the review.) Study characteristics We included 133 studies (844,206 participants) which investigated the accuracy of the seven tests above, plus 69 other common tests and 32 test combinations, in detection of difficult airways. Key results For difficult laryngoscopy, the average sensitivity (percentage of correctly identified difficult airways) ranged from 22% (mouth opening test) to 63% (upper lip bite test). The average specificity (percentage of correctly classified patients without difficult airways) ranged from 80% (modified Mallampati test) to 95% (Wilson risk score). The upper lip bite test had the highest sensitivity of all tests considered. For difficult tube insertion, the average sensitivity ranged from 24% (thyromental distance) to 51% (modified Mallampati test) and the average specificity ranged from 87% (modified Mallampati test) to 93% (mouth opening test). The modified Mallampati test had the highest sensitivity of all tests considered. For difficult face mask ventilation (another indication of a difficult airway), there were only enough data to calculate average sensitivity of 17% and specificity 90% for the modified Mallampati test. Quality of the evidence Overall, the evidence from the studies was of moderate to high quality. The likelihood of the studies providing reliable results was generally high, although in half of them, the intubating physician knew the result of the preceding test, which may have influenced results, but this is the normal situation in routine clinical care. The characteristics of patients, tests, and conditions were comparable to those seen in a wide range of everyday clinical settings. The results of this review should apply to standard preoperative airway assessments in apparently normal hospital patients worldwide. Conclusion The bedside screening tests examined in this review are not well suited for the purpose of detecting unanticipated difficult airways because they missed a large number of people who had a difficult airway.","5","John Wiley & Sons, Ltd","1465-1858","*Intubation, Intratracheal [statistics & numerical data]; *Laryngoscopy [statistics & numerical data]; Adult; Airway Management [statistics & numerical data]; Humans; Physical Examination [*methods]; Point‐of‐Care Systems [statistics & numerical data]; Sensitivity and Specificity; Treatment Failure","10.1002/14651858.CD008874.pub2","http://dx.doi.org/10.1002/14651858.CD008874.pub2","Anaesthesia"
"CD012080.PUB2","Best, LMJ; Takwoingi, Y; Siddique, S; Selladurai, A; Gandhi, A; Low, B; Yaghoobi, M; Gurusamy, KS","Non‐invasive diagnostic tests for   Helicobacter pylori infection","Cochrane Database of Systematic Reviews","2018","Abstract - Background Helicobacter pylori  ( H pylori ) infection has been implicated in a number of malignancies and non‐malignant conditions including peptic ulcers, non‐ulcer dyspepsia, recurrent peptic ulcer bleeding, unexplained iron deficiency anaemia, idiopathic thrombocytopaenia purpura, and colorectal adenomas. The confirmatory diagnosis of  H pylori  is by endoscopic biopsy, followed by histopathological examination using haemotoxylin and eosin (H & E) stain or special stains such as Giemsa stain and Warthin‐Starry stain. Special stains are more accurate than H & E stain. There is significant uncertainty about the diagnostic accuracy of non‐invasive tests for diagnosis of  H pylori . Objectives To compare the diagnostic accuracy of urea breath test, serology, and stool antigen test, used alone or in combination, for diagnosis of  H pylori  infection in symptomatic and asymptomatic people, so that eradication therapy for  H pylori  can be started. Search methods We searched MEDLINE, Embase, the Science Citation Index and the National Institute for Health Research Health Technology Assessment Database on 4 March 2016. We screened references in the included studies to identify additional studies. We also conducted citation searches of relevant studies, most recently on 4 December 2016. We did not restrict studies by language or publication status, or whether data were collected prospectively or retrospectively. Selection criteria We included diagnostic accuracy studies that evaluated at least one of the index tests (urea breath test using isotopes such as  13 C or  14 C, serology and stool antigen test) against the reference standard (histopathological examination using H & E stain, special stains or immunohistochemical stain) in people suspected of having  H pylori  infection. Data collection and analysis Two review authors independently screened the references to identify relevant studies and independently extracted data. We assessed the methodological quality of studies using the QUADAS‐2 tool. We performed meta‐analysis by using the hierarchical summary receiver operating characteristic (HSROC) model to estimate and compare SROC curves. Where appropriate, we used bivariate or univariate logistic regression models to estimate summary sensitivities and specificities. Main results We included 101 studies involving 11,003 participants, of which 5839 participants (53.1%) had  H pylori  infection. The prevalence of  H pylori  infection in the studies ranged from 15.2% to 94.7%, with a median prevalence of 53.7% (interquartile range 42.0% to 66.5%). Most of the studies (57%) included participants with dyspepsia and 53 studies excluded participants who recently had proton pump inhibitors or antibiotics.There was at least an unclear risk of bias or unclear applicability concern for each study. Of the 101 studies, 15 compared the accuracy of two index tests and two studies compared the accuracy of three index tests. Thirty‐four studies (4242 participants) evaluated serology; 29 studies (2988 participants) evaluated stool antigen test; 34 studies (3139 participants) evaluated urea breath test‐ 13 C; 21 studies (1810 participants) evaluated urea breath test‐ 14 C; and two studies (127 participants) evaluated urea breath test but did not report the isotope used. The thresholds used to define test positivity and the staining techniques used for histopathological examination (reference standard) varied between studies. Due to sparse data for each threshold reported, it was not possible to identify the best threshold for each test. Using data from 99 studies in an indirect test comparison, there was statistical evidence of a difference in diagnostic accuracy between urea breath test‐ 13 C, urea breath test‐ 14 C, serology and stool antigen test (P = 0.024). The diagnostic odds ratios for urea breath test‐ 13 C, urea breath test‐ 14 C, serology, and stool antigen test were 153 (95% confidence interval (CI) 73.7 to 316), 105 (95% CI 74.0 to 150), 47.4 (95% CI 25.5 to 88.1) and 45.1 (95% CI 24.2 to 84.1). The sensitivity (95% CI) estimated at a fixed specificity of 0.90 (median from studies across the four tests), was 0.94 (95% CI 0.89 to 0.97) for urea breath test‐ 13 C, 0.92 (95% CI 0.89 to 0.94) for urea breath test‐ 14 C, 0.84 (95% CI 0.74 to 0.91) for serology, and 0.83 (95% CI 0.73 to 0.90) for stool antigen test. This implies that on average, given a specificity of 0.90 and prevalence of 53.7% (median specificity and prevalence in the studies), out of 1000 people tested for  H pylori  infection, there will be 46 false positives (people without  H pylori  infection who will be diagnosed as having  H pylori i nfection). In this hypothetical cohort, urea breath test‐ 13 C, urea breath test‐ 14 C, serology, and stool antigen test will give 30 (95% CI 15 to 58), 42 (95% CI 30 to 58), 86 (95% CI 50 to 140), and 89 (95% CI 52 to 146) false negatives respectively (people with  H pylori  infection for whom the diagnosis of  H pylori  will be missed). Direct comparisons were based on few head‐to‐head studies. The ratios of diagnostic odds ratios (DORs) were 0.68 (95% CI 0.12 to 3.70; P = 0.56) for urea breath test‐ 13 C versus serology (seven studies), and 0.88 (95% CI 0.14 to 5.56; P = 0.84) for urea breath test‐ 13 C versus stool antigen test (seven studies). The 95% CIs of these estimates overlap with those of the ratios of DORs from the indirect comparison. Data were limited or unavailable for meta‐analysis of other direct comparisons. Authors' conclusions In people without a history of gastrectomy and those who have not recently had antibiotics or proton ,pump inhibitors, urea breath tests had high diagnostic accuracy while serology and stool antigen tests were less accurate for diagnosis of  Helicobacter pylori  infection.This is based on an indirect test comparison (with potential for bias due to confounding), as evidence from direct comparisons was limited or unavailable. The thresholds used for these tests were highly variable and we were unable to identify specific thresholds that might be useful in clinical practice. We need further comparative studies of high methodological quality to obtain more reliable evidence of relative accuracy between the tests. Such studies should be conducted prospectively in a representative spectrum of participants and clearly reported to ensure low risk of bias. Most importantly, studies should prespecify and clearly report thresholds used, and should avoid inappropriate exclusions. Plain language summary Accuracy of different non‐invasive methods for identifying  Helicobacter pylori Why is it important to know whether someone has  Helicobacter pylori ? Helicobacter pylori (H pylori)  is a type of bacteria which may be present in the stomach of some people.  H pylori  is believed to cause a number of cancers, including stomach cancer, pancreatic cancer, and throat cancer.  H pylori  is also linked with other diseases including stomach ulcers, heart burn, and a bloated feeling. If  H pylori  is found in an individual, appropriate treatment can be started. What is the aim of this review? To compare the accuracy of three different types of test for  H pylori.  These are: urea breath tests, blood tests (the specific blood test is called serology), and stool tests (in faeces). What was studied in this review? There are two types of urea breath test which use two different forms of carbon known as  13 C and  14 C, as well as multiple versions of serology and stool tests. What are the main results of the review? We found 101 studies which included 11,003 people who were tested for  H pylori . Of these 11,003 participants, 5839 (53.1%) had  H pylori  infection. All the studies used one of the three tests listed above and compared these test results with the diagnosis given by endoscopic biopsy. Endoscopic biopsy involves obtaining tissue from the stomach using a thin flexible tube introduced through the mouth and testing for the presence of  H pylori  under the microscope. It is currently the most accurate available test, however it causes physical discomfort to the patient, with associated risks for harm. This is in contrast to the alternative non‐invasive tests in this review which are significantly less uncomfortable and have minimal or no risk of harm, making them desirable alternatives if they can be shown to be as accurate at diagnosing  H pylori  as endoscopic biopsy. Most of the studies included participants with heart burn or similar problems in the stomach and excluded participants who had previously undergone partial removal of the stomach and those having treatment for  H pylori . Thirty‐four studies (4242 participants) used serology; 29 studies (2988 participants) used stool antigen test; 34 studies (3139 participants) used urea breath test‐ 13 C; 21 studies (1810 participants) used urea breath test‐ 14 C; and two studies (127 participants) used urea breath test but did not report the type of carbon used. Studies varied in the limit they used before saying a test was positive for  H pylori  infection and the type of stains used to examine the biopsy material. When we looked at all the data we found that urea breath tests were more accurate than blood and stool tests. The results mean that, on average, if 1000 people are tested, there will be 46 people without  H pylori  who will be misdiagnosed as having  H pylori . Also, there will be 30, 42, 86, and 89 people with  H pylori  infection for whom the diagnosis of  H pylori  infection will be missed by urea breath test‐ 13 C, urea breath test‐ 14 C, serology, and stool antigen test, respectively. When we looked at the seven studies which compared urea breath test‐ 13 C and serology, or urea breath test‐ 13 C and stool antigen tests in the same participants, the results were uncertain and we cannot tell which test is more accurate. How reliable are the results of the studies? Except for one study, all the studies were of poor methodological quality, which makes their results unreliable. Who do the results of this review apply to? These results apply to children and adults with suspected  H pylori  infection, but only in those who have not previously undergone stomach operations and those who have not recently had antibiotics or treatment for  H pylori  infection. What are the implications of this review? Urea breath tests, blood tests, and stool tests may be suitable for identifying whether someone has  H pylori  infection. However, the level of the result of urea breath test, blood test, or stool test which should be used to make a diagnosis of  H pylori  infection remains unclear. How up‐to‐date is the review? We performed a thorough literature search for studies reporting the accuracy of these different tests until 4 March 2016.","3","John Wiley & Sons, Ltd","1465-1858","*Helicobacter pylori [immunology]; Adult; Antigens, Bacterial [analysis]; Biomarkers [analysis]; Breath Tests [*methods]; Child; Feces [*chemistry]; Helicobacter Infections [blood, *diagnosis, epidemiology]; Humans; Prevalence; Urea [*analysis]","10.1002/14651858.CD012080.pub2","http://dx.doi.org/10.1002/14651858.CD012080.pub2","Gut"
"CD011912.PUB2","Plana, MN; Zamora, J; Suresh, G; Fernandez‐Pineda, L; Thangaratinam, S; Ewer, AK","Pulse oximetry screening for critical congenital heart defects","Cochrane Database of Systematic Reviews","2018","Abstract - Background Health outcomes are improved when newborn babies with critical congenital heart defects (CCHDs) are detected before acute cardiovascular collapse. The main screening tests used to identify these babies include prenatal ultrasonography and postnatal clinical examination; however, even though both of these methods are available, a significant proportion of babies are still missed. Routine pulse oximetry has been reported as an additional screening test that can potentially improve detection of CCHD. Objectives • To determine the diagnostic accuracy of pulse oximetry as a screening method for detection of CCHD in asymptomatic newborn infants • To assess potential sources of heterogeneity, including: ○ characteristics of the population: inclusion or exclusion of antenatally detected congenital heart defects; ○ timing of testing: < 24 hours versus ≥ 24 hours after birth; ○ site of testing: right hand and foot (pre‐ductal and post‐ductal) versus foot only (post‐ductal); ○ oxygen saturation: functional versus fractional; ○ study design: retrospective versus prospective design, consecutive versus non‐consecutive series; and ○ risk of bias for the ""flow and timing"" domain of QUADAS‐2. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL; 2017, Issue 2) in the Cochrane Library and the following databases: MEDLINE, Embase, the Cumulative Index to Nursing and Allied Health Literature (CINAHL), and Health Services Research Projects in Progress (HSRProj), up to March 2017. We searched the reference lists of all included articles and relevant systematic reviews to identify additional studies not found through the electronic search. We applied no language restrictions. Selection criteria We selected studies that met predefined criteria for design, population, tests, and outcomes. We included cross‐sectional and cohort studies assessing the diagnostic accuracy of pulse oximetry screening for diagnosis of CCHD in term and late preterm asymptomatic newborn infants. We considered all protocols of pulse oximetry screening (eg, different saturation thresholds to define abnormality, post‐ductal only or pre‐ductal and post‐ductal measurements, test timing less than or greater than 24 hours). Reference standards were diagnostic echocardiography (echocardiogram) and clinical follow‐up, including postmortem findings, mortality, and congenital anomaly databases. Data collection and analysis We extracted accuracy data for the threshold used in primary studies. We explored between‐study variability and correlation between indices visually through use of forest and receiver operating characteristic (ROC) plots. We assessed risk of bias in included studies using the QUADAS‐2 tool. We used the bivariate model to calculate random‐effects pooled sensitivity and specificity values. We investigated sources of heterogeneity using subgroup analyses and meta‐regression. Main results Twenty‐one studies met our inclusion criteria (N = 457,202 participants). Nineteen studies provided data for the primary analysis (oxygen saturation threshold < 95% or ≤ 95%; N = 436,758 participants). The overall sensitivity of pulse oximetry for detection of CCHD was 76.3% (95% confidence interval [CI] 69.5 to 82.0)  (low certainty of the evidence) . Specificity was 99.9% (95% CI 99.7 to 99.9), with a false‐positive rate of 0.14% (95% CI 0.07 to 0.22)  (high certainty of the evidence) . Summary positive and negative likelihood ratios were 535.6 (95% CI 280.3 to 1023.4) and 0.24 (95% CI 0.18 to 0.31), respectively. These results showed that out of 10,000 apparently healthy late preterm or full‐term newborn infants, six will have CCHD (median prevalence in our review). Screening by pulse oximetry will detect five of these infants as having CCHD and will miss one case. In addition, screening by pulse oximetry will falsely identify another 14 infants out of the 10,000 as having suspected CCHD when they do not have it. The false‐positive rate for detection of CCHD was lower when newborn pulse oximetry was performed longer than 24 hours after birth than when it was performed within 24 hours (0.06%, 95% CI 0.03 to 0.13, vs 0.42%, 95% CI 0.20 to 0.89; P = 0.027). Forest and ROC plots showed greater variability in estimated sensitivity than specificity across studies. We explored heterogeneity by conducting subgroup analyses and meta‐regression of inclusion or exclusion of antenatally detected congenital heart defects, timing of testing, and risk of bias for the ""flow and timing"" domain of QUADAS‐2, and we did not find an explanation for the heterogeneity in sensitivity. Authors' conclusions Pulse oximetry is a highly specific and moderately sensitive test for detection of CCHD with very low false‐positive rates. Current evidence supports the introduction of routine screening for CCHD in asymptomatic newborns before discharge from the well‐baby nursery. Plain language summary Pulse oximetry for diagnosis of critical congenital heart defects Review question We reviewed evidence on the accuracy of pulse oximetry for detection of critical congenital heart defects (CCHDs) in asymptomatic newborn infants. Background CCHDs occur in around two in 1000 newborn infants and are a leading cause of infant death. Timely diagnosis is crucial for best outcomes for these babies, but current screening methods may miss up to 50% of affected newborn infants before birth, and those sent home before diagnosis frequently die or endure major morbidity. However, babies with CCHD often have low blood oxygen levels, which can be detected quickly and non‐invasively by pulse oximetry, using a sensor placed on the newborn infant’s hand or foot. A pulse oximeter is a machine that can measure, non‐invasively, the amount of oxygen carried around the body by red blood cells. Oxygen from the lungs is bound to hemoglobin in red blood cells, forming oxyhemoglobin. If oxygen is not bound, de‐oxyhemoglobin is formed. In health, almost all hemoglobin is oxyhemoglobin, and so oxygen saturation (ie, the percentage of hemoglobin that has bound oxygen) is close to 100%. The pulse oximeter measures this by passing light through peripheral blood vessels (eg, a fingertip in an adult, in a hand or foot in a baby). Oxyhemoglobin and de‐oxyhemoglobin absorb this light in different ways, and the proportion of light absorbed can be analyzed by software within the oximeter, which then calculates the percentage of hemoglobin saturated with oxygen. Study characteristics We searched until March 2017 for evidence on use of pulse oximetry to detect CCHD in newborn infants and found 21 studies. These studies used different thresholds to define a pulse oximetry test as positive. We combined all studies using a threshold around 95% (19 studies with 436,758 newborn infants). Key results This review found that for every 10,000 apparently healthy newborn infants screened, around six of them will have CCHD. The pulse oximetry test will correctly identify five of these newborn infants with CCHD (but will miss one case). Newborn infants who are missed could die or experience major morbidity. For every 10,000 apparently healthy newborn infants screened, 9994 will not have CCHD. The pulse oximetry test will correctly identify 9980 of them (but 14 newborn infants will be investigated for suspected CCHD). Some of these infants may be exposed to unnecessary additional tests and a prolonged hospital stay, but a proportion will have a potentially serious non‐cardiac illness. The number of newborn infants incorrectly investigated for CCHD decreases when pulse oximetry is performed longer than 24 hours after birth. Certainty of evidence We judged the included studies to be mainly at low or unclear risk of bias for several of the certainty domains assessed. Some studies used less robust methods to verify negative results. We considered the overall certainty of the evidence as moderate.","3","John Wiley & Sons, Ltd","1465-1858","*Asymptomatic Diseases; Data Accuracy; False Positive Reactions; Heart Defects, Congenital [*diagnosis]; Humans; Infant, Newborn; Oximetry [*methods]; Sensitivity and Specificity","10.1002/14651858.CD011912.pub2","http://dx.doi.org/10.1002/14651858.CD011912.pub2","Neonatal"
"CD011551.PUB2","Abrigo, JM; Fountain, DM; Provenzale, JM; Law, EK; Kwong, JSW; Hart, MG; Tam, WWS","Magnetic resonance perfusion for differentiating low‐grade from high‐grade gliomas at first presentation","Cochrane Database of Systematic Reviews","2018","Abstract - Background Gliomas are the most common primary brain tumour. They are graded using the WHO classification system, with Grade II‐IV astrocytomas, oligodendrogliomas and oligoastrocytomas. Low‐grade gliomas (LGGs) are WHO Grade II infiltrative brain tumours that typically appear solid and non‐enhancing on magnetic resonance imaging (MRI) scans. People with LGG often have little or no neurologic deficit, so may opt for a watch‐and‐wait‐approach over surgical resection, radiotherapy or both, as surgery can result in early neurologic disability. Occasionally, high‐grade gliomas (HGGs, WHO Grade III and IV) may have the same MRI appearance as LGGs. Taking a watch‐and‐wait approach could be detrimental for the patient if the tumour progresses quickly. Advanced imaging techniques are increasingly used in clinical practice to predict the grade of the tumour and to aid clinical decision of when to intervene surgically. One such advanced imaging technique is magnetic resonance (MR) perfusion, which detects abnormal haemodynamic changes related to increased angiogenesis and vascular permeability, or ""leakiness"" that occur with aggressive tumour histology. These are reflected by changes in cerebral blood volume (CBV) expressed as rCBV (ratio of tumoural CBV to normal appearing white matter CBV) and permeability, measured by  K trans . Objectives To determine the diagnostic test accuracy of MR perfusion for identifying patients with primary solid and non‐enhancing LGGs (WHO Grade II) at first presentation in children and adults. In performing the quantitative analysis for this review, patients with LGGs were considered disease positive while patients with HGGs were considered disease negative. To determine what clinical features and methodological features affect the accuracy of MR perfusion. Search methods Our search strategy used two concepts: (1) glioma and the various histologies of interest, and (2) MR perfusion. We used structured search strategies appropriate for each database searched, which included: MEDLINE (Ovid SP), Embase (Ovid SP), and Web of Science Core Collection (Science Citation Index Expanded and Conference Proceedings Citation Index). The most recent search for this review was run on 9 November 2016. We also identified 'grey literature' from online records of conference proceedings from the American College of Radiology, European Society of Radiology, American Society of Neuroradiology and European Society of Neuroradiology in the last 20 years. Selection criteria The titles and abstracts from the search results were screened to obtain full‐text articles for inclusion or exclusion. We contacted authors to clarify or obtain missing/unpublished data. We included cross‐sectional studies that performed dynamic susceptibility (DSC) or dynamic contrast‐enhanced (DCE) MR perfusion or both of untreated LGGs and HGGs, and where rCBV and/or  K trans  values were reported. We selected participants with solid and non‐enhancing gliomas who underwent MR perfusion within two months prior to histological confirmation. We excluded studies on participants who received radiation or chemotherapy before MR perfusion, or those without histologic confirmation. Data collection and analysis Two review authors extracted information on study characteristics and data, and assessed the methodological quality using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. We present a summary of the study characteristics and QUADAS‐2 results, and rate studies as good quality when they have low risk of bias in the domains of reference standard of tissue diagnosis and flow and timing between MR perfusion and tissue diagnosis. In the quantitative analysis, LGGs were considered disease positive, while HGGs were disease negative. The sensitivity refers to the proportion of LGGs detected by MR perfusion, and specificity as the proportion of detected HGGs. We constructed two‐by‐two tables with true positives and false negatives as the number of correctly and incorrectly diagnosed LGG, respectively, while true negatives and false positives are the number of correctly and incorrectly diagnosed HGG, respectively. Meta‐analysis was performed on studies with two‐by‐two tables, with further sensitivity analysis using good quality studies. Limited data precluded regression analysis to explore heterogeneity but subgroup analysis was performed on tumour histology groups. Main results Seven studies with small sample sizes (4 to 48) met our inclusion criteria. These were mostly conducted in university hospitals and mostly recruited adult patients. All studies performed DSC MR perfusion and described heterogeneous acquisition and post‐processing methods. Only one study performed DCE MR perfusion, precluding quantitative analysis. Using patient‐level data allowed selection of individual participants relevant to the review, with generally low risks of bias for the participant selection, reference standard and flow and timing domains. Most studies did not use a pre‐specified threshold, which was considered a significant source of bias, however this did not affect quantitative analysis as we adopted a common rCBV threshold of 1.75 for the review. Concerns regarding applicability were low. From published and unpublished data, 115 participants were selected and included in the meta‐analysis. Average rCBV (range) of 83 LGGs and 32 HGGs were 1.29 (0.01 to 5.10) and 1.89 (0.30 to 6.51), respectively. Using the widely accepted rCBV threshold of  < 1.75 to differentiate LGG from HGG, the summary sensitivity/specificity estimates were 0.83 (95% CI 0.66 to 0.93)/0.48 (95% CI 0.09 to 0.90). Sensitivity analysis using five good quality studies yielded sensitivity/specificity of 0.80 (95% CI 0.61 to 0.91)/0.67 (95% CI 0.07 to 0.98). Subgroup analysis for tumour histology showed sensitivity/specificity of 0.92 (95% CI 0.55 to 0.99)/0.42 (95% CI 0.02 to 0.95) in astrocytomas (6 studies, 55 participants) and 0.77 (95% CI 0.46 to 0.93)/0.53 (95% CI 0.14 to 0.88) in oligodendrogliomas+oligoastrocytomas (6 studies, 56 participants). Data were too sparse to investigate any differences across subgroups. Authors' conclusions The limited available evidence precludes reliable estimation of the performance of DSC MR perfusion‐derived rCBV for the identification of grade in untreated solid and non‐enhancing LGG from that of HGG. Pooled data yielded a wide range of estimates for both sensitivity (range 66% to 93% for detection of LGGs) and specificity (range 9% to 90% for detection of HGGs). Other clinical and methodological features affecting accuracy of the technique could not be determined from the limited data. A larger sample size of both LGG and HGG, preferably using a standardised scanning approach and with an updated reference standard incorporating molecular profiles, is required for a definite conclusion. Plain language summary How accurate is MR perfusion, an advanced MRI method, for differentiating low‐grade gliomas from high‐grade gliomas in children and adults? Why is differentiation of low‐grade and high‐grade gliomas important? Low‐grade gliomas (LGGs) are slow growing brain tumours that have a typical appearance on standard MRI. Patients with LGGs who have few or no symptoms may prefer to delay treatment until such time they experience progression of their symptoms or appearance of the tumour on MRI; this is called the watch‐and‐wait approach. However occasionally, high‐grade gliomas (HGGs), which are aggressive and require early treatment, can mimic the appearance of LGGs. It is only by examining tissues obtained by surgery ‐ either through sampling (biopsy) or removal of tumour (resection) ‐ can LGG and HGG be definitively differentiated. But a patient with few or no symptoms may want to avoid risking early neurologic disability resulting from surgery. Thus an accurate noninvasive method to differentiate gliomas can aid patients' decision making whether to opt for a watch‐and‐wait approach or undergo early treatment. What is the aim of this review? The review aims to determine how accurate MR perfusion is for differentiating LGGs and HGGs, and what factors affect its accuracy. Researchers in Cochrane included seven studies to answer this question. What was studied in this review? An advanced MRI technique called MR perfusion was studied. This method detects abnormal blood vessels which are increased from low‐ to high‐grade gliomas. Unlike surgery, MR perfusion is noninvasive and allows clinicians to determine if a watch‐and‐wait approach can be adopted by patients, i.e. delay treatment including the initial tissue examination which requires surgery. What are the main results of the review? The analysis included results from 115 patients. The results indicate that in theory, if MR perfusion were to be used in 100 patients with brain tumours that look like LGG on standard MRI scan, of whom 72 actually have LGG, then:  ‐ an estimated 74 will have an MR perfusion result indicating that they have LGGs, and of these 15 will have HGGs;  ‐ an estimated 26 will have an MR perfusion result indicating that they have HGGs, and of these 13 will have LGGs. How reliable are the results of the studies in this review? In the included studies, the diagnosis of LGG or HGG was made by assessing all patients with tissue examination, and a majority underwent resection. This is considered a reliable method for deciding whether patients actually had LGGs or HGGs. The small number of patients that were included in this review is a major limitation to the analysis. Estimates from individual studies and pooled data were variable and/or had a wide range. The numbers reported in the main results above are an average across studies in the review, but it is unknown if MR perfusion will always produce these results. Further, the included studies differed in how MR perfusion was performed, and pooling of data for the analysis may be inappropriate. Who do the results of this review apply to? The included studies were carried out in Europe (Italy, Sweden, Spain, France), Asia (Japan) and South America (Brazil) and MR perfusion was mostly performed in university hospitals. Most studies recruited adults so the results may not be representative of children. What are the implications of this review? Our results based on 115 patients showed that MR perfusion may detect 66% to 93% of LGGs, which means that 7% to 34% of people with LGGs may be misclassified as having HGGs and thus may undergo early invasive treatment with an accompanying risk of adverse events. Meanwhile, around half of people with HGGs may be misclassified as having LGGs, and thus may suffer from delayed treatment. Due to uncertainty in the estimates this may range from 9% to 90% of patients. Given the wide range of estimates, currently, it cannot be determined how accurate MR perfusion is for differentiating LGGs and HGGs. Future studies to inform evidence would need to include larger numbers of patients with LGG and HGG. How up to date is this review? We searched for and used studies published from 1990 to November 2016.","1","John Wiley & Sons, Ltd","1465-1858","*Magnetic Resonance Imaging; Adult; Astrocytoma [diagnostic imaging]; Brain Neoplasms [*diagnostic imaging]; Child; Cross‐Sectional Studies; Glioma [*diagnostic imaging]; Humans; Oligodendroglioma [diagnostic imaging]; Sensitivity and Specificity","10.1002/14651858.CD011551.pub2","http://dx.doi.org/10.1002/14651858.CD011551.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD012216.PUB2","Martínez, G; Vernooij, RWM; Fuentes Padilla, P; Zamora, J; Bonfill Cosp, X; Flicker, L","18F PET with florbetapir for the early diagnosis of Alzheimer’s disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2017","Abstract - Background  18 F‐florbetapir uptake by brain tissue measured by positron emission tomography (PET) is accepted by regulatory agencies like the Food and Drug Administration (FDA) and the European Medicine Agencies (EMA) for assessing amyloid load in people with dementia. Its added value is mainly demonstrated by excluding Alzheimer's pathology in an established dementia diagnosis. However, the National Institute on Aging and Alzheimer's Association (NIA‐AA) revised the diagnostic criteria for Alzheimer's disease and confidence in the diagnosis of mild cognitive impairment (MCI) due to Alzheimer's disease may be increased when using amyloid biomarkers tests like  18 F‐florbetapir. These tests, added to the MCI core clinical criteria, might increase the diagnostic test accuracy (DTA) of a testing strategy. However, the DTA of  18 F‐florbetapir to predict the progression from MCI to Alzheimer’s disease dementia (ADD) or other dementias has not yet been systematically evaluated. Objectives To determine the DTA of the  18 F‐florbetapir PET scan for detecting people with MCI at time of performing the test who will clinically progress to ADD, other forms of dementia (non‐ADD), or any form of dementia at follow‐up. Search methods This review is current to May 2017. We searched MEDLINE (OvidSP), Embase (OvidSP), PsycINFO (OvidSP), BIOSIS Citation Index (Thomson Reuters Web of Science), Web of Science Core Collection, including the Science Citation Index (Thomson Reuters Web of Science) and the Conference Proceedings Citation Index (Thomson Reuters Web of Science), LILACS (BIREME), CINAHL (EBSCOhost), ClinicalTrials.gov ( https://clinicaltrials.gov ), and the World Health Organization International Clinical Trials Registry Platform (WHO ICTRP) ( http://www.who.int/ictrp/search/en/ ). We also searched ALOIS, the Cochrane Dementia & Cognitive Improvement Group’s specialised register of dementia studies ( http://www.medicine.ox.ac.uk/alois/ ). We checked the reference lists of any relevant studies and systematic reviews, and performed citation tracking using the Science Citation Index to identify any additional relevant studies. No language or date restrictions were applied to the electronic searches. Selection criteria We included studies that had prospectively defined cohorts with any accepted definition of MCI at time of performing the test and the use of  18 F‐florbetapir scan to evaluate the DTA of the progression from MCI to ADD or other forms of dementia. In addition, we only selected studies that applied a reference standard for Alzheimer’s dementia diagnosis, for example, National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer’s Disease and Related Disorders Association (NINCDS‐ADRDA) or Diagnostic and Statistical Manual of Mental Disorders‐IV (DSM‐IV) criteria. Data collection and analysis We screened all titles and abstracts identified in electronic‐database searches. Two review authors independently selected studies for inclusion and extracted data to create two‐by‐two tables, showing the binary test results cross‐classified with the binary reference standard. We used these data to calculate sensitivities, specificities, and their 95% confidence intervals. Two independent assessors performed quality assessment using the QUADAS‐2 tool plus some additional items to assess the methodological quality of the included studies. Main results We included three studies, two of which evaluated the progression from MCI to ADD, and one evaluated the progression from MCI to any form of dementia. Progression from MCI to ADD was evaluated in 448 participants. The studies reported data on 401 participants with 1.6 years of follow‐up and in 47 participants with three years of follow‐up. Sixty‐one (15.2%) participants converted at 1.6 years follow‐up; nine (19.1%) participants converted at three years of follow‐up. Progression from MCI to any form of dementia was evaluated in five participants with 1.5 years of follow‐up, with three (60%) participants converting to any form of dementia. There were concerns regarding applicability in the reference standard in all three studies. Regarding the domain of flow and timing, two studies were considered at high risk of bias. MCI to ADD ; Progression from MCI to ADD in those with a follow‐up between two to less than four years had a sensitivity of 67% (95% CI 30 to 93) and a specificity of 71% (95% CI 54 to 85) by visual assessment (n = 47, 1 study). Progression from MCI to ADD in those with a follow‐up between one to less than two years had a sensitivity of 89% (95% CI 78 to 95) and a specificity of 58% (95% CI 53 to 64) by visual assessment, and a sensitivity of 87% (95% CI 76 to 94) and a specificity of 51% (95% CI 45 to 56) by quantitative assessment by the standardised uptake value ratio (SUVR)(n = 401, 1 study). MCI to any form of dementia ; Progression from MCI to any form of dementia in those with a follow‐up between one to less than two years had a sensitivity of 67% (95% CI 9 to 99) and a specificity of 50% (95% CI 1 to 99) by visual assessment (n = 5, 1 study). MCI to any other forms of dementia (non‐ADD) ; There was no information regarding the progression from MCI to any other form of dementia (non‐ADD). Authors' conclusions Although sensitivity was good in one included study, considering the poor specificity and the limited data available in the literature, we cannot recommend routine use of  18 F‐florbetapir PET in clinical practice to predict the progression from MCI to ADD. Because of the poor sensitivity and specificity, limited number of included participants, and the limited data available in the literature, we cannot recommend its routine use in clinical practice to predict the progression from MCI to any form of dementia. Because of the high financial costs of  18 F‐florbetapir, clearly demonstrating the DTA and standardising the process of this modality are important prior to its wider use. Plain language summary 18 F‐florbetapir PET scan for the early diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment Review question:  In people with mild cognitive impairment (MCI), does using a  18 F PET scan with florbetapir predict the progression to Alzheimer's disease dementia (ADD) and other dementias? Background Due to global ageing, the number of people with dementia is expected to increase dramatically in the next few decades. Diagnosing dementia at an early stage is desirable, but there is no widespread agreement on the best approach. A range of simple pen and paper tests used by healthcare professionals can assess people with poor memory or cognitive impairment. Whether or not using special PET scans that detect amyloid —one of the hallmarks of Alzheimer's disease— improves our ability to predict the progression from MCI to ADD or other forms of dementia remains unclear. Since these tests are expensive, it is important that they provide additional benefits. Aim We aimed to evaluate the accuracy of the  18 F‐florbetapir PET scan in identifying those people with MCI who clinically progress to ADD, other types of dementia, or any form of dementia over a period of time. Study characteristics The evidence is current to May 2017. We found three studies including 453 participants with MCI. Two studies evaluated the progression from MCI to ADD and one study evaluated the progression from MCI to any form of dementia. Regarding the two studies that evaluated the progression from MCI to ADD, one study had 401 participants with a follow‐up of 1.6 years and the mean age was 72 years. The other study had 47 participants with a follow‐up of three years, and the mean age was 72 years. The other study that looked at any form of dementia included 5 participants over 90 years old. Two of the studies were funded by the test manufacturer. Quality of the evidence The main limitation of this review was that our findings were based on only three studies, with insufficient detail on how the people were selected, whether the information from the scan was assessed separately from the final diagnosis. The studies were considered to be at high risk of bias due to potential conflicts of interest detected. Key findings In this review, we found the following results based on the three studies. At a follow‐up of 1.6 years, using visual assessment, the scan correctly classified 89% of the participants who progressed to ADD but only 58% of the participants who did not progress to ADD. This means that in a group of 100 people with MCI, 15% of whom will develop ADD, we would expect 13 of 15 people to have a positive result and the other 2 participants to be falsely negative. Also 49 people who will not develop ADD would have a negative result, but 36 people who will not develop ADD would have a positive result (false positives). In the study that followed up people for three years and used visual assessment, the scan correctly classified 67% of people who progressed to ADD and 71% who did not progress to ADD. This means that in a group of 100 people with MCI, 19 of whom will develop ADD, we would expect 13 people to have a positive result of the scan and 6 people to have a falsely negative result. In addition, 58 of 81 participants who will not progress to ADD would have a negative result, but 23 people who will not develop ADD would have a positive result (false positives). The small number of participants evaluated at three years lowered our confidence on these estimates of accuracy. Regarding progression to any form of dementia, the extremely small number of participants meant that we were unable to provide meaningful estimates of accuracy. We conclude that  18 F‐florbetapir PET scans cannot be recommended for routine use in clinical practice to predict the progression from MCI to ADD or any form of dementia based on the currently available data. More studies are needed to demonstrate its usefulness.","11","John Wiley & Sons, Ltd","1465-1858","*Aniline Compounds; *Ethylene Glycols; *Fluorine Radioisotopes; *Positron‐Emission Tomography; Alzheimer Disease [diagnostic imaging, etiology]; Cognitive Dysfunction [*complications]; Dementia [*diagnostic imaging, etiology]; Diagnosis, Differential; Disease Progression; Early Diagnosis; Humans; Sensitivity and Specificity","10.1002/14651858.CD012216.pub2","http://dx.doi.org/10.1002/14651858.CD012216.pub2","Dementia and Cognitive Improvement"
"CD012883","Martínez, G; Vernooij, RWM; Fuentes Padilla, P; Zamora, J; Flicker, L; Bonfill Cosp, X","18F PET with florbetaben for the early diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2017","Abstract - Background  18 F‐florbetaben uptake by brain tissue, measured by positron emission tomography (PET), is accepted by regulatory agencies like the Food and Drug Administration (FDA) and the European Medicine Agencies (EMA) for assessing amyloid load in people with dementia. Its added value is mainly demonstrated by excluding Alzheimer's pathology in an established dementia diagnosis. However, the National Institute on Aging and Alzheimer's Association (NIA‐AA) revised the diagnostic criteria for Alzheimer's disease and confidence in the diagnosis of mild cognitive impairment (MCI) due to Alzheimer's disease may be increased when using some amyloid biomarkers tests like  18 F‐florbetaben. These tests, added to the MCI core clinical criteria, might increase the diagnostic test accuracy (DTA) of a testing strategy. However, the DTA of  18 F‐florbetaben to predict the progression from MCI to Alzheimer’s disease dementia (ADD) or other dementias has not yet been systematically evaluated. Objectives To determine the DTA of the  18 F‐florbetaben PET scan for detecting people with MCI at time of performing the test who will clinically progress to ADD, other forms of dementia (non‐ADD), or any form of dementia at follow‐up. Search methods The most recent search for this review was performed in May 2017. We searched MEDLINE (OvidSP), Embase (OvidSP), PsycINFO (OvidSP), BIOSIS Citation Index (Thomson Reuters Web of Science), Web of Science Core Collection, including the Science Citation Index (Thomson Reuters Web of Science) and the Conference Proceedings Citation Index (Thomson Reuters Web of Science), LILACS (BIREME), CINAHL (EBSCOhost), ClinicalTrials.gov ( https://clinicaltrials.gov ), and the World Health Organization International Clinical Trials Registry Platform (WHO ICTRP) ( http://www.who.int/ictrp/search/en/ ). We also searched ALOIS, the Cochrane Dementia & Cognitive Improvement Group’s specialised register of dementia studies ( http://www.medicine.ox.ac.uk/alois/ ). We checked the reference lists of any relevant studies and systematic reviews, and performed citation tracking using the Science Citation Index to identify any additional relevant studies. No language or date restrictions were applied to electronic searches. Selection criteria We included studies that had prospectively defined cohorts with any accepted definition of MCI at time of performing the test and the use of  18 F‐florbetaben scan to evaluate the DTA of the progression from MCI to ADD or other forms of dementia. In addition, we only selected studies that applied a reference standard for Alzheimer’s dementia diagnosis, for example, the National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer’s Disease and Related Disorders Association (NINCDS‐ADRDA) or Diagnostic and Statistical Manual of Mental Disorders‐IV (DSM‐IV) criteria. Data collection and analysis We screened all titles and abstracts identified in electronic‐database searches. Two review authors independently selected studies for inclusion and extracted data to create two‐by‐two tables, showing the binary test results cross‐classified with the binary reference standard. We used these data to calculate sensitivities, specificities, and their 95% confidence intervals. Two independent assessors performed quality assessment using the QUADAS‐2 tool plus some additional items to assess the methodological quality of the included studies. Main results Progression from MCI to ADD, any other form of dementia, and any form of dementia was evaluated in one study (Ong 2015). It reported data on 45 participants at four years of follow‐up; 21 participants met NINCDS‐ADRDA criteria for Alzheimer’s disease dementia at four years of follow‐up, the proportion converting to ADD was 47% of the 45 participants, and 11% of the 45 participants met criteria for other types of dementias (three cases of FrontoTemporal Dementia (FTD), one of Dementia with Lewy body (DLB), and one of Progressive Supranuclear Palsy (PSP)). We considered the study to be at high risk of bias in the domains of the reference standard, flow, and timing (QUADAS‐2). MCI to ADD ;  18 F‐florbetaben PET scan analysed visually: the sensitivity was 100% (95% confidence interval (CI) 84% to 100%) and the specificity was 83% (95% CI 63% to 98%) (n = 45, 1 study). Analysed quantitatively: the sensitivity was 100% (95% CI 84% to 100%) and the specificity was 88% (95% CI 68% to 97%) for the diagnosis of ADD at follow‐up (n = 45, 1 study). MCI to any other form of dementia (non‐ADD);   18 F‐florbetaben PET scan analysed visually: the sensitivity was 0% (95% CI 0% to 52%) and the specificity was 38% (95% CI 23% to 54%) (n = 45, 1 study). Analysed quantitatively: the sensitivity was 0% (95% CI 0% to 52%) and the specificity was 40% (95% CI 25% to 57%) for the diagnosis of any other form of dementia at follow‐up (n = 45, 1 study). MCI to any form of dementia ; 18 F‐florbetaben PET scan analysed visually: the sensitivity was 81% (95% CI 61% to 93%) and the specificity was 79% (95% CI 54% to 94%) (n = 45, 1 study). Analysed quantitatively: the sensitivity was 81% (95% CI 61% to 93%) and the specificity was 84% (95% CI 60% to 97%) for the diagnosis of any form of dementia at follow‐up (n = 45, 1 study). Authors' conclusions Although we were able to calculate one estimation of DTA in, especially, the prediction of progression from MCI to ADD at four years follow‐up, the small number of participants implies imprecision of sensitivity and specificity estimates. We cannot make any recommendation regarding the routine use of  18 F‐florbetaben in clinical practice based on one single study with 45 participants.  18 F‐florbetaben has high financial costs, therefore, clearly demonstrating its DTA and standardising the process of the  18 F‐florbetaben modality are important prior to its wider use. Plain language summary 18 F PET with florbetaben for the early diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment Review question:  In people with mild cognitive impairment (MCI), does using a  18 F PET scan with florbetaben predict progression to Alzheimer's disease dementia (ADD) and other dementias? Background   Due to global ageing, the number of people with dementia is expected to increase dramatically in the next few decades. Diagnosing dementia at an early stage is desirable, but there is no widespread agreement on the best approach. A range of simple pen and paper tests used by healthcare professionals can assess people with poor memory or cognitive impairment. Whether or not using special PET scans that detect amyloid —one of the hallmarks of Alzheimer's disease— improves our ability to predict the progression from MCI to ADD or other forms of dementia remains unclear. Since these tests are expensive, it is important that they provide additional benefits. Aim We aimed to evaluate the accuracy of the  18 F‐florbetaben PET scan in identifying those people with MCI who clinically progress to ADD, other types of dementia, or any form of dementia over a period of time. Study characteristics   The evidence is current to May 2017. We found 1 study including 45 participants with MCI with a follow‐up of 4 years; gender was not reported and the median age for those with a PET‐positive scan by quantitative assessment was 73.5 years old. For those with a PET‐negative scan the mean age was 71.8 years old. Participants were mainly recruited from local memory clinics. Study funding sources: the study was funded by the test manufacturer. Quality of the evidence   The main limitation of this review was that our findings were based on only one study, with not enough details on how the participants were selected. The study was considered to be at high risk of bias, since the final ADD diagnosis was not established separately from the scan results, and due to potential conflicts of interest detected. Key findings   In this review, based on only one study, we found that the  18 F‐florbetaben PET scan, as a single test with visual assessment, correctly classified 100% of the participants who will progress to ADD and 83% of the participants who did not progress to ADD at four years follow‐up. This means that in a cohort with 100 participants with MCI, 47 of whom will progress to ADD, we would expect that all those 47 MCI participants would test positive with the  18 F‐florbetaben scan and that 0 participants would be falsely negative (i.e. none of the 47 participants would have a negative test and yet progress to ADD). In addition, we would expect 44 of 53 participants who did not progress to ADD to be  18 F‐florbetaben‐negative and 9 to be falsely positive (i.e. 9 of the 53 participants would have a positive test but not progress to ADD). The small size of the included study lowered our confidence on these estimates of accuracy and it is still possible that the test is considerably less accurate than these results suggest. We conclude that  18 F‐florbetaben imaging is a promising test to predict the progression from MCI to ADD; however, we need more studies to clearly demonstrate its accuracy.","11","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.CD012883","http://dx.doi.org/10.1002/14651858.CD012883","Dementia and Cognitive Improvement"
"CD012884","Martínez, G; Vernooij, RWM; Fuentes Padilla, P; Zamora, J; Flicker, L; Bonfill Cosp, X","18F PET with flutemetamol for the early diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2017","Abstract - Background  18 F‐flutemetamol uptake by brain tissue, measured by positron emission tomography (PET), is accepted by regulatory agencies like the Food and Drug Administration (FDA) and the European Medicine Agencies (EMA) for assessing amyloid load in people with dementia. Its added value is mainly demonstrated by excluding Alzheimer's pathology in an established dementia diagnosis. However, the National Institute on Aging and Alzheimer's Association (NIA‐AA) revised the diagnostic criteria for Alzheimer's disease and the confidence in the diagnosis of mild cognitive impairment (MCI) due to Alzheimer's disease may be increased when using some amyloid biomarkers tests like  18 F‐flutemetamol. These tests, added to the MCI core clinical criteria, might increase the diagnostic test accuracy (DTA) of a testing strategy. However, the DTA of  18 F‐flutemetamol to predict the progression from MCI to Alzheimer’s disease dementia (ADD) or other dementias has not yet been systematically evaluated. Objectives To determine the DTA of the  18 F‐flutemetamol PET scan for detecting people with MCI at time of performing the test who will clinically progress to ADD, other forms of dementia (non‐ADD) or any form of dementia at follow‐up. Search methods The most recent search for this review was performed in May 2017. We searched MEDLINE (OvidSP), Embase (OvidSP), PsycINFO (OvidSP), BIOSIS Citation Index (Thomson Reuters Web of Science), Web of Science Core Collection, including the Science Citation Index (Thomson Reuters Web of Science) and the Conference Proceedings Citation Index (Thomson Reuters Web of Science), LILACS (BIREME), CINAHL (EBSCOhost), ClinicalTrials.gov ( https://clinicaltrials.gov ), and the World Health Organization International Clinical Trials Registry Platform (WHO ICTRP) ( http://www.who.int/ictrp/search/en/ ). We also searched ALOIS, the Cochrane Dementia & Cognitive Improvement Group’s specialised register of dementia studies ( http://www.medicine.ox.ac.uk/alois/ ). We checked the reference lists of any relevant studies and systematic reviews, and performed citation tracking using the Science Citation Index to identify any additional relevant studies. No language or date restrictions were applied to the electronic searches. Selection criteria We included studies that had prospectively defined cohorts with any accepted definition of MCI at time of performing the test and the use of  18 F‐flutemetamol scan to evaluate the DTA of the progression from MCI to ADD or other forms of dementia. In addition, we only selected studies that applied a reference standard for Alzheimer’s dementia diagnosis, for example, National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer’s Disease and Related Disorders Association (NINCDS‐ADRDA) or Diagnostic and Statistical Manual of Mental Disorders‐IV (DSM‐IV) criteria. Data collection and analysis We screened all titles and abstracts identified in electronic‐database searches. Two review authors independently selected studies for inclusion and extracted data to create two‐by‐two tables, showing the binary test results cross‐classified with the binary reference standard. We used these data to calculate sensitivities, specificities, and their 95% confidence intervals. Two independent assessors performed quality assessment using the QUADAS‐2 tool plus some additional items to assess the methodological quality of the included studies. Main results Progression from MCI to ADD was evaluated in 243 participants from two studies. The studies reported data on 19 participants with two years of follow‐up and on 224 participants with three years of follow‐up. Nine (47.4%) participants converted at two years follow‐up and 81 (36.2%) converted at three years of follow‐up. There were concerns about participant selection and sampling in both studies. The index test domain in one study was considered unclear and in the second study it was considered at low risk of bias. For the reference standard domain, one study was considered at low risk and the second study was considered to have an unclear risk of bias. Regarding the domains of flow and timing, both studies were considered at high risk of bias. MCI to ADD ; Progression from MCI to ADD at two years of follow‐up had a sensitivity of 89% (95% CI 52 to 100) and a specificity of 80% (95% CI 44 to 97) by quantitative assessment by SUVR (n = 19, 1 study). Progression from MCI to ADD at three years of follow‐up had a sensitivity of 64% (95% CI 53 to 75) and a specificity of 69% (95% CI 60 to 76) by visual assessment (n = 224, 1 study). There was no information regarding the other two objectives in this systematic review (SR): progression from MCI to other forms of dementia and progression to any form of dementia at follow‐up. Authors' conclusions Due to the varying sensitivity and specificity for predicting the progression from MCI to ADD and the limited data available, we cannot recommend routine use of  18 F‐flutemetamol in clinical practice.  18 F‐flutemetamol has high financial costs; therefore, clearly demonstrating its DTA and standardising the process of the  18 F‐flutemetamol modality is important prior to its wider use. Plain language summary 18 F PET with flutemetamol for the early diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment Review question:  In people with mild cognitive impairment (MCI), does using a  18 F PET scan with flutemetamol predict the progression to Alzheimer's disease dementia (ADD) and other dementias? Background   Due to global ageing, the number of people with dementia is expected to increase dramatically in the next few decades. Diagnosing dementia at an early stage is desirable, but there is no widespread agreement on the best approach. A range of simple pen and paper tests used by healthcare professionals can assess people with poor memory or cognitive impairment. Whether or not using special PET scans that detect amyloid —one of the hallmarks of Alzheimer's disease— improves our ability to predict the progression from MCI to ADD or other forms of dementia remains unclear. Since these tests are expensive, it is important that they provide additional benefits. Aim We aimed to evaluate the accuracy of the  18 F‐flutemetamol PET scan in identifying those people with MCI who clinically progress to ADD, other types of dementia, or any form of dementia over a period of time. Study characteristics The evidence is current to May 2017. We found two studies evaluating the progression from MCI to ADD. The studies included 252 MCI eligible participants, with 243 participants that had follow‐up. Of these, 127 were women. The average age in one study with two years of follow‐up was 72.7  +  7.09 years. In the other study with three years of follow‐up, the average age was 71.1  +  8.62 years. The setting in one study was memory clinics. Study funding sources: both studies were funded by the test manufacturer. Quality of the evidence The main limitation of this review was that our findings were based on only two studies, with not enough details on how the people were selected, how the interpretation of the PET scan was made in one study, how the clinical diagnosis of dementia was established in the other study. The studies were considered to be at high risk of bias due to potential conflicts of interest detected. Key findings In this review, we found that the  18 F‐flutemetamol PET scan, as a single test, in one study with 19 participants included with 2 years of follow‐up, had a sensitivity of 89% and a specificity of 80%. This means that in a cohort with 100 participants with MCI and a proportion of progression in this study of 47%, we would expect 42 of 47 MCI participants with a positive result for  18 F‐flutemetamol scan to progress to ADD, and 5 participants to be falsely positive. In addition, we would expect 42 of 53 participants who will not progress to ADD to have a negative result for  18 F‐flutemetamol, and 11 to be falsely negative. In the other study with 224 participants included in the analysis with 3 years follow‐up, the sensitivity was 64% and the specificity was 69%. This means that in a cohort with 100 participants with MCI and a proportion of progression in this study of 36%, we would expect 23 of 36 MCI participants with a positive result for  18 F‐flutemetamol to progress to ADD, and 13 participants to be falsely positive. In addition, we would expect 44 of 64 participants who will not progress to ADD to have a negative result for  18 F‐flutemetamol, and 20 to be falsely negative. There was no information regarding the progression from MCI to other forms of dementia and progression to any form of dementia at follow‐up. We conclude that  18 F‐flutemetamol PET imaging cannot be recommended for routine use in clinical practice to predict the progression from MCI to ADD based on the currently available data. More studies are needed to clearly demonstrate its usefulness.","11","John Wiley & Sons, Ltd","1465-1858","*Positron‐Emission Tomography; Aged; Alzheimer Disease [*diagnostic imaging, metabolism]; Amyloid; Aniline Compounds [economics, *pharmacokinetics]; Benzothiazoles [economics, *pharmacokinetics]; Biomarkers; Cognitive Dysfunction [complications, *diagnostic imaging, metabolism]; Confidence Intervals; Disease Progression; Early Diagnosis; False Negative Reactions; False Positive Reactions; Female; Follow‐Up Studies; Humans; Male; Radiopharmaceuticals [economics, *pharmacokinetics]; Reference Standards; Sensitivity and Specificity; Time Factors","10.1002/14651858.CD012884","http://dx.doi.org/10.1002/14651858.CD012884","Dementia and Cognitive Improvement"
"CD011767.PUB2","Badeau, M; Lindsay, C; Blais, J; Nshimyumukiza, L; Takwoingi, Y; Langlois, S; Légaré, F; Giguère, Y; Turgeon, AF; Witteman, W; Rousseau, F","Genomics‐based non‐invasive prenatal testing for detection of fetal chromosomal aneuploidy in pregnant women","Cochrane Database of Systematic Reviews","2017","Abstract - Background Common fetal aneuploidies include Down syndrome (trisomy 21 or T21), Edward syndrome (trisomy 18 or T18), Patau syndrome (trisomy 13 or T13), Turner syndrome (45,X), Klinefelter syndrome (47,XXY), Triple X syndrome (47,XXX) and 47,XYY syndrome (47,XYY). Prenatal screening for fetal aneuploidies is standard care in many countries, but current biochemical and ultrasound tests have high false negative and false positive rates. The discovery of fetal circulating cell‐free DNA (ccfDNA) in maternal blood offers the potential for genomics‐based non‐invasive prenatal testing (gNIPT) as a more accurate screening method. Two approaches used for gNIPT are massively parallel shotgun sequencing (MPSS) and targeted massively parallel sequencing (TMPS). Objectives To evaluate and compare the diagnostic accuracy of MPSS and TMPS for gNIPT as a first‐tier test in unselected populations of pregnant women undergoing aneuploidy screening or as a second‐tier test in pregnant women considered to be high risk after first‐tier screening for common fetal aneuploidies. The gNIPT results were confirmed by a reference standard such as fetal karyotype or neonatal clinical examination. Search methods We searched 13 databases (including MEDLINE, Embase and Web of Science) from 1 January 2007 to 12 July 2016 without any language, search filter or publication type restrictions. We also screened reference lists of relevant full‐text articles, websites of private prenatal diagnosis companies and conference abstracts. Selection criteria Studies could include pregnant women of any age, ethnicity and gestational age with singleton or multifetal pregnancy. The women must have had a screening test for fetal aneuploidy by MPSS or TMPS and a reference standard such as fetal karyotype or medical records from birth. Data collection and analysis Two review authors independently carried out study selection, data extraction and quality assessment (using the QUADAS‐2 tool). Where possible, hierarchical models or simpler alternatives were used for meta‐analysis. Main results Sixty‐five studies of 86,139 pregnant women (3141 aneuploids and 82,998 euploids) were included. No study was judged to be at low risk of bias across the four domains of the QUADAS‐2 tool but applicability concerns were generally low. Of the 65 studies, 42 enrolled pregnant women at high risk, five recruited an unselected population and 18 recruited cohorts with a mix of prior risk of fetal aneuploidy. Among the 65 studies, 44 evaluated MPSS and 21 evaluated TMPS; of these, five studies also compared gNIPT with a traditional screening test (biochemical, ultrasound or both). Forty‐six out of 65 studies (71%) reported gNIPT assay failure rate, which ranged between 0% and 25% for MPSS, and between 0.8% and 7.5% for TMPS. In the population of unselected pregnant women, MPSS was evaluated by only one study; the study assessed T21, T18 and T13. TMPS was assessed for T21 in four studies involving unselected cohorts; three of the studies also assessed T18 and 13. In pooled analyses (88 T21 cases, 22 T18 cases, eight T13 cases and 20,649 unaffected pregnancies (non T21, T18 and T13)), the clinical sensitivity (95% confidence interval (CI)) of TMPS was 99.2% (78.2% to 100%), 90.9% (70.0% to 97.7%) and 65.1% (9.16% to 97.2%) for T21, T18 and T13, respectively. The corresponding clinical specificity was above 99.9% for T21, T18 and T13. In high‐risk populations, MPSS was assessed for T21, T18, T13 and 45,X in 30, 28, 20 and 12 studies, respectively. In pooled analyses (1048 T21 cases, 332 T18 cases, 128 T13 cases and 15,797 unaffected pregnancies), the clinical sensitivity (95% confidence interval (CI)) of MPSS was 99.7% (98.0% to 100%), 97.8% (92.5% to 99.4%), 95.8% (86.1% to 98.9%) and 91.7% (78.3% to 97.1%) for T21, T18, T13 and 45,X, respectively. The corresponding clinical specificities (95% CI) were 99.9% (99.8% to 100%), 99.9% (99.8% to 100%), 99.8% (99.8% to 99.9%) and 99.6% (98.9% to 99.8%). In this risk group, TMPS was assessed for T21, T18, T13 and 45,X in six, five, two and four studies. In pooled analyses (246 T21 cases, 112 T18 cases, 20 T13 cases and 4282 unaffected pregnancies), the clinical sensitivity (95% CI) of TMPS was 99.2% (96.8% to 99.8%), 98.2% (93.1% to 99.6%), 100% (83.9% to 100%) and 92.4% (84.1% to 96.5%) for T21, T18, T13 and 45,X respectively. The clinical specificities were above 100% for T21, T18 and T13 and 99.8% (98.3% to 100%) for 45,X. Indirect comparisons of MPSS and TMPS for T21, T18 and 45,X showed no statistical difference in clinical sensitivity, clinical specificity or both. Due to limited data, comparative meta‐analysis of MPSS and TMPS was not possible for T13. We were unable to perform meta‐analyses of gNIPT for 47,XXX, 47,XXY and 47,XYY because there were very few or no studies in one or more risk groups. Authors' conclusions These results show that MPSS and TMPS perform similarly in terms of clinical sensitivity and specificity for the detection of fetal T31, T18, T13 and sex chromosome aneuploidy (SCA). However, no study compared the two approaches head‐to‐head in the same cohort of patients. The accuracy of gNIPT as a prenatal screening test has been mainly evaluated as a second‐tier screening test to identify pregnancies at very low risk of fetal aneuploidies (T21, T18 and T13), thus avoiding invasive procedures. Genomics‐based non‐invasive prenatal testing methods appear to be sensitive and highly specific for detection of fetal trisomies 21, 18 and 13 in high‐risk populations. There is paucity of data on the accuracy of gNIPT as a first‐tier aneuploidy screening test in a population of unselected pregnant women. With respect to the replacement of invasive tests, the performance of gNIPT observed in this review is not sufficient to replace current invasive diagnostic tests. We conclude that given the current data on the performance of gNIPT, invasive fetal karyotyping is still the required diagnostic approach to confirm the presence of a chromosomal abnormality prior to making irreversible decisions relative to the pregnancy outcome. However, most of the gNIPT studies were prone to bias, especially in terms of the selection of participants. Plain language summary Accuracy of gNIPT for identifying genetic abnormalities in unborn babies What is the issue? How accurate is the new test (genomics‐based non‐invasive prenatal testing (gNIPT)) for detecting abnormal chromosome number in an unborn baby's genetic material (DNA) found in the mother's blood? We assessed the accuracy for the screening of Down syndrome (trisomy 21), Edward syndrome (trisomy 18), Patau syndrome (trisomy 13), Turner syndrome (45,X), Klinefelter syndrome (47,XXY), Triple X syndrome (47,XXX) and 47,XYY syndrome. There are different methods in use for gNIPT. We assessed MPSS (massively parallel shotgun sequencing) that tests whole DNA and TMPS (targeted massively parallel sequencing) that tests targeted DNA. Background There are 46 chromosomes (23 pairs) in humans. Abnormal numbers of chromosomes can cause genetic disorders for which there are no cures. Having an extra chromosome is called trisomy and an excess (or less) of sexual chromosome is called sex chromosome abnormality (SCA). The most common trisomy is Down syndrome which occurs in about one in 1000 babies. Children with Downs have slow growth, characteristic facial features and mild to moderate intellectual disability, with some requiring specialist education later in life. However, the symptoms vary from mild to severe so that some infants lead relatively normal lives. The other trisomy or SCA conditions have varying degrees of disability but the chance of a baby being affected is much less. Current screening tests for these conditions require confirmation if the baby has the condition or not and for this an invasive test like amniocentesis is used. Amniocentesis is where fetal cells that float in the fluid surrounding the unborn baby are collected by putting a fine needle through the mother’s abdomen and collecting the fluid. Alternatively, tissue can be collected from the placenta (chorionic villus sampling (CVS)). With these invasive tests, pregnant women are exposed to a higher chance of losing their baby even if the baby is unaffected by Down syndrome. So, this invasive test is only offered to women who are thought to have a higher chance of having an affected unborn baby What we did We looked for studies that included women of any age, ethnicity and gestational age who were carrying either a single baby or more than one. We searched for studies (up to July 2016) that assessed the accuracy of the new test. What we found We found 65 studies with a total of 86,139 pregnant women, including 3141 affected pregnancies. Forty‐two studies (65%) enrolled pregnant women with a high chance of having babies with abnormal chromosome number. Forty‐eight (74%) studies included only women with a singleton pregnancy. Forty‐four studies (68%) used MPSS and 21 studies (32%) used TMPS. gNIPT seems to be accurate for screening unborn babies (either singletons or twins), especially for detecting Down syndrome, trisomy 18 and trisomy 13. However, there were some problems with how the studies were conducted which makes us cautious about our findings. This may result in gNIPT appearing to perform better than it really does. Other important information to consider gNIPT method appears to perform well in identifying unborn babies with abnormal number of chromosomes. However, when a gNIPT detects an abnormal chromosome number, then a confirmation using invasive tests (like amniocentesis or CVS) is still needed before pregnancy‐related decisions can be made. It is important that pregnant women are given full information on the possible health problems that might arise for babies affected by an additional chromosome. For example, with Down syndrome though some children have considerable disability, others can lead relatively normal lives. In addition, in this review most studies enrolled pregnant women with increased chance of having babies with abnormal chromosome number, so our findings do not directly apply to general populations of pregnant women.","11","John Wiley & Sons, Ltd","1465-1858","*Aneuploidy; Cell‐Free Nucleic Acids [*blood]; Chromosome Disorders [*diagnosis, genetics]; Disorders of Sex Development [diagnosis, genetics]; Female; Fetal Diseases [*diagnosis, genetics]; High‐Throughput Nucleotide Sequencing [*methods]; Humans; Pregnancy; Pregnancy, High‐Risk; Prenatal Diagnosis [*methods]","10.1002/14651858.CD011767.pub2","http://dx.doi.org/10.1002/14651858.CD011767.pub2","Pregnancy and Childbirth"
"CD011221.PUB2","Hull, S; Tailor, V; Balduzzi, S; Rahi, J; Schmucker, C; Virgili, G; Dahlmann‐Noor, A","Tests for detecting strabismus in children aged 1 to 6 years in the community","Cochrane Database of Systematic Reviews","2017","Abstract - Background Strabismus (misalignment of the eyes) is a risk factor for impaired visual development both of visual acuity and of stereopsis. Detection of strabismus in the community by non‐expert examiners may be performed using a number of different index tests that include direct measures of misalignment (corneal or fundus reflex tests), or indirect measures such as stereopsis and visual acuity. The reference test to detect strabismus by trained professionals is the cover‒uncover test. Objectives To assess and compare the accuracy of tests, alone or in combination, for detection of strabismus in children aged 1 to 6 years, in a community setting by non‐expert screeners or primary care professionals to inform healthcare commissioners setting up childhood screening programmes. Secondary objectives were to investigate sources of heterogeneity of diagnostic accuracy. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL; 2016, Issue 12) (which contains the Cochrane Eyes and Vision Trials Register) in the Cochrane Library, the Health Technology Assessment Database (HTAD) in the Cochrane Library (2016, Issue 4), MEDLINE Ovid (1946 to 5 January 2017), Embase Ovid (1947 to 5 January 2017), CINAHL (January 1937 to 5 January 2017), Web of Science Conference Proceedings Citation Index‐Science (CPCI‐S) (January 1990 to 5 January 2017), BIOSIS Previews (January 1969 to 5 January 2017), MEDION (to 18 August 2014), the Aggressive Research Intelligence Facility database (ARIF) (to 5 January 2017), the ISRCTN registry ( www.isrctn.com/editAdvancedSearch ); searched 5 January 2017, ClinicalTrials.gov ( www.clinicaltrials.gov ); searched 5 January 2017 and the World Health Organization (WHO) International Clinical Trials Registry Platform (ICTRP) ( www.who.int/ictrp/search/en ); searched 5 January 2017. We did not use any date or language restrictions in the electronic searches for trials. In addition, orthoptic journals and conference proceedings without electronic listings were searched. Selection criteria All prospective or retrospective population‐based test accuracy studies of consecutive participants were included. Studies compared a single or combination of index tests with the reference test. Only those studies with sufficient data for analysis were included specifically to calculate sensitivity and specificity and determine diagnostic accuracy. Participants were aged 1 to 6 years. Studies reporting participants outside this range were included if subgroup data were available. Permitted settings included population‐based vision screening programmes or opportunistic screening programmes, such as those performed in schools. Data collection and analysis We used standard methodological procedures expected by Cochrane. In brief, two review authors independently assessed titles and abstracts for eligibility and extracted the data, with a third senior author resolving any disagreement. We analysed data primarily for specificity and sensitivity. Main results One study from a total of 1236 papers, abstracts and trials was eligible for inclusion with a total number of participants of 335 of which 271 completed both the screening test and the gold standard test. The screening test using an automated photoscreener had a sensitivity of 0.46 (95% confidence interval (CI) 0.19 to 0.75) and specificity of 0.97 (CI 0.94 to 0.99). The overall number affected by strabismus was low at 13 (4.8%). Authors' conclusions There is very limited data in the literature to ascertain the accuracy of tests for detecting strabismus in the community as performed by non‐expert screeners. A large prospective study to compare methods would be required to determine which tests have the greatest accuracy. Plain language summary Tests for detecting strabismus in children aged one to six years in the community Review aim   The aim of this Cochrane Review was to find out how well different tests work to detect strabismus in children aged 1 to 6 years old outside of eye departments. These tests were used in the community and were performed by screeners who were not eye specialists. Background   Strabismus (also known as squint) occurs when the eyes are not aligned. It can lead to reduced vision and failure of the eyes to work properly together, including for 3D vision. A number of different tests can be used to screen for strabismus directly, by measuring the misalignment; or indirectly, by measuring the level of vision in each eye (visual acuity); or by measuring 3D vision (stereopsis). It is unknown which of these tests is the most accurate in correctly identifying children with strabismus. Results and conclusion   Only one study was found that met the standards to be included in this review. This study used a photoscreener (a type of camera that measures refractive error and misalignment). Following screening, all children were offered an examination by an eye‐care specialist to confirm which children did have strabismus. The photoscreener was very accurate in identifying those children without strabismus (highly specific) but not accurate in correctly identifying those children with strabismus (low sensitivity only). As only one study could be included in this review, it was not possible to conclude which test is the most accurate for screening for strabismus. Further studies would be needed to determine this. However, they would need to include very large numbers of children to be able to make statistically valid conclusions. How up to date is this review?     Cochrane researchers searched for studies that had been published up to 5 January 2017.","11","John Wiley & Sons, Ltd","1465-1858","Blinking; Child; Child, Preschool; Depth Perception; Fundus Oculi; Humans; Infant; Prospective Studies; Retrospective Studies; Sensitivity and Specificity; Strabismus [*diagnosis]; Vision Screening [*methods]; Visual Acuity","10.1002/14651858.CD011221.pub2","http://dx.doi.org/10.1002/14651858.CD011221.pub2","Eyes and Vision"
"CD008587.PUB2","Koliopoulos, G; Nyaga, VN; Santesso, N; Bryant, A; Martin‐Hirsch, PPL; Mustafa, RA; Schünemann, H; Paraskevaidis, E; Arbyn, M","Cytology versus HPV testing for cervical cancer screening in the general population","Cochrane Database of Systematic Reviews","2017","Abstract - Background Cervical cancer screening has traditionally been based on cervical cytology. Given the aetiological relationship between human papillomavirus (HPV) infection and cervical carcinogenesis, HPV testing has been proposed as an alternative screening test. Objectives To determine the diagnostic accuracy of HPV testing for detecting histologically confirmed cervical intraepithelial neoplasias (CIN) of grade 2 or worse (CIN 2+), including adenocarcinoma in situ, in women participating in primary cervical cancer screening; and how it compares to the accuracy of cytological testing (liquid‐based and conventional) at various thresholds. Search methods We performed a systematic literature search of articles in MEDLINE and Embase (1992 to November 2015) containing quantitative data and handsearched the reference lists of retrieved articles. Selection criteria We included comparative test accuracy studies if all women received both HPV testing and cervical cytology followed by verification of the disease status with the reference standard, if positive for at least one screening test. The studies had to include women participating in a cervical cancer screening programme who were not being followed up for previous cytological abnormalities. Data collection and analysis We completed a 2 x 2 table with the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives for each screening test (HPV test and cytology) used in each study. We calculated the absolute and relative sensitivities and the specificities of the tests for the detection of CIN 2+ and CIN 3+ at various thresholds and computed sensitivity (TP/(TP + TN) and specificity (TN/ (TN + FP) for each test separately. Relative sensitivity and specificity of one test compared to another test were defined as sensitivity of test‐1 over sensitivity of test‐2 and specificity of test‐1 over specificity of test‐2, respectively. To assess bias in the studies, we used the Quality Assessment of Diagnostic test Accuracy Studies (QUADAS) tool. We used a bivariate random‐effects model for computing pooled accuracy estimates. This model takes into account the within‐ and between‐study variability and the intrinsic correlation between sensitivity and specificity. Main results We included a total of 40 studies in the review, with more than 140,000 women aged between 20 and 70 years old. Many studies were at low risk of bias. There were a sufficient number of included studies with adequate methodology to perform the following test comparisons: hybrid capture 2 (HC2) (1 pg/mL threshold) versus conventional cytology (CC) (atypical squamous cells of undetermined significance (ASCUS)+ and low‐grade squamous intraepithelial lesions (LSIL)+ thresholds) or liquid‐based cytology (LBC) (ASCUS+ and LSIL+ thresholds), other high‐risk HPV tests versus conventional cytology (ASCUS+ and LSIL+ thresholds) or LBC (ASCUS+ and LSIL+ thresholds). For CIN 2+, pooled sensitivity estimates for HC2, CC and LBC (ASCUS+) were 89.9%, 62.5% and 72.9%, respectively, and pooled specificity estimates were 89.9%, 96.6%, and 90.3%, respectively. The results did not differ by age of women (less than or greater than 30 years old), or in studies with verification bias. Accuracy of HC2 was, however, greater in European countries compared to other countries. The results for the sensitivity of the tests were heterogeneous ranging from 52% to 94% for LBC, and 61% to 100% for HC2. Overall, the quality of the evidence for the sensitivity of the tests was moderate, and high for the specificity. The relative sensitivity of HC2 versus CC for CIN 2+ was 1.52 (95% CI: 1.24 to 1.86) and the relative specificity 0.94 (95% CI: 0.92 to 0.96), and versus LBC for CIN 2+ was 1.18 (95% CI: 1.10 to 1.26) and the relative specificity 0.96 (95% CI: 0.95 to 0.97). The relative sensitivity of HC2 versus CC for CIN 3+ was 1.46 (95% CI: 1.12 to 1.91) and the relative specificity 0.95 (95% CI: 0.93 to 0.97). The relative sensitivity of HC2 versus LBC for CIN 3+ was 1.17 (95% CI: 1.07 to 1.28) and the relative specificity 0.96 (95% CI: 0.95 to 0.97). Authors' conclusions Whilst HPV tests are less likely to miss cases of CIN 2+ and CIN 3+, these tests do lead to more unnecessary referrals. However, a negative HPV test is more reassuring than a negative cytological test, as the cytological test has a greater chance of being falsely negative, which could lead to delays in receiving the appropriate treatment. Evidence from prospective longitudinal studies is needed to establish the relative clinical implications of these tests. Plain language summary Human papillomavirus (HPV) test compared to the Papanicolaou (Pap) test to screen for cervical cancer Review question   We assessed studies comparing two tests to screen for cervical cancer: the HPV test (Human papillomavirus test) and the Pap test otherwise known as cervical smear or Papanicolaou test. The aim was to find out which test detects precancerous changes of the cervix more accurately. Background    The HPV and the Pap tests are tests that a doctor performs to check for the development of cervical cancer or precancerous changes to the cells of the cervix (called lesions). These lesions can develop into cervical cancer within about 10 to 20 years. The HPV test checks whether a woman has an HPV infection which may lead to cervical cancer. If the HPV test is positive, it may mean that there are precancerous changes in the cervix. There are many types of HPV tests. One of them is called the HC2 test. The Pap test checks for whether cells in the cervix are abnormal. Abnormal cervical cells that are tested as ‘low grade to high grade’ may mean that there are precancerous changes in the cervix that may lead to cervical cancer. One type of Pap test is ‘conventional cytology' and another is 'liquid‐based cytology'. Depending on the test, if it is positive a woman may need to have the cervix examined or could receive surgery to have the precancerous lesion removed. Study characteristics   We searched for all relevant studies up to November 2015. Forty studies compared the HPV test to the Pap test on over 140,000 women between 20 to 70 years old who attended for their routine cervical screening. The studies examined which test can detect precancerous cervical changes which are called cervical intraepithelial neoplasias (CIN 2 and CIN 3). Quality of the evidence   There were enough studies with enough women in them to allow us to draw conclusions. However, some of the results from the studies were different from each other. For example, tests were more accurate in studies in Europe than in Asia or Central or South America. Overall, the quality of the evidence was moderate to high. Key results   A perfect test would correctly say if a woman has precancerous changes or if a woman does not. But most tests are not perfect. This review found that for every 1000 women screened, around 20 women will have precancerous changes. The HPV test will correctly identify 18 of these women (but will miss 2 women). The Pap test will identify 15 of the women (but will miss 5 women). The women who are missed could develop cervical cancer. For every 1000 women screened, there will be 980 women who will not have precancerous changes. The HPV test will correctly identify 881 women (but 99 women will be incorrectly told that they have a lesion). The Pap test will correctly identify 885 women (but 95 will be incorrectly told that they have a lesion). Women who are incorrectly told that they have a lesion may have their cervix examined or may receive surgery unnecessarily.","8","John Wiley & Sons, Ltd","1465-1858","Adult; Aged; Early Detection of Cancer [methods]; Female; Humans; Middle Aged; Papillomavirus Infections [*diagnosis]; Polymerase Chain Reaction; Precancerous Conditions [*diagnosis, pathology, virology]; Sensitivity and Specificity; Uterine Cervical Dysplasia [*diagnosis, pathology, virology]; Uterine Cervical Neoplasms [*diagnosis, pathology, virology]; Vaginal Smears [methods]","10.1002/14651858.CD008587.pub2","http://dx.doi.org/10.1002/14651858.CD008587.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD010296.PUB2","Abraha, I; Luchetta, ML; De Florio, R; Cozzolino, F; Casazza, G; Duca, P; Parente, B; Orso, M; Germani, A; Eusebi, P; Montedori, A","Ultrasonography for endoleak detection after endoluminal abdominal aortic aneurysm repair","Cochrane Database of Systematic Reviews","2017","Abstract - Background People with abdominal aortic aneurysm who receive endovascular aneurysm repair (EVAR) need lifetime surveillance to detect potential endoleaks. Endoleak is defined as persistent blood flow within the aneurysm sac following EVAR. Computed tomography (CT) angiography is considered the reference standard for endoleak surveillance. Colour duplex ultrasound (CDUS) and contrast‐enhanced CDUS (CE‐CDUS) are less invasive but considered less accurate than CT. Objectives To determine the diagnostic accuracy of colour duplex ultrasound (CDUS) and contrast‐enhanced‐colour duplex ultrasound (CE‐CDUS) in terms of sensitivity and specificity for endoleak detection after endoluminal abdominal aortic aneurysm repair (EVAR). Search methods We searched MEDLINE, Embase, LILACS, ISI Conference Proceedings, Zetoc, and trial registries in June 2016 without language restrictions and without use of filters to maximize sensitivity. Selection criteria Any cross‐sectional diagnostic study evaluating participants who received EVAR by both ultrasound (with or without contrast) and CT scan assessed at regular intervals. Data collection and analysis Two pairs of review authors independently extracted data and assessed quality of included studies using the QUADAS 1 tool. A third review author resolved discrepancies. The unit of analysis was number of participants for the primary analysis and number of scans performed for the secondary analysis. We carried out a meta‐analysis to estimate sensitivity and specificity of CDUS or CE‐CDUS using a bivariate model. We analysed each index test separately. As potential sources of heterogeneity, we explored year of publication, characteristics of included participants (age and gender), direction of the study (retrospective, prospective), country of origin, number of CDUS operators, and ultrasound manufacturer. Main results We identified 42 primary studies with 4220 participants. Twenty studies provided accuracy data based on the number of individual participants (seven of which provided data with and without the use of contrast). Sixteen of these studies evaluated the accuracy of CDUS. These studies were generally of moderate to low quality: only three studies fulfilled all the QUADAS items; in six (40%) of the studies, the delay between the tests was unclear or longer than four weeks; in eight (50%), the blinding of either the index test or the reference standard was not clearly reported or was not performed; and in two studies (12%), the interpretation of the reference standard was not clearly reported. Eleven studies evaluated the accuracy of CE‐CDUS. These studies were of better quality than the CDUS studies: five (45%) studies fulfilled all the QUADAS items; four (36%) did not report clearly the blinding interpretation of the reference standard; and two (18%) did not clearly report the delay between the two tests. Based on the bivariate model, the summary estimates for CDUS were 0.82 (95% confidence interval (CI) 0.66 to 0.91) for sensitivity and 0.93 (95% CI 0.87 to 0.96) for specificity whereas for CE‐CDUS the estimates were 0.94 (95% CI 0.85 to 0.98) for sensitivity and 0.95 (95% CI 0.90 to 0.98) for specificity. Regression analysis showed that CE‐CDUS was superior to CDUS in terms of sensitivity (LR Chi 2  = 5.08, 1 degree of freedom (df); P = 0.0242 for model improvement). Seven studies provided estimates before and after administration of contrast. Sensitivity before contrast was 0.67 (95% CI 0.47 to 0.83) and after contrast was 0.97 (95% CI 0.92 to 0.99). The improvement in sensitivity with of contrast use was statistically significant (LR Chi 2  = 13.47, 1 df; P = 0.0002 for model improvement). Regression testing showed evidence of statistically significant effect bias related to year of publication and study quality within individual participants based CDUS studies. Sensitivity estimates were higher in the studies published before 2006 than the estimates obtained from studies published in 2006 or later (P < 0.001); and studies judged as low/unclear quality provided higher estimates in sensitivity. When regression testing was applied to the individual based CE‐CDUS studies, none of the items, namely direction of the study design, quality, and age, were identified as a source of heterogeneity. Twenty‐two studies provided accuracy data based on number of scans performed (of which four provided data with and without the use of contrast). Analysis of the studies that provided scan based data showed similar results. Summary estimates for CDUS (18 studies) showed 0.72 (95% CI 0.55 to 0.85) for sensitivity and 0.95 (95% CI 0.90 to 0.96) for specificity whereas summary estimates for CE‐CDUS (eight studies) were 0.91 (95% CI 0.68 to 0.98) for sensitivity and 0.89 (95% CI 0.71 to 0.96) for specificity. Authors' conclusions This review demonstrates that both ultrasound modalities (with or without contrast) showed high specificity. For ruling in endoleaks, CE‐CDUS appears superior to CDUS. In an endoleak surveillance programme CE‐CDUS can be introduced as a routine diagnostic modality followed by CT scan only when the ultrasound is positive to establish the type of endoleak and the subsequent therapeutic management. Plain language summary Ultrasonography versus computed tomography scan for endoleak detection after endoluminal abdominal aortic aneurysm repair Background An abdominal aortic aneurysm (AAA) is a localised swelling or widening of a major vessel that carries blood to the abdomen (tummy), pelvis, and legs. People with AAA are at risk from sudden death due to AAA rupture (bursting). Once detected, intervention (treatment) is recommended once the AAA is bigger than about 5 cm in diameter. Most repairs are now performed using a new vessel lining inside the aneurysm guided by x‐ray control (endovascular aneurysm repair or EVAR). Once the new lining is in place, the seals at either end may leak or vessel branches arising from the aneurysm wall may bleed backwards into the AAA sac. These are collectively referred to as endoleaks. Endoleaks are common after EVAR, developing in about 40% of people during monitoring (follow‐up). Endoleaks can be associated with late aneurysm rupture and, therefore, detection and monitoring is essential. Ultrasound (uses high‐frequency sound waves), computed tomography (uses x‐rays), and magnetic resonance scans (uses strong magnetic fields and radio waves) have all been used to detect and monitor endoleaks. Sometimes, dye (contrast) is injected into a vein to improve the accuracy of ultrasound (contrast‐enhanced ultrasound). Study characteristics We collected the most recent evidence (to July 2016) and conducted a meta‐analysis according to the most appropriate methods for diagnostic tests. We included 42 studies with 4220 participants in the review. Key results The analyses measured sensitivity (how well a test identified people with endoleak correctly) and specificity (how well a test identified people without endoleak correctly). The summary accuracy estimates were sensitivity 82% (95% confidence interval 66% to 91%) and specificity 93% (95% confidence interval 87% to 96%) for ultrasonography without contrast; and sensitivity 94% (95% confidence interval 85% to 98%) and specificity 95% (95% confidence interval 90% to 98%) for ultrasonography with contrast. Use of contrast improved the sensitivity of ultrasound significantly. Based on these results, we would expect 94% of people with endoleaks will be correctly identified by contrast‐enhanced ultrasound. Quality of the evidence Studies that evaluated contrast‐enhanced ultrasound used better methods than the studies that evaluated ultrasound alone.","6","John Wiley & Sons, Ltd","1465-1858","*Contrast Media; *Endovascular Procedures; *Ultrasonography, Doppler, Duplex; Aortic Aneurysm, Abdominal [*surgery]; Endoleak [*diagnostic imaging]; Female; Humans; Male; Sensitivity and Specificity; Stents [statistics & numerical data]; Tomography, X‐Ray Computed","10.1002/14651858.CD010296.pub2","http://dx.doi.org/10.1002/14651858.CD010296.pub2","Vascular"
"CD008892.PUB2","Wijedoru, L; Mallett, S; Parry, CM","Rapid diagnostic tests for typhoid and paratyphoid (enteric) fever","Cochrane Database of Systematic Reviews","2017","Abstract - Background Differentiating both typhoid ( Salmonella  Typhi) and paratyphoid ( Salmonella  Paratyphi A) infection from other causes of fever in endemic areas is a diagnostic challenge. Although commercial point‐of‐care rapid diagnostic tests (RDTs) for enteric fever are available as alternatives to the current reference standard test of blood or bone marrow culture, or to the widely used Widal Test, their diagnostic accuracy is unclear. If accurate, they could potentially replace blood culture as the World Health Organization (WHO)‐recommended main diagnostic test for enteric fever. Objectives To assess the diagnostic accuracy of commercially available rapid diagnostic tests (RDTs) and prototypes for detecting  Salmonella  Typhi or Paratyphi A infection in symptomatic persons living in endemic areas. Search methods We searched the Cochrane Infectious Diseases Group Specialized Register, MEDLINE, Embase, Science Citation Index, IndMED, African Index Medicus, LILACS, ClinicalTrials.gov, and the World Health Organization (WHO) International Clinical Trials Registry Platform (ICTRP) up to 4 March 2016. We manually searched WHO reports, and papers from international conferences on  Salmonella  infections. We also contacted test manufacturers to identify studies. Selection criteria We included diagnostic accuracy studies of enteric fever RDTs in patients with fever or with symptoms suggestive of enteric fever living in endemic areas. We classified the reference standard used as either Grade 1 (result from a blood culture and a bone marrow culture) or Grade 2 (result from blood culture and blood polymerase chain reaction, or from blood culture alone). Data collection and analysis Two review authors independently extracted the test result data. We used a modified QUADAS‐2 extraction form to assess methodological quality. We performed a meta‐analysis when there were sufficient studies for the test and heterogeneity was reasonable. Main results Thirty‐seven studies met the inclusion criteria and included a total of 5080 participants (range 50 to 1732). Enteric fever prevalence rates in the study populations ranged from 1% to 75% (median prevalence 24%, interquartile range (IQR) 11% to 46%). The included studies evaluated 16 different RDTs, and 16 studies compared two or more different RDTs. Only three studies used the Grade 1 reference standard, and only 11 studies recruited unselected febrile patients. Most included studies were from Asia, with five studies from sub‐Saharan Africa. All of the RDTs were designed to detect  S. Typhi infection only. Most studies evaluated three RDTs and their variants: TUBEX in 14 studies; Typhidot (Typhidot, Typhidot‐M, and TyphiRapid‐Tr02) in 22 studies; and the Test‐It Typhoid immunochromatographic lateral flow assay, and its earlier prototypes (dipstick, latex agglutination) developed by the Royal Tropical Institute, Amsterdam (KIT) in nine studies. Meta‐analyses showed an average sensitivity of 78% (95% confidence interval (CI) 71% to 85%) and specificity of 87% (95% CI 82% to 91%) for TUBEX; and an average sensitivity of 69% (95% CI 59% to 78%) and specificity of 90% (95% CI 78% to 93%) for all Test‐It Typhoid and prototype tests (KIT). Across all forms of the Typhidot test, the average sensitivity was 84% (95% CI 73% to 91%) and specificity was 79% (95% CI 70% to 87%). When we based the analysis on the 13 studies of the Typhidot test that either reported indeterminate test results or where the test format means there are no indeterminate results, the average sensitivity was 78% (95% CI 65% to 87%) and specificity was 77% (95% CI 66% to 86%). We did not identify any difference in either sensitivity or specificity between TUBEX, Typhidot, and Test‐it Typhoid tests when based on comparison to the 13 Typhidot studies where indeterminate results are either reported or not applicable. If TUBEX and Test‐it Typhoid are compared to all Typhidot studies, the sensitivity of Typhidot was higher than Test‐it Typhoid (15% (95% CI 2% to 28%), but other comparisons did not show a difference at the 95% level of CIs. In a hypothetical cohort of 1000 patients presenting with fever where 30% (300 patients) have enteric fever, on average Typhidot tests reporting indeterminate results or where tests do not produce indeterminate results will miss the diagnosis in 66 patients with enteric fever, TUBEX will miss 66, and Test‐It Typhoid and prototype (KIT) tests will miss 93. In the 700 people without enteric fever, the number of people incorrectly diagnosed with enteric fever would be 161 with Typhidot tests, 91 with TUBEX, and 70 with Test‐It Typhoid and prototype (KIT) tests. The CIs around these estimates were wide, with no difference in false positive results shown between tests. The quality of the data for each study was evaluated using a standardized checklist called QUADAS‐2. Overall, the certainty of the evidence in the studies that evaluated enteric fever RDTs was low. Authors' conclusions In 37 studies that evaluated the diagnostic accuracy of RDTs for enteric fever, few studies were at a low risk of bias. The three main RDT tests and variants had moderate diagnostic accuracy. There was no evidence of a difference between the average sensitivity and specificity of the three main RDT tests. More robust evaluations of alternative RDTs for enteric fever are needed. 2 April 2019 Up to date All studies incorporated from most recent search All eligible published studies found in the last search (4 Mar, 2016) were included Plain language summary The accuracy of rapid diagnostic tests for detecting typhoid and paratyphoid (enteric) fever Cochrane researchers assessed the accuracy of commercially‐available rapid diagnostic tests and their prototypes (including TUBEX, Typhidot, Typhidot‐M, Test‐it Typhoid, and other tests) for detecting typhoid and paratyphoid (enteric) fever in people living in countries where the estimated number of individuals with the disease at any one time is greater than 10 per 100,000 population. If accurate, they could replace the current World Health Organization (WHO)‐recommended diagnostic test: culture (growing the bacteria that causes the infection from a patient’s blood or bone marrow). Background Typhoid fever and paratyphoid fever are infections caused by the bacteria  Salmonella  Typhi and  Salmonella  Paratyphi A respectively. The term ‘enteric fever’ is used to describe both infections. Enteric fever can be difficult to diagnose as the signs and symptoms are similar to those of other infectious diseases that cause fever such as malaria. The recommended test to confirm if a person has enteric fever is to grow the  Salmonella  from their blood. It takes at least 48 hours to give a result, so cannot help healthcare workers make a diagnosis the same day the blood culture is taken. Blood cultures may give a negative result even though a person has enteric fever. The test also requires a laboratory and trained staff, which are often unavailable in communities where enteric fever is common. Rapid diagnostic tests (RDTs) are designed to be easy to use, and to deliver a quick result without the need for a blood culture laboratory. The cost of an enteric fever RDT would be significantly less than a blood culture, and requires less training to perform. Study characteristics Cochrane researchers searched the available literature up to 4 March 2016 and included 37 studies. Most studies recruited participants from South Asia. Most participants were adults, with 22 studies including children. All of the RDTs evaluated detected  Salmonella  Typhi (typhoid fever) only. Quality of the evidence The Cochrane researchers evaluated the quality of the data for each study using a standardized checklist called QUADAS‐2. High quality studies that compared different types of RDT in the same patients were few in number. Two‐thirds of the included studies did not evaluate the RDTs in the context of patients who are typically tested for the disease. Many studies utilized a particular study design (a case control study) which risks overestimating RDT accuracy. In the studies evaluating the Typhidot RDT, it was often unclear how many test results were indeterminate, when the test cannot distinguish a current episode of infection from a previous disease episode. Overall, the certainty of the evidence in the studies that evaluated enteric fever RDTs was low. Key results Sensitivity indicates the percentage of patients with a positive test result who are correctly diagnosed with disease. Specificity indicates the percentage of patients who are correctly identified as not having disease. TUBEX showed an average sensitivity of 78% and specificity of 87%. Typhidot studies, grouped together to include Typhidot, Typhidot‐M, and TyphiRapid‐Tr02, showed an average sensitivity of 84% and specificity of 79%. When Typhidot studies with clear reporting of indeterminate results are considered, the average sensitivity and specificity of Typhidot was 78% and 77% respectively. Test‐It Typhoid and prototypes (KIT) showed an average sensitivity of 69% and specificity of 90%. Based on these results, in 1000 patients with fever where 30% (300 patients) have enteric fever, we would expect Typhidot tests reporting indeterminate results or where tests do not produce indeterminate results to, on average, miss the diagnosis (give a false negative result) in 66 patients with enteric fever, TUBEX to miss 66, and Test‐It Typhoid and prototypes (KIT) to miss 93. In the 700 people without enteric fever, the number of people incorrectly given a diagnosis of enteric fever (a false positive result) would be on average 161 with these Typhidot tests, 91 with TUBEX, and 70 with the Test‐It Typhoid and prototypes (KIT). These differences in the number of false negative and false positive results in patients from the different tests are not statistically important. The RDTs evaluated are not sufficiently accurate to replace blood culture as a diagnostic test for enteric fever.","5","John Wiley & Sons, Ltd","1465-1858","Adult; Child; False Negative Reactions; False Positive Reactions; Humans; Immunoassay [*methods]; Paratyphoid Fever [blood, *diagnosis]; Polymerase Chain Reaction [standards]; Reagent Kits, Diagnostic [*standards]; Reference Standards; Sensitivity and Specificity; Typhoid Fever [blood, *diagnosis]","10.1002/14651858.CD008892.pub2","http://dx.doi.org/10.1002/14651858.CD008892.pub2","Infectious Diseases"
"CD011126.PUB2","Nieuwenhuis, LL; Hermans, FJR; Bij de Vaate, AJM; Leeflang, MMG; Brölmann, HAM; Hehenkamp, WJK; Mol, BWJ; Clark, TJ; Huirne, JAF","Three‐dimensional saline infusion sonography compared to two‐dimensional saline infusion sonography for the diagnosis of focal intracavitary lesions","Cochrane Database of Systematic Reviews","2017","Abstract - Background Focal abnormalities most commonly acquired within the uterine cavity include endometrial polyps (arising from the endometrium) and submucous fibroids (arising from the myometrium). These benign abnormalities can cause several problems, including abnormal uterine bleeding (AUB) and subfertility. Two‐dimensional saline infusion sonography (2D SIS) is a minimally invasive test that can be used to diagnose these pathologies, but it is less accurate than hysteroscopy, which is a more invasive procedure by which an endoscope allows direct visualisation of the uterine cavity. Three‐dimensional (3D) SIS appears to enhance sonographic visualisation within the uterine cavity, thereby offering a potentially more accurate minimally invasive diagnostic test. Objectives Primary objectives • To evaluate the diagnostic accuracy of 3D SIS (index test 1) compared with 2D SIS for the diagnosis of focally growing lesions (presence or not) in women with AUB or subfertility, with hysteroscopy performed as the reference test . • To evaluate the diagnostic accuracy of 2D+3D SIS (index test 2) compared with 2D SIS for the diagnosis of focally growing lesions (presence or not) in women with AUB or subfertility, with hysteroscopy performed as the reference test .  In this case, any abnormality on either modality was regarded as a positive result (‘OR’ approach). Secondary objectives • To evaluate the diagnostic accuracy of 3D SIS (index test 1) compared with 2D SIS according to type of abnormality and discrimination between uterine polyps and submucous fibroids in women with AUB or subfertility, with hysteroscopy and histology used as the reference. • To evaluate the diagnostic accuracy of 2D+3D SIS (index test 2) compared with 2D SIS according to type of abnormality and discrimination between uterine polyps and submucous fibroids in women with AUB or subfertility, with hysteroscopy and histology used as the reference. Search methods We searched the following databases: Cochrane Central Register of Studies Online (CENTRAL CRSO), MEDLINE, Embase, PubMed, Cochrane Gynaecology and Fertility Group (CGF) Specialised Register and CGFG Diagnostic Test Accuracy (DTA) Specialised Register, clinicaltrials.gov and the World Health Organization (WHO) International Clinical Trials Registry Platform (ICTRP). Screening reference lists of appropriate studies was also performed. We screened for eligibility all studies identified from inception until March 2016. We performed searches with no date or language restrictions. Selection criteria The population of interest consisted of premenopausal women with AUB or subfertility and postmenopausal women with AUB. Diagnostic test accuracy studies, randomised controlled trials (RCTs) and prospective cohort studies were eligible for inclusion if they evaluated the accuracy of both 2D SIS and 3D SIS for the diagnosis of acquired intracavitary abnormalities with hysteroscopy used as the reference standard. In light of the lack of data for 3D SIS, we also included studies that evaluated the accuracy of 3D SIS alone. Data collection and analysis Two review authors read all potentially eligible references after performing a first screening by title and abstract (LLN and FJRH). They independently extracted data to construct 2×2 tables from eligible studies and assessed studies for methodological quality using the QUADAS‐2 tool (revised tool for quality assessment of diagnostic accuracy studies). To describe and visually present results, we produced in RevMan forest plots showing pairs of sensitivity and specificity together with 95% confidence intervals from each study, as well as raw receiver operating characteristic (ROC) plots. We displayed paired analyses in an ROC plot by linking sensitivity‐specificity pairs from each study by using a dashed line. To compare 3D SIS versus 2D SIS, we restricted analyses to studies that provided 2×2 tables for both tests and used the bivariate meta‐analysis of sensitivity and specificity. Main results Thirteen studies (1053 women) reported the accuracy of 3D SIS for focal uterine abnormalities; 11 of these (846 women) were suitable for meta‐analysis, and eight reported accuracy according to the type of focal abnormality. The design of the included studies seems applicable. The main problem involving the quality of included studies is insufficient reporting of study methods, resulting in unclear risk of bias for several of the quality domains assessed. Therefore, we considered the overall quality of the evidence as low. The summary estimate (11 studies reporting absence or presence of abnormality at 3D SIS) for sensitivity was 94.5% (95% confidence interval (CI) 90.6% to 96.9%) and for specificity 99.4% (95% CI 96.2% to 99.9%). Meta‐analysis of the eight studies (N = 716) directly comparing 2D SIS versus 3D SIS showed summary sensitivity of 96.9% (95% CI 91.9% to 98.8%) and summary specificity of 99.5% (95% CI 96.1% to 100%) for 3D SIS. For 2D SIS, summary sensitivity was 90.9% (95% CI 81.2% to 95.8%) and summary specificity was 96.3% (95% CI 86.1% to 99.1%). The difference in accuracy between 2D SIS and 3D SIS was non‐significant (P values of 0.07 for sensitivity and 0.10 for specificity). Authors' conclusions Low‐quality evidence suggests that 3D SIS may be very accurate in detecting intracavitary abnormalities. Meta‐analysis revealed no statistically significant differences between 2D SIS and 3D SIS. Summary sensitivity and summary specificity are higher for 3D SIS, but margins of improvement are limited because 2D SIS is already very accurate. When the technology and appropriate expertise are available, 3D SIS offers an alternative to 2D SIS. Both 2D SIS and 3D SIS should be considered alternatives to diagnostic hysteroscopy when intracavitary pathology is suspected in subfertile women and in those with abnormal uterine bleeding. Plain language summary Is three‐dimensional saline infusion sonography (3D SIS) better than two‐dimensional (2D) SIS for detecting polyps and fibroids? Review question Is three‐dimensional saline infusion sonography (3D SIS) better than two‐dimensional (2D) SIS for detecting polyps and fibroids? Background The womb (uterus) is one of the female reproductive organs. Inside the cavity of the womb, abnormalities such as polyps and fibroids can grow. Polyps and fibroids can cause problems such as abnormal menstrual bleeding and difficulty getting pregnant. The presence of these polyps and fibroids may be a reason for clinicians to start drug therapy or remove the polyps and fibroids during surgery. Ultrasonography can provide a picture of the womb and of possible fibroids or polyps. Saline or gel inside the cavity of the womb makes the ultrasound image more clear. This technique is called saline infusion sonography (SIS). Usually, this picture is only two‐dimensional. Nowadays, it is possible to make a three‐dimensional picture so the type of abnormality can be better seen. Study characteristics Review authors searched for studies published from inception until March 2016 and found 13 studies (in total 1053 women), eight of which directly compared 3D SIS versus 2D SIS. Data included all women reporting abnormal menstrual bleeding or difficulty getting pregnant. The number of patients in these studies varied from 23 to 180 women. Quality of the evidence In all studies, researchers checked the results of 2D SIS and 3D SIS against results obtained when a camera was used to look inside the womb (hysteroscopy); this is expected to give the true picture but is also more painful for the patient. All studies were performed in the usual way. Some studies did not report several items that might have influenced the results. For example, not all studies made it clear that the person evaluating the ultrasound pictures was unaware of the hysteroscopy results, and vice versa. The main problem involving the quality of included studies is insufficient reporting of study methods, resulting in unclear risk of bias for several of the quality domains assessed. Therefore, review authors considered the overall quality of the evidence as low. Key results Low‐quality evidence suggests that 3D SIS may be very accurate in detecting polyps and fibroids. Our analysis revealed no clear differences between 2D SIS and 3D SIS. Summary results are higher for 3D SIS but margins of improvement are limited because 2D SIS is already very accurate. Results show that 2D SIS missed a fibroid or polyp in 9 of 100 women and 3D SIS missed a polyp or fibroid in 3 of 100 women who had them. In 4 of 100 women, 2D SIS indicated the presence of polyps or fibroids when there were none, and in less than 1 in 100 women, 3D SIS was wrong. In theory, if both tests were used in a group of 1000 women with abnormal menstrual bleeding, 300 with fibroids or polyps, 27 of the 300 women with polyps/fibroids will be missed by 2D SIS, and 9 of 300 will be missed by 3D SIS. 3D SIS is an alternative to 2D SIS for which the technology and appropriate expertise are available. Both 2D SIS and 3D SIS should be considered alternatives to diagnostic hysteroscopy when intracavitary pathology is suspected in subfertile women and in those with abnormal uterine bleeding.","5","John Wiley & Sons, Ltd","1465-1858","Endometrium [*diagnostic imaging]; Female; Humans; Hysteroscopy [standards]; Leiomyoma [*diagnostic imaging]; Observational Studies as Topic; Polyps [*diagnostic imaging]; Prospective Studies; ROC Curve; Randomized Controlled Trials as Topic; Reference Standards; Sensitivity and Specificity; Sodium Chloride [*administration & dosage]; Solutions [administration & dosage]; Ultrasonography, Interventional [*methods, standards]; Uterine Diseases [*diagnostic imaging]; Uterine Hemorrhage [etiology]","10.1002/14651858.CD011126.pub2","http://dx.doi.org/10.1002/14651858.CD011126.pub2","Gynaecology and Fertility"
"CD008759.PUB2","Colli, A; Gana, JC; Yap, J; Adams‐Webber, T; Rashkovan, N; Ling, SC; Casazza, G","Platelet count, spleen length, and platelet count‐to‐spleen length ratio for the diagnosis of oesophageal varices in people with chronic liver disease or portal vein thrombosis","Cochrane Database of Systematic Reviews","2017","Abstract - Background Current guidelines recommend screening of people with oesophageal varices via oesophago‐gastro‐duodenoscopy at the time of diagnosis of hepatic cirrhosis. This requires that people repeatedly undergo unpleasant invasive procedures with their attendant risks, although half of these people have no identifiable oesophageal varices 10 years after the initial diagnosis of cirrhosis. Platelet count, spleen length, and platelet count‐to‐spleen length ratio are non‐invasive tests proposed as triage tests for the diagnosis of oesophageal varices. Objectives Primary objectives To determine the diagnostic accuracy of platelet count, spleen length, and platelet count‐to‐spleen length ratio for the diagnosis of oesophageal varices of any size in paediatric or adult patients with chronic liver disease or portal vein thrombosis, irrespective of aetiology. To investigate the accuracy of these non‐invasive tests as triage or replacement of oesophago‐gastro‐duodenoscopy. Secondary objectives To compare the diagnostic accuracy of these same tests for the diagnosis of high‐risk oesophageal varices in paediatric or adult patients with chronic liver disease or portal vein thrombosis, irrespective of aetiology. We aimed to perform pair‐wise comparisons between the three index tests, while considering predefined cut‐off values. We investigated sources of heterogeneity. Search methods The Cochrane Hepato‐Biliary Group Controlled Trials Register, the Cochrane Hepato‐Biliary Group Diagnostic Test Accuracy Studies Register, the Cochrane Library, MEDLINE (OvidSP), Embase (OvidSP), and Science Citation Index ‐ Expanded (Web of Science) (14 June 2016). We applied no language or document‐type restrictions. Selection criteria Studies evaluating the diagnostic accuracy of platelet count, spleen length, and platelet count‐to‐spleen length ratio for the diagnosis of oesophageal varices via oesophago‐gastro‐duodenoscopy as the reference standard in children or adults of any age with chronic liver disease or portal vein thrombosis, who did not have variceal bleeding. Data collection and analysis Standard Cochrane methods as outlined in the  Cochrane Handbook for Diagnostic Test of Accuracy Reviews. Main results We included 71 studies, 67 of which enrolled only adults and four only children. All included studies were cross‐sectional and were undertaken at a tertiary care centre. Eight studies reported study results in abstracts or letters. We considered all but one of the included studies to be at high risk of bias. We had major concerns about defining the cut‐off value for the three index tests; most included studies derived the best cut‐off values a posteriori, thus overestimating accuracy; 16 studies were designed to validate the 909 (n/mm 3) /mm cut‐off value for platelet count‐to‐spleen length ratio. Enrolment of participants was not consecutive in six studies and was unclear in 31 studies. Thirty‐four studies assessed enrolment consecutively. Eleven studies excluded some included participants from the analyses, and in only one study, the time interval between index tests and the reference standard was longer than three months. Diagnosis of varices of any size.  Platelet count showed sensitivity of 0.71 (95% confidence interval (CI) 0.63 to 0.77) and specificity of 0.80 (95% CI 0.69 to 0.88) (cut‐off value of around 150,000/mm 3  from 140,000 to 150,000/mm 3 ; 10 studies, 2054 participants). When examining potential sources of heterogeneity, we found that of all predefined factors, only aetiology had a role: studies including participants with chronic hepatitis C reported different results when compared with studies including participants with mixed aetiologies (P = 0.036). Spleen length showed sensitivity of 0.85 (95% CI 0.75 to 0.91) and specificity of 0.54 (95% CI 0.46 to 0.62) (cut‐off values of around 110 mm, from 110 to 112.5 mm; 13 studies, 1489 participants). Summary estimates for detection of varices of any size showed sensitivity of 0.93 (95% CI 0.83 to 0.97) and specificity of 0.84 (95% CI 0.75 to 0.91) in 17 studies, and 2637 participants had a cut‐off value for platelet count‐to‐spleen length ratio of 909 (n/mm 3 )/mm. We found no effect of predefined sources of heterogeneity. An overall indirect comparison of the HSROCs of the three index tests showed that platelet count‐to‐spleen length ratio was the most accurate index test when compared with platelet count (P < 0.001) and spleen length (P < 0.001). Diagnosis of varices at high risk of bleeding.  Platelet count showed sensitivity of 0.80 (95% CI 0.73 to 0.85) and specificity of 0.68 (95% CI 0.57 to 0.77) (cut‐off value of around 150,000/mm 3  from 140,000 to 160,000/mm 3 ; seven studies, 1671 participants). For spleen length, we obtained only a summary ROC curve as we found no common cut‐off between studies (six studies, 883 participants). Platelet count‐to‐spleen length ratio showed sensitivity of 0.85 (95% CI 0.72 to 0.93) and specificity of 0.66 (95% CI 0.52 to 0.77) (cut‐off value of around 909 (n/mm 3 )/mm; from 897 to 921 (n/mm 3 )/mm; seven studies, 642 participants). An overall indirect comparison of the HSROCs of the three index tests showed that platelet count‐to‐spleen length ratio was the most accurate index test when compared with platelet count (P = 0.003) and spleen length (P < 0.001). DIagnosis of varices of any size in children.  We found four studies including 277 children with different liver diseases and or portal vein thrombosis. Platelet count showed sensitivity of 0.71 (95% CI 0.60 to 0.80) and specificity of 0.83 (95% CI 0.70 to 0.91) (cut‐off value of around 115,000/mm 3 ; four studies, 277 participants). Platelet count‐to‐spleen length z‐score ratio showed sensitivity of 0.74 (95% CI 0.65 to 0.81) and specificity of 0.64 (95% CI 0.36 to 0.84) (cut‐off value of 25; two studies, 197 participants). Authors' conclusions Platelet count‐to‐spleen length ratio could be used to stratify the risk of oesophageal varices. This test can be used as a triage test before endoscopy, thus ruling out adults without varices. In the case of a ratio > 909 (n/mm 3 )/mm, the presence of oesophageal varices of any size can be excluded and only 7% of adults with varices of any size would be missed, allowing investigators to spare the number of oesophago‐gastro‐duodenoscopy examinations. This test is not accurate enough for identification of oesophageal varices at high risk of bleeding that require primary prophylaxis. Future studies should assess the diagnostic accuracy of this test in specific subgroups of patients, as well as its ability to predict variceal bleeding. New non‐invasive tests should be examined. Plain language summary Platelet count, spleen length, and platelet‐to‐spleen length ratio for the diagnosis of oesophageal varices in people with liver disease Background Hepatic cirrhosis is a severe disease with scars and nodules on the liver tissue. As a result, the normal function of the liver is impaired. Whatever the cause of cirrhosis, changes in the structure of and blood flow within the liver increase pressure in the portal vein (called portal vein hypertension), which is the vein that drains blood from the bowels to the liver. Portal hypertension induces dilatation (extension) of veins within the wall of the oesophagus (food pipe or gullet), which often rupture (break) with severe bleeding. Thus, when liver cirrhosis is diagnosed, an oesophago‐gastro‐duodenoscopy (OGD) is recommended to detect the presence of oesophageal varices (areas of abnormal dilatation of veins). During OGD, a small camera at the end of a tube is inserted down the oesophagus from the mouth and pictures are relayed back to a screen. Large varices or red signs on even small varices show high risks of rupture and bleeding. If high‐risk varices are found, treatment with beta‐blockers is effective in reducing the risk of bleeding. Three simple non‐invasive tests could be used to identify people with liver diease at high risk of having oesophageal varices: platelet count ‐ a simple laboratory test on a blood sample by which the number of platelets (a blood element ensuring coagulation) is measured; length (maximal diameter) of the spleen measured during ultrasound examination of the abdomen; and ratio of platelet count to spleen length. Study characteristics We searched scientific databases for clinical studies comparing platelet count, spleen length, or platelet count‐to‐spleen length ratio versus oesophago‐gastro‐duodenoscopy in detecting the presence of varices in children or adults with chronic liver disease or portal vein thrombosis (narrowing of the portal vein). The evidence is current to June 2016. Key results We found 25 studies with 5096 participants assessing the use of platelet count to diagnose the presence of varices and grade the risk of bleeding, and comparing platelet count versus oesophago‐gastro‐duodenoscopy in adults with cirrhosis: 13 studies with 1489 participants assessed the diagnostic ability of spleen length, and 38 studies with 5235 participants assessed the diagnostic ability of platelet count‐to‐spleen length ratio. Platelet count‐to‐spleen length ratio was the most accurate and could be used to identify people with liver disease who were at high risk of having oesophageal varices. Particularly, in people with hepatic cirrhosis among whom 580 out of 1000 people are expected to have oesophageal varices, only 41 (7% of 580) people will be missed as having varices and will have no appropriate preventive treatment or follow‐up. Thus, if platelet count‐to‐spleen length ratio is lower than 909 (n/mm 3 )/mm (the most used threshold), the presence of oesophageal varices can be excluded. Thus, it is possible to reduce the number of endoscopic examinations needed to find a person with oesophageal varices. On the contrary, this ratio is not accurate enough to replace endoscopy for identification of high risk of bleeding oesophageal varices. Quality of the evidence All but one study had problems of risk of bias involving mainly the definition of positive or negative index tests (platelet count, spleen length, and their ratio), which should be defined before and not after data analyses, and blinding of test results to the endoscopists who performed oesophago‐gastro‐duodenoscopy. Hence, these problems could impair the accuracy estimates of the three tests.","4","John Wiley & Sons, Ltd","1465-1858","*Duodenoscopy; *Platelet Count; *Portal Vein; Adult; Child; Chronic Disease; Esophageal and Gastric Varices [blood, *diagnosis, pathology]; Hepatitis C, Chronic [complications]; Humans; Liver Diseases [*complications]; Organ Size; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Spleen [*anatomy & histology]; Triage; Venous Thrombosis [*complications]","10.1002/14651858.CD008759.pub2","http://dx.doi.org/10.1002/14651858.CD008759.pub2","Hepato-Biliary"
"CD012010.PUB2","Rompianesi, G; Hann, A; Komolafe, O; Pereira, SP; Davidson, BR; Gurusamy, KS","Serum amylase and lipase and urinary trypsinogen and amylase for diagnosis of acute pancreatitis","Cochrane Database of Systematic Reviews","2017","Abstract - Background The treatment of people with acute abdominal pain differs if they have acute pancreatitis. It is important to know the diagnostic accuracy of serum amylase, serum lipase, urinary trypsinogen‐2, and urinary amylase for the diagnosis of acute pancreatitis, so that an informed decision can be made as to whether the person with abdominal pain has acute pancreatitis. There is currently no Cochrane review of the diagnostic test accuracy of serum amylase, serum lipase, urinary trypsinogen‐2, and urinary amylase for the diagnosis of acute pancreatitis. Objectives To compare the diagnostic accuracy of serum amylase, serum lipase, urinary trypsinogen‐2, and urinary amylase, either alone or in combination, in the diagnosis of acute pancreatitis in people with acute onset of a persistent, severe epigastric pain or diffuse abdominal pain. Search methods We searched MEDLINE, Embase, Science Citation Index Expanded, National Institute for Health Research (NIHR HTA and DARE), and other databases until March 2017. We searched the references of the included studies to identify additional studies. We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. We also performed a 'related search' and 'citing reference' search in MEDLINE and Embase. Selection criteria We included all studies that evaluated the diagnostic test accuracy of serum amylase, serum lipase, urinary trypsinogen‐2, and urinary amylase for the diagnosis of acute pancreatitis. We excluded case‐control studies because these studies are prone to bias. We accepted any of the following reference standards: biopsy, consensus conference definition, radiological features of acute pancreatitis, diagnosis of acute pancreatitis during laparotomy or autopsy, and organ failure. At least two review authors independently searched and screened the references located by the search to identify relevant studies. Data collection and analysis Two review authors independently extracted data from the included studies. The thresholds used for the diagnosis of acute pancreatitis varied in the trials, resulting in sparse data for each index test. Because of sparse data, we used ‐2 log likelihood values to determine which model to use for meta‐analysis. We calculated and reported the sensitivity, specificity, post‐test probability of a positive and negative index test along with 95% confidence interval (CI) for each cutoff, but have reported only the results of the recommended cutoff of three times normal for serum amylase and serum lipase, and the manufacturer‐recommended cutoff of 50 mg/mL for urinary trypsinogen‐2 in the abstract. Main results Ten studies including 5056 participants met the inclusion criteria for this review and assessed the diagnostic accuracy of the index tests in people presenting to the emergency department with acute abdominal pain. The risk of bias was unclear or high for all of the included studies. The study that contributed approximately two‐thirds of the participants included in this review was excluded from the results of the analysis presented below due to major concerns about the participants included in the study. We have presented only the results where at least two studies were included in the analysis. Serum amylase, serum lipase, and urinary trypsinogen‐2 at the standard threshold levels of more than three times normal for serum amylase and serum lipase, and a threshold of 50 ng/mL for urinary trypsinogen‐2 appear to have similar sensitivities (0.72 (95% CI 0.59 to 0.82); 0.79 (95% CI 0.54 to 0.92); and 0.72 (95% CI 0.56 to 0.84), respectively) and specificities (0.93 (95% CI 0.66 to 0.99); 0.89 (95% CI 0.46 to 0.99); and 0.90 (95% CI 0.85 to 0.93), respectively). At the median prevalence of 22.6% of acute pancreatitis in the studies, out of 100 people with positive test, serum amylase (more than three times normal), serum lipase (more than three times normal), and urinary trypsinogen (more than 50 ng/mL), 74 (95% CI 33 to 94); 68 (95% CI 21 to 94); and 67 (95% CI 57 to 76) people have acute pancreatitis, respectively; out of 100 people with negative test, serum amylase (more than three times normal), serum lipase (more than three times normal), and urinary trypsinogen (more than 50 ng/mL), 8 (95% CI 5 to 12); 7 (95% CI 3 to 15); and 8 (95% CI 5 to 13) people have acute pancreatitis, respectively. We were not able to compare these tests formally because of sparse data. Authors' conclusions As about a quarter of people with acute pancreatitis fail to be diagnosed as having acute pancreatitis with the evaluated tests, one should have a low threshold to admit the patient and treat them for acute pancreatitis if the symptoms are suggestive of acute pancreatitis, even if these tests are normal. About 1 in 10 patients without acute pancreatitis may be wrongly diagnosed as having acute pancreatitis with these tests, therefore it is important to consider other conditions that require urgent surgical intervention, such as perforated viscus, even if these tests are abnormal. The diagnostic performance of these tests decreases even further with the progression of time, and one should have an even lower threshold to perform additional investigations if the symptoms are suggestive of acute pancreatitis. Plain language summary Blood and urine tests for the diagnosis of acute pancreatitis (sudden inflammation of pancreas) Background The pancreas is an organ in the abdomen (tummy) that secretes several digestive enzymes (substances that break down the food we eat) into the pancreatic ductal system, which empties into the small bowel. The pancreas also contains the islets of Langerhans, which secrete several hormones such as insulin (which helps regulate blood sugar). Acute pancreatitis is sudden inflammation of the pancreas, which can lead to damage of the heart, lungs, and kidneys and cause them to fail. Acute pancreatitis usually manifests as upper abdominal pain radiating to the back. However, there are several potential causes of upper abdominal pain. It is important to determine if someone with abdominal pain has acute pancreatitis or another illness in order to start appropriate treatment. Blood tests such as serum amylase and serum lipase, as well as urine tests such as urinary trypsinogen‐2 and urinary amylase, can be used to determine if someone with abdominal pain has acute pancreatitis. It is usually the case that a patient is considered to have acute pancreatitis only when amylase or lipase levels are three times the upper limit of normal. With regard to urinary trypsinogen‐2, a level of more than 50 ng/mL of trypsinogen‐2 in the urine is considered an indication of acute pancreatitis. With regard to urinary amylase, there is no clear‐cut level beyond which someone with abdominal pain is considered to have acute pancreatitis. At present it is unclear whether these tests are equally effective or if one of the tests is better than the other in the diagnosis of acute pancreatitis in people with sudden‐onset abdominal pain. We determined to resolve this question by performing a literature search for studies reporting the accuracy of the above mentioned blood and urine tests. We included studies reported until 20 March 2017. Study characteristics We identified 10 studies reporting information on 5056 people with abdominal pain that started suddenly. The studies included pancreatitis due to all causes. Quality of evidence All of the studies were of unclear or low methodological quality, which may result in arriving at false conclusions. We excluded the study that contributed approximately two‐thirds of the participants included in this review from the results of the analysis presented below due to concerns about whether the participants included in the study are typical of those seen in the emergency department. Key results The accuracy of serum amylase, serum lipase, and urinary trypsinogen‐2 in making the diagnosis of acute pancreatitis was similar. About a quarter of people with acute pancreatitis fail to be diagnosed as having acute pancreatitis with these tests. The patient should be admitted and treated as having acute pancreatitis, even if these tests are normal, if there is a suspicion of acute pancreatitis. As about 1 in 10 patients without acute pancreatitis may be wrongly diagnosed as having acute pancreatitis with these tests, it is important to consider other conditions that require urgent surgery, even if these tests are abnormal. The diagnostic performance of these tests decreases even further with the progression of time, and additional investigations should be performed if there is a suspicion of acute pancreatitis.","4","John Wiley & Sons, Ltd","1465-1858","Acute Disease; Amylases [*blood, *urine]; Biomarkers [blood, urine]; Diagnostic Errors [statistics & numerical data]; Humans; Lipase [*blood]; Pancreatitis [*diagnosis]; Trypsin [blood, urine]; Trypsinogen [blood, *urine]","10.1002/14651858.CD012010.pub2","http://dx.doi.org/10.1002/14651858.CD012010.pub2","Gut"
"CD012645","Komolafe, O; Pereira, SP; Davidson, BR; Gurusamy, KS","Serum C‐reactive protein, procalcitonin, and lactate dehydrogenase for the diagnosis of pancreatic necrosis","Cochrane Database of Systematic Reviews","2017","Abstract - Background The treatment of people with pancreatic necrosis differs from that of people with oedematous pancreatitis. It is important to know the diagnostic accuracy of serum C‐reactive protein (CRP), serum procalcitonin, and serum lactate dehydrogenase (LDH) as a triage test for the detection of pancreatic necrosis in people with acute pancreatitis, so that an informed decision can be made as to whether the person with pancreatic necrosis needs further investigations such as computed tomography (CT) scan or magnetic resonance imaging (MRI) scan and treatment for pancreatic necrosis started. There is currently no standard clinical practice, although CRP, particularly an increasing trend of CRP, is often used as a triage test to determine whether the person requires further imaging. There is also currently no systematic review of the diagnostic test accuracy of CRP, procalcitonin, and LDH for the diagnosis of pancreatic necrosis in people with acute pancreatitis. Objectives To compare the diagnostic accuracy of CRP, procalcitonin, or LDH (index test), either alone or in combination, in the diagnosis of necrotising pancreatitis in people with acute pancreatitis and without organ failure. Search methods We searched MEDLINE, Embase, Science Citation Index Expanded, National Institute for Health Research (NIHR HTA and DARE), and other databases until March 2017. We searched the references of the included studies to identify additional studies. We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. We also performed a 'related search' and 'citing reference' search in MEDLINE and Embase. Selection criteria We included all studies that evaluated the diagnostic test accuracy of CRP, procalcitonin, and LDH for the diagnosis of pancreatic necrosis in people with acute pancreatitis using the following reference standards, either alone or in combination: radiological features of pancreatic necrosis (contrast‐enhanced CT or MRI), surgeon's judgement of pancreatic necrosis during surgery, or histological confirmation of pancreatic necrosis. Had we found case‐control studies, we planned to exclude them because they are prone to bias; however, we did not locate any. Two review authors independently identified the relevant studies from the retrieved references. Data collection and analysis Two review authors independently extracted data, including methodological quality assessment, from the included studies. As the included studies reported CRP, procalcitonin, and LDH on different days of admission and measured at different cut‐off levels, it was not possible to perform a meta‐analysis using the bivariate model as planned. We have reported the sensitivity, specificity, post‐test probability of a positive and negative index test along with 95% confidence interval (CI) on each of the different days of admission and measured at different cut‐off levels. Main results A total of three studies including 242 participants met the inclusion criteria for this review. One study reported the diagnostic performance of CRP for two threshold levels (> 200 mg/L and > 279 mg/L) without stating the day on which the CRP was measured. One study reported the diagnostic performance of procalcitonin on day 1 (1 day after admission) using a threshold level of 0.5 ng/mL. One study reported the diagnostic performance of CRP on day 3 (3 days after admission) using a threshold level of 140 mg/L and LDH on day 5 (5 days after admission) using a threshold level of 290 U/L. The sensitivities and specificities varied: the point estimate of the sensitivities ranged from 0.72 to 0.88, while the point estimate of the specificities ranged from 0.75 to 1.00 for the different index tests on different days of hospital admission. However, the confidence intervals were wide: confidence intervals of sensitivities ranged from 0.51 to 0.97, while those of specificities ranged from 0.18 to 1.00 for the different tests on different days of hospital admission. Overall, none of the tests assessed in this review were sufficiently accurate to suggest that they could be useful in clinical practice. Authors' conclusions The paucity of data and methodological deficiencies in the studies meant that it was not possible to arrive at any conclusions regarding the diagnostic test accuracy of the index test because of the uncertainty of the results. Further well‐designed diagnostic test accuracy studies with prespecified index test thresholds of CRP, procalcitonin, LDH; appropriate follow‐up (for at least two weeks to ensure that the person does not have pancreatic necrosis, as early scans may not indicate pancreatic necrosis); and clearly defined reference standards (of surgical or radiological confirmation of pancreatic necrosis) are important to reliably determine the diagnostic accuracy of CRP, procalcitonin, and LDH. Plain language summary Blood tests for the diagnosis of pancreatic necrosis (pancreatic destruction due to inflammation of pancreas) Background The pancreas is an organ in the abdomen (tummy) that secretes several digestive enzymes (substances that break down the food that we eat) into the pancreatic ductal system, which empties into the small bowel. The pancreas also contains the islets of Langerhans, which secrete several hormones including insulin (which helps regulate blood sugar). Acute pancreatitis is a sudden inflammation of the pancreas that can lead to destruction of the pancreas (pancreatic necrosis). The treatment of people with pancreatic necrosis differs from that of people without pancreatic necrosis. Blood tests such as C‐reactive protein (CRP), procalcitonin, and lactate dehydrogenase (LDH) may be used to find out whether a person with acute pancreatitis has pancreatic necrosis. This is usually followed by CT scan to confirm that the person has pancreatic necrosis. If the person is found to have pancreatic necrosis, the intensity of care is increased and additional treatments are performed as required. At present it is unclear whether measuring the levels of CRP, procalcitonin, or LDH is useful in identifying pancreatic necrosis. Study characteristics We performed a thorough literature search for studies reporting the accuracy of CRP, procalcitonin, or LDH in identifying pancreatic necrosis. We included studies reported until 20 March 2017. We identified three studies reporting information on 242 people with pancreatitis. The studies included pancreatitis due to all causes. Key results Variations in when the studies carried out the blood tests and what level was considered abnormal meant that we were unable to combine the data to provide the overall results. It was not possible to arrive at any firm conclusions about how accurate the tests are for the following reasons. • The studies included few participants. As a result, there was significant uncertainty in the results. • The studies were of poor methodological quality, which introduced additional uncertainty in the results. • For the results to be trusted, they must be reproduced in another group of participants. Since this was not done, there was uncertainty in the results. Quality of evidence All of the studies were of unclear or low methodological quality, which may result in arriving at false conclusions.","4","John Wiley & Sons, Ltd","1465-1858","Acute Disease; Biomarkers [blood]; Calcitonin [*blood]; Confidence Intervals; C‐Reactive Protein [*analysis]; Diagnosis, Differential; Humans; L‐Lactate Dehydrogenase [*blood]; Pancreatitis [blood, diagnosis, enzymology]; Pancreatitis, Acute Necrotizing [blood, *diagnosis, enzymology]; Sensitivity and Specificity","10.1002/14651858.CD012645","http://dx.doi.org/10.1002/14651858.CD012645","Gut"
"CD010213.PUB2","Best, LMJ; Rawji, V; Pereira, SP; Davidson, BR; Gurusamy, KS","Imaging modalities for characterising focal pancreatic lesions","Cochrane Database of Systematic Reviews","2017","Abstract - Background Increasing numbers of incidental pancreatic lesions are being detected each year. Accurate characterisation of pancreatic lesions into benign, precancerous, and cancer masses is crucial in deciding whether to use treatment or surveillance. Distinguishing benign lesions from precancerous and cancerous lesions can prevent patients from undergoing unnecessary major surgery. Despite the importance of accurately classifying pancreatic lesions, there is no clear algorithm for management of focal pancreatic lesions. Objectives To determine and compare the diagnostic accuracy of various imaging modalities in detecting cancerous and precancerous lesions in people with focal pancreatic lesions. Search methods We searched the CENTRAL, MEDLINE, Embase, and Science Citation Index until 19 July 2016. We searched the references of included studies to identify further studies. We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. Selection criteria We planned to include studies reporting cross‐sectional information on the index test (CT (computed tomography), MRI (magnetic resonance imaging), PET (positron emission tomography), EUS (endoscopic ultrasound), EUS elastography, and EUS‐guided biopsy or FNA (fine‐needle aspiration)) and reference standard (confirmation of the nature of the lesion was obtained by histopathological examination of the entire lesion by surgical excision, or histopathological examination for confirmation of precancer or cancer by biopsy and clinical follow‐up of at least six months in people with negative index tests) in people with pancreatic lesions irrespective of language or publication status or whether the data were collected prospectively or retrospectively. Data collection and analysis Two review authors independently searched the references to identify relevant studies and extracted the data. We planned to use the bivariate analysis to calculate the summary sensitivity and specificity with their 95% confidence intervals and the hierarchical summary receiver operating characteristic (HSROC) to compare the tests and assess heterogeneity, but used simpler models (such as univariate random‐effects model and univariate fixed‐effect model) for combining studies when appropriate because of the sparse data. We were unable to compare the diagnostic performance of the tests using formal statistical methods because of sparse data. Main results We included 54 studies involving a total of 3,196 participants evaluating the diagnostic accuracy of various index tests. In these 54 studies, eight different target conditions were identified with different final diagnoses constituting benign, precancerous, and cancerous lesions. None of the studies was of high methodological quality. None of the comparisons in which single studies were included was of sufficiently high methodological quality to warrant highlighting of the results. For differentiation of cancerous lesions from benign or precancerous lesions, we identified only one study per index test. The second analysis, of studies differentiating cancerous versus benign lesions, provided three tests in which meta‐analysis could be performed. The sensitivities and specificities for diagnosing cancer were: EUS‐FNA: sensitivity 0.79 (95% confidence interval (CI) 0.07 to 1.00), specificity 1.00 (95% CI 0.91 to 1.00); EUS: sensitivity 0.95 (95% CI 0.84 to 0.99), specificity 0.53 (95% CI 0.31 to 0.74); PET: sensitivity 0.92 (95% CI 0.80 to 0.97), specificity 0.65 (95% CI 0.39 to 0.84). The third analysis, of studies differentiating precancerous or cancerous lesions from benign lesions, only provided one test (EUS‐FNA) in which meta‐analysis was performed. EUS‐FNA had moderate sensitivity for diagnosing precancerous or cancerous lesions (sensitivity 0.73 (95% CI 0.01 to 1.00) and high specificity 0.94 (95% CI 0.15 to 1.00), the extremely wide confidence intervals reflecting the heterogeneity between the studies). The fourth analysis, of studies differentiating cancerous (invasive carcinoma) from precancerous (dysplasia) provided three tests in which meta‐analysis was performed. The sensitivities and specificities for diagnosing invasive carcinoma were: CT: sensitivity 0.72 (95% CI 0.50 to 0.87), specificity 0.92 (95% CI 0.81 to 0.97); EUS: sensitivity 0.78 (95% CI 0.44 to 0.94), specificity 0.91 (95% CI 0.61 to 0.98); EUS‐FNA: sensitivity 0.66 (95% CI 0.03 to 0.99), specificity 0.92 (95% CI 0.73 to 0.98). The fifth analysis, of studies differentiating cancerous (high‐grade dysplasia or invasive carcinoma) versus precancerous (low‐ or intermediate‐grade dysplasia) provided six tests in which meta‐analysis was performed. The sensitivities and specificities for diagnosing cancer (high‐grade dysplasia or invasive carcinoma) were: CT: sensitivity 0.87 (95% CI 0.00 to 1.00), specificity 0.96 (95% CI 0.00 to 1.00); EUS: sensitivity 0.86 (95% CI 0.74 to 0.92), specificity 0.91 (95% CI 0.83 to 0.96); EUS‐FNA: sensitivity 0.47 (95% CI 0.24 to 0.70), specificity 0.91 (95% CI 0.32 to 1.00); EUS‐FNA carcinoembryonic antigen 200 ng/mL: sensitivity 0.58 (95% CI 0.28 to 0.83), specificity 0.51 (95% CI 0.19 to 0.81); MRI: sensitivity 0.69 (95% CI 0.44 to 0.86), specificity 0.93 (95% CI 0.43 to 1.00); PET: sensitivity 0.90 (95% CI 0.79 to 0.96), specificity 0.94 (95% CI 0.81 to 0.99). The sixth analysis, of studies differentiating cancerous (invasive carcinoma) from precancerous (low‐grade dysplasia) provided no tests in which meta‐analysis was performed. The seventh analysis, of studies differentiating precancerous or cancerous (intermediate‐ or high‐grade dysplasia or invasive carcinoma) from precancerous (low‐grade dysplasia) provided two tests in which meta‐analysis was performed. The sensitivity and specificity for diagnosing cancer were: CT: sensitivity 0.83 (95% CI 0.68 to 0.92), specificity 0.83 (95% CI 0.64 to 0.93) and MRI: sensitivity 0.80 (95% CI 0.58 to 0.92), specificity 0.81 (95% CI 0.53 to 0.95), respectively. The eighth analysis, of studies differentiating precancerous or cancerous (intermediate‐ or high‐grade dysplasia or invasive carcinoma) from precancerous (low‐grade dysplasia) or benign lesions provided no test in which meta‐analysis was performed. There were no major alterations in the subgroup analysis of cystic pancreatic focal lesions (42 studies; 2086 participants). None of the included studies evaluated EUS elastography or sequential testing. Authors' conclusions We were unable to arrive at any firm conclusions because of the differences in the way that study authors classified focal pancreatic lesions into cancerous, precancerous, and benign lesions; the inclusion of few studies with wide confidence intervals for each comparison; poor methodological quality in the studies; and heterogeneity in the estimates within comparisons. Plain language summary Accuracy of different imaging techniques for determining whether a pancreatic tumour is cancerous Background The pancreas is an organ in the abdomen that secretes pancreatic juice, which aids digestion and contains cells that produce important hormones such as insulin. Increasingly, abnormalities in the pancreas are noted in people undergoing routine scans, such as ultrasound or computed tomography (CT) scans, in the form of what are known as 'shadows', which may be described as focal pancreatic lesion, pancreatic mass, pancreatic tumour, pancreatic cyst, or pancreatic nodule. A significant proportion of focal pancreatic lesions are benign (non‐cancerous) lesions requiring no treatment. Surgical removal of the tumour is the main method of treatment for precancerous (i.e. focal pancreatic lesions that are not full‐blown cancer and do not have the ability to spread like cancer, but can turn into cancer) and cancerous focal pancreatic lesions. New methods are being developed for treating precancerous lesions, such as using heat to destroy the tumour. Surgical removal remains the only potentially curative treatment for people with limited pancreatic cancer. It is thus important to characterise whether a focal pancreatic lesion is non‐cancerous, precancerous, or cancerous. A number of scans are available for characterising the nature of the focal pancreatic lesion, which include the following. • Computed tomography (CT) scan: a series of X‐rays taken from different angles, which are then reconstructed using a computer. • Magnetic resonance imaging (MRI): the use of a powerful magnet to produce images of different tissues of the body. • Positron emission tomography (PET): the use of a small amount of radioactive glucose (sugar) to differentiate between different tissues. It takes advantage of the tendency of cancer cells to use more glucose than normal cells. • Endoscopic ultrasound (also known as endosonography or EUS): the use of an endoscope, a camera introduced into a body cavity to view the inside of the body. An ultrasound (high‐energy sound waves) probe at the end of the endoscope is used to differentiate tissues. • EUS elastography: this measures the stiffness of the lesion, which is used to identify whether the lesion is cancerous. • EUS‐guided biopsy: the removal of cells or tissues for examination under a microscope or to perform other tests on the cells or tissue. At present it is unclear how effective different scans are in characterising focal pancreatic lesions. Study characteristics We performed a thorough literature search for studies reporting the accuracy of different scans until 19 July 2016. We identified 54 studies reporting information on 3196 people with focal pancreatic lesions. These studies evaluated one or more of the above tests and compared these test results with the eventual diagnosis provided by surgical removal of the lesion and examination under microscope. There were no diagnostic test accuracy studies of EUS elastography or studies that looked at multiple scans rather than single scans. Key results Variations in how studies defined precancerous and cancerous lesions meant that we were not able to combine the data to provide the overall results for many tests. We were unable to arrive at any firm conclusions for the following reasons. • The way that study authors classified focal pancreatic lesions into cancerous, precancerous, and benign lesions was not consistent in different studies. • The studies included few participants, leading to significant uncertainty in the results. • The studies were of poor methodological quality, which introduced additional uncertainty in the results. • Even among the studies that classified focal pancreatic lesions into cancerous, precancerous, and benign lesions in a similar manner, the results were not consistent. Quality of evidence All of the studies were of low methodological quality, which may result in arriving at false conclusions.","4","John Wiley & Sons, Ltd","1465-1858","Diagnostic Imaging [*methods]; Elasticity Imaging Techniques; Endoscopic Ultrasound‐Guided Fine Needle Aspiration; Endosonography; Humans; Magnetic Resonance Imaging; Pancreatic Diseases [diagnostic imaging]; Pancreatic Neoplasms [*diagnostic imaging]; Positron‐Emission Tomography; Precancerous Conditions [*diagnostic imaging]; Sensitivity and Specificity; Tomography, X‐Ray Computed","10.1002/14651858.CD010213.pub2","http://dx.doi.org/10.1002/14651858.CD010213.pub2","Gut"
"CD012009.PUB2","Davidson, TBU; Yaghoobi, M; Davidson, BR; Gurusamy, KS","Amylase in drain fluid for the diagnosis of pancreatic leak in post‐pancreatic resection","Cochrane Database of Systematic Reviews","2017","Abstract - Background The treatment of people with clinically significant postoperative pancreatic leaks is different from those without clinically significant pancreatic leaks. It is important to know the diagnostic accuracy of drain fluid amylase as a triage test for the detection of clinically significant pancreatic leaks, so that an informed decision can be made as to whether the patient with a suspected pancreatic leak needs further investigations and treatment. There is currently no systematic review of the diagnostic test accuracy of drain fluid amylase for the diagnosis of clinically relevant pancreatic leak. Objectives To determine the diagnostic accuracy of amylase in drain fluid at 48 hours or more for the diagnosis of pancreatic leak in people who had undergone pancreatic resection. Search methods We searched MEDLINE, Embase, the Science Citation Index Expanded, and the National Institute for Health Research Health Technology Assessment (NIHR HTA) websites up to 20 February 2017. We searched the references of the included studies to identify additional studies. We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. We also performed a 'related search' and 'citing reference' search in MEDLINE and Embase. Selection criteria We included all studies that evaluated the diagnostic test accuracy of amylase in the drain fluid at 48 hours or more for the diagnosis of pancreatic leak in people who had undergone pancreatic resection excluding total pancreatectomy. We planned to exclude case‐control studies because these studies are prone to bias, but did not find any. At least two authors independently searched and screened the references produced by the search to identify relevant studies. Data collection and analysis Two review authors independently extracted data from the included studies. The included studies reported drain fluid amylase on different postoperative days and measured at different cut‐off levels, so it was not possible to perform a meta‐analysis using the bivariate model as planned. We have reported the sensitivity, specificity, post‐test probability of a positive and negative drain fluid amylase along with 95% confidence interval (CI) on each of the different postoperative days and measured at different cut‐off levels. Main results A total of five studies including 868 participants met the inclusion criteria for this review. The five studies included in this review reported the value of drain fluid amylase at different thresholds and different postoperative days. The sensitivities and specificities were variable; the sensitivities ranged between 0.72 and 1.00 while the specificities ranged between 0.73 and 0.99 for different thresholds on different postoperative days. At the median prevalence (pre‐test probability) of 15.9%, the post‐test probabilities for pancreatic leak ranged between 35.9% and 95.4% for a positive drain fluid amylase test and ranged between 0% and 5.5% for a negative drain fluid amylase test. None of the studies used the reference standard of confirmation by surgery or by a combination of surgery and clinical follow‐up, but used the International Study Group on Pancreatic Fistula (ISGPF) grade B and C as the reference standard. The overall methodological quality was unclear or high in all the studies. Authors' conclusions Because of the paucity of data and methodological deficiencies in the studies, we are uncertain whether drain fluid amylase should be used as a method for testing for pancreatic leak in an unselected population after pancreatic resection; and we judge that the optimal cut‐off of drain fluid amylase for making the diagnosis of pancreatic leak is also not clear. Further well‐designed diagnostic test accuracy studies with pre‐specified index test threshold of drain fluid amylase (at three times more on postoperative day 5 or another suitable pre‐specified threshold), appropriate follow‐up (for at least six to eight weeks to ensure that there are no pancreatic leaks), and clearly defined reference standards (of surgical, clinical, and radiological confirmation of pancreatic leak) are important to reliably determine the diagnostic accuracy of drain fluid amylase in the diagnosis of pancreatic leak. Plain language summary Amylase in drain fluid for the diagnosis of pancreatic leak after partial removal of the pancreas Background The pancreas is an organ in the abdomen that secretes pancreatic juice that aids digestion; and it contains cells that produce important hormones such as insulin. Partial removal of the pancreas (pancreatic resection) is performed to remove cancerous and non‐cancerous growths in the pancreas. During this process, new connections (anastomoses) are made between the pancreas and intestines and bile duct (a tube that transports bile from the liver to the intestines). These connections may break down and result in leakage of pancreatic content into the abdomen; this can lead to severe infections within the abdomen and in the blood stream, which can even lead to the death of the patient. At the end of the operation, a drainage tube is inserted into the abdomen for two purposes: firstly, the detection of any fluid collections within the abdomen (intra‐abdominal collections), usually resulting from the pancreatic leaks; and secondly, as the treatment of intra‐abdominal collections, so that fluid collection decreases or, at least, does not worsen within the abdomen. The fluids from the drain can be tested for amylase (one of the contents of the pancreatic juice which digests carbohydrates) to find out whether the fluid in the drain is because of a pancreatic leak. If there is a high suspicion of a pancreatic leak, further scans are performed to confirm it or to rule it out. If the leak is major and the patient is unwell, urgent reoperation may be required. Moderate leaks can lead to intra‐abdominal infections: patients may need antibiotics, drugs that decrease pancreatic secretion, insertion of a new drainage tube or repositioning of the existing drainage tube to drain the infected collection, and supportive care to recover. Currently, it is unclear whether measuring the amylase content in the fluid from the drain inserted after pancreatic resection is useful in identifying pancreatic leaks. Study characteristics We performed a thorough literature search for studies reporting the accuracy of drain fluid amylase in identifying pancreatic leaks. We included studies reported up to 20 February 2017. We identified five studies reporting information on 868 people who underwent pancreatic resections for cancer and non‐cancerous growths. Most studies included only people in whom the head of the pancreas (right side of the pancreas) was removed. Key results Variations in when the studies measured the amylase content in the drain and what level was considered abnormal meant that we were not able to combine the data to provide the overall results. We are uncertain whether drain fluid amylase is useful in identifying pancreatic leaks because of the following reasons. 1. The way that study authors confirmed that a participant had or did not have pancreatic leak was itself subject to error (i.e. there was no true 'gold standard'). 2. The studies included few participants. As a result, there was significant uncertainty in the results. 3. The studies were of poor methodological quality. This introduced additional uncertainty in the results. Quality of evidence All of the studies were of unclear or low methodological quality, which may result in arriving at false conclusions.","4","John Wiley & Sons, Ltd","1465-1858","*Pancreatectomy; *Pancreaticoduodenectomy; Aged; Amylases [*analysis]; Anastomotic Leak [*diagnosis]; Biomarkers [analysis]; Clinical Enzyme Tests [*standards]; Drainage; Female; Humans; Male; Middle Aged; Pancreas [surgery]; Prospective Studies; Retrospective Studies; Sensitivity and Specificity","10.1002/14651858.CD012009.pub2","http://dx.doi.org/10.1002/14651858.CD012009.pub2","Gut"
"CD010803.PUB2","Ritchie, C; Smailagic, N; Noel‐Storr, AH; Ukoumunne, O; Ladds, EC; Martin, S","CSF tau and the CSF tau/ABeta ratio for the diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2017","Abstract - Background Research suggests that measurable change in cerebrospinal fluid (CSF) biomarkers occurs years in advance of the onset of clinical symptoms ( Beckett 2010 ). In this review, we aimed to assess the ability of CSF tau biomarkers (t‐tau and p‐tau) and the CSF tau (t‐tau or p‐tau)/ABeta ratio to enable the detection of Alzheimer’s disease pathology in patients with mild cognitive impairment (MCI). These biomarkers have been proposed as important in new criteria for Alzheimer's disease dementia that incorporate biomarker abnormalities. Objectives To determine the diagnostic accuracy of 1) CSF t‐tau, 2) CSF p‐tau, 3) the CSF t‐tau/ABeta ratio and 4) the CSF p‐tau/ABeta ratio index tests for detecting people with MCI at baseline who would clinically convert to Alzheimer’s disease dementia or other forms of dementia at follow‐up. Search methods The most recent search for this review was performed in January 2013. We searched MEDLINE (OvidSP), Embase (OvidSP), BIOSIS Previews (Thomson Reuters Web of Science), Web of Science Core Collection, including Conference Proceedings Citation Index (Thomson Reuters Web of Science), PsycINFO (OvidSP), and LILACS (BIREME). We searched specialized sources of diagnostic test accuracy studies and reviews. We checked reference lists of relevant studies and reviews for additional studies. We contacted researchers for possible relevant but unpublished data. We did not apply any language or data restriction to the electronic searches. We did not use any methodological filters as a method to restrict the search overall. Selection criteria We selected those studies that had prospectively well‐defined cohorts with any accepted definition of MCI and with CSF t‐tau or p‐tau and CSF tau (t‐tau or p‐tau)/ABeta ratio values, documented at or around the time the MCI diagnosis was made. We also included studies which looked at data from those cohorts retrospectively, and which contained sufficient data to construct two by two tables expressing those biomarker results by disease status. Moreover, studies were only selected if they applied a reference standard for Alzheimer's disease dementia diagnosis, for example, the NINCDS‐ADRDA or Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM‐IV) criteria. Data collection and analysis We screened all titles generated by the electronic database searches. Two review authors independently assessed the abstracts of all potentially relevant studies, and the full papers for eligibility. Two independent assessors performed data extraction and quality assessment. Where data allowed, we derived estimates of sensitivity at fixed values of specificity from the model we fitted to produce the summary receiver operating characteristic (ROC) curve. Main results In total, 1282 participants with MCI at baseline were identified in the 15 included studies of which 1172 had analysable data; 430 participants converted to Alzheimer’s disease dementia and 130 participants to other forms of dementia. Follow‐up ranged from less than one year to over four years for some participants, but in the majority of studies was in the range one to three years. Conversion to Alzheimer’s disease dementia The accuracy of the CSF t‐tau was evaluated in seven studies (291 cases and 418 non‐cases).The sensitivity values ranged from 51% to 90% while the specificity values ranged from 48% to 88%. At the median specificity of 72%, the estimated sensitivity was 75% (95% CI 67 to 85), the positive likelihood ratio was 2.72 (95% CI 2.43 to 3.04), and the negative likelihood ratio was 0.32 (95% CI 0.22 to 0.47). Six studies (164 cases and 328 non‐cases) evaluated the accuracy of the CSF p‐tau. The sensitivities were between 40% and 100% while the specificities were between 22% and 86%. At the median specificity of 47.5%, the estimated sensitivity was 81% (95% CI: 64 to 91), the positive likelihood ratio was 1.55 (CI 1.31 to 1.84), and the negative likelihood ratio was 0.39 (CI: 0.19 to 0.82). Five studies (140 cases and 293 non‐cases) evaluated the accuracy of the CSF p‐tau/ABeta ratio. The sensitivities were between 80% and 96% while the specificities were between 33% and 95%. We did not conduct a meta‐analysis because the studies were few and small. Only one study reported the accuracy of CSF t‐tau/ABeta ratio. Our findings are based on studies with poor reporting. A significant number of studies had unclear risk of bias for the reference standard, participant selection and flow and timing domains. According to the assessment of index test domain, eight of 15 studies were of poor methodological quality. The accuracy of these CSF biomarkers for ‘other dementias’ had not been investigated in the included primary studies. Investigation of heterogeneity The main sources of heterogeneity were thought likely to be reference standards used for the target disorders, sources of recruitment, participant sampling, index test methodology and aspects of study quality (particularly, inadequate blinding). We were not able to formally assess the effect of each potential source of heterogeneity as planned, due to the small number of studies available to be included. Authors' conclusions The insufficiency and heterogeneity of research to date primarily leads to a state of uncertainty regarding the value of CSF testing of t‐tau, p‐tau or p‐tau/ABeta ratio for the diagnosis of Alzheimer's disease in current clinical practice. Particular attention should be paid to the risk of misdiagnosis and overdiagnosis of dementia (and therefore over‐treatment) in clinical practice. These tests, like other biomarker tests which have been subject to Cochrane DTA reviews, appear to have better sensitivity than specificity and therefore might have greater utility in ruling out Alzheimer's disease as the aetiology to the individual's evident cognitive impairment, as opposed to ruling it in. The heterogeneity observed in the few studies awaiting classification suggests our initial summary will remain valid. However, these tests may have limited clinical value until uncertainties have been addressed. Future studies with more uniformed approaches to thresholds, analysis and study conduct may provide a more homogenous estimate than the one that has been available from the included studies we have identified. Plain language summary Proteins in cerebrospinal fluids (CSF) for early prediction of developing Alzheimer’s disease or other dementia in people with mild cognitive problems Background The numbers of people with dementia and other cognitive problems are increasing globally. A diagnosis of dementia at early stage is recommended but there is no agreement on the best approach. A range of tests have been developed which healthcare professionals can use to assess people with poor memory or cognitive impairment. In this review, we have focused on the cerebrospinal fluid (CSF) diagnostic tests. Review question We reviewed the evidence about the accuracy of CSF tests in identifying those people presenting with mild cognitive impairment (MCI) who would develop Alzheimer’s disease dementia or other forms of dementia over a period of time. Study characteristics The evidence is current to January 2013. We included 15 studies containing a total of 1282 participants with MCI. The majority of studies (n = 9) were published between 2010 and 2013. The remaining six studies were published between 2004 and 2009. All of the included studies were conducted in Europe. Study sizes varied and ranged from 15 to 231 participants.The mean (range) age of the youngest sample was 64 years (45 to 76) and the mean (standard deviation) age of the oldest sample was 73.4 (6.6) years. Quality of the evidence Our findings are based on studies with poor reporting, with a majority of studies at unclear risk of bias due to insufficient details given on how participants were selected and how the clinical diagnosis of dementia was established. According to the assessment of how the CSF tests were conducted and analysed, eight of 15 studies were of poor methodological quality. Key findings Below is a summary of key findings for the tests: CSF t‐tau test for conversion from MCI to Alzheimer’s disease dementia The sensitivity values in seven individual studies ranged from 51% to 90% while the specificity values ranged from 48% to 88%. The statistical analysis of those studies showed that, at the fixed specificity of 72%, the estimated sensitivity was 77%, and, at the prevalence of 37%, the positive predictive value was 62% and the negative predictive value was 84%. Based on these results, on average 62 out of 100 people with MCI and a positive index test result would convert to Alzheimer's disease dementia but 38 would not; on average, 84 out of 100 people with MCI and with a negative index test result would not convert to Alzheimer's disease dementia, but 16 would. CSF p‐tau test for conversion from MCI to Alzheimer’s disease dementia The sensitivity values in six individual studies ranged from 40% to 100% while the specificity values ranged from 22% to 86%. The statistical analysis of those studies showed that, at the fixed specificity of 48%, the estimated sensitivity was 81%, and, at the prevalence of 37%, the positive predictive value was 48% and the negative predictive value was 81%. Based on these results, on average 48 out of 100 people with MCI and with a positive index test result would convert to Alzheimer's disease dementia, but 52 would not; on average, 81 out of 100 people with MCI with a negative index test result would not convert to Alzheimer's disease dementia, but 19 would. We found that the cerebrospinal fluid (CSF) diagnostic test, as a single test, lacks the accuracy to identify those people with mild cognitive impairment (MCI) who would develop Alzheimer’s disease dementia or other forms of dementia over a period of time. The data suggested that a negative CSF test, in people with MCI, almost indicates the absence of Alzheimer's disease as the cause of their clinical symptoms. However, a positive CSF test does not confirm the presence of Alzheimer's disease as the aetiology (cause) of their clinical symptoms. There were methodological problems in the included studies that did not allow for a clear answer to the review question. The main limitations of the review were poor reporting in the included studies, lack of a widely accepted threshold of the CSF diagnostic tests in people with MCI, variability in length of follow‐up, and the marked variation in CSF tests’ accuracy between the included studies.","3","John Wiley & Sons, Ltd","1465-1858","Aged; Alzheimer Disease [diagnosis]; Amyloid beta‐Peptides [*cerebrospinal fluid]; Biomarkers [cerebrospinal fluid]; Cognition Disorders [diagnosis]; Humans; Middle Aged; Sensitivity and Specificity; tau Proteins [*cerebrospinal fluid]","10.1002/14651858.CD010803.pub2","http://dx.doi.org/10.1002/14651858.CD010803.pub2","Dementia and Cognitive Improvement"
"CD012599","Alldred, SK; Takwoingi, Y; Guo, B; Pennant, M; Deeks, JJ; Neilson, JP; Alfirevic, Z","First and second trimester serum tests with and without first trimester ultrasound tests for Down's syndrome screening","Cochrane Database of Systematic Reviews","2017","Abstract - Background Down's syndrome occurs when a person has three copies of chromosome 21 (or the specific area of chromosome 21 implicated in causing Down's syndrome) rather than two. It is the commonest congenital cause of mental disability. Non‐invasive screening based on biochemical analysis of maternal serum or urine, or fetal ultrasound measurements, allows estimates of the risk of a pregnancy being affected and provides information to guide decisions about definitive testing.   Before agreeing to screening tests, parents need to be fully informed about the risks, benefits and possible consequences of such a test. This includes subsequent choices for further tests they may face, and the implications of both false positive (i.e. invasive diagnostic testing, and the possibility that a miscarried fetus may be chromosomally normal) and false negative screening tests (i.e. a fetus with Down’s syndrome will be missed). The decisions that may be faced by expectant parents inevitably engender a high level of anxiety at all stages of the screening process, and the outcomes of screening can be associated with considerable physical and psychological morbidity. No screening test can predict the severity of problems a person with Down's syndrome will have. Objectives To estimate and compare the accuracy of first and second trimester serum markers with and without first trimester ultrasound markers for the detection of Down’s syndrome in the antenatal period, as combinations of markers. Search methods We conducted a sensitive and comprehensive literature search of MEDLINE (1980 to 25 August 2011), Embase (1980 to 25 August 2011), BIOSIS via EDINA (1985 to 25 August 2011), CINAHL via OVID (1982 to 25 August 2011), the Database of Abstracts of Reviews of Effectiveness (the Cochrane Library 25 August 2011), MEDION (25 August 2011), the Database of Systematic Reviews and Meta‐Analyses in Laboratory Medicine (25 August 2011), the National Research Register (Archived 2007), and Health Services Research Projects in Progress database (25 August 2011). We did not apply a diagnostic test search filter. We did forward citation searching in ISI citation indices, Google Scholar and PubMed ‘related articles’. We also searched reference lists of retrieved articles Selection criteria Studies evaluating tests of combining first and second trimester maternal serum markers in women up to 24 weeks of gestation for Down's syndrome, with or without first trimester ultrasound markers, compared with a reference standard, either chromosomal verification or macroscopic postnatal inspection. Data collection and analysis Data were extracted as test positive/test negative results for Down's and non‐Down's pregnancies allowing estimation of detection rates (sensitivity) and false positive rates (1‐specificity). We performed quality assessment according to QUADAS criteria. We used hierarchical summary ROC meta‐analytical methods to analyse test performance and compare test accuracy. Analysis of studies allowing direct comparison between tests was undertaken. We investigated the impact of maternal age on test performance in subgroup analyses. Main results Twenty‐two studies (reported in 25 publications) involving 228,615 pregnancies (including 1067 with Down's syndrome) were included. Studies were generally high quality, although differential verification was common with invasive testing of only high risk pregnancies. Ten studies made direct comparisons between tests. Thirty‐two different test combinations were evaluated formed from combinations of eight different tests and maternal age; first trimester nuchal translucency (NT) and the serum markers AFP, uE3, total hCG, free βhCG, Inhibin A, PAPP‐A and ADAM 12. We looked at tests combining first and second trimester markers with or without ultrasound as complete tests, and we also examined stepwise and contingent strategies. Meta‐analysis of the six most frequently evaluated test combinations showed that a test strategy involving maternal age and a combination of first trimester NT and PAPP‐A, and second trimester total hCG, uE3, AFP and Inhibin A significantly outperformed other test combinations that involved only one serum marker or NT in the first trimester, detecting about nine out of every 10 Down's syndrome pregnancies at a 5% false positive rate. However, the evidence was limited in terms of the number of studies evaluating this strategy, and we therefore cannot recommend one single screening strategy. Authors' conclusions Tests involving first trimester ultrasound with first and second trimester serum markers in combination with maternal age are significantly better than those without ultrasound, or those evaluating first trimester ultrasound in combination with second trimester serum markers, without first trimester serum markers. We cannot make recommendations about a specific strategy on the basis of the small number of studies available. Plain language summary Screening tests for Down’s syndrome in the first 24 weeks of pregnancy Background   Down's syndrome (also known as Down's or Trisomy 21) is an incurable genetic disorder that causes significant physical and mental health problems, and disabilities. However, there is wide variation in how Down's affects people. Some individuals are severely affected whilst others have mild problems and are able to lead relatively normal lives. There is no way of predicting how badly a baby might be affected.    Expectant parents are given the choice to be tested for Down’s syndrome during pregnancy to assist them in making decisions. If a mother is carrying a baby with Down’s syndrome, then there is the decision about whether to terminate or continue with the pregnancy. The information offers parents the opportunity to plan for life with a child with Down’s syndrome.    The most accurate tests for Down’s syndrome involve testing fluid from around the baby (amniocentesis) or tissue from the placenta (chorionic villus sampling (CVS)) for the abnormal chromosomes associated with Down’s syndrome. Both these tests involve inserting needles through the mother's abdomen and are known to increase the risk of miscarriage. Thus, the tests may not be suitable for all pregnant women. Rather, tests that measure markers in the mother’s blood, urine, or on ultrasound scans of the baby are used for screening. These screening tests are not perfect as they can miss cases of Down’s syndrome and also give high risk test results to a number of women whose babies are not affected by Down’s syndrome. Thus, pregnancies identified as high risk using these screening tests require further testing using amniocentesis or CVS to confirm a diagnosis of Down’s syndrome. What we did   We assessed combinations of first trimester (up to 14 weeks' gestation) and second trimester serum screening tests (up to 24 weeks' gestation), with or without first trimester ultrasound screening tests, Our aim was to identify the most accurate test(s) for predicting the risk of a pregnancy being affected by Down's syndrome. We looked at one ultrasound marker (nuchal translucency) and seven different serum markers (PAPP‐A, total hCG, free βhCG, uE3, AFP, inhibin A, ADAM 12) that can be used alone, in ratios or in combination, taken before 24 weeks' gestation, thus creating 32 screening tests for Down’s. We found 22 studies, involving 228,615 pregnancies (including 1067 fetuses affected by Down's syndrome). What we found   For Down's syndrome screening, where tests were carried out in the first and second trimester and combined to give an overall risk, we found that a test comprised of first trimester nuchal translucency and PAPP‐A, and second trimester total hCG, uE3, AFP and Inhibin A was the most sensitive test, detecting nine out of 10 pregnancies affected by Down's syndrome. Five per cent of pregnant women receiving a high risk test result based on this combination would not be affected by Down's syndrome. There were relatively few studies assessing these tests and therefore we cannot make a strong recommendation about the best test.    Other important information to consider   The ultrasound tests themselves have no adverse effects for the woman, and blood tests can cause discomfort, bruising and, rarely, infection. However, some women who have a high risk screening test result, and are given amniocentesis or CVS have a risk of miscarrying a baby unaffected by Down’s. Parents will need to weigh up this risk when deciding whether or not to have an amniocentesis or CVS following a high risk screening test result.","3","John Wiley & Sons, Ltd","1465-1858","*Nuchal Translucency Measurement; Biomarkers [blood]; Chorionic Gonadotropin [blood]; Down Syndrome [*blood, *diagnosis, diagnostic imaging]; Estriol [blood]; False Positive Reactions; Female; Humans; Inhibins [blood]; Maternal Age; Pregnancy; Pregnancy Trimester, First [*blood]; Pregnancy Trimester, Second [*blood]; Pregnancy‐Associated Plasma Protein‐A [analysis]; Sensitivity and Specificity; alpha‐Fetoproteins [analysis]","10.1002/14651858.CD012599","http://dx.doi.org/10.1002/14651858.CD012599","Pregnancy and Childbirth"
"CD012600","Alldred, SK; Takwoingi, Y; Guo, B; Pennant, M; Deeks, JJ; Neilson, JP; Alfirevic, Z","First trimester ultrasound tests alone or in combination with first trimester serum tests for Down's syndrome screening","Cochrane Database of Systematic Reviews","2017","Abstract - Background Down's syndrome occurs when a person has three, rather than two copies of chromosome 21; or the specific area of chromosome 21 implicated in causing Down's syndrome. It is the commonest congenital cause of mental disability and also leads to numerous metabolic and structural problems. It can be life‐threatening, or lead to considerable ill health, although some individuals have only mild problems and can lead relatively normal lives. Having a baby with Down’s syndrome is likely to have a significant impact on family life. Non‐invasive screening based on biochemical analysis of maternal serum or urine, or fetal ultrasound measurements, allows estimates of the risk of a pregnancy being affected and provides information to guide decisions about definitive testing. Before agreeing to screening tests, parents need to be fully informed about the risks, benefits and possible consequences of such a test. This includes subsequent choices for further tests they may face, and the implications of both false positive and false negative screening tests (i.e. invasive diagnostic testing, and the possibility that a miscarried fetus may be chromosomally normal). The decisions that may be faced by expectant parents inevitably engender a high level of anxiety at all stages of the screening process, and the outcomes of screening can be associated with considerable physical and psychological morbidity. No screening test can predict the severity of problems a person with Down's syndrome will have. Objectives To estimate and compare the accuracy of first trimester ultrasound markers alone, and in combination with first trimester serum tests for the detection of Down’s syndrome. Search methods We carried out extensive literature searches including MEDLINE (1980 to 25 August 2011), Embase (1980 to 25 August 2011), BIOSIS via EDINA (1985 to 25 August 2011), CINAHL via OVID (1982 to 25 August 2011), and The Database of Abstracts of Reviews of Effects (the Cochrane Library 2011, Issue 7). We checked reference lists and published review articles for additional potentially relevant studies. Selection criteria Studies evaluating tests of first trimester ultrasound screening, alone or in combination with first trimester serum tests (up to 14 weeks' gestation) for Down's syndrome, compared with a reference standard, either chromosomal verification or macroscopic postnatal inspection. Data collection and analysis Data were extracted as test positive/test negative results for Down's and non‐Down's pregnancies allowing estimation of detection rates (sensitivity) and false positive rates (1‐specificity). We performed quality assessment according to QUADAS criteria. We used hierarchical summary ROC meta‐analytical methods to analyse test performance and compare test accuracy. Analysis of studies allowing direct comparison between tests was undertaken. We investigated the impact of maternal age on test performance in subgroup analyses. Main results We included 126 studies (152 publications) involving 1,604,040 fetuses (including 8454 Down's syndrome cases). Studies were generally good quality, although differential verification was common with invasive testing of only high‐risk pregnancies. Sixty test combinations were evaluated formed from combinations of 11 different ultrasound markers (nuchal translucency (NT), nasal bone, ductus venosus Doppler, maxillary bone length, fetal heart rate, aberrant right subclavian artery, frontomaxillary facial angle, presence of mitral gap, tricuspid regurgitation, tricuspid blood flow and iliac angle 90 degrees); 12 serum tests (inhibin A, alpha‐fetoprotein (AFP), free beta human chorionic gonadotrophin (ßhCG), total hCG, pregnancy‐associated plasma protein A (PAPP‐A), unconjugated oestriol (uE3), disintegrin and metalloprotease 12 (ADAM 12), placental growth factor (PlGF), placental growth hormone (PGH), invasive trophoblast antigen (ITA) (synonymous with hyperglycosylated hCG), growth hormone binding protein (GHBP) and placental protein 13 (PP13)); and maternal age. The most frequently evaluated serum markers in combination with ultrasound markers were PAPP‐A and free ßhCG. Comparisons of the 10 most frequently evaluated test strategies showed that a combined NT, PAPP‐A, free ßhCG and maternal age test strategy significantly outperformed ultrasound markers alone (with or without maternal age) except nasal bone, detecting about nine out of every 10 Down's syndrome pregnancies at a 5% false positive rate (FPR). In both direct and indirect comparisons, the combined NT, PAPP‐A, free ßhCG and maternal age test strategy showed superior diagnostic accuracy to an NT and maternal age test strategy (P < 0.0001). Based on the indirect comparison of all available studies for the two tests, the sensitivity (95% confidence interval) estimated at a 5% FPR for the combined NT, PAPP‐A, free ßhCG and maternal age test strategy (69 studies; 1,173,853 fetuses including 6010 with Down's syndrome) was 87% (86 to 89) and for the NT and maternal age test strategy (50 studies; 530,874 fetuses including 2701 Down's syndrome pregnancies) was 71% (66 to 75). Combinations of NT with other ultrasound markers, PAPP‐A and free ßhCG were evaluated in one or two studies and showed sensitivities of more than 90% and specificities of more than 95%. High‐risk populations (defined before screening was done, mainly due to advanced maternal age of 35 years or more, or previous pregnancies affected with Down's syndrome) showed lower detection rates compared to routine screening populations at a 5% FPR. Women who miscarried in the over 35 group were more likely to have been offered an invasive test to verify a negative screening results, whereas those under 35 were usually not offered invasive testing for a negative screening result. Pregnancy loss in women under 35 therefore leads to under‐ascertainment of screening results, potentially missing a proportion of affected pregnancies and affecting test sensitivity. Conversely, for the NT, PAPP‐A, free ßhCG and maternal age test strategy, detection rates and false positive rates increased with maternal age in the five studies that provided data separately for the subset of women aged 35 years or more. Authors' conclusions Test strategies that combine ultrasound markers with serum markers, especially PAPP‐A and free ßhCG, and maternal age were significantly better than those involving only ultrasound markers (with or without maternal age) except nasal bone. They detect about nine out of 10 Down’s affected pregnancies for a fixed 5% FPR. Although the absence of nasal bone appeared to have a high diagnostic accuracy, only five out of 10 affected Down's pregnancies were detected at a 1% FPR. Plain language summary Screening tests for Down’s syndrome in first 24 weeks of pregnancy Background   Down's syndrome (also known as Down's or Trisomy 21) is an incurable genetic disorder that causes significant physical and mental health problems, and disabilities. However, there is wide variation in how Down's affects people. Some individuals are severely affected whilst others have mild problems and are able to lead relatively normal lives. There is no way of predicting how badly a baby might be affected. Expectant parents are given the choice to be tested for Down’s during pregnancy to assist them in making decisions. If a mother is carrying a baby with Down’s, then there is the decision about whether to terminate or continue with the pregnancy. The information offers parents the opportunity to plan for life with a Down’s child. The most accurate tests for Down’s involve testing fluid from around the baby (amniocentesis) or tissue from the placenta (chorionic villus sampling (CVS)) for the abnormal chromosomes associated with Down’s. Both these tests involve inserting needles through the mother's abdomen and are known to increase the risk of miscarriage. Thus the tests are not suitable for offering to all pregnant women. Rather, tests that measure markers in the mother’s blood, urine or on ultrasound scans of the baby are used for screening. These screening tests are not perfect, they can miss cases of Down’s and also give a ‘high risk’ test results to a number of women whose babies are not affected by Down’s. Thus pregnancies identified as ‘high risk’ using these screening tests require further testing using amniocentesis (from 15 weeks' gestation) or CVS (from 10 + 0 to 13 + 6 weeks' gestation) to confirm a diagnosis of Down’s. What we did   The aim of this review was to find out which of the first trimester ultrasound screening tests, with or without first trimester serum tests done during the first 14 weeks of pregnancy are the most accurate at predicting the risk of a pregnancy being affected by Down's. We looked at 11 different ultrasound markers and 12 different serum markers that can be used alone, in ratios or in combination, taken before 14 weeks' gestation, thus creating 60 screening tests for Down’s. We found 126 studies, involving 1,604,040 fetuses (including 8454 fetuses affected by Down's syndrome). What we found   For the first 14 weeks of pregnancy, the evidence supports the use of first trimester ultrasound tests in combination with two serum (blood) markers ‐ especially pregnancy‐associated plasma protein A (PAPP‐A) and free beta human chorionic gonadotrophin (ßhCG) ‐ and maternal age, for Down's syndrome screening. In general, these tests are better than ultrasound markers on their own. They detect nine out of 10 pregnancies affected by Down's syndrome. Five per cent of women undertaking the test will have a high risk test result, however the majority of these pregnancies will not be affected by Down's syndrome.     Other important information to consider   The ultrasound tests themselves have no adverse effects for the woman, blood tests can cause discomfort, bruising and rarely infection. However some women who have a ‘high risk’ screening test result, and are given amniocentesis or CVS have a risk of miscarrying a baby unaffected by Down’s. Parents will need to weigh up this risk when deciding whether or not to have an amniocentesis or CVS following a ‘high risk’ screening test result.","3","John Wiley & Sons, Ltd","1465-1858","*Ultrasonography, Prenatal; Biomarkers [blood]; Chorionic Gonadotropin [blood]; Chorionic Gonadotropin, beta Subunit, Human [blood]; Down Syndrome [*blood, *diagnosis, diagnostic imaging]; False Positive Reactions; Female; Humans; Maternal Age; Nasal Bone [diagnostic imaging]; Pregnancy; Pregnancy Trimester, First [*blood]; Pregnancy‐Associated Plasma Protein‐A [analysis]; Sensitivity and Specificity","10.1002/14651858.CD012600","http://dx.doi.org/10.1002/14651858.CD012600","Pregnancy and Childbirth"
"CD011926.PUB2","Pammi, M; Flores, A; Versalovic, J; Leeflang, MMG","Molecular assays for the diagnosis of sepsis in neonates","Cochrane Database of Systematic Reviews","2017","Abstract - Background Microbial cultures for diagnosis of neonatal sepsis have low sensitivity and reporting delay. Advances in molecular microbiology have fostered new molecular assays that are rapid and may improve neonatal outcomes. Objectives To assess the diagnostic accuracy of various molecular methods for the diagnosis of culture‐positive bacterial and fungal sepsis in neonates and to explore heterogeneity among studies by analyzing subgroups classified by gestational age and type of sepsis onset and compare molecular tests with one another. Search methods We performed the systematic review as recommended by the Cochrane Diagnostic Test Accuracy Working Group. On 19 January 2016, we searched electronic bibliographic databases (the Cochrane Library, PubMed (from 1966), Embase (from 1982), and CINAHL (from 1982)) ,  conference proceedings of the Pediatric Academic Societies annual conference (from 1990), clinical trial registries (ClinicalTrials.gov, International Standard Randomised Controlled Trial Number (ISRCTN) registry, and World Health Organization (WHO) International Clinical Trials Platform (ICTRP) Search portal), and Science Citation Index. We contacted experts in the field for studies. Selection criteria We included studies that were prospective or retrospective, cohort or cross‐sectional design, which evaluated molecular assays (index test) in neonates with suspected sepsis (participants) in comparison with microbial cultures (reference standard). Data collection and analysis Two review authors independently assessed the methodologic quality of the studies and extracted data. We performed meta‐analyses using the bivariate and hierarchical summary receiver operating characteristic (HSROC) models and entered data into Review Manager 5. Main results Thirty‐five studies were eligible for inclusion and the summary estimate of sensitivity was 0.90 (95% confidence interval (CI) 0.82 to 0.95) and of specificity was 0.93 (95% CI 0.89 to 0.96) (moderate quality evidence). We explored heterogeneity by subgroup analyses of type of test, gestational age, type of sepsis onset, and prevalence of sepsis and we did not find sufficient explanations for the heterogeneity (moderate to very low quality evidence). Sensitivity analyses by including studies that analyzed blood samples and by good methodology revealed similar results (moderate quality evidence). Authors' conclusions Molecular assays have the advantage of producing rapid results and may perform well as 'add‐on' tests. Plain language summary Molecular tests to detect infections in newborn babies Review question:  Do molecular tests detect infection better than the standard culture methods for detecting infection in newborn babies? Background The current method of detecting infection (illness caused by germs) in newborn babies is to obtain blood or other body fluids (or both) and culture (grow) the bacteria (germs) in a laboratory. However, culture methods may miss some infections and take a long time to produce results (48 to 72 hours). Newer methods of detecting infection are based on detecting DNA (a molecule that carries the genetic instructions used in growth, development, functioning, and reproduction) from bacteria and other organisms that cause infections. Advances in microbiology have introduced new molecular tests for detecting infections. Molecular tests are rapid and may detect more infections compared to the traditional culture methods. Study characteristics We searched for evidence for the use of the molecular methods to detect infection in newborn babies. We found 35 studies that compared the new molecular methods to culture methods of the blood and spinal fluid to diagnose infection. Study funding sources None. Key results We found that the molecular methods may be very helpful additional tests because they provide rapid results. Quality of evidence Although there were some issues with selection of newborn babies for this review, overall the methods used by the studies were adequate. We rated the quality of the evidence as moderate to low.","2","John Wiley & Sons, Ltd","1465-1858","DNA, Bacterial [blood, cerebrospinal fluid, isolation & purification]; DNA, Fungal [blood, cerebrospinal fluid, isolation & purification]; Humans; Infant, Newborn; Infant, Premature; Polymerase Chain Reaction [methods]; Sepsis [*diagnosis, microbiology]","10.1002/14651858.CD011926.pub2","http://dx.doi.org/10.1002/14651858.CD011926.pub2","Neonatal"
"CD011053.PUB2","van Mens, TE; Scheres, LJJ; de Jong, PG; Leeflang, MMG; Nijkeuter, M; Middeldorp, S","Imaging for the exclusion of pulmonary embolism in pregnancy","Cochrane Database of Systematic Reviews","2017","Abstract - Background Pulmonary embolism is a leading cause of pregnancy‐related death. An accurate diagnosis in pregnant patients is crucial to prevent untreated pulmonary embolism as well as unnecessary anticoagulant treatment and future preventive measures. Applied imaging techniques might perform differently in these younger patients with less comorbidity and altered physiology, who largely have been excluded from diagnostic studies. Objectives To determine the diagnostic accuracy of computed tomography pulmonary angiography (CTPA), lung scintigraphy and magnetic resonance angiography (MRA) for the diagnosis of pulmonary embolism during pregnancy. Search methods We searched MEDLINE and Embase until July 2015. We used included studies as seeds in citations searches and in 'find similar' functions and searched reference lists. We approached experts in the field to help us identify non‐indexed studies. Selection criteria We included consecutive series of pregnant patients suspected of pulmonary embolism who had undergone one of the index tests (computed tomography (CT) pulmonary angiography, lung scintigraphy or MRA) and clinical follow‐up or pulmonary angiography as a reference test. Data collection and analysis Two review authors performed data extraction and quality assessment. We contacted investigators of potentially eligible studies to obtain missing information. In the primary analysis, we regarded inconclusive index test results as a negative reference test, and treatment for pulmonary embolism after an inconclusive index test as a positive reference test. Main results We included 11 studies (four CTPA, five lung scintigraphy, two both) with a total of 695 CTPA and 665 lung scintigraphy results. Lung scintigraphy was applied by different techniques. No MRA studies matched our inclusion criteria. Overall, risk of bias and concerns regarding applicability were high in all studies as judged in light of the review research question, as was heterogeneity in study methods. We did not undertake meta‐analysis. All studies used clinical follow‐up as a reference standard, none in a manner that enabled reliable identification of false positives. Sensitivity and negative predictive value were therefore the only valid test accuracy measures. The median negative predictive value for CTPA was 100% (range 96% to 100%). Median sensitivity was 83% (range 0% to 100%). The median negative predictive value for lung scintigraphy was 100% (range 99% to 100%). Median sensitivity was 100% (range 0% to 100%). The median frequency of inconclusive results was 5.9% (range 0.9% to 36%) for CTPA and 4.0% (range 0% to 23%) for lung scintigraphy. The overall median prevalence of pulmonary embolism was 3.3% (range 0.0% to 8.7%). Authors' conclusions Both CTPA and lung scintigraphy seem appropriate for exclusion of pulmonary embolism during pregnancy. However, the quality of the evidence mandates cautious adoption of this conclusion. Important limitations included poor reference standards, necessary assumptions in the analysis regarding inconclusive test results and the inherent inability of included studies to identify false positives. It is unclear which test has the highest accuracy. There is a need for direct comparisons between diagnostic methods, including MR, in prospective randomized diagnostic studies. Plain language summary Scanning imaging techniques for the exclusion of pulmonary embolism during pregnancy Pulmonary embolism is a blood clot that blocks blood flow to a portion of the lungs. Pregnant women are at high risk of pulmonary embolism, and it is a leading cause of death during pregnancy. Women at risk are treated with blood thinning medication. It is important that no cases are missed, and that treatment is prevented in women without the disease. Pulmonary embolism can be diagnosed through different scanning techniques. Little is known about the performance of these tests during pregnancy, which might be different from their performance outside pregnancy. We performed this review to establish the accuracy of the following imaging tests for diagnosing pulmonary embolism during pregnancy: computed tomography pulmonary angiography, lung scintigraphy and magnetic resonance angiography. We found 11 studies (current until July 2015) that described 695 computed tomography pulmonary angiography results, 665 lung scintigraphy results and no magnetic resonance angiography results. Studies on lung scintigraphy used varying techniques. Overall, these studies were of poor quality; therefore, we could not analyse results together to obtain a single estimate of their accuracy. The identified studies followed‐up patients clinically to confirm the absence of pulmonary embolism as revealed on the initial scan, so information could be used to draw conclusions only on the ability of these imaging tests to exclude pulmonary embolism, not on their ability to establish the diagnosis. Both computed tomography pulmonary angiography and lung scintigraphy appear appropriate for excluding pulmonary embolism in pregnancy. Almost no cases were missed, especially when the imaging test indicated the absence of disease without a doubt. However, this result should be interpreted with care because of the low quality of and variation between identified studies. Around 5% of the scans were unclear, but this percentage was as high as 36% in one study. About 3% of all women included in the studies had pulmonary embolism. We do not know which of the tests is better because tests were not directly compared in the same patients, and because aspects besides test accuracy need to be taken into account. Major limitations of this review include the use of clinical follow‐up within studies to confirm the absence of disease, unclear test results and the inability of studies to provide information on the accuracy of these tests in establishing rather than rejecting the diagnosis. High‐quality research is needed to investigate the use of computed tomography pulmonary angiography, lung scintigraphy and magnetic pulmonary angiography in the same patient groups.","1","John Wiley & Sons, Ltd","1465-1858","Angiography [standards, statistics & numerical data]; Female; Humans; Magnetic Resonance Angiography; Positron‐Emission Tomography [*standards, statistics & numerical data]; Pregnancy; Pregnancy Complications, Hematologic [*diagnostic imaging]; Pulmonary Embolism [*diagnostic imaging]; Radionuclide Imaging [standards, statistics & numerical data]; Sensitivity and Specificity; Tomography, X‐Ray Computed [*standards, statistics & numerical data]","10.1002/14651858.CD011053.pub2","http://dx.doi.org/10.1002/14651858.CD011053.pub2","Vascular"
"CD011515.PUB2","Tamburrino, D; Riviere, D; Yaghoobi, M; Davidson, BR; Gurusamy, KS","Diagnostic accuracy of different imaging modalities following computed tomography (CT) scanning for assessing the resectability with curative intent in pancreatic and periampullary cancer","Cochrane Database of Systematic Reviews","2016","Abstract - Background Periampullary cancer includes cancer of the head and neck of the pancreas, cancer of the distal end of the bile duct, cancer of the ampulla of Vater, and cancer of the second part of the duodenum. Surgical resection is the only established potentially curative treatment for pancreatic and periampullary cancer. A considerable proportion of patients undergo unnecessary laparotomy because of underestimation of the extent of the cancer on computed tomography (CT) scanning. Other imaging methods such as magnetic resonance imaging (MRI), positron emission tomography (PET), PET‐CT, and endoscopic ultrasound (EUS) have been used to detect local invasion or distant metastases not visualised on CT scanning which could prevent unnecessary laparotomy. No systematic review or meta‐analysis has examined the role of different imaging modalities in assessing the resectability with curative intent in patients with pancreatic and periampullary cancer. Objectives To determine the diagnostic accuracy of MRI, PET scan, and EUS performed as an add‐on test or PET‐CT as a replacement test to CT scanning in detecting curative resectability in pancreatic and periampullary cancer. Search methods We searched MEDLINE, Embase, Science Citation Index Expanded, and Health Technology Assessment (HTA) databases up to 5 November 2015. Two review authors independently screened the references and selected the studies for inclusion. We also searched for articles related to the included studies by performing the ""related search"" function in MEDLINE (OvidSP) and Embase (OvidSP) and a ""citing reference"" search (by searching the articles that cite the included articles). Selection criteria We included diagnostic accuracy studies of MRI, PET scan, PET‐CT, and EUS in patients with potentially resectable pancreatic and periampullary cancer on CT scan. We accepted any criteria of resectability used in the studies. We included studies irrespective of language, publication status, or study design (prospective or retrospective). We excluded case‐control studies. Data collection and analysis Two review authors independently performed data extraction and quality assessment using the QUADAS‐2 (quality assessment of diagnostic accuracy studies ‐ 2) tool. Although we planned to use bivariate methods for analysis of sensitivities and specificities, we were able to fit only the univariate fixed‐effect models for both sensitivity and specificity because of the paucity of data. We calculated the probability of unresectability in patients who had a positive index test (post‐test probability of unresectability in people with a positive test result) and in those with negative index test (post‐test probability of unresectability in people with a positive test result) using the mean probability of unresectability (pre‐test probability) from the included studies and the positive and negative likelihood ratios derived from the model. The difference between the pre‐test and post‐test probabilities gave the overall added value of the index test compared to the standard practice of CT scan staging alone. Main results Only two studies (34 participants) met the inclusion criteria of this systematic review. Both studies evaluated the diagnostic test accuracy of EUS in assessing the resectability with curative intent in pancreatic cancers. There was low concerns about applicability for most domains in both studies. The overall risk of bias was low in one study and unclear or high in the second study. The mean probability of unresectable disease after CT scan across studies was 60.5% (that is 61 out of 100 patients who had resectable cancer after CT scan had unresectable disease on laparotomy). The summary estimate of sensitivity of EUS for unresectability was 0.87 (95% confidence interval (CI) 0.54 to 0.97) and the summary estimate of specificity for unresectability was 0.80 (95% CI 0.40 to 0.96). The positive likelihood ratio and negative likelihood ratio were 4.3 (95% CI 1.0 to 18.6) and 0.2 (95% CI 0.0 to 0.8) respectively. At the mean pre‐test probability of 60.5%, the post‐test probability of unresectable disease for people with a positive EUS (EUS indicating unresectability) was 86.9% (95% CI 60.9% to 96.6%) and the post‐test probability of unresectable disease for people with a negative EUS (EUS indicating resectability) was 20.0% (5.1% to 53.7%). This means that 13% of people (95% CI 3% to 39%) with positive EUS have potentially resectable cancer and 20% (5% to 53%) of people with negative EUS have unresectable cancer. Authors' conclusions Based on two small studies, there is significant uncertainty in the utility of EUS in people with pancreatic cancer found to have resectable disease on CT scan. No studies have assessed the utility of EUS in people with periampullary cancer. There is no evidence to suggest that it should be performed routinely in people with pancreatic cancer or periampullary cancer found to have resectable disease on CT scan. Plain language summary Diagnostic accuracy of different scans following a CT scan for assessing whether pancreatic and periampullary cancer is resectable Review question How well do different scans identify whether pancreatic and periampullary cancer is resectable (can be surgically removed) in patients with pancreatic cancer in whom computed tomography (CT) scan suggests that the cancer can be removed? CT scan involves a series of X‐rays which are combined by a computer to provide detailed images of the area of the body X‐rayed. Background The pancreas is an organ situated in the abdomen close to the junction of the stomach and small bowel. It secretes digestive juices that are necessary for the digestion of all food materials. The digestive juices secreted in the pancreas drain into the upper part of the small bowel via the pancreatic duct. The bile duct is a tube which drains bile from the liver and gallbladder. The pancreatic and bile ducts share a common path just before they drain into the small bowel. This area is called the periampullary region. Surgical removal is the only potentially curative treatment for cancers arising from the pancreatic and periampullary regions. A considerable proportion of patients undergo unnecessary major open abdominal exploratory operation (laparotomy) because their CT scan has underestimated the spread of cancer. If the cancer is spread within the abdomen as identified during the major open operation, the main treatment is chemotherapy which does not cure the cancer but may improve survival. Thus the major open abdominal operation with its associated risks can be avoided if the spread of cancer within the abdomen is known before the major operation. Determining the extent of cancer is called ""staging"" the cancer. Usually the minimum test used for staging is the CT scan. However, CT scan can understage the cancer, i.e. it can underestimate the spread of cancer. Various other scans can be used in addition to CT scan in order to find out if pancreatic cancer is resectable (able to be surgically removed). These include the following tests. 1. Magnetic resonance imaging (MRI): use of a powerful magnet to produce images of different tissues of the body. 2. Positron emission tomography (PET scan): small amount of radioactive glucose (sugar) is used to differentiate between different tissues. It utilises the property that cancer cells often use more glucose than normal cells). 3. Endoscopic ultrasound (EUS); the use of an endoscope, a camera introduced into the body cavities to view the inside of the body. An ultrasound (high‐energy sound waves) probe at the end of the endoscope is used to differentiate different tissues. In addition, a combination of PET‐CT may be performed instead of CT. Different studies report different accuracy of these tests in assessing whether the cancer can be removed. In this review, we identified all such studies and used appropriate mathematical methods to identify the average diagnostic accuracy of these tests for staging pancreatic and periampullary cancers considered to be removable after a CT scan. Study characteristics We included two studies with a total of 34 patients in this review. Both studies evaluated the diagnostic performance of EUS. This evidence is current to 5 November 2015. Quality of the evidence Of the two studies, one study was conducted as well as such a study could be conducted. The methodological quality of the other study was poor. Key results The two included studies showed that in those people with pancreatic cancer in whom CT alone showed their cancer was capable of being fully surgically removed, 61% (61 out of 100) would prove to have cancer that was too fully spread to make this possible when a laparotomy was attempted. Due to the small sample size, there is significant uncertainty in the utility of EUS in people with pancreatic cancer found to have resectable disease on CT scan. There is no evidence to suggest that it should be performed routinely in people with pancreatic cancer found to have resectable disease on CT scan.","9","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.CD011515.pub2","http://dx.doi.org/10.1002/14651858.CD011515.pub2","Gut"
"CD010680.PUB2","Crawford, F; Welch, K; Andras, A; Chappell, FM","Ankle brachial index for the diagnosis of lower limb peripheral arterial disease","Cochrane Database of Systematic Reviews","2016","Abstract - Background Peripheral arterial disease (PAD) of the lower limb is common, with prevalence of both symptomatic and asymptomatic disease estimated at 13% in the over 50 age group. Symptomatic PAD affects about 5% of individuals in Western populations between the ages of 55 and 74 years. The most common initial symptom of PAD is muscle pain on exercise that is relieved by rest and is attributed to reduced lower limb blood flow due to atherosclerotic disease (intermittent claudication). The ankle brachial index (ABI) is widely used by a variety of healthcare professionals, including specialist nurses, physicians, surgeons and podiatrists working in primary and secondary care settings, to assess signs and symptoms of PAD. As the ABI test is non‐invasive and inexpensive and is in widespread clinical use, a systematic review of its diagnostic accuracy in people presenting with leg pain suggestive of PAD is highly relevant to routine clinical practice. Objectives To estimate the diagnostic accuracy of the ankle brachial index (ABI) ‐ also known as the ankle brachial pressure index (ABPI) ‐ for the diagnosis of peripheral arterial disease in people who experience leg pain on walking that is alleviated by rest. Search methods We carried out searches of the following databases in August 2013: MEDLINE (Ovid SP),Embase (Ovid SP), the Cumulative Index to Nursing and Allied Health Literature (CINAHL) (EBSCO), Latin American and Caribbean Health Sciences (LILACS) (Bireme), Database of Abstracts of Reviews of Effects and the Health Technology Assessment Database in  The Cochrane Library,  the Institute for Scientific Information (ISI) Conference Proceedings Citation Index ‐ Science, the British Library Zetoc Conference search and Medion. Selection criteria We included cross‐sectional studies of ABI in which duplex ultrasonography or angiography was used as the reference standard. We also included cross‐sectional or diagnostic test accuracy (DTA) cohort studies consisting of both prospective and retrospective studies. Participants were adults presenting with leg pain on walking that was relieved by rest, who were tested in primary care settings or secondary care settings (hospital outpatients only) and who did not have signs or symptoms of critical limb ischaemia (rest pain, ischaemic ulcers or gangrene). The index test was ABI, also called the ankle brachial pressure index (ABPI) or the Ankle Arm Index (AAI), which was performed with a hand‐held doppler or oscillometry device to detect ankle vessels. We included data collected via sphygmomanometers (both manual and aneroid) and digital equipment. Data collection and analysis Two review authors independently replicated data extraction by using a standard form, which included an assessment of study quality, and resolved disagreements by discussion. Two review authors extracted participant‐level data when available to populate 2×2 contingency tables (true positives, true negatives, false positives and false negatives). After a pilot phase involving two review authors working independently, we used the methodological quality assessment tool the Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2), which incorporated our review question ‐ along with a flow diagram to aid reviewers' understanding of the conduct of the study when necessary and an assessment of risk of bias and applicability judgements. Main results We screened 17,055 records identified through searches of databases. We obtained 746 full‐text articles and assessed them for relevance. We scrutinised 49 studies to establish their eligibility for inclusion in the review and excluded 48, primarily because participants were not patients presenting solely with exertional leg pain, investigators used no reference standard or investigators used neither angiography nor duplex ultrasonography as the reference standard. We excluded most studies for more than one reason. Only one study met the eligibility criteria and provided limb‐level accuracy data from just 85 participants (158 legs). This prospective study compared the manual doppler method of obtaining an ABI (performed by untrained personnel) with the automated oscillometric method. Limb‐level data, as reported by the study, indicated that the accuracy of the ABI in detecting significant arterial disease on angiography is superior when stenosis is present in the femoropopliteal vessels, with sensitivity of 97% (95% confidence interval (CI) 93% to 99%) and specificity of 89% (95% CI 67% to 95%) for oscillometric ABI, and sensitivity of 95% (95% CI 89% to 97%) and specificity of 56% (95% CI 33% to 70%) for doppler ABI. The ABI threshold was not reported. Investigators attributed the lower specificity for doppler to the fact that a tibial or dorsalis pedis pulse could not be detected by doppler in 12 of 27 legs with normal vessels or non‐significant lesions. The superiority of the oscillometric (automated) method for obtaining an ABI reading over the manual method with a doppler probe used by inexperienced operators may be a clinically important finding. Authors' conclusions Evidence about the accuracy of the ankle brachial index for the diagnosis of PAD in people with leg pain on exercise that is alleviated by rest is sparse. The single study included in our review provided only limb‐level data from a few participants. Well‐designed cross‐sectional studies are required to evaluate the accuracy of ABI in patients presenting with early symptoms of peripheral arterial disease in all healthcare settings. Another systematic review of existing studies assessing the use of ABI in alternative patient groups, including asymptomatic, high‐risk patients, is required. Plain language summary Ankle brachial index for the diagnosis of lower limb peripheral arterial disease Peripheral arterial disease (PAD) of the legs affects 13% of people over 50 years of age. Sometimes PAD is ""silent"" and people are unaware they have it, but PAD can cause pain in the legs, especially with walking, and this type of symptomatic PAD affects about 5% of people in the Western world between the ages of 55 and 74 years. In PAD, fatty deposits (atherosclerosis) and blood clots cause the arteries to narrow and block. This leads to poor blood flow to the muscles during exercise, causing the classical symptom of muscle pain during walking that goes away after rest (intermittent claudication). In severe cases of PAD, symptoms of rest pain, ulceration and gangrene may develop and, if untreated, can lead to lower limb amputation. People with PAD are also at higher risk for cardiovascular disease and stroke. The ankle brachial index (ABI) is a test that is used to facilitate diagnosis of PAD. This test uses a device for measuring blood pressure with an inflatable cuff, and blood pressure measurements are taken at the upper arm and the ankle. The equipment can be manual or digital with automatic electronic calculation of blood pressure. The ABI is widely used for assessment of PAD by specialist nurses, physicians, surgeons and podiatrists working in hospitals. Dividing blood pressure recorded at the ankle by that recorded at the arm produces a ratio. Ratios of 0.90 to 1.30 are considered normal for adults, and ratios less than 0.8 indicate that PAD is present. Lower readings (< 0.7) suggest that the disease is severe and people might develop ulcers and gangrene. People with mild to moderate PAD can arrive at a diagnosis by several routes when using the ABI: during routine diabetic foot checks in general practice, in community health clinic or hospital settings, as a screening test for PAD in people who have no symptoms and during assessment of people presenting with exertional leg pain suggestive of PAD. Once a diagnosis of PAD is established, treatment will include prescribed secondary prevention therapy and lifestyle advice (exercise, smoking cessation, diet, weight), and for those with impaired quality of life, treatment may include supervised exercise therapy, or revascularisation, which commonly involves endovascular treatment rather than surgery. In hospitals, other tests may be used to diagnose PAD. Duplex ultrasound (DUS) shows blood flow in the arteries and is non‐invasive, but only an experienced radiologist can achieve useful images. Hospital staff can use other tests to image the blood vessels, namely, computerised tomography angiography (CTA), magnetic resonance angiography (MRA) and catheter angiography. The ABI test is non‐invasive and inexpensive and is widely used clinically; therefore, we have reviewed all available reports obtained from a wide search of databases of medical literature to estimate its accuracy in identifying PAD in people who experience pain on walking that goes away after rest. Two review authors independently assessed studies that met inclusion criteria of the review, including use of a cross‐sectional study design; enrolment of participants with pain on walking that got better with rest; and use of duplex ultrasonography or angiography to check that results of the ABI test were accurate. One study met our criteria and provided data from 85 participants (158 limbs). Investigators compared the manual doppler method of measuring ABI with the automated method. Researchers provided only data for legs as opposed to data for patients; we were therefore unable to recalculate the analysis at the whole‐participant level. In conclusion, we found little evidence about the accuracy of the ankle brachial index for diagnosing PAD in people presenting with exertional leg pain. The study included in our review had some flaws, and well‐designed cross‐sectional studies are needed to measure the accuracy of the ABI for diagnosing PAD in patients with early symptoms.","9","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.CD010680.pub2","http://dx.doi.org/10.1002/14651858.CD010680.pub2","Vascular"
"CD010705.PUB3","Theron, G; Peter, J; Richardson, M; Warren, R; Dheda, K; Steingart, KR","GenoType® MTBDRsl assay for resistance to second‐line anti‐tuberculosis drugs","Cochrane Database of Systematic Reviews","2016","Abstract - Background Genotype® MTBDR sl  (MTBDR sl ) is a rapid DNA‐based test for detecting specific mutations associated with resistance to fluoroquinolones and second‐line injectable drugs (SLIDs) in  Mycobacterium tuberculosis  complex. MTBDR sl  version 2.0 (released in 2015) identifies the mutations detected by version 1.0, as well as additional mutations. The test may be performed on a culture isolate or a patient specimen, which eliminates delays associated with culture. Version 1.0 requires a smear‐positive specimen, while version 2.0 may use a smear‐positive or ‐negative specimen. We performed this updated review as part of a World Health Organization process to develop updated guidelines for using MTBDR sl . Objectives To assess and compare the diagnostic accuracy of MTBDR sl  for: 1. fluoroquinolone resistance, 2. SLID resistance, and 3. extensively drug‐resistant tuberculosis, indirectly on a  M. tuberculosis  isolate grown from culture or directly on a patient specimen. Participants were people with rifampicin‐resistant or multidrug‐resistant tuberculosis. The role of MTBDR sl  would be as the initial test, replacing culture‐based drug susceptibility testing (DST), for detecting second‐line drug resistance. Search methods We searched the following databases without language restrictions up to 21 September 2015: the Cochrane Infectious Diseases Group Specialized Register; MEDLINE; Embase OVID; Science Citation Index Expanded, Conference Proceedings Citation Index‐Science, and BIOSIS Previews (all three from Web of Science); LILACS; and SCOPUS; registers for ongoing trials; and ProQuest Dissertations & Theses A&I. We reviewed references from included studies and contacted specialists in the field. Selection criteria We included cross‐sectional and case‐control studies that determined MTBDR sl  accuracy against a defined reference standard (culture‐based DST, genetic sequencing, or both). Data collection and analysis Two review authors independently extracted data and assessed quality using the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool. We synthesized data for versions 1.0 and 2.0 separately. We estimated MTBDR sl  sensitivity and specificity for fluoroquinolone resistance, SLID resistance, and extensively drug‐resistant tuberculosis when the test was performed indirectly or directly (smear‐positive specimen for version 1.0, smear‐positive or ‐negative specimen for version 2.0). We explored the influence on accuracy estimates of individual drugs within a drug class and of different reference standards. We performed most analyses using a bivariate random‐effects model with culture‐based DST as reference standard. Main results We included 27 studies. Twenty‐six studies evaluated version 1.0, and one study version 2.0. Of 26 studies stating specimen country origin, 15 studies (58%) evaluated patients from low‐ or middle‐income countries. Overall, we considered the studies to be of high methodological quality. However, only three studies (11%) had low risk of bias for the reference standard; these studies used World Health Organization (WHO)‐recommended critical concentrations for all drugs in the culture‐based DST reference standard. MTBDR sl  version 1.0 Fluoroquinolone resistance:  indirect testing, MTBDR sl  pooled sensitivity and specificity (95% confidence interval (CI)) were 85.6% (79.2% to 90.4%) and 98.5% (95.7% to 99.5%), (19 studies, 2223 participants); direct testing (smear‐positive specimen), pooled sensitivity and specificity were 86.2% (74.6% to 93.0%) and 98.6% (96.9% to 99.4%), (nine studies, 1771 participants,  moderate quality evidence ). SLID resistance:  indirect testing, MTBDR sl  pooled sensitivity and specificity were 76.5% (63.3% to 86.0%) and 99.1% (97.3% to 99.7%), (16 studies, 1921 participants); direct testing (smear‐positive specimen), pooled sensitivity and specificity were 87.0% (38.1% to 98.6%) and 99.5% (93.6% to 100.0%), (eight studies, 1639 participants,  low quality evidence ). Extensively drug‐resistant tuberculosis:  indirect testing, MTBDR sl  pooled sensitivity and specificity were 70.9% (42.9% to 88.8%) and 98.8% (96.1% to 99.6%), (eight studies, 880 participants); direct testing (smear‐positive specimen), pooled sensitivity and specificity were 69.4% (38.8% to 89.0%) and 99.4% (95.0% to 99.3%), (six studies, 1420 participants,  low quality evidence ). Similar to the original Cochrane review, we found no evidence of a significant difference in MTBDR sl  version 1.0 accuracy between indirect and direct testing for fluoroquinolone resistance, SLID resistance, and extensively drug‐resistant tuberculosis. MTBDR sl  version 2.0 Fluoroquinolone resistance:  direct testing, MTBDR sl  sensitivity and specificity were 97% (83% to 100%) and 98% (93% to 100%), smear‐positive specimen; 80% (28% to 99%) and 100% (40% to 100%), smear‐negative specimen. SLID resistance:  direct testing, MTBDR sl  sensitivity and specificity were 89% (72% to 98%) and 90% (84% to 95%), smear‐positive specimen; 80% (28% to 99%) and 100% (40% to 100%), smear‐negative specimen. Extensively drug‐resistant tuberculosis:  direct testing, MTBDR sl  sensitivity and specificity were 79% (49% to 95%) and 97% (93% to 99%), smear‐positive specimen; 50% (1% to 99%) and 100% (59% to 100%), smear‐negative specimen. We had insufficient data to estimate summary sensitivity and specificity of version 2.0 (smear‐positive and ‐negative specimens) or to compare accuracy of the two versions. A limitation was that most included studies did not consistently use the World Health Organization (WHO)‐recommended concentrations for drugs in the culture‐based DST reference standard. Authors' conclusions In people with rifampicin‐resistant or multidrug‐resistant tuberculosis, MTBDR sl  performed on a culture isolate or smear‐positive specimen may be useful in detecting second‐line drug resistance. MTBDR sl  (smear‐positive specimen) correctly classified around six in seven people as having fluoroquinolone or SLID resistance, although the sensitivity estimates for SLID resistance varied. The test rarely gave a positive result for people without drug resistance. However, when second‐line drug resistance is not detected (MTBDR sl  result is negative), conventional DST can still be used to evaluate patients for resistance to the fluoroquinolones or SLIDs. We recommend that future work evaluate MTBDR sl  version 2.0, in particular on smear‐negative specimens and in different settings to account for different resistance‐causing mutations that may vary by strain. Researchers should also consider incorporating WHO‐recommended critical concentrations into their culture‐based reference standards. 12 April 2019 No update planned Other The CIDG anticipates this may be updated if policymakers consider this a priority. All eligible published studies found in the last search (21 Sep, 2015) were included. Plain language summary The rapid test GenoType ®  MTBDR sl  for testing resistance to second‐line TB drugs Background Different drugs are available to treat tuberculosis (TB), but resistance to these drugs is a growing problem. People with drug‐resistant TB require second‐line TB drugs that, compared with first‐line TB drugs, must be taken for longer and may be associated with more harms. Detecting TB drug resistance quickly is important for improving health, reducing deaths, and decreasing the spread of drug‐resistant TB. Definitions   Multidrug‐resistant TB (MDR‐TB) is caused by TB bacteria that are resistant to at least isoniazid and rifampicin, the two most potent TB drugs. Extensively drug‐resistant TB (XDR‐TB) is a type of MDR‐TB that is resistant to nearly all TB drugs. What test is evaluated by this review? GenoType ®  MTBDR sl  (MTBDR sl ) is a rapid test for detecting resistance to second‐line TB drugs. In people with MDR‐TB, MTBDR sl  is used to detect additional drug resistance. The test may be performed on TB bacteria grown in culture from a patient specimen (indirect testing) or on a patient specimen (direct testing), which eliminates delays associated with culture. MTBDR sl  version 1.0 requires a specimen to be smear‐positive by microscopy, while version 2.0 (released in 2015) may use a smear‐positive or ‐negative specimen. What are the aims of the review? We wanted to find out how accurate MTBDR sl  is for detecting drug resistance; to compare indirect and direct testing; and to compare the two test versions. How up‐to‐date is the review? We searched for and used studies that had been published up to 21 September 2015. What are the main results of the review? We found 27 studies; 26 studies evaluated MTBDR sl  version 1.0 and one study evaluated version 2.0. Fluoroquinolone drugs MTBDR sl  version 1.0 (smear‐positive specimen) detected 86% of people with fluoroquinolone resistance and rarely gave a positive result for people without resistance ( GRADE, moderate quality evidence) . Second‐line injectable drugs MTBDR sl  version 1.0 (smear‐positive specimen) detected 87% of people with second‐line injectable drug resistance and rarely gave a positive result for people without resistance ( GRADE, low quality evidence) . XDR‐TB MTBDR sl  version 1.0 (smear‐positive specimen) detected 69% of people with XDR‐TB and rarely gave a positive result for people without resistance ( GRADE ,  low quality evidence) . For MTBDR sl  version 1.0, we found similar results for indirect and direct testing (smear‐positive specimen). As we identified only one study evaluating MTBDR sl  version 2.0, we could not be sure of the diagnostic accuracy of version 2.0. Also, we could not compare accuracy of the two versions. What is the methodological quality of the evidence? We used the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) tool to assess study quality. Overall, we considered the included studies to be of high quality; however, we had concerns about how the reference standard (the benchmark against which MTBDR sl  was measured) was applied. What are the authors' conclusions? MTBDR sl  (smear‐positive specimen) identified most of the patients with second‐line drug resistance. When the test reports a negative result, conventional testing for drug resistance can still be used.","9","John Wiley & Sons, Ltd","1465-1858","Adult; Antitubercular Agents [*therapeutic use]; Cross‐Sectional Studies; Extensively Drug‐Resistant Tuberculosis [*diagnosis, drug therapy]; Fluoroquinolones [therapeutic use]; Genotype; Humans; Microbial Sensitivity Tests [*methods, standards]; Mycobacterium tuberculosis [*drug effects]; Sensitivity and Specificity","10.1002/14651858.CD010705.pub3","http://dx.doi.org/10.1002/14651858.CD010705.pub3","Infectious Diseases"
"CD010864.PUB2","Crawford, F; Andras, A; Welch, K; Sheares, K; Keeling, D; Chappell, FM","D‐dimer test for excluding the diagnosis of pulmonary embolism","Cochrane Database of Systematic Reviews","2016","Abstract - Background Pulmonary embolism (PE) can occur when a thrombus (blood clot) travels through the veins and lodges in the arteries of the lungs, producing an obstruction. People who are thought to be at risk include those with cancer, people who have had a recent surgical procedure or have experienced long periods of immobilisation and women who are pregnant. The clinical presentation can vary, but unexplained respiratory symptoms such as difficulty breathing, chest pain and an increased respiratory rate are common. D‐dimers are fragments of protein released into the circulation when a blood clot breaks down as a result of normal body processes or with use of prescribed fibrinolytic medication. The D‐dimer test is a laboratory assay currently used to rule out the presence of high D‐dimer plasma levels and, by association, venous thromboembolism (VTE). D‐dimer tests are rapid, simple and inexpensive and can prevent the high costs associated with expensive diagnostic tests. Objectives To investigate the ability of the D‐dimer test to rule out a diagnosis of acute PE in patients treated in hospital outpatient and accident and emergency (A&E) settings who have had a pre‐test probability (PTP) of PE determined according to a clinical prediction rule (CPR), by estimating the accuracy of the test according to estimates of sensitivity and specificity. The review focuses on those patients who are not already established on anticoagulation at the time of study recruitment. Search methods We searched 13 databases from conception until December 2013. We cross‐checked the reference lists of relevant studies. Selection criteria Two review authors independently applied exclusion criteria to full papers and resolved disagreements by discussion. We included cross‐sectional studies of D‐dimer in which ventilation/perfusion (V/Q) scintigraphy, computerised tomography pulmonary angiography (CTPA), selective pulmonary angiography and magnetic resonance pulmonary angiography (MRPA) were used as the reference standard. • Participants: Adults who were managed in hospital outpatient and A&E settings and were suspected of acute PE were eligible for inclusion in the review if they had received a pre‐test probability score based on a CPR. • Index tests: quantitative, semi quantitative and qualitative D‐dimer tests. • Target condition: acute symptomatic PE. • Reference standards: We included studies that used pulmonary angiography, V/Q scintigraphy, CTPA and MRPA as reference standard tests. Data collection and analysis Two review authors independently extracted data and assessed quality using Quality Assessment of Diagnostic Accuracy Studies‐2 (QUADAS‐2). We resolved disagreements by discussion. Review authors extracted patient‐level data when available to populate 2 × 2 contingency tables (true‐positives (TPs), true‐negatives (TNs), false‐positives (FPs) and false‐negatives (FNs)). Main results We included four studies in the review (n = 1585 patients). None of the studies were at high risk of bias in any of the QUADAS‐2 domains, but some uncertainty surrounded the validity of studies in some domains for which the risk of bias was uncertain. D‐dimer assays demonstrated high sensitivity in all four studies, but with high levels of false‐positive results, especially among those over the age of 65 years. Estimates of sensitivity ranged from 80% to 100%, and estimates of specificity from 23% to 63%. Authors' conclusions A negative D‐dimer test is valuable in ruling out PE in patients who present to the A&E setting with a low PTP. Evidence from one study suggests that this test may have less utility in older populations, but no empirical evidence was available to support an increase in the diagnostic threshold of interpretation of D‐dimer results for those over the age of 65 years. Plain language summary D‐dimer for excluding pulmonary embolism in hospital outpatient and accident and emergency populations Review question To investigate the ability of the D‐dimer test to rule out a diagnosis of acute pulmonary embolism (PE) in patients treated in hospital outpatient and accident and emergency (A&E) departments. Background Pulmonary embolism (PE) is a serious, potentially fatal condition that occurs when a blood clot becomes lodged in the blood vessels of the lungs. When people arrive to hospital A&E departments reporting difficulty breathing, breathlessness and chest pain, several explanations are possible but a quick diagnosis is needed. Tests that are available to detect blood clots in the lungs can be invasive and time‐consuming, can carry a radiation burden and may be costly. Quick, easy‐to‐use and inexpensive tests that can be used to rule out the diagnosis would be very valuable. One such test is the D‐dimer test, which is so named because it detects small pieces of protein in the blood, which are called D‐dimer. When someone with symptoms of breathlessness and chest pain arrives to the hospital A&E department, the staff conducts an examination and asks questions about the patient's medical history and lifestyle. This helps them to calculate a score for the patient's risk that symptoms are due to a PE. If the score shows that they are at high risk of a blood clot in the lungs, patients undergo diagnostic scanning immediately (or are treated while test results are awaited). A D‐dimer test can be ordered for people in low or moderate (or unlikely) risk groups; a negative D‐dimer result might rule out the diagnosis of PE without the need for imaging. Study characteristics This review considered all evidence provided by studies that assess the ability of D‐dimer to rule out PE in people attending hospital outpatient and A&E departments. We assessed all available reports from a wide search of databases of medical literature. Two review authors independently assessed studies that met the review criteria, including use of a study design called a cross‐sectional study; inclusion of people with symptoms of PE who attended hospital outpatient and A&E departments; use of a risk score and then a D‐dimer test; and comparison of results of the D‐dimer test against the results of the very best available tests ‐ ventilation/perfusion scanning (V/Q scanning), pulmonary angiography, computerised tomography pulmonary angiography, and magnetic resonance pulmonary angiography. Key results Four studies met our criteria, and data from 1585 patients were available. We found evidence that negative (disease absent) D‐dimer tests are very good at ruling out PE and identifying people without PE, but high numbers of false‐positive test results suggest that people with a raised D‐dimer may not in fact have a PE; therefore, a positive result needs to be followed by imaging. In one study, false‐positives were more common among people older than 65 years of age. Quality of the evidence The flow of patients and the timing of D‐dimer and reference standard tests were of greatest methodological concern; no study authors provided a flow diagram to show the flow of patients throughout their study, and only one study clearly reported the time between administration of index and reference standard tests. In the remaining three studies, timing between conduct of the index test and completion of the reference standard was not clearly reported, leading to an unclear classification of bias. Conclusions Limited evidence from the studies included in this review suggests that quantitative D‐dimer tests used in emergency departments have few false‐negatives but very high levels of false‐positive results, with a high level of sensitivity consistently evident across all age groups. This makes the test useful as a rule‐out test but means that a positive result requires diagnostic imaging.","8","John Wiley & Sons, Ltd","1465-1858","Acute Disease; Adult; Biomarkers [blood]; Cross‐Sectional Studies; False Negative Reactions; False Positive Reactions; Fibrin Fibrinogen Degradation Products [*analysis]; Humans; Pulmonary Embolism [blood, *diagnosis]; Reference Standards; Venous Thromboembolism [blood, diagnosis]","10.1002/14651858.CD010864.pub2","http://dx.doi.org/10.1002/14651858.CD010864.pub2","Vascular"
"CD012281","Nisenblat, V; Prentice, L; Bossuyt, PMM; Farquhar, C; Hull, ML; Johnson, N","Combination of the non‐invasive tests for the diagnosis of endometriosis","Cochrane Database of Systematic Reviews","2016","Abstract - Background About 10% of women of reproductive age suffer from endometriosis, a costly chronic disease causing pelvic pain and subfertility. Laparoscopy is the gold standard diagnostic test for endometriosis, but is expensive and carries surgical risks. Currently, there are no non‐invasive tests available in clinical practice to accurately diagnose endometriosis. This review assessed the diagnostic accuracy of combinations of different non‐invasive testing modalities for endometriosis and provided a summary of all the reviews in the non‐invasive tests for endometriosis series. Objectives To estimate the diagnostic accuracy of any combination of non‐invasive tests for the diagnosis of pelvic endometriosis (peritoneal and/or ovarian or deep infiltrating) compared to surgical diagnosis as a reference standard. The combined tests were evaluated as replacement tests for diagnostic surgery and triage tests to assist decision‐making to undertake diagnostic surgery for endometriosis. Search methods We did not restrict the searches to particular study designs, language or publication dates. We searched CENTRAL to July 2015, MEDLINE and EMBASE to May 2015, as well as the following databases to April 2015: CINAHL, PsycINFO, Web of Science, LILACS, OAIster, TRIP, ClinicalTrials.gov, DARE and PubMed. Selection criteria We considered published, peer‐reviewed, randomised controlled or cross‐sectional studies of any size, including prospectively collected samples from any population of women of reproductive age suspected of having one or more of the following target conditions: ovarian, peritoneal or deep infiltrating endometriosis (DIE). We included studies comparing the diagnostic test accuracy of a combination of several testing modalities with the findings of surgical visualisation of endometriotic lesions. Data collection and analysis Three review authors independently collected and performed a quality assessment of the data from each study by using the QUADAS‐2 tool. For each test, the data were classified as positive or negative for the surgical detection of endometriosis and sensitivity and specificity estimates were calculated. The bivariate model was planned to obtain pooled estimates of sensitivity and specificity whenever sufficient data were available. The predetermined criteria for a clinically useful test to replace diagnostic surgery were a sensitivity of 0.94 and a specificity of 0.79 to detect endometriosis. We set the criteria for triage tests at a sensitivity of 0.95 and above and a specificity of 0.50 and above, which 'rules out' the diagnosis with high accuracy if there is a negative test result (SnOUT test), or a sensitivity of 0.50 and above and a specificity of 0.95 and above, which 'rules in' the diagnosis with high accuracy if there is a positive result (SpIN test). Main results Eleven eligible studies included 1339 participants. All the studies were of poor methodological quality. Seven studies evaluated pelvic endometriosis, one study considered DIE and/or ovarian endometrioma, two studies differentiated endometrioma from other ovarian cysts and one study addressed mapping DIE at specific anatomical sites. Fifteen different diagnostic combinations were assessed, including blood, urinary or endometrial biomarkers, transvaginal ultrasound (TVUS) and clinical history or examination. We did not pool estimates of sensitivity and specificity, as each study analysed independent combinations of the non‐invasive tests. Tests that met the criteria for a replacement test were: a combination of serum IL‐6 (cut‐off >15.4 pg/ml) and endometrial PGP 9.5 for pelvic endometriosis (sensitivity 1.00 (95% confidence interval (CI) 0.91 to 1.00), specificity 0.93 (95% CI, 0.80, 0.98) and the combination of vaginal examination and transvaginal ultrasound (TVUS) for rectal endometriosis (sensitivity 0.96 (95% CI 0.86 to 0.99), specificity 0.98 (95% CI 0.94 to 1.00)). Tests that met the criteria for SpIN triage tests for pelvic endometriosis were: 1. a multiplication of urine vitamin‐D‐binding protein (VDBP) and serum CA‐125 (cut‐off >2755) (sensitivity 0.74 (95% CI 0.60 to 0.84), specificity 0.97 (95% CI 0.86 to 1.00)) and 2. a combination of history (length of menses), serum CA‐125 (cut‐off >35 U/ml) and endometrial leukocytes (sensitivity 0.61 (95% CI 0.54 to 0.69), specificity 0.95 (95% CI 0.91 to 0.98)). For endometrioma, the following combinations qualified as SpIN test: 1. TVUS and either serum CA‐125 (cut‐off ≥25 U/ml) or CA 19.9 (cut‐off ≥12 U/ml) (sensitivity 0.79 (95% CI 0.64 to 0.91), specificity 0.97 (95% CI 0.91 to 1.00)); 2. TVUS and serum CA 19.9 (cut‐off ≥12 U/ml) (sensitivity 0.54 (95% CI 0.37 to 0.70), specificity 0.97 (95% CI 0.91 to 1.0)); 3‐4. TVUS and serum CA‐125 (cut‐off ≥20 U/ml or cut‐off ≥25 U/ml) (sensitivity 0.69 (95% CI 0.49 to 0.85), specificity 0.96 (95% CI 0.88 to 0.99)); 5. TVUS and serum CA‐125 (cut‐off ≥35 U/ml) (sensitivity 0.52 (95% CI 0.33 to 0.71), specificity 0.97 (95% CI 0.90 to 1.00)). A combination of vaginal examination and TVUS reached the threshold for a SpIN test for obliterated pouch of Douglas (sensitivity 0.87 (95% CI 0.69 to 0.96), specificity 0.98 (95% CI 0.95 to 1.00)), vaginal wall endometriosis (sensitivity 0.82 (95% CI 0.60 to 0.95), specificity 0.99 (95% CI 0.97 to 1.0)) and rectovaginal septum endometriosis (sensitivity 0.88 (95% CI 0.47 to 1.00), specificity 0.99 (95% CI 0.96 to 1.00)). All the tests were evaluated in individual studies and displayed wide CIs. Due to the heterogeneity and high risk of bias of the included studies, the clinical utility of the studied combination diagnostic tests for endometriosis remains unclear. Authors' conclusions None of the biomarkers evaluated in this review could be evaluated in a meaningful way and there was insufficient or poor‐quality evidence. Laparoscopy remains the gold standard for the diagnosis of endometriosis and using any non‐invasive tests should only be undertaken in a research setting. Plain language summary Combination of different types of tests for the non‐invasive diagnosis of endometriosis Review Question Can any combination of non‐invasive tests be accurate enough to replace or reduce the need for surgery in the diagnosis of endometriosis? Background Women with endometriosis have endometrial tissue (the tissue that lines the womb and is shed during menstruation) growing outside the womb within the pelvic cavity. This tissue responds to reproductive hormones, causing painful periods, chronic lower abdominal pain and difficulty conceiving. Currently, the only reliable way of diagnosing endometriosis is to perform keyhole surgery and visualise the endometrial deposits inside the abdomen. Because surgery is risky and expensive, combinations of various tests have been evaluated for their ability to detect endometriosis non‐invasively. An accurate test could lead to the diagnosis of endometriosis without the need for surgery or it could reduce the need for diagnostic surgery so only women who were most likely to have endometriosis would require it. Study characteristics The evidence included in this review is current to April 2015. We included 11 studies on combinations of several testing methods involving 1339 participants. All studies evaluated women of reproductive age who were undertaking diagnostic surgery to investigate symptoms of endometriosis or for other indications. Fifteen combinations of different blood, endometrial and urinary biomarkers were studied, incorporating ultrasound, clinical history and examination. Each combination of tests was assessed in small individual studies. Key results and quality of evidence Several studies identified the combined tests that might be of value in diagnosing endometriosis, but there are too few reports to be sure of their diagnostic benefit. The reports were of low methodological quality, which is why these results cannot be considered reliable unless confirmed in large high‐quality studies. Overall, there is not enough evidence to demonstrate benefit of any combined non‐invasive test for use in clinical practice for the diagnosis of endometriosis over the current ‘gold standard’ of diagnostic laparoscopy. Future research More high‐quality research studies are needed to accurately assess the diagnostic potential of any type of non‐invasive tests or their combinations that were identified in only a few studies as possibly having value in the detection of endometriosis.","7","John Wiley & Sons, Ltd","1465-1858","Aromatase [analysis]; Biomarkers [*analysis]; CA‐125 Antigen [blood]; CA‐19‐9 Antigen [blood]; Endometriosis [*diagnosis, diagnostic imaging]; Female; Humans; Interleukin‐6 [blood]; Leukocytes [cytology]; Ovarian Diseases [*diagnosis, diagnostic imaging]; Pelvis [diagnostic imaging]; Peritoneal Diseases [*diagnosis, diagnostic imaging]; Phosphopyruvate Hydratase [urine]; Sensitivity and Specificity; Ubiquitin Thiolesterase [analysis]; Ultrasonography; Vitamin D‐Binding Protein [urine]","10.1002/14651858.CD012281","http://dx.doi.org/10.1002/14651858.CD012281","Gynaecology and Fertility"
"CD009323.PUB3","Allen, VB; Gurusamy, KS; Takwoingi, Y; Kalia, A; Davidson, BR","Diagnostic accuracy of laparoscopy following computed tomography (CT) scanning for assessing the resectability with curative intent in pancreatic and periampullary cancer","Cochrane Database of Systematic Reviews","2016","Abstract - Background Surgical resection is the only potentially curative treatment for pancreatic and periampullary cancer. A considerable proportion of patients undergo unnecessary laparotomy because of underestimation of the extent of the cancer on computed tomography (CT) scanning. Laparoscopy can detect metastases not visualised on CT scanning, enabling better assessment of the spread of cancer (staging of cancer). This is an update to a previous Cochrane Review published in 2013 evaluating the role of diagnostic laparoscopy in assessing the resectability with curative intent in people with pancreatic and periampullary cancer. Objectives To determine the diagnostic accuracy of diagnostic laparoscopy performed as an add‐on test to CT scanning in the assessment of curative resectability in pancreatic and periampullary cancer. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE via PubMed, EMBASE via OvidSP (from inception to 15 May 2016), and Science Citation Index Expanded (from 1980 to 15 May 2016). Selection criteria We included diagnostic accuracy studies of diagnostic laparoscopy in people with potentially resectable pancreatic and periampullary cancer on CT scan, where confirmation of liver or peritoneal involvement was by histopathological examination of suspicious (liver or peritoneal) lesions obtained at diagnostic laparoscopy or laparotomy. We accepted any criteria of resectability used in the studies. We included studies irrespective of language, publication status, or study design (prospective or retrospective). We excluded case‐control studies. Data collection and analysis Two review authors independently performed data extraction and quality assessment using the QUADAS‐2 tool. The specificity of diagnostic laparoscopy in all studies was 1 because there were no false positives since laparoscopy and the reference standard are one and the same if histological examination after diagnostic laparoscopy is positive. The sensitivities were therefore meta‐analysed using a univariate random‐effects logistic regression model. The probability of unresectability in people who had a negative laparoscopy (post‐test probability for people with a negative test result) was calculated using the median probability of unresectability (pre‐test probability) from the included studies, and the negative likelihood ratio derived from the model (specificity of 1 assumed). The difference between the pre‐test and post‐test probabilities gave the overall added value of diagnostic laparoscopy compared to the standard practice of CT scan staging alone. Main results We included 16 studies with a total of 1146 participants in the meta‐analysis. Only one study including 52 participants had a low risk of bias and low applicability concern in the patient selection domain. The median pre‐test probability of unresectable disease after CT scanning across studies was 41.4% (that is 41 out of 100 participants who had resectable cancer after CT scan were found to have unresectable disease on laparotomy). The summary sensitivity of diagnostic laparoscopy was 64.4% (95% confidence interval (CI) 50.1% to 76.6%). Assuming a pre‐test probability of 41.4%, the post‐test probability of unresectable disease for participants with a negative test result was 0.20 (95% CI 0.15 to 0.27). This indicates that if a person is said to have resectable disease after diagnostic laparoscopy and CT scan, there is a 20% probability that their cancer will be unresectable compared to a 41% probability for those receiving CT alone. A subgroup analysis of people with pancreatic cancer gave a summary sensitivity of 67.9% (95% CI 41.1% to 86.5%). The post‐test probability of unresectable disease after being considered resectable on both CT and diagnostic laparoscopy was 18% compared to 40.0% for those receiving CT alone. Authors' conclusions Diagnostic laparoscopy may decrease the rate of unnecessary laparotomy in people with pancreatic and periampullary cancer found to have resectable disease on CT scan. On average, using diagnostic laparoscopy with biopsy and histopathological confirmation of suspicious lesions prior to laparotomy would avoid 21 unnecessary laparotomies in 100 people in whom resection of cancer with curative intent is planned. Plain language summary What is the diagnostic accuracy of laparoscopic staging following a CT scan for assessing whether pancreatic and periampullary cancer is resectable? Background The pancreas is an organ situated in the abdomen close to the junction of the stomach and small bowel. It secretes digestive juices which are necessary for the digestion of all food materials. The digestive juices secreted in the pancreas drain into the upper part of the small bowel via the pancreatic duct. The bile duct is a tube which drains bile from the liver and gallbladder. The pancreatic and bile ducts share a common path just before they drain into the small bowel. This area is called the periampullary region. Surgical removal is the only potentially curative treatment for cancers arising from the pancreatic and periampullary regions. A considerable proportion of patients undergo unnecessary major open abdominal exploratory operation (laparotomy) because their CT scan has underestimated the spread of cancer. If during the major open operation the cancer is found to have spread within the abdomen, patients are referred for alternate treatments such as chemotherapy, which do not cure the cancer but may improve survival. This major open abdominal operation can be avoided if the spread of cancer within the abdomen is known, called 'staging' the cancer. The minimum test used for staging is usually the computed tomography (CT) scan. However, CT scan can understage the cancer, that is it can underestimate the spread of cancer. Laparoscopy, a procedure whereby a small telescope is inserted inside the abdomen through a small (keyhole) surgical incision, can detect spread not identified on CT scanning. Different studies report different accuracy of laparoscopy in assessing whether the cancer can be removed. Our aim therefore was to find out the average diagnostic accuracy of laparoscopy for staging pancreatic and periampullary cancers considered to be removable after a CT scan. This review is an update of our previous review. A glossary of terms is provided in  Appendix 1 . Study characteristics We performed a thorough literature search to identify studies published up to 15 May 2016. We identified 16 studies reporting information on 1146 people with pancreatic or periampullary cancers which were considered to be eligible for potentially curative surgery based on CT scan staging. These studies evaluated diagnostic laparoscopy and compared results of the procedure with the eventual diagnosis by the surgeon that the cancer was not resectable during major abdominal operation or examination under microscope. Quality of evidence All of the studies were of unclear or low methodological quality in one or more aspects, which may undermine the validity of our findings. Key results Of those people with what CT suggests seems to be a potentially surgically curable cancer, the percentage in whom more extensive cancer was found on further staging with diagnostic laparoscopy or laparotomy ranged between 17% and 82% across studies. The median percentage of people in whom cancer spread was not detected by CT scan was 41%. Adding staging laparoscopy to CT scan might decrease the number of people with unremovable disease undergoing unnecessary major operations to 20% compared to those who undergo unnecessary major operation after CT scan alone (41%). This means that using diagnostic laparoscopy could halve the rate of unnecessary major open operations in people undergoing major surgery for potentially surgically curable pancreatic cancer.","7","John Wiley & Sons, Ltd","1465-1858","*Ampulla of Vater; *Unnecessary Procedures; Common Bile Duct Neoplasms [diagnostic imaging, pathology, *surgery]; Humans; Laparoscopy [*methods]; Laparotomy [*statistics & numerical data]; Neoplasm Staging [methods]; Pancreatic Neoplasms [diagnostic imaging, pathology, *surgery]; Randomized Controlled Trials as Topic; Tomography, X‐Ray Computed","10.1002/14651858.CD009323.pub3","http://dx.doi.org/10.1002/14651858.CD009323.pub3","Gut"
"CD010657.PUB2","Shaikh, N; Spingarn, RB; Hum, SW","Dimercaptosuccinic acid scan or ultrasound in screening for vesicoureteral reflux among children with urinary tract infections","Cochrane Database of Systematic Reviews","2016","Abstract - Background There is considerable interest in detecting vesicoureteral reflux (VUR) because its presence, especially when severe, has been linked to an increased risk of urinary tract infections and renal scarring. Voiding cystourethrography (VCUG), also known as micturating cystourethrography, is the gold standard for the diagnosis of VUR, and the grading of its severity. Because VCUG requires bladder catheterisation and exposes children to radiation, there has been a growing interest in other screening strategies that could identify at‐risk children without the risks and discomfort associated with VCUG. Objectives The objective of this review is to evaluate the accuracy of two alternative imaging tests ‐ the dimercaptosuccinic acid renal scan (DMSA) and renal‐bladder ultrasound (RBUS) ‐ in diagnosing VUR and high‐grade VUR (Grade III‐V VUR). Search methods We searched MEDLINE, EMBASE, BIOSIS, and the Cochrane Register of Diagnostic Test Accuracy Studies from 1985 to 31 March 2016. The reference lists of relevant review articles were searched to identify additional studies not found through the electronic search. Selection criteria We considered published cross‐sectional or cohort studies that compared the results of the index tests (DMSA scan or RBUS) with the results of radiographic VCUG in children less than 19 years of age with a culture‐confirmed urinary tract infection. Data collection and analysis Two authors independently applied the selection criteria to all citations and independently abstracted data. We used the bivariate model to calculate summary sensitivity and specificity values. Main results A total of 42 studies met our inclusion criteria. Twenty studies reported data on the test performance of RBUS in detecting VUR; the summary sensitivity and specificity estimates were 0.44 (95% CI 0.34 to 0.54) and 0.78 (95% CI 0.68 to 0.86), respectively. A total of 11 studies reported data on the test performance of RBUS in detecting high‐grade VUR; the summary sensitivity and specificity estimates were 0.59 (95% CI 0.45 to 0.72) and 0.79 (95% CI 0.65 to 0.87), respectively. A total of 19 studies reported data on the test performance of DMSA in detecting VUR; the summary sensitivity and specificity estimates were 0.75 (95% CI 0.67 to 0.81) and 0.48 (95% CI 0.38 to 0.57), respectively. A total of 10 studies reported data on the accuracy of DMSA in detecting high‐grade VUR. The summary sensitivity and specificity estimates were 0.93 (95% CI 0.77 to 0.98) and 0.44 (95% CI 0.33 to 0.56), respectively. Authors' conclusions Neither the renal ultrasound nor the DMSA scan is accurate enough to detect VUR (of all grades). Although a child with a negative DMSA test has an < 1% probability of having high‐grade VUR, performing a screening DMSA will result in a large number of children falsely labelled as being at risk for high‐grade VUR. Accordingly, the usefulness of the DMSA as a screening test for high‐grade VUR should be questioned. Plain language summary The accuracy of two imaging tests in detecting vesicoureteral reflux Some children are born with an anatomic abnormality that allows backwards flow of urine from the bladder to the kidney. This is called vesicoureteral reflux or VUR. Children with VUR have more urinary tract infections and develop more renal scars than children without VUR. This is especially the case if VUR is severe. As such, clinicians are interested in finding out which children have VUR. Unfortunately, testing for VUR (using a voiding cystourethrogram or a VCUG or MCUG) involves bladder catheterisation and exposure to radiation. Accordingly, clinicians are interested in finding alternative tests that could replace the VCUG. The authors compared the accuracy of two other imaging tests (ultrasound and DMSA renal scan) to see whether these could replace the VCUG test. Neither test was found to be sufficiently accurate to replace the VCUG test. Although the DMSA scan seems to be good at ruling out high‐grade VUR, it falsely labels many children as being at risk for high‐grade VUR. Accordingly, DMSA does not appear to be useful as a screening test.","7","John Wiley & Sons, Ltd","1465-1858","*Radiopharmaceuticals; *Technetium Tc 99m Dimercaptosuccinic Acid; Adolescent; Child; Child, Preschool; Cohort Studies; Cross-Sectional Studies; Humans; Infant; Infant, Newborn; ROC Curve; Radionuclide Imaging; Sensitivity and Specificity; Severity of Illness Index; Ultrasonography; Urinary Tract Infections [*complications]; Vesico-Ureteral Reflux [complications, *diagnostic imaging]; Young Adult","10.1002/14651858.CD010657.pub2","http://dx.doi.org/10.1002/14651858.CD010657.pub2","Kidney and Transplant"
"CD010502.PUB2","Cohen, JF; Bertille, N; Cohen, R; Chalumeau, M","Rapid antigen detection test for group A streptococcus in children with pharyngitis","Cochrane Database of Systematic Reviews","2016","Abstract - Background Group A streptococcus (GAS) accounts for 20% to 40% of cases of pharyngitis in children; the remaining cases are caused by viruses. Compared with throat culture, rapid antigen detection tests (RADTs) offer diagnosis at the point of care (within five to 10 minutes). Objectives To determine the diagnostic accuracy of RADTs for diagnosing GAS in children with pharyngitis. To assess the relative diagnostic accuracy of the two major types of RADTs (enzyme immunoassays (EIA) and optical immunoassays (OIA)) by indirect and direct comparison. Search methods We searched CENTRAL, MEDLINE, EMBASE, Web of Science, CDSR, DARE, MEDION and TRIP (January 1980 to July 2015). We also conducted related citations tracking via PubMed, handsearched reference lists of included studies and relevant review articles, and screened all articles citing included studies via Google Scholar. Selection criteria We included studies that compared RADT for GAS pharyngitis with throat culture on a blood agar plate in a microbiology laboratory in children seen in ambulatory care. Data collection and analysis Two review authors independently screened titles and abstracts for relevance, assessed full texts for inclusion, and carried out data extraction and quality assessment using the QUADAS‐2 tool. We used bivariate meta‐analysis to estimate summary sensitivity and specificity, and to investigate heterogeneity across studies. We compared the accuracy of EIA and OIA tests using indirect and direct evidence. Main results We included 98 unique studies in the review (116 test evaluations; 101,121 participants). The overall methodological quality of included studies was poor, mainly because many studies were at high risk of bias regarding patient selection and the reference standard used (in 73% and 43% of test evaluations, respectively). In studies in which all participants underwent both RADT and throat culture (105 test evaluations; 58,244 participants; median prevalence of participants with GAS was 29.5%), RADT had a summary sensitivity of 85.6%; 95% confidence interval (CI) 83.3 to 87.6 and a summary specificity of 95.4%; 95% CI 94.5 to 96.2. There was substantial heterogeneity in sensitivity across studies; specificity was more stable. There was no evidence of a trade‐off between sensitivity and specificity. Heterogeneity in accuracy was not explained by study‐level characteristics such as whether an enrichment broth was used before plating, mean age and clinical severity of participants, and GAS prevalence. The sensitivity of EIA and OIA tests was comparable (summary sensitivity 85.4% versus 86.2%). Sensitivity analyses showed that summary estimates of sensitivity and specificity were stable in low risk of bias studies. Authors' conclusions In a population of 1000 children with a GAS prevalence of 30%, 43 patients with GAS will be missed. Whether or not RADT can be used as a stand‐alone test to rule out GAS will depend mainly on the epidemiological context. The sensitivity of EIA and OIA tests seems comparable. RADT specificity is sufficiently high to ensure against unnecessary use of antibiotics. Based on these results, we would expect that amongst 100 children with strep throat, 86 would be correctly detected with the rapid test while 14 would be missed and not receive antibiotic treatment. Plain language summary What is the performance of rapid tests for the diagnosis of strep throat in children? Background and aims Sore throat is very common in children. It can be caused by viruses or bacteria. The bacterium most frequently identified during sore throat in children is group A streptococcus ('strep throat'). Amongst children with sore throat, antibiotic treatment is only useful in those with strep throat. Simple, rapid tests for the diagnosis of strep throat have been available since the 1980s. Physicians can do a rapid test at the point of care by swabbing the throat. Based on the result of the rapid test, they can then decide if antibiotics are needed. We reviewed the evidence about the performance of rapid tests for correctly detecting strep throat in children seen in Outpatient departments with a main complaint of sore throat. Study characteristics We searched for studies published in any language from January 1980 to July 2015. We found 98 unique studies, for a total of 116 test evaluations, involving 101,121 children. The number of participants ranged from 42 to 11,644 across test evaluations. The proportion of children with strep throat ranged from 9.5% to 66.6% across test evaluations. Quality of the evidence Important study design features were frequently not reported. The overall methodological quality of included studies was poor. For most studies, we had concerns about the ways in which participants were selected. Key results On average, rapid tests for strep throat had a sensitivity (ability to correctly detect people with the disease) of 86% and a specificity (ability to correctly identify people who do not have the disease) of 95%. There was substantial variability in rapid test performance across studies, which was not explained by study characteristics, including methodological quality. The two types of rapid tests under evaluation seemed to have comparable sensitivity (85.4% versus 86.2% for enzyme immunoassays and optical immunoassays, respectively). Based on these results, we would expect that amongst 100 children with strep throat, 86 would be correctly detected with the rapid test while 14 would be missed and not receive antibiotic treatment. Of 100 children with non‐streptococcal sore throat, 95 would be correctly classified as such with the rapid test while 5 would be misdiagnosed as having strep throat and receive unnecessary antibiotics.","7","John Wiley & Sons, Ltd","1465-1858","Adolescent; Antigens, Bacterial [*analysis]; Child; Humans; Immunoenzyme Techniques [*standards, statistics & numerical data]; Pharyngitis [*microbiology]; Reference Standards; Sensitivity and Specificity; Streptococcal Infections [*diagnosis]; Streptococcus pyogenes [*immunology]","10.1002/14651858.CD010502.pub2","http://dx.doi.org/10.1002/14651858.CD010502.pub2","Acute Respiratory Infections"
"CD012179","Nisenblat, V; Bossuyt, PMM; Shaikh, R; Farquhar, C; Jordan, V; Scheffers, CS; Mol, BWJ; Johnson, N; Hull, ML","Blood biomarkers for the non‐invasive diagnosis of endometriosis","Cochrane Database of Systematic Reviews","2016","Abstract - Background About 10% of reproductive‐aged women suffer from endometriosis, a costly chronic disease causing pelvic pain and subfertility. Laparoscopy is the gold standard diagnostic test for endometriosis, but is expensive and carries surgical risks. Currently, there are no non‐invasive or minimally invasive tests available in clinical practice to accurately diagnose endometriosis. Although other reviews have assessed the ability of blood tests to diagnose endometriosis, this is the first review to use Cochrane methods, providing an update on the rapidly expanding literature in this field. Objectives To evaluate blood biomarkers as replacement tests for diagnostic surgery and as triage tests to inform decisions on surgery for endometriosis. Specific objectives include: 1. To provide summary estimates of the diagnostic accuracy of blood biomarkers for the diagnosis of peritoneal, ovarian and deep infiltrating pelvic endometriosis, compared to surgical diagnosis as a reference standard. 2. To assess the diagnostic utility of biomarkers that could differentiate ovarian endometrioma from other ovarian masses. Search methods We did not restrict the searches to particular study designs, language or publication dates. We searched CENTRAL to July 2015, MEDLINE and EMBASE to May 2015, as well as these databases to 20 April 2015: CINAHL, PsycINFO, Web of Science, LILACS, OAIster, TRIP, ClinicalTrials.gov, DARE and PubMed. Selection criteria We considered published, peer‐reviewed, randomised controlled or cross‐sectional studies of any size, including prospectively collected samples from any population of reproductive‐aged women suspected of having one or more of the following target conditions: ovarian, peritoneal or deep infiltrating endometriosis (DIE). We included studies comparing the diagnostic test accuracy of one or more blood biomarkers with the findings of surgical visualisation of endometriotic lesions. Data collection and analysis Two authors independently collected and performed a quality assessment of data from each study. For each diagnostic test, we classified the data as positive or negative for the surgical detection of endometriosis, and we calculated sensitivity and specificity estimates. We used the bivariate model to obtain pooled estimates of sensitivity and specificity whenever sufficient datasets were available. The predetermined criteria for a clinically useful blood test to replace diagnostic surgery were a sensitivity of 0.94 and a specificity of 0.79 to detect endometriosis. We set the criteria for triage tests at a sensitivity of ≥ 0.95 and a specificity of ≥ 0.50, which 'rules out' the diagnosis with high accuracy if there is a negative test result (SnOUT test), or a sensitivity of ≥ 0.50 and a specificity of ≥ 0.95, which 'rules in' the diagnosis with high accuracy if there is a positive result (SpIN test). Main results We included 141 studies that involved 15,141 participants and evaluated 122 blood biomarkers. All the studies were of poor methodological quality. Studies evaluated the blood biomarkers either in a specific phase of the menstrual cycle or irrespective of the cycle phase, and they tested for them in serum, plasma or whole blood. Included women were a selected population with a high frequency of endometriosis (10% to 85%), in which surgery was indicated for endometriosis, infertility work‐up or ovarian mass. Seventy studies evaluated the diagnostic performance of 47 blood biomarkers for endometriosis (44 single‐marker tests and 30 combined tests of two to six blood biomarkers). These were angiogenesis/growth factors, apoptosis markers, cell adhesion molecules, high‐throughput markers, hormonal markers, immune system/inflammatory markers, oxidative stress markers, microRNAs, tumour markers and other proteins. Most of these biomarkers were assessed in small individual studies, often using different cut‐off thresholds, and we could only perform meta‐analyses on the data sets for anti‐endometrial antibodies, interleukin‐6 (IL‐6), cancer antigen‐19.9 (CA‐19.9) and CA‐125. Diagnostic estimates varied significantly between studies for each of these biomarkers, and CA‐125 was the only marker with sufficient data to reliably assess sources of heterogeneity. The mean sensitivities and specificities of anti‐endometrial antibodies (4 studies, 759 women) were 0.81 (95% confidence interval (CI) 0.76 to 0.87) and 0.75 (95% CI 0.46 to 1.00). For IL‐6, with a cut‐off value of > 1.90 to 2.00 pg/ml (3 studies, 309 women), sensitivity was 0.63 (95% CI 0.52 to 0.75) and specificity was 0.69 (95% CI 0.57 to 0.82). For CA‐19.9, with a cut‐off value of > 37.0 IU/ml (3 studies, 330 women), sensitivity was 0.36 (95% CI 0.26 to 0.45) and specificity was 0.87 (95% CI 0.75 to 0.99). Studies assessed CA‐125 at different thresholds, demonstrating the following mean sensitivities and specificities: for cut‐off > 10.0 to 14.7 U/ml: 0.70 (95% CI 0.63 to 0.77) and 0.64 (95% CI 0.47 to 0.82); for cut‐off > 16.0 to 17.6 U/ml: 0.56 (95% CI 0.24, 0.88) and 0.91 (95% CI 0.75, 1.00); for cut‐off > 20.0 U/ml: 0.67 (95% CI 0.50 to 0.85) and 0.69 (95% CI 0.58 to 0.80); for cut‐off > 25.0 to 26.0 U/ml: 0.73 (95% CI 0.67 to 0.79) and 0.70 (95% CI 0.63 to 0.77); for cut‐off > 30.0 to 33.0 U/ml: 0.62 (95% CI 0.45 to 0.79) and 0.76 (95% CI 0.53 to 1.00); and for cut‐off > 35.0 to 36.0 U/ml: 0.40 (95% CI 0.32 to 0.49) and 0.91 (95% CI 0.88 to 0.94). We could not statistically evaluate other biomarkers meaningfully, including biomarkers that were assessed for their ability to differentiate endometrioma from other benign ovarian cysts. Eighty‐two studies evaluated 97 biomarkers that did not differentiate women with endometriosis from disease‐free controls. Of these, 22 biomarkers demonstrated conflicting results, with some studies showing differential expression and others no evidence of a difference between the endometriosis and control groups. Authors' conclusions Of the biomarkers that were subjected to meta‐analysis, none consistently met the criteria for a replacement or triage diagnostic test. A subset of blood biomarkers could prove useful either for detecting pelvic endometriosis or for differentiating ovarian endometrioma from other benign ovarian masses, but there was insufficient evidence to draw meaningful conclusions. Overall, none of the biomarkers displayed enough accuracy to be used clinically outside a research setting. We also identified blood biomarkers that demonstrated no diagnostic value in endometriosis and recommend focusing research resources on evaluating other more clinically useful biomarkers. Plain language summary Blood biomarkers for the non‐invasive diagnosis of endometriosis Review Question How accurate are blood tests in detecting endometriosis? Can any blood test be accurate enough to replace or reduce the need for surgery in the diagnosis of endometriosis? Background Women with endometriosis have endometrial tissue (the tissue that lines the womb and is shed during menstruation) growing outside the womb within the pelvic cavity. This tissue responds to reproductive hormones, causing painful periods, chronic lower abdominal pain and difficulty conceiving. Currently, the only reliable way of diagnosing endometriosis is to perform keyhole surgery and visualise the endometrial deposits inside the abdomen. Because surgery is risky and expensive, we evaluated whether the results of blood tests (blood biomarkers) can help to detect endometriosis non‐invasively. An accurate blood test could lead to the diagnosis of endometriosis without the need for surgery, or it could reduce the need for diagnostic surgery to a group of women who were most likely to have endometriosis. Separate Cochrane reviews from this series evaluate other non‐invasive ways of diagnosing endometriosis using urine, imaging, endometrial and combination tests. Study characteristics The evidence included in this review is current to July 2015. We included 141 studies involving 15,141 participants. All studies evaluated reproductive‐aged women who were undertaking diagnostic surgery because they were suspected of having one or more of the following target conditions: ovarian, peritoneal or deep infiltrating endometriosis (DIE). Cancer antigen‐125 (CA‐125) was the most common blood biomarker studied. Seventy studies evaluated 47 blood biomarkers that were expressed differently in women with and without endometriosis, and 82 studies identified 97 biomarkers that did not distinguish between the two groups. Twenty‐two biomarkers were in both categories. Key results Only four of the assessed biomarkers (anti‐endometrial Abs (anti‐endometrial autoantibodies), interleukin‐6 (IL‐6), CA‐19.9 and CA‐125) were evaluated by enough studies to provide a meaningful assessment of test accuracy. None of these tests was accurate enough to replace diagnostic surgery. Several studies identified biomarkers that might be of value in diagnosing endometriosis, but there are too few reports to be sure of their diagnostic benefit. Overall, there is not enough evidence to recommend testing for any blood biomarker in clinical practice to diagnose endometriosis. Quality of the evidence Generally, the reports were of low methodological quality, and most blood tests were only assessed by a single or a small number of studies. When the same biomarker was studied, there were significant differences in how studies were conducted, the group of women studied and the cut‐offs used to determine a positive result. Future research More high quality research trials are necessary to accurately assess the diagnostic potential of certain blood biomarkers, whose diagnostic value for endometriosis was suggested by a limited number of studies.","5","John Wiley & Sons, Ltd","1465-1858","Adult; Autoantibodies [blood]; Biomarkers [*blood]; CA-125 Antigen [blood]; CA-19-9 Antigen [blood]; Endometriosis [*diagnosis]; Endometrium [immunology]; Female; Humans; Interleukin-6 [blood]; Ovarian Diseases [*diagnosis]; Pelvis; Peritoneal Diseases [*diagnosis]; Randomized Controlled Trials as Topic","10.1002/14651858.CD012179","http://dx.doi.org/10.1002/14651858.CD012179","Gynaecology and Fertility"
"CD012165","Gupta, D; Hull, ML; Fraser, I; Miller, L; Bossuyt, PMM; Johnson, N; Nisenblat, V","Endometrial biomarkers for the non‐invasive diagnosis of endometriosis","Cochrane Database of Systematic Reviews","2016","Abstract - Background About 10% of reproductive‐aged women suffer from endometriosis, which is a costly, chronic disease that causes pelvic pain and subfertility. Laparoscopy is the gold standard diagnostic test for endometriosis, but it is expensive and carries surgical risks. Currently, there are no non‐invasive tests available in clinical practice that accurately diagnose endometriosis. This is the first diagnostic test accuracy review of endometrial biomarkers for endometriosis that utilises Cochrane methodologies, providing an update on the rapidly expanding literature in this field. Objectives To determine the diagnostic accuracy of the endometrial biomarkers for pelvic endometriosis, using a surgical diagnosis as the reference standard. We evaluated the tests as replacement tests for diagnostic surgery and as triage tests to inform decisions to undertake surgery for endometriosis. Search methods We did not restrict the searches to particular study designs, language or publication dates. To identify trials, we searched the following databases: CENTRAL (2015, July), MEDLINE (inception to May 2015), EMBASE (inception to May 2015), CINAHL (inception to April 2015), PsycINFO (inception to April 2015), Web of Science (inception to April 2015), LILACS (inception to April 2015), OAIster (inception to April 2015), TRIP (inception to April 2015) and ClinicalTrials.gov (inception to April 2015). We searched DARE and PubMed databases up to April 2015 to identify reviews and guidelines as sources of references to potentially relevant studies. We also performed searches for papers recently published and not yet indexed in the major databases. The search strategies incorporated words in the title, abstract, text words across the record and the medical subject headings (MeSH). Selection criteria We considered published peer‐reviewed, randomised controlled or cross‐sectional studies of any size that included prospectively collected samples from any population of reproductive‐aged women suspected of having one or more of the following target conditions: ovarian, peritoneal or deep infiltrating endometriosis (DIE). Data collection and analysis Two authors independently extracted data from each study and performed a quality assessment. For each endometrial diagnostic test, we classified the data as positive or negative for the surgical detection of endometriosis and calculated the estimates of sensitivity and specificity. We considered two or more tests evaluated in the same cohort as separate data sets. We used the bivariate model to obtain pooled estimates of sensitivity and specificity whenever sufficient data were available. The predetermined criteria for a clinically useful test to replace diagnostic surgery was one with a sensitivity of 94% and a specificity of 79%. The criteria for triage tests were set at sensitivity at or above 95% and specificity at or above 50%, which in case of negative results rules out the diagnosis (SnOUT test) or sensitivity at or above 50% with specificity at or above 95%, which in case of positive result rules in the diagnosis (SpIN test). Main results We included 54 studies involving 2729 participants, most of which were of poor methodological quality. The studies evaluated endometrial biomarkers either in specific phases of the menstrual cycle or outside of it, and the studies tested the biomarkers either in menstrual fluid, in whole endometrial tissue or in separate endometrial components. Twenty‐seven studies evaluated the diagnostic performance of 22 endometrial biomarkers for endometriosis. These were angiogenesis and growth factors (PROK‐1), cell‐adhesion molecules (integrins α3β1, α4β1, β1 and α6), DNA‐repair molecules (hTERT), endometrial and mitochondrial proteome, hormonal markers (CYP19, 17βHSD2, ER‐α, ER‐β), inflammatory markers (IL‐1R2), myogenic markers (caldesmon, CALD‐1), neural markers (PGP 9.5, VIP, CGRP, SP, NPY, NF) and tumour markers (CA‐125). Most of these biomarkers were assessed in single studies, whilst only data for PGP 9.5 and CYP19 were available for meta‐analysis. These two biomarkers demonstrated significant diversity for the diagnostic estimates between the studies; however, the data were too limited to reliably determine the sources of heterogeneity. The mean sensitivities and specificities of PGP 9.5 (7 studies, 361 women) were 0.96 (95% confidence interval (CI) 0.91 to 1.00) and 0.86 (95% CI 0.70 to 1.00), after excluding one outlier study, and for CYP19 (8 studies, 444 women), they were were 0.77 (95% CI 0.70 to 0.85) and 0.74 (95% CI 0.65 to 84), respectively. We could not statistically evaluate other biomarkers in a meaningful way. An additional 31 studies evaluated 77 biomarkers that showed no evidence of differences in expression levels between the groups of women with and without endometriosis. Authors' conclusions We could not statistically evaluate most of the biomarkers assessed in this review in a meaningful way. In view of the low quality of most of the included studies, the findings of this review should be interpreted with caution. Although PGP 9.5 met the criteria for a replacement test, it demonstrated considerable inter study heterogeneity in diagnostic estimates, the source of which could not be determined. Several endometrial biomarkers, such as endometrial proteome, 17βHSD2, IL‐1R2, caldesmon and other neural markers (VIP, CGRP, SP, NPY and combination of VIP, PGP 9.5 and SP) showed promising evidence of diagnostic accuracy, but there was insufficient or poor quality evidence for any clinical recommendations. Laparoscopy remains the gold standard for the diagnosis of endometriosis, and using any non‐invasive tests should only be undertaken in a research setting. We have also identified a number of biomarkers that demonstrated no diagnostic value for endometriosis. We recommend that researchers direct future studies towards biomarkers with high diagnostic potential in good quality diagnostic studies. Plain language summary Endometrial biomarkers for the non‐invasive diagnosis of endometriosis Review question Can physicians use biomarkers (distinctive molecules, genes or other characteristics that appear in certain conditions) to reduce the need to surgically diagnose endometriosis? Background The endometrium refers to the tissue that lines the womb and is shed during menstruation. Women with endometriosis have endometrial tissue growing outside the womb, within the pelvic cavity. This tissue responds to reproductive hormones causing painful periods, chronic lower abdominal pain and difficulty conceiving. Currently the only reliable way of diagnosing endometriosis is to perform keyhole surgery and visualise the endometriotic deposits inside the abdomen. Because surgery is risky and expensive, various tests within the endometrium that can be obtained during an in‐office womb sampling procedure have been assessed for their ability to detect endometriosis non‐invasively or with minimal invasion. An accurate test could lead to the diagnosis of endometriosis without the need for surgery, or it could reduce the need for diagnostic surgery so only women who were most likely to have endometriosis would require it. Review teams have also evaluated other non‐invasive ways of diagnosing endometriosis using blood, urine and imaging tests as well as a combination of several testing methods in separate Cochrane reviews within this series. Study characteristics The evidence in this review is current to April 2015. We included 54 studies involving 2729 participants. All studies evaluated reproductive‐aged women who were undertaking diagnostic surgery to investigate symptoms of endometriosis or for other indications. Twenty‐six studies evaluated the role of 22 different biomarkers in diagnosing endometriosis, and 31 studies identified 77 additional biomarkers that had no value in differentiating between women with and without the disease. Key results and quality of evidence Only two of the assessed biomarkers, a neural fibre marker PGP 9.5 and hormonal marker CYP19, were assessed in sufficient number of studies to obtain meaningful results. PGP 9.5 identified endometriosis with enough accuracy to replace surgical diagnosis. Several additional biomarkers (endometrial proteome, 17βHSD2, IL‐1R2, caldesmon and other neural markers) show promise in detecting endometriosis, but there are too few studies to be sure of their diagnostic value. The studies differed in how they were conducted, which groups of women were studied and how the surgery was undertaken. The reports were of low methodological quality, which is why readers cannot consider these results to be reliable unless confirmed in large, high quality studies. Overall, there is not enough evidence to recommend any endometrial test for use in clinical practice for the diagnosis of endometriosis. Future research Further high quality research is necessary to accurately evaluate the diagnostic potential of the endometrial biomarkers for the diagnosis of endometriosis.","4","John Wiley & Sons, Ltd","1465-1858","Biomarkers [*analysis]; Endometriosis [*diagnosis]; Endometrium [*chemistry]; Female; Humans; Menstrual Cycle; Menstruation [metabolism]","10.1002/14651858.CD012165","http://dx.doi.org/10.1002/14651858.CD012165","Gynaecology and Fertility"
"CD011602.PUB2","Pavlov, CS; Casazza, G; Semenistaia, M; Nikolova, D; Tsochatzis, E; Liusina, E; Ivashkin, VT; Gluud, C","Ultrasonography for diagnosis of alcoholic cirrhosis in people with alcoholic liver disease","Cochrane Database of Systematic Reviews","2016","Abstract - Background Heavy alcohol consumption causes alcoholic liver disease and is a causal factor of many types of liver injuries and concomitant diseases. It is a true systemic disease that may damage the digestive tract, the nervous system, the heart and vascular system, the bone and skeletal muscle system, and the endocrine and immune system, and can lead to cancer. Liver damage in turn, can present as multiple alcoholic liver diseases, including fatty liver, steatohepatitis, fibrosis, alcoholic cirrhosis, and hepatocellular carcinoma, with presence or absence of hepatitis B or C virus infection. There are three scarring types (fibrosis) that are most commonly found in alcoholic liver disease: centrilobular scarring, pericellular fibrosis, and periportal fibrosis. When liver fibrosis progresses, alcoholic cirrhosis occurs. Hepatocellular carcinoma occurs in 5% to 15% of people with alcoholic cirrhosis, but people in whom hepatocellular carcinoma has developed are often co‐infected with hepatitis B or C virus. Abstinence from alcohol may help people with alcoholic disease in improving their prognosis of survival at any stage of their disease; however, the more advanced the stage, the higher the risk of complications, co‐morbidities, and mortality, and lesser the effect of abstinence. Being abstinent one month after diagnosis of early cirrhosis will improve the chance of a seven‐year life expectancy by 1.6 times. Liver transplantation is the only radical method that may change the prognosis of a person with alcoholic liver disease; however, besides the difficulties of finding a suitable liver transplant organ, there are many other factors that may influence a person's survival. Ultrasound is an inexpensive method that has been used for years in clinical practice to diagnose alcoholic cirrhosis. Ultrasound parameters for assessing cirrhosis in people with alcoholic liver disease encompass among others liver size, bluntness of the liver edge, coarseness of the liver parenchyma, nodularity of the liver surface, size of the lymph nodes around the hepatic artery, irregularity and narrowness of the inferior vena cava, portal vein velocity, and spleen size. Diagnosis of cirrhosis by ultrasound, especially in people who are asymptomatic, may have its advantages for the prognosis, motivation, and treatment of these people to decrease their alcohol consumption or become abstinent. Timely diagnosis of alcoholic cirrhosis in people with alcoholic liver disease is the cornerstone for evaluation of prognosis or choosing treatment strategies. Objectives To determine the diagnostic accuracy of ultrasonography for detecting the presence or absence of cirrhosis in people with alcoholic liver disease compared with liver biopsy as reference standard. To determine the diagnostic accuracy of any of the ultrasonography tests, B‐mode or echo‐colour Doppler ultrasonography, used singly or combined, or plus ultrasonography signs, or a combination of these, for detecting hepatic cirrhosis in people with alcoholic liver disease compared with liver biopsy as a reference standard, irrespective of sequence. Search methods We performed searches in The Cochrane Hepato‐Biliary Group Controlled Trials Register, The Cochrane Hepato‐Biliary Group Diagnostic Test Accuracy Studies Register,  The Cochrane Library  (Wiley), MEDLINE (OvidSP), EMBASE (OvidSP), and the Science Citation Index Expanded to 8 January 2015. We applied no language limitations. We screened study references of the retrieved studies to identify other potentially relevant studies for inclusion in the review and read abstract and poster publications. Selection criteria Three review authors independently identified studies for possible inclusion in the review. We excluded references not fulfilling the inclusion criteria of the review protocol. We sent e‐mails to study authors. The included studies had to evaluate ultrasound in the diagnosis of hepatic cirrhosis using only liver biopsy as the reference standard. The maximum time interval of investigation with liver biopsy and ultrasonography should not have exceeded six months. In addition, ultrasonography could have been performed before or after liver biopsy. Data collection and analysis We followed the  Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy . Main results The review included two studies that provided numerical data regarding alcoholic cirrhosis in 205 men and women with alcoholic liver disease. Although there were no applicability concerns in terms of participant selection, index text, and reference standard, we judged the two studies at high risk of bias. Participants in both studies had undergone both liver biopsy and ultrasonography investigations. The studies shared only a few comparable clinical signs and symptoms (index tests). We decided to not perform a meta‐analysis due to the high risk of bias and the high degree of heterogeneity of the included studies. Authors' conclusions As the accuracy of ultrasonography in the two included studies was not informative enough, we could not recommend the use of ultrasonography as a diagnostic tool for liver cirrhosis in people with alcoholic liver disease. In order to be able to answer the review questions, we need diagnostic ultrasonography prospective studies of adequate sample size, enrolling only participants with alcoholic liver disease. The design and report of the studies should follow the Standards for Reporting of Diagnostic Accuracy. The sonographic features, with validated cut‐offs, which may help identify clinical signs used for diagnosis of fibrosis in alcoholic liver disease, should be carefully selected to achieve maximum diagnostic accuracy on ultrasonography. Plain language summary Ultrasonography for diagnosis of alcoholic cirrhosis in people with alcoholic liver disease Background   Heavy alcohol consumption causes alcoholic liver disease and may lead to a number of other concomitant diseases. Alcohol may damage the function of body organs and can cause cancer. Liver damage due to excessive alcohol consumption is usually presented as fatty liver (build‐up of fats in the liver), steatohepatitis (inflammation of the liver with concurrent fat accumulation in the liver), fibrosis (fibrous degeneration), alcoholic cirrhosis (scarring of the liver), and hepatocellular carcinoma (most common type of liver cancer). When liver fibrosis progresses, alcoholic cirrhosis occurs. Abstinence from alcohol may help people with alcoholic disease to improve their health at any stage of their disease; however, the more advanced the stage, the higher the risk of complications, co‐morbidities (presence of other diseases), and mortality (death), and lesser the effect of abstinence. Abstinence from alcohol one month after diagnosis of early cirrhosis will improve the chance of a seven‐year life expectancy by 1.6 times. Liver transplantation (replacement of a diseased liver) is the only radical method that may change the prognosis of a person with alcoholic liver disease; however, besides the difficulties of finding a suitable liver transplant organ, there are many other factors that may influence a person's survival after transplantation. Ultrasound is an inexpensive method that has been used for years in clinical practice to diagnose alcoholic cirrhosis. Ultrasound parameters for assessing cirrhosis in people with alcoholic liver disease encompass among others liver size, bluntness of the liver edge, coarseness of the liver parenchyma (part of the liver that filters blood to remove toxins), nodularity (unevenness) of the liver surface, size of the lymph nodes (small glands that filter lymph) around the hepatic artery (which supplies oxygenated blood to the liver), irregularity and narrowness of the inferior vena cava (which carries blood from the lower body to the heart), portal vein velocity, and spleen size. Diagnosis of cirrhosis by ultrasound, especially in people who have no symptoms, may have its advantages for the prognosis, motivation, and treatment of these people to decrease their alcohol consumption or become abstinent. Timely diagnosis of alcoholic cirrhosis in people with alcoholic liver disease is important for evaluation of prognosis or choosing treatment strategies. Aim   The primary review aim was to determine the diagnostic accuracy of ultrasound for detecting the presence or absence of cirrhosis in people with alcoholic liver disease compared with liver biopsy (where a small needle is inserted into the liver to collect a sample, which is then examined in a laboratory) as reference standard (i.e., the best available test). The secondary aim of the review was to determine the diagnostic accuracy of any of the ultrasound tests, B‐mode (a two‐dimensional ultrasound image display composed of bright dots representing the ultrasound echoes) or echo‐colour Doppler ultrasound (a colour ultrasound image showing blood flow through the liver), used singly or combined, or plus ultrasound signs, or a combination of these, for detecting hepatic cirrhosis in people with alcoholic liver disease compared with liver biopsy as a reference standard. Methods   We searched the medical literature to retrieve studies for the review to 8 January 2015. Results   We identified two studies; one from 1985, performed in France, and the other from 2013, performed in South Korea. We could not analyse the data as the two studies with 205 participants in total were very different and they shared only a few clinical signs and symptoms for assessment of cirrhosis. We considered the studies at high risk of bias (the quality of the evidence was low). Funding   One of the two studies was sponsored by a grant from the Ministry of Health and Welfare, Republic of Korea. Conclusions   The review authors cannot recommend the use of ultrasound as a diagnostic tool for liver cirrhosis in people with alcoholic liver disease as the obtained study data were insufficient for analysis. Diagnostic ultrasound prospective studies with a large number of people and similar signs and features on ultrasound imaging are needed to establish how good the test is in detecting cirrhosis in people with alcoholic liver disease.","3","John Wiley & Sons, Ltd","1465-1858","Female; Humans; Liver Cirrhosis, Alcoholic [*diagnostic imaging]; Liver Diseases, Alcoholic [*complications]; Male; Prospective Studies; Ultrasonography","10.1002/14651858.CD011602.pub2","http://dx.doi.org/10.1002/14651858.CD011602.pub2","Hepato-Biliary"
"CD010360.PUB2","Ratnavelu, NDG; Brown, AP; Mallett, S; Scholten, RJPM; Patel, A; Founta, C; Galaal, K; Cross, P; Naik, R","Intraoperative frozen section analysis for the diagnosis of early stage ovarian cancer in suspicious pelvic masses","Cochrane Database of Systematic Reviews","2016","Abstract - Background Women with suspected early‐stage ovarian cancer need surgical staging which involves taking samples from areas within the abdominal cavity and retroperitoneal lymph nodes in order to inform further treatment. One potential strategy is to surgically stage all women with suspicious ovarian masses, without any histological information during surgery. This avoids incomplete staging, but puts more women at risk of potential surgical over‐treatment. A second strategy is to perform a two‐stage procedure to remove the pelvic mass and subject it to paraffin sectioning, which involves formal tissue fixing with formalin and paraffin embedding, prior to ultrathin sectioning and multiple site sampling of the tumour. Surgeons may then base further surgical staging on this histology, reducing the rate of over‐treatment, but conferring additional surgical and anaesthetic morbidity. A third strategy is to perform a rapid histological analysis on the ovarian mass during surgery, known as 'frozen section'. Tissues are snap frozen to allow fine tissue sections to be cut and basic histochemical staining to be performed. Surgeons can perform or avoid the full surgical staging procedure depending on the results. However, this is a relatively crude test compared to paraffin sections, which take many hours to perform. With frozen section there is therefore a risk of misdiagnosing malignancy and understaging women subsequently found to have a presumed early‐stage malignancy (false negative), or overstaging women without a malignancy (false positive). Therefore it is important to evaluate the accuracy and usefulness of adding frozen section to the clinical decision‐making process. Objectives To assess the diagnostic test accuracy of frozen section (index test) to diagnose histopathological ovarian cancer in women with suspicious pelvic masses as verified by paraffin section (reference standard). Search methods We searched MEDLINE (January 1946 to January 2015), EMBASE (January 1980 to January 2015) and relevant Cochrane registers. Selection criteria Studies that used frozen section for intraoperative diagnosis of ovarian masses suspicious of malignancy, provided there was sufficient data to construct 2 x 2 tables. We excluded articles without an available English translation. Data collection and analysis Authors independently assessed the methodological quality of included studies using the Quality Assessment of Diagnostic Accuracy Studies tool (QUADAS‐2) domains: patient selection, index test, reference standard, flow and timing. Data extraction converted 3 x 3 tables of per patient results presented in articles into 2 x 2 tables, for two index test thresholds. Main results All studies were retrospective, and the majority reported consecutive sampling of cases. Sensitivity and specificity results were available from 38 studies involving 11,181 participants (3200 with invasive cancer, 1055 with borderline tumours and 6926 with benign tumours, determined by paraffin section as the reference standard). The median prevalence of malignancy was 29% (interquartile range (IQR) 23% to 36%, range 11% to 63%). We assessed test performance using two thresholds for the frozen section test. Firstly, we used a test threshold for frozen sections, defining positive test results as invasive cancer and negative test results as borderline and benign tumours. The average sensitivity was 90.0% (95% confidence interval (CI) 87.6% to 92.0%; with most studies typically reporting range of 71% to 100%), and average specificity was 99.5% (95% CI 99.2% to 99.7%; range 96% to 100%). Similarly, we analysed sensitivity and specificity using a second threshold for frozen section, where both invasive cancer and borderline tumours were considered test positive and benign cases were classified as negative. Average sensitivity was 96.5% (95% CI 95.5% to 97.3%; typical range 83% to 100%), and average specificity was 89.5% (95% CI 86.6% to 91.9%; typical range 58% to 99%). Results were available from the same 38 studies, including the subset of 3953 participants with a frozen section result of either borderline or invasive cancer, based on final diagnosis of malignancy. Studies with small numbers of disease‐negative cases (borderline cases) had more variation in estimates of specificity. Average sensitivity was 94.0% (95% CI 92.0% to 95.5%; range 73% to 100%), and average specificity was 95.8% (95% CI 92.4% to 97.8%; typical range 81% to 100%). Our additional analyses showed that, if the frozen section showed a benign or invasive cancer, the final diagnosis would remain the same in, on average, 94% and 99% of cases, respectively. In cases where the frozen section diagnosis was a borderline tumour, on average 21% of the final diagnoses would turn out to be invasive cancer. In three studies, the same pathologist interpreted the index and reference standard tests, potentially causing bias. No studies reported blinding pathologists to index test results when reporting paraffin sections. In heterogeneity analyses, there were no statistically significant differences between studies with pathologists of different levels of expertise. Authors' conclusions In a hypothetical population of 1000 patients (290 with cancer and 80 with a borderline tumour), if a frozen section positive test result for invasive cancer alone was used to diagnose cancer, on average 261 women would have a correct diagnosis of a cancer, and 706 women would be correctly diagnosed without a cancer. However, 4 women would be incorrectly diagnosed with a cancer (false positive), and 29 with a cancer would be missed (false negative). If a frozen section result of either an invasive cancer or a borderline tumour was used as a positive test to diagnose cancer, on average 280 women would be correctly diagnosed with a cancer and 635 would be correctly diagnosed without. However, 75 women would be incorrectly diagnosed with a cancer and 10 women with a cancer would be missed. The largest discordance is within the reporting of frozen section borderline tumours. Investigation into factors leading to discordance within centres and standardisation of criteria for reporting borderline tumours may help improve accuracy. Some centres may choose to perform surgical staging in women with frozen section diagnosis of a borderline ovarian tumour to reduce the number of false positives. In their interpretation of this review, readers should evaluate results from studies most typical of their population of patients. Plain language summary Is a 'quick diagnosis' test on an ovarian mass during surgery accurate? The issue When women go to their doctor with a mass that could be ovarian cancer, they are normally referred for surgery, since the mass may need to be removed and examined microscopically in a laboratory in a procedure known as paraffin section histopathology. A third of women with ovarian cancer present with a cyst or mass without any visible evidence of spread elsewhere. However, in these apparently early‐stage cancers (confined to the ovary) surgical staging is required to decide if chemotherapy is required. This staging consists of sampling tissues within the abdomen, including lymph nodes. Different staging strategies exist. One is to perform surgical staging for all women who might have a cancer, to get information about spread. This may result in complications due to additional surgical procedures that may turn out to be unnecessary in approximately two thirds of women. A second strategy is to perform an operation to remove just the suspicious mass and await the paraffin section diagnosis. This may result in needing a further operation in one third of women if cancer is confirmed, putting them at increased risks from another operation. A third strategy is to send the mass to the laboratory during the operation for a quick diagnosis, known as 'frozen section'. This helps the surgeon decide if further surgical treatment is required during a single operation. Why is this review important? Frozen section is not as accurate as the traditional slower paraffin section examination, and it entails a risk of incorrect diagnosis, meaning that some women may not have all the samples taken at the initial surgery and may need to undergo a second operation; and others may undergo unnecessary surgical sampling. How was this review conducted? We searched all available studies reporting use of frozen section in women with suspicious ovarian masses. We excluded studies without an English translation and studies without enough information to allow us to analyse the data. What are the findings? We included 38 studies (11,181 women), reporting three types of diagnoses from the frozen section test. 1. Cancer, which occurred in an average of 29% of women. 2. Borderline tumour, which occurred in 8% of women. 3. Benign mass. In a hypothetical group of 1000 patients where 290 have cancer and 80 have a borderline tumour, 261 women would receive a correct diagnosis of a cancer and 706 women would be correctly diagnosed without a cancer based on a frozen section result. However, 4 women would be incorrectly diagnosed as having a cancer where none existed (false positive), and 29 women with cancer would be missed and potentially need further treatment (false negative). If surgeons used a frozen section result of either a cancer or a borderline tumour to diagnose cancer, 280 women would be correctly diagnosed with a cancer and 635 women would be correctly diagnosed without a cancer. However, 75 women would be incorrectly diagnosed as having a cancer, and 10 women with cancer would be missed on the initial test and found to have a cancer after surgery. If the frozen section result reported the mass as benign or malignant, the final diagnosis would remain the same in, on average, 94% and 99% of the cases, respectively. In cases where the frozen section diagnosis was a borderline tumour, there is a chance that the final diagnosis would turn out to be a cancer in, on average, 21% of women. What does this mean? Where the frozen section diagnosis is a borderline tumour, the diagnosis is less accurate than for benign or malignant tumours. Surgeons may choose to perform additional surgery in this group of women at the time of their initial surgery in order to reduce the need for a second operation if the final diagnosis turns out to be a cancer, as it would on average in one out of five of these women.","3","John Wiley & Sons, Ltd","1465-1858","Diagnostic Errors [statistics & numerical data]; False Negative Reactions; False Positive Reactions; Female; Frozen Sections [*methods]; Humans; Intraoperative Period; Neoplasm Staging [*methods]; Ovarian Neoplasms [*pathology, surgery]; Paraffin Embedding; Pelvic Neoplasms [pathology]; Retrospective Studies; Sensitivity and Specificity","10.1002/14651858.CD010360.pub2","http://dx.doi.org/10.1002/14651858.CD010360.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD009591.PUB2","Nisenblat, V; Bossuyt, PMM; Farquhar, C; Johnson, N; Hull, ML","Imaging modalities for the non‐invasive diagnosis of endometriosis","Cochrane Database of Systematic Reviews","2016","Abstract - Background About 10% of women of reproductive age suffer from endometriosis. Endometriosis is a costly chronic disease that causes pelvic pain and subfertility. Laparoscopy, the gold standard diagnostic test for endometriosis, is expensive and carries surgical risks. Currently, no non‐invasive tests that can be used to accurately diagnose endometriosis are available in clinical practice. This is the first review of diagnostic test accuracy of imaging tests for endometriosis that uses Cochrane methods to provide an update on the rapidly expanding literature in this field. Objectives • To provide estimates of the diagnostic accuracy of imaging modalities for the diagnosis of pelvic endometriosis, ovarian endometriosis and deeply infiltrating endometriosis (DIE) versus surgical diagnosis as a reference standard. • To describe performance of imaging tests for mapping of deep endometriotic lesions in the pelvis at specific anatomical sites. Imaging tests were evaluated as replacement tests for diagnostic surgery and as triage tests that would assist decision making regarding diagnostic surgery for endometriosis. Search methods We searched the following databases to 20 April 2015: MEDLINE, CENTRAL, EMBASE, CINAHL, PsycINFO, Web of Science, LILACS, OAIster, TRIP, ClinicalTrials.gov, MEDION, DARE, and PubMed. Searches were not restricted to a particular study design or language nor to specific publication dates. The search strategy incorporated words in the title, abstracts, text words across the record and medical subject headings (MeSH). Selection criteria We considered published peer‐reviewed cross‐sectional studies and randomised controlled trials of any size that included prospectively recruited women of reproductive age suspected of having one or more of the following target conditions: endometrioma, pelvic endometriosis, DIE or endometriotic lesions at specific intrapelvic anatomical locations. We included studies that compared the diagnostic test accuracy of one or more imaging modalities versus findings of surgical visualisation of endometriotic lesions. Data collection and analysis Two review authors independently collected and performed a quality assessment of data from each study. For each imaging test, data were classified as positive or negative for surgical detection of endometriosis, and sensitivity and specificity estimates were calculated. If two or more tests were evaluated in the same cohort, each was considered as a separate data set. We used the bivariate model to obtain pooled estimates of sensitivity and specificity when sufficient data sets were available. Predetermined criteria for a clinically useful imaging test to replace diagnostic surgery included sensitivity ≥ 94% and specificity ≥ 79%. Criteria for triage tests were set at sensitivity ≥ 95% and specificity ≥ 50%, ruling out the diagnosis with a negative result (SnNout test ‐ if sensitivity is high, a negative test rules out pathology) or at sensitivity ≥ 50% with specificity ≥ 95%, ruling in the diagnosis with a positive result (SpPin test ‐ if specificity is high, a positive test rules in pathology). Main results We included 49 studies involving 4807 women: 13 studies evaluated pelvic endometriosis, 10 endometriomas and 15 DIE, and 33 studies addressed endometriosis at specific anatomical sites. Most studies were of poor methodological quality. The most studied modalities were transvaginal ultrasound (TVUS) and magnetic resonance imaging (MRI), with outcome measures commonly demonstrating diversity in diagnostic estimates; however, sources of heterogeneity could not be reliably determined. No imaging test met the criteria for a replacement or triage test for detecting pelvic endometriosis, albeit TVUS approached the criteria for a SpPin triage test. For endometrioma, TVUS (eight studies, 765 participants; sensitivity 0.93 (95% confidence interval (CI) 0.87, 0.99), specificity 0.96 (95% CI 0.92, 0.99)) qualified as a SpPin triage test and approached the criteria for a replacement and SnNout triage test, whereas MRI (three studies, 179 participants; sensitivity 0.95 (95% CI 0.90, 1.00), specificity 0.91 (95% CI 0.86, 0.97)) met the criteria for a replacement and SnNout triage test and approached the criteria for a SpPin test. For DIE, TVUS (nine studies, 12 data sets, 934 participants; sensitivity 0.79 (95% CI 0.69, 0.89) and specificity 0.94 (95% CI 0.88, 1.00)) approached the criteria for a SpPin triage test, and MRI (six studies, seven data sets, 266 participants; sensitivity 0.94 (95% CI 0.90, 0.97), specificity 0.77 (95% CI 0.44, 1.00)) approached the criteria for a replacement and SnNout triage test. Other imaging tests assessed in small individual studies could not be statistically evaluated. TVUS met the criteria for a SpPin triage test in mapping DIE to uterosacral ligaments, rectovaginal septum, vaginal wall, pouch of Douglas (POD) and rectosigmoid. MRI met the criteria for a SpPin triage test for POD and vaginal and rectosigmoid endometriosis. Transrectal ultrasonography (TRUS) might qualify as a SpPin triage test for rectosigmoid involvement but could not be adequately assessed for other anatomical sites because heterogeneous data were scant. Multi‐detector computerised tomography enema (MDCT‐e) displayed the highest diagnostic performance for rectosigmoid and other bowel endometriosis and met the criteria for both SpPin and SnNout triage tests, but studies were too few to provide meaningful results. Diagnostic accuracies were higher for TVUS with bowel preparation (TVUS‐BP) and rectal water contrast (RWC‐TVS) and for 3.0TMRI than for conventional methods, although the paucity of studies precluded statistical evaluation. Authors' conclusions None of the evaluated imaging modalities were able to detect overall pelvic endometriosis with enough accuracy that they would be suggested to replace surgery. Specifically for endometrioma, TVUS qualified as a SpPin triage test. MRI displayed sufficient accuracy to suggest utility as a replacement test, but the data were too scant to permit meaningful conclusions. TVUS could be used clinically to identify additional anatomical sites of DIE compared with MRI, thus facilitating preoperative planning. Rectosigmoid endometriosis was the only site that could be accurately mapped by using TVUS, TRUS, MRI or MDCT‐e. Studies evaluating recent advances in imaging modalities such as TVUS‐BP, RWC‐TVS, 3.0TMRI and MDCT‐e were observed to have high diagnostic accuracies but were too few to allow prudent evaluation of their diagnostic role. In view of the low quality of most of the included studies, the findings of this review should be interpreted with caution. Future well‐designed diagnostic studies undertaken to compare imaging tests for diagnostic test accuracy and costs are recommended. Plain language summary Imaging tests for the non‐invasive diagnosis of endometriosis Review question How accurate are imaging tests in detecting endometriosis? Can any imaging test be accurate enough to replace or reduce the need for surgery in the diagnosis of endometriosis? Background Women with endometriosis have endometrial tissue (the tissue that lines the womb and is shed during menstruation) growing outside the womb within the pelvis, causing chronic abdominal pain and difficulty conceiving. Currently, the only reliable way of diagnosing endometriosis is to perform laparoscopic surgery and visualise the endometrial deposits inside the abdomen. Because surgery is risky and expensive, imaging tests have been assessed for their ability to detect endometriosis non‐invasively. An accurate imaging test could lead to the diagnosis of endometriosis without the need for surgery, or it could reduce the need for surgery, so only women who were most likely to have endometriosis would require it. Furthermore, if imaging tests could accurately predict the location of endometriotic lesions, surgeons would have the information they need to plan and improve their surgical approach. Other non‐invasive ways of diagnosing endometriosis by using urine, blood and endometrial and combination tests have been evaluated in separate Cochrane reviews from this series. Study characteristics Evidence included in this review is current to April 2015. We included 49 studies involving 4807 participants. Thirteen studies evaluated pelvic endometriosis, 10 studies ovarian endometrioma, 15 studies deep endometriosis (endometriosis deeply situated in tissues in the pelvis) and 33 studies endometriosis at specific sites within the pelvic cavity. All studies included women of reproductive age who were undergoing diagnostic surgery because they had symptoms of endometriosis. Key results None of the imaging methods was accurate enough to provide this information on overall pelvic endometriosis. Transvaginal ultrasound identified ovarian endometriosis with enough accuracy to help surgeons determine whether surgery was needed, and magnetic resonance imaging (MRI) was sufficiently accurate to replace surgery in diagnosing endometrioma but was evaluated in only a small number of studies. Other imaging tests were assessed in small individual studies and could not be evaluated in a meaningful way. Transvaginal ultrasound could be used to locate more anatomical sites of deep endometriosis when compared with MRI, helping surgeons better plan an operative procedure. Endometriosis in the lower bowel appears to be relatively accurately identified by both transvaginal and transrectal ultrasound, by MRI and by multi‐detector computerised tomography enema. New types of ultrasound and MRI show a lot of promise in detecting endometriosis but studies are too few to clearly show their diagnostic value. Quality of the evidence Generally the studies were of low methodological quality, and most imaging techniques were assessed by only a small number of studies. Differences between studies involved how they were run, groups of women studied, ways imaging tests were performed and how surgery was undertaken. Future research Additional high‐quality research is needed to accurately evaluate the diagnostic potential of non‐invasive imaging tests for endometriosis.","2","John Wiley & Sons, Ltd","1465-1858","Chronic Disease; Cross‐Sectional Studies; Diagnostic Imaging [*methods]; Endometriosis [*diagnosis, pathology]; Female; Humans; Magnetic Resonance Imaging; Ovarian Diseases [diagnosis, surgery]; Pelvis; Positron‐Emission Tomography; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Ultrasonography","10.1002/14651858.CD009591.pub2","http://dx.doi.org/10.1002/14651858.CD009591.pub2","Gynaecology and Fertility"
"CD011145.PUB2","Creavin, ST; Wisniewski, S; Noel‐Storr, AH; Trevelyan, CM; Hampton, T; Rayment, D; Thom, VM; Nash, KJE; Elhamoui, H; Milligan, R; Patel, AS; Tsivos, DV; Wing, T; Phillips, E; Kellman, SM; Shackleton, HL; Singleton, GF; Neale, BE; Watton, ME; Cullum, S","Mini‐Mental State Examination (MMSE) for the detection of dementia in clinically unevaluated people aged 65 and over in community and primary care populations","Cochrane Database of Systematic Reviews","2016","Abstract - Background The Mini Mental State Examination (MMSE) is a cognitive test that is commonly used as part of the evaluation for possible dementia. Objectives To determine the diagnostic accuracy of the Mini‐Mental State Examination (MMSE) at various cut points for dementia in people aged 65 years and over in community and primary care settings who had not undergone prior testing for dementia. Search methods We searched the specialised register of the Cochrane Dementia and Cognitive Improvement Group, MEDLINE (OvidSP), EMBASE (OvidSP), PsycINFO (OvidSP), LILACS (BIREME), ALOIS, BIOSIS previews (Thomson Reuters Web of Science), and Web of Science Core Collection, including the Science Citation Index and the Conference Proceedings Citation Index (Thomson Reuters Web of Science). We also searched specialised sources of diagnostic test accuracy studies and reviews: MEDION (Universities of Maastricht and Leuven, www.mediondatabase.nl), DARE (Database of Abstracts of Reviews of Effects, via the Cochrane Library), HTA Database (Health Technology Assessment Database, via the Cochrane Library), and ARIF (University of Birmingham, UK, www.arif.bham.ac.uk). We attempted to locate possibly relevant but unpublished data by contacting researchers in this field. We first performed the searches in November 2012 and then fully updated them in May 2014. We did not apply any language or date restrictions to the electronic searches, and we did not use any methodological filters as a method to restrict the search overall. Selection criteria We included studies that compared the 11‐item (maximum score 30) MMSE test (at any cut point) in people who had not undergone prior testing versus a commonly accepted clinical reference standard for all‐cause dementia and subtypes (Alzheimer disease dementia, Lewy body dementia, vascular dementia, frontotemporal dementia). Clinical diagnosis included all‐cause (unspecified) dementia, as defined by any version of the  Diagnostic and Statistical Manual of Mental Disorders  (DSM); International Classification of Diseases (ICD) and the Clinical Dementia Rating. Data collection and analysis At least three authors screened all citations.Two authors handled data extraction and quality assessment. We performed meta‐analysis using the hierarchical summary receiver‐operator curves (HSROC) method and the bivariate method. Main results We retrieved 24,310 citations after removal of duplicates. We reviewed the full text of 317 full‐text articles and finally included 70 records, referring to 48 studies, in our synthesis. We were able to perform meta‐analysis on 28 studies in the community setting (44 articles) and on 6 studies in primary care (8 articles), but we could not extract usable 2 x 2 data for the remaining 14 community studies, which we did not include in the meta‐analysis. All of the studies in the community were in asymptomatic people, whereas two of the six studies in primary care were conducted in people who had symptoms of possible dementia. We judged two studies to be at high risk of bias in the patient selection domain, three studies to be at high risk of bias in the index test domain and nine studies to be at high risk of bias regarding flow and timing. We assessed most studies as being applicable to the review question though we had concerns about selection of participants in six studies and target condition in one study. The accuracy of the MMSE for diagnosing dementia was reported at 18 cut points in the community (MMSE score 10, 14‐30 inclusive) and 10 cut points in primary care (MMSE score 17‐26 inclusive). The total number of participants in studies included in the meta‐analyses ranged from 37 to 2727, median 314 (interquartile range (IQR) 160 to 647). In the community, the pooled accuracy at a cut point of 24 (15 studies) was sensitivity 0.85 (95% confidence interval (CI) 0.74 to 0.92), specificity 0.90 (95% CI 0.82 to 0.95); at a cut point of 25 (10 studies), sensitivity 0.87 (95% CI 0.78 to 0.93), specificity 0.82 (95% CI 0.65 to 0.92); and in seven studies that adjusted accuracy estimates for level of education, sensitivity 0.97 (95% CI 0.83 to 1.00), specificity 0.70 (95% CI 0.50 to 0.85). There was insufficient data to evaluate the accuracy of the MMSE for diagnosing dementia subtypes.We could not estimate summary diagnostic accuracy in primary care due to insufficient data. Authors' conclusions The MMSE contributes to a diagnosis of dementia in low prevalence settings, but should not be used in isolation to confirm or exclude disease. We recommend that future work evaluates the diagnostic accuracy of tests in the context of the diagnostic pathway experienced by the patient and that investigators report how undergoing the MMSE changes patient‐relevant outcomes. Plain language summary Mini‐Mental State Examination (MMSE) for the detection of dementia in people aged over 65 The term 'dementia' covers a group of brain problems that cause gradual deterioration of brain function, thinking skills, and ability to perform everyday tasks (e.g. washing and dressing). People with dementia may also develop problems with their mental health (mood and emotions) and behaviour that are difficult for other people to manage or deal with. The process that causes dementia in the brain is often degenerative (due to brain damage over time). Subtypes of dementia include Alzheimer's disease dementia, vascular dementia, dementia with Lewy bodies and frontotemporal dementia. We aimed to assess the accuracy of the Mini‐Mental State Examination (MMSE), which is commonly used as part of the process when considering a diagnosis of dementia, according to the definition in the  Diagnostic and Statistical Manual of Mental Disorders  ( DSM ). The MMSE is a paper‐based test with a maximum score of 30, with lower scores indicating more severe cognitive problems. The cut point established for the MMSE defines 'normal' cognitive function and is usually set at 24, although theoretically it could fall anywhere from 1 to 30. We searched a wide range of resources and found 24,310 unique citations (hits). We reviewed the full text of 317 academic papers and finally included 70 articles, referring to 48 studies in our review. We included community studies (by which we mean people living in the community who have ) and primary care studies (by which we mean studies that had an office‐based first contact care with a non specialist clinician ‐ which would often be a GP). Two of the studies had serious design weaknesses with regard to their methods for selecting participants, three with regard to the application of the test (MMSE), and nine with regard to the presentation of flow and timing. We were able to do a combined statistical analysis (meta‐analysis) on 28 studies in the community setting (44 articles) and 6 studies in primary care (8 articles), but we could not extract usable data for the remaining 14 community studies. Two of the six studies in primary care were conducted in people who had symptoms of possible dementia. We were able to calculate the summary diagnostic accuracy of the MMSE at three cut points in community‐based studies, but we didn't have enough data to do this in the primary care studies. A perfect test would have sensitivity (ability to identify anyone with dementia) of 1.0 (100%) and specificity (ability to identify people without dementia) of 1.0 (100%). For the MMSE, the summary accuracy at a cut point of 25 (10 studies) was sensitivity 0.87 and specificity 0.82. In seven studies that adjusted accuracy estimates for level of education, we found that the test had a sensitivity of 0.97 and specificity of 0.70. The summary accuracy at a cut point of 24 (15 studies) was sensitivity 0.85 and specificity 0.90. Based on these results, we would expect 85% of people with dementia to be correctly identified with the MMSE, while 15% would be wrongly classified as not having dementia; 90% of those tested would be correctly identified as not having dementia whilst 10% would be false positives and might be referred for further testing. Our results support the use of the MMSE as part of the process for deciding whether or not someone has dementia, but the results of the test should be interpreted in broader context of the individual patient, such as their personality, behaviour and how they are managing at home and in daily life.","1","John Wiley & Sons, Ltd","1465-1858","Aged; Alzheimer Disease [diagnosis]; Community Health Services; Dementia [*diagnosis]; Dementia, Vascular [diagnosis]; Humans; Lewy Body Disease [diagnosis]; Mental Status Schedule; Neuropsychological Tests [*standards]; Primary Health Care; Randomized Controlled Trials as Topic","10.1002/14651858.CD011145.pub2","http://dx.doi.org/10.1002/14651858.CD011145.pub2","Dementia and Cognitive Improvement"
"CD007394.PUB2","Leeflang, MMG; Debets‐Ossenkopp, YJ; Wang, J; Visser, CE; Scholten, RJPM; Hooft, L; Bijlmer, HA; Reitsma, JB; Zhang, M; Bossuyt, PMM; Vandenbroucke‐Grauls, CM","Galactomannan detection for invasive aspergillosis in immunocompromised patients","Cochrane Database of Systematic Reviews","2015","Abstract - Background Invasive aspergillosis is the most common life‐threatening opportunistic invasive mycosis in immunocompromised patients. A test for invasive aspergillosis should neither be too invasive nor too great a burden for the already weakened patient. The serum galactomannan enzyme‐linked immunosorbent assay (ELISA) seems to have the potential to meet both requirements. Objectives To obtain summary estimates of the diagnostic accuracy of galactomannan detection in serum for the diagnosis of invasive aspergillosis. Search methods We searched MEDLINE, EMBASE and Web of Science with both MeSH terms and text words for both aspergillosis and the sandwich ELISA. We checked the reference lists of included studies and review articles for additional studies. We conducted the searches in February 2014. Selection criteria We included cross‐sectional studies, case‐control designs and consecutive series of patients assessing the diagnostic accuracy of galactomannan detection for the diagnosis of invasive aspergillosis in patients with neutropenia or patients whose neutrophils are functionally compromised. The reference standard was composed of the criteria given by the European Organization for Research and Treatment of Cancer (EORTC) and the Mycoses Study Group (MSG). Data collection and analysis Two review authors independently assessed quality and extracted data. We carried out meta‐analysis using the bivariate method. We investigated sources of heterogeneity by adding potential sources of heterogeneity to the model as covariates. Main results We included 54 studies in the review (50 in the meta‐analyses), containing 5660 patients, of whom 586 had proven or probable invasive aspergillosis. When using an optical density index (ODI) of 0.5 as a cut‐off value, the sensitivity of the test was 78% (70% to 85%) and the specificity was 85% (78% to 91%). At a cut‐off value of 1.0 ODI, the sensitivity was 71% (63% to 78%) and the specificity was 90% (86% to 93%). At a cut‐off value of 1.5 ODI, the sensitivity was 63% (49% to 78%) and the specificity was 93% (89% to 97%). None of the potential sources of heterogeneity had a statistically significant effect on either sensitivity or specificity. Authors' conclusions If we used the test at a cut‐off value of 0.5 ODI in a population of 100 patients with a disease prevalence of 11% (overall median prevalence), two patients who have invasive aspergillosis would be missed (sensitivity 78%, 22% false negatives), and 13 patients would be treated unnecessarily or referred unnecessarily for further testing (specificity 85%, 15% false negatives). If we used the test at a cut‐off value of 1.5 in the same population, that would mean that four invasive aspergillosis patients would be missed (sensitivity 61%, 39% false negatives), and six patients would be treated or referred for further testing unnecessarily (specificity 93%, 7% false negatives). These numbers should, however, be interpreted with caution because the results were very heterogeneous. Plain language summary Measurement of serum galactomannan to detect invasive aspergillosis in immunocompromised patients When the immune system of a patient is unable to fight infections (for example because of prolonged corticosteroid therapy, immunosuppressive drugs, haematological malignancies or HIV/AIDS) invasive or systemic aspergillosis can be a life‐threatening mycotic (fungal) infection. Establishing a diagnosis of invasive aspergillosis at an early stage of infection allows early antifungal treatment, but a definitive diagnosis can only be established after death. To enable early diagnosis in a way that is not burdensome for the already weakened patient, galactomannan testing may be promising. Galactomannan is a cell wall component of  Aspergillus  that is excreted by the fungus. Study design The authors of this systematic review found 54 studies that looked at the error rates of this galactomannan test. These studies compared the results of the galactomannan test with the results of a more elaborate diagnostic workup, so that the percentages of false positive results (patients without invasive aspergillosis, according to the elaborate testing, but with a positive galactomannan test) and false negative results (patients with invasive aspergillosis, according to the elaborate testing, but with a negative galactomannan test) could be calculated. The galactomannan test does not result in a yes/no answer, but in a so‐called 'optical density index' (ODI). The authors of the different studies defined the galactomannan test as positive when the ODI was above 0.5, 1.0 or 1.5. Four studies used a different ODI and these were not included in the meta‐analysis. Studies and results When an ODI of 0.5 or higher was said to be positive, the galactomannan test missed 22 out of every 100 patients with invasive aspergillosis and it resulted in a false positive test in 15 out of every 100 patients without invasive aspergillosis. When an ODI of 1.0 or higher was said to be positive, the galactomannan test missed 29 out of every 100 patients with invasive aspergillosis and it resulted in a false positive test in 10 out of every 100 patients without invasive aspergillosis. When an ODI of 1.5 or higher was said to be positive, the galactomannan test missed 37 out of every 100 patients with invasive aspergillosis and it resulted in a false positive test in only 7 out of every 100 patients without invasive aspergillosis. Limitations The studies showed variable results and had small numbers of patients with invasive aspergillosis.","12","John Wiley & Sons, Ltd","1465-1858","*Immunocompromised Host; Aspergillosis [*diagnosis, immunology]; Biomarkers [blood]; Galactose [analogs & derivatives]; Humans; Mannans [*blood]; Opportunistic Infections [*diagnosis, immunology]; Sensitivity and Specificity","10.1002/14651858.CD007394.pub2","http://dx.doi.org/10.1002/14651858.CD007394.pub2","Airways"
"CD012019","Liu, E; Nisenblat, V; Farquhar, C; Fraser, I; Bossuyt, PMM; Johnson, N; Hull, ML","Urinary biomarkers for the non‐invasive diagnosis of endometriosis","Cochrane Database of Systematic Reviews","2015","Abstract - Background About 10% of reproductive‐aged women suffer from endometriosis which is a costly chronic disease that causes pelvic pain and subfertility. Laparoscopy is the 'gold standard' diagnostic test for endometriosis, but it is expensive and carries surgical risks. Currently, there are no simple non‐invasive or minimally‐invasive tests available in clinical practice that accurately diagnoses endometriosis. Objectives 1. To provide summary estimates of the diagnostic accuracy of urinary biomarkers for the diagnosis of pelvic endometriosis compared to surgical diagnosis as a reference standard. 2. To assess the diagnostic utility of biomarkers that could differentiate ovarian endometrioma from other ovarian masses. Urinary biomarkers were evaluated as replacement tests for surgical diagnosis and as triage tests to inform decisions to undertake surgery for endometriosis. Search methods The searches were not restricted to particular study design, language or publication dates. We searched the following databases to 20 April ‐ 31 July 2015: CENTRAL, MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, LILACS, OAIster, TRIP and ClinicalTrials.gov (trial register). MEDION, DARE, and PubMed were also searched to identify reviews and guidelines as reference sources of potentially relevant studies. Recently published papers not yet indexed in the major databases were also sought. The search strategy incorporated words in the title, abstract, text words across the record and the medical subject headings (MeSH) and was modified for each database. Selection criteria Published peer‐reviewed, randomised controlled or cross‐sectional studies of any size were considered, which included prospectively collected samples from any population of reproductive‐aged women suspected of having one or more of the following target conditions: ovarian, peritoneal or deep infiltrating endometriosis (DIE). We included studies comparing the diagnostic test accuracy of one or more urinary biomarkers with surgical visualisation of endometriotic lesions. Data collection and analysis Two authors independently collected and performed a quality assessment of the data from each study. For each diagnostic test, the data were classified as positive or negative for the surgical detection of endometriosis and sensitivity and specificity estimates were calculated. If two or more tests were evaluated in the same cohort, each was considered as a separate data set. The bivariate model was used to obtain pooled estimates of sensitivity and specificity whenever sufficient data sets were available. The predetermined criteria for a clinically useful urine test to replace diagnostic surgery was one with a sensitivity of 94% and a specificity of 79% to detect endometriosis. The criteria for triage tests were set at sensitivity of equal or greater than 95% and specificity of equal or greater than 50%, which in case of negative result rules out the diagnosis (SnOUT test) or sensitivity of equal or greater than 50% with specificity of equal or greater than 95%, which in case of positive result rules the diagnosis in (SpIN test). Main results We included eight studies involving 646 participants, most of which were of poor methodological quality. The urinary biomarkers were evaluated either in a specific phase of menstrual cycle or irrespective of the cycle phase. Five studies evaluated the diagnostic performance of four urinary biomarkers for endometriosis, including three biomarkers distinguishing women with and without endometriosis (enolase 1 (NNE); vitamin D binding protein (VDBP); and urinary peptide profiling); and one biomarker (cytokeratin 19 (CK 19)) showing no significant difference between the two groups. All of these biomarkers were assessed in small individual studies and could not be statistically evaluated in a meaningful way. None of the biomarkers met the criteria for a replacement test or a triage test. Three studies evaluated three biomarkers that did not differentiate women with endometriosis from disease‐free controls. Authors' conclusions There was insufficient evidence to recommend any urinary biomarker for use as a replacement or triage test in clinical practice for the diagnosis of endometriosis. Several urinary biomarkers may have diagnostic potential, but require further evaluation before being introduced into routine clinical practice. Laparoscopy remains the gold standard for the diagnosis of endometriosis, and diagnosis of endometriosis using urinary biomarkers should only be undertaken in a research setting. Plain language summary Urinary biomarkers for the non‐invasive diagnosis of endometriosis Review Question How accurate are urine test in detecting endometriosis? Can any urine test be accurate enough to replace or reduce the need for surgery in the diagnosis of endometriosis? Background Women with endometriosis have endometrial tissue (the tissue that lines the womb and is shed during menstruation) growing outside the womb within the pelvic cavity. This tissue responds to reproductive hormones causing painful periods, chronic lower abdominal pain and difficulty conceiving. Currently, the only reliable way of diagnosing endometriosis is to perform laparoscopic surgery and visualise the endometrial deposits inside the abdomen. Because surgery is risky and expensive, urine tests have been evaluated for their ability to detect endometriosis non‐invasively. An accurate urine test could lead to the diagnosis of endometriosis without the need for surgery; or it could reduce the need for diagnostic surgery, so only women who were most likely to have endometriosis would require it. Other non‐invasive ways of diagnosing endometriosis using blood, imaging, endometrial and combination tests are evaluated in separate Cochrane reviews from this series. Study characteristics The evidence included in this review is current to July 2015. We included eight studies involving 646 participants. All studies evaluated reproductive‐aged women who were undertaking diagnostic surgery to investigate symptoms of endometriosis or for other indications. Five studies evaluated the diagnostic accuracy of four urinary biomarkers, including four biomarkers that were expressed differently in women with and without endometriosis and one showing no difference between the two groups. Three other studies just identified three biomarkers that did not distinguish the two groups. Key results None of the assessed biomarkers, including cytokeratin 19 (CK 19), enolase 1 (NNE), vitamin D binding protein (VDBP) and urinary peptide profiling have been evaluated by enough studies to provide a meaningful assessment of test accuracy. None of the tests were accurate enough to replace diagnostic surgery. Several studies identified biomarkers that might be of value in diagnosing endometriosis, but there are too few reports to be sure of their diagnostic benefit. There is not enough evidence to recommend any urinary biomarker for use in clinical practice for the diagnosis of endometriosis. Quality of the evidence Generally, the reports were of low methodological quality and urine tests were only assessed in small individual studies. Future research More high quality research trials are needed to accurately assess the diagnostic potential of urinary biomarkers identified in small numbers of studies as having value in detecting endometriosis.","12","John Wiley & Sons, Ltd","1465-1858","Biomarkers [urine]; Diagnosis, Differential; Endometriosis [*diagnosis]; Female; Humans; Keratin‐19 [*urine]; Peptide Fragments [urine]; Peptides [*urine]; Phosphopyruvate Hydratase [*urine]; Proteomics; Vitamin D‐Binding Protein [*urine]","10.1002/14651858.CD012019","http://dx.doi.org/10.1002/14651858.CD012019","Gynaecology and Fertility"
"CD011984","Alldred, SK; Guo, B; Takwoingi, Y; Pennant, M; Wisniewski, S; Deeks, JJ; Neilson, JP; Alfirevic, Z","Urine tests for Down's syndrome screening","Cochrane Database of Systematic Reviews","2015","Abstract - Background Down's syndrome occurs when a person has three copies of chromosome 21, or the specific area of chromosome 21 implicated in causing Down's syndrome, rather than two. It is the commonest congenital cause of mental disability and also leads to numerous metabolic and structural problems. It can be life‐threatening, or lead to considerable ill health, although some individuals have only mild problems and can lead relatively normal lives. Having a baby with Down's syndrome is likely to have a significant impact on family life. The risk of a Down's syndrome affected pregnancy increases with advancing maternal age. Noninvasive screening based on biochemical analysis of maternal serum or urine, or fetal ultrasound measurements, allows estimates of the risk of a pregnancy being affected and provides information to guide decisions about definitive testing. Before agreeing to screening tests, parents need to be fully informed about the risks, benefits and possible consequences of such a test. This includes subsequent choices for further tests they may face, and the implications of both false positive and false negative screening tests (i.e. invasive diagnostic testing, and the possibility that a miscarried fetus may be chromosomally normal). The decisions that may be faced by expectant parents inevitably engender a high level of anxiety at all stages of the screening process, and the outcomes of screening can be associated with considerable physical and psychological morbidity. No screening test can predict the severity of problems a person with Down's syndrome will have. Objectives To estimate and compare the accuracy of first and second trimester urine markers for the detection of Down's syndrome. Search methods We carried out a sensitive and comprehensive literature search of MEDLINE (1980 to 25 August 2011), EMBASE (1980 to 25 August 2011), BIOSIS via EDINA (1985 to 25 August 2011), CINAHL via OVID (1982 to 25 August 2011), The Database of Abstracts of Reviews of Effectiveness ( The Cochrane Library  2011, Issue 7), MEDION (25 August 2011), The Database of Systematic Reviews and Meta‐Analyses in Laboratory Medicine (25 August 2011), The National Research Register (archived 2007), Health Services Research Projects in Progress database (25 August 2011). We studied reference lists and published review articles. Selection criteria Studies evaluating tests of maternal urine in women up to 24 weeks of gestation for Down's syndrome, compared with a reference standard, either chromosomal verification or macroscopic postnatal inspection. Data collection and analysis We extracted data as test positive or test negative results for Down's and non‐Down's pregnancies allowing estimation of detection rates (sensitivity) and false positive rates (1‐specificity). We performed quality assessment according to QUADAS (Quality Assessment of Diagnostic Accuracy Studies) criteria. We used hierarchical summary ROC (receiver operating characteristic) meta‐analytical methods to analyse test performance and compare test accuracy. We performed analysis of studies allowing direct comparison between tests. We investigated the impact of maternal age on test performance in subgroup analyses. Main results We included 19 studies involving 18,013 pregnancies (including 527 with Down's syndrome). Studies were generally of high quality, although differential verification was common with invasive testing of only high‐risk pregnancies. Twenty‐four test combinations were evaluated formed from combinations of the following seven different markers with and without maternal age: AFP (alpha‐fetoprotein), ITA (invasive trophoblast antigen), ß‐core fragment, free ßhCG (beta human chorionic gonadotrophin), total hCG, oestriol, gonadotropin peptide and various marker ratios. The strategies evaluated included three double tests and seven single tests in combination with maternal age, and one triple test, two double tests and 11 single tests without maternal age. Twelve of the 19 studies only evaluated the performance of a single test strategy while the remaining seven evaluated at least two test strategies. Two marker combinations were evaluated in more than four studies; second trimester ß‐core fragment (six studies), and second trimester ß‐core fragment with maternal age (five studies). In direct test comparisons, for a 5% false positive rate (FPR), the diagnostic accuracy of the double marker second trimester ß‐core fragment and oestriol with maternal age test combination was significantly better (ratio of diagnostic odds ratio (RDOR): 2.2 (95% confidence interval (CI) 1.1 to 4.5), P = 0.02) (summary sensitivity of 73% (CI 57 to 85) at a cut‐point of 5% FPR) than that of the single marker test strategy of second trimester ß‐core fragment and maternal age (summary sensitivity of 56% (CI 45 to 66) at a cut‐point of 5% FPR), but was not significantly better (RDOR: 1.5 (0.8 to 2.8), P = 0.21) than that of the second trimester ß‐core fragment to oestriol ratio and maternal age test strategy (summary sensitivity of 71% (CI 51 to 86) at a cut‐point of 5% FPR). Authors' conclusions Tests involving second trimester ß‐core fragment and oestriol with maternal age are significantly more sensitive than the single marker second trimester ß‐core fragment and maternal age, however, there were few studies. There is a paucity of evidence available to support the use of urine testing for Down's syndrome screening in clinical practice where alternatives are available. Plain language summary Screening tests for Down’s syndrome in first 24 weeks of pregnancy Background   Down's syndrome (also known as Down's or Trisomy 21) is an incurable genetic disorder that causes significant physical and mental health problems, and disabilities. However, there is wide variation in how Down's affects people. Some individuals are severely affected whilst others have mild problems and are able to lead relatively normal lives. There is no way of predicting how badly a baby might be affected. Expectant parents are given the choice to be tested for Down’s during pregnancy to assist them in making decisions. If a mother is carrying a baby with Down’s, then there is the decision about whether to terminate or continue with the pregnancy. The information offers parents the opportunity to plan for life with a Down’s child. The most accurate tests for Down’s involve testing fluid from around the baby (amniocentesis) or tissue from the placenta (chorionic villus sampling (CVS)) for the abnormal chromosomes associated with Down’s. Both these tests involve inserting needles through the mother's abdomen and are known to increase the risk of miscarriage. Thus, the tests are not suitable for offering to all pregnant women. Rather, tests that measure markers in the mother’s blood, urine or on ultrasound scans of the baby are used for screening. These screening tests are not perfect, they can miss cases of Down’s and also give a ‘high risk’ test results to a number of women whose babies are not affected by Down’s. Thus, pregnancies identified as ‘high risk’ using these screening tests require further testing using amniocentesis or CVS to confirm a diagnosis of Down’s. What we did   The aim of this review was to find out which of the urine screening tests done during the first 24 weeks of pregnancy are the most accurate at predicting the risk of a pregnancy being affected by Down's. We looked at seven different urine markers that can be used alone, in ratios or in combination, taken before 24 weeks' gestation, thus creating 24 screening tests for Down’s. We found 19 studies, involving 18,013 pregnancies of which 527 had pregnancies affected by Down's. What we found   For the first 24 weeks of pregnancy, the evidence does not support the use of urine tests for Down's syndrome screening. The amount of evidence is limited. These tests are not offered in routine clinical practice. Other important information to consider   The urine tests themselves have no adverse effects for the woman. However, some women who have a ‘high risk’ screening test result, and are given amniocentesis or CVS have a risk of miscarrying a baby unaffected by Down’s. Parents will need to weigh up this risk when deciding whether or not to have an amniocentesis or CVS following a ‘high risk’ screening test result.","12","John Wiley & Sons, Ltd","1465-1858","Biomarkers [*urine]; Chorionic Gonadotropin [urine]; Down Syndrome [*diagnosis]; Estriol [urine]; False Positive Reactions; Female; Gonadotropins [urine]; Humans; Maternal Age; Predictive Value of Tests; Pregnancy; Pregnancy Trimester, First [*urine]; Pregnancy Trimester, Second [*urine]; Sensitivity and Specificity; alpha‐Fetoproteins [urine]","10.1002/14651858.CD011984","http://dx.doi.org/10.1002/14651858.CD011984","Pregnancy and Childbirth"
"CD011134.PUB2","Nicholson, BD; Shinkins, B; Pathiraja, I; Roberts, NW; James, TJ; Mallett, S; Perera, R; Primrose, JN; Mant, D","Blood CEA levels for detecting recurrent colorectal cancer","Cochrane Database of Systematic Reviews","2015","Abstract - Background Testing for carcino‐embryonic antigen (CEA) in the blood is a recommended part of follow‐up to detect recurrence of colorectal cancer following primary curative treatment. There is substantial clinical variation in the cut‐off level applied to trigger further investigation. Objectives To determine the diagnostic performance of different blood CEA levels in identifying people with colorectal cancer recurrence in order to inform clinical practice. Search methods We conducted all searches to January 29 2014. We applied no language limits to the searches, and translated non‐English manuscripts. We searched for relevant reviews in the MEDLINE, EMBASE, MEDION and DARE databases. We searched for primary studies (including conference abstracts) in the Cochrane Central Register of Controlled Trials (CENTRAL), in MEDLINE, EMBASE, and the Science Citation Index & Conference Proceedings Citation Index – Science. We identified ongoing studies by searching WHO ICTRP and the ASCO meeting library. Selection criteria We included cross‐sectional diagnostic test accuracy studies, cohort studies, and randomised controlled trials (RCTs) of post‐resection colorectal cancer follow‐up that compared CEA to a reference standard. We included studies only if we could extract 2 x 2 accuracy data. We excluded case‐control studies, as the ratio of cases to controls is determined by the study design, making the data unsuitable for assessing test accuracy. Data collection and analysis Two review authors (BDN, IP) assessed the quality of all articles independently, discussing any disagreements. Where we could not reach consensus, a third author (BS) acted as moderator. We assessed methodological quality against QUADAS‐2 criteria. We extracted binary diagnostic accuracy data from all included studies as 2 x 2 tables. We conducted a bivariate meta‐analysis. We used the xtmelogit command in Stata to produce the pooled estimates of sensitivity and specificity and we also produced hierarchical summary ROC plots. Main results In the 52 included studies, sensitivity ranged from 41% to 97% and specificity from 52% to 100%. In the seven studies reporting the impact of applying a threshold of 2.5 µg/L, pooled sensitivity was 82% (95% confidence interval (CI) 78% to 86%) and pooled specificity 80% (95% CI 59% to 92%). In the 23 studies reporting the impact of applying a threshold of 5 µg/L, pooled sensitivity was 71% (95% CI 64% to 76%) and pooled specificity 88% (95% CI 84% to 92%). In the seven studies reporting the impact of applying a threshold of 10 µg/L, pooled sensitivity was 68% (95% CI 53% to 79%) and pooled specificity 97% (95% CI 90% to 99%). Authors' conclusions CEA is insufficiently sensitive to be used alone, even with a low threshold. It is therefore essential to augment CEA monitoring with another diagnostic modality in order to avoid missed cases. Trying to improve sensitivity by adopting a low threshold is a poor strategy because of the high numbers of false alarms generated. We therefore recommend monitoring for colorectal cancer recurrence with more than one diagnostic modality but applying the highest CEA cut‐off assessed (10 µg/L). Plain language summary Detecting recurrent colorectal cancer by testing for blood carcino‐embryonic antigen (CEA). Background After surgery for cancer in the colon or rectum (colorectal cancer), most people are intensively followed up for at least five years to monitor for signs of the cancer returning. When this occurs, it usually causes a rise in a blood protein called CEA (carcino‐embryonic antigen). An increased level of CEA can be picked up by a blood test, which is normally done every three to six months after colorectal cancer surgery. Those people with raised CEA levels are further investigated by x‐ray imaging (usually a scan of the chest, abdomen and pelvis). We conducted this review to help decide what level of blood CEA should lead to further investigation. Key Results This review shows that setting a low cut‐off point will increase the number of genuine cases of colorectal cancer recurrence that are detected (true positives), but a low cut‐off will also cause unnecessary alarm by incorrectly classifying too many cases that are not actually recurrences (false positives). In addition, this review shows that a rise in CEA does not occur in up to 20% of patients with a true recurrence (false negatives). The current evidence supports using the highest cut‐off point assessed (10 µg/L), but that adding another diagnostic modality (e.g. a single scan of the chest, abdomen and pelvis at 12 to 18 months) is necessary in order to avoid the missed cases.","12","John Wiley & Sons, Ltd","1465-1858","Carcinoembryonic Antigen [*blood]; Colorectal Neoplasms [blood, *diagnosis]; Humans; Neoplasm Recurrence, Local [blood, *diagnosis]; Sensitivity and Specificity","10.1002/14651858.CD011134.pub2","http://dx.doi.org/10.1002/14651858.CD011134.pub2","Colorectal"
"CD011975","Alldred, SK; Takwoingi, Y; Guo, B; Pennant, M; Deeks, JJ; Neilson, JP; Alfirevic, Z","First trimester serum tests for Down's syndrome screening","Cochrane Database of Systematic Reviews","2015","Abstract - Background Down's syndrome occurs when a person has three, rather than two copies of chromosome 21; or the specific area of chromosome 21 implicated in causing Down's syndrome. It is the commonest congenital cause of mental disability and also leads to numerous metabolic and structural problems. It can be life‐threatening, or lead to considerable ill health, although some individuals have only mild problems and can lead relatively normal lives. Having a baby with Down’s syndrome is likely to have a significant impact on family life. Noninvasive screening based on biochemical analysis of maternal serum or urine, or fetal ultrasound measurements, allows estimates of the risk of a pregnancy being affected and provides information to guide decisions about definitive testing. However, no test can predict the severity of problems a person with Down’s syndrome will have. Objectives The aim of this review was to estimate and compare the accuracy of first trimester serum markers for the detection of Down’s syndrome in the antenatal period, both as individual markers and as combinations of markers. Accuracy is described by the proportion of fetuses with Down’s syndrome detected by screening before birth (sensitivity or detection rate) and the proportion of women with a low risk (normal) screening test result who subsequently had a baby unaffected by Down's syndrome (specificity). Search methods We conducted a sensitive and comprehensive literature search of MEDLINE (1980 to 25 August 2011), Embase (1980 to 25 August 2011), BIOSIS via EDINA (1985 to 25 August 2011), CINAHL via OVID (1982 to 25 August 2011), The Database of Abstracts of Reviews of Effectiveness ( The Cochrane Library  25 August 2011), MEDION (25 August 2011), The Database of Systematic Reviews and Meta‐Analyses in Laboratory Medicine (25 August 2011), The National Research Register (Archived 2007), Health Services Research Projects in Progress database (25 August 2011). We did forward citation searching ISI citation indices, Google Scholar and PubMed ‘related articles’. We did not apply a diagnostic test search filter. We also searched reference lists and published review articles.   Selection criteria We included studies in which all women from a given population had one or more index test(s) compared to a reference standard (either chromosomal verification or macroscopic postnatal inspection). Both consecutive series and diagnostic case‐control study designs were included. Randomised trials where individuals were randomised to different screening strategies and all verified using a reference standard were also eligible for inclusion. Studies in which test strategies were compared head‐to‐head either in the same women, or between randomised groups were identified for inclusion in separate comparisons of test strategies. We excluded studies if they included less than five Down's syndrome cases, or more than 20% of participants were not followed up. Data collection and analysis We extracted data as test positive or test negative results for Down's and non‐Down's pregnancies allowing estimation of detection rates (sensitivity) and false positive rates (1‐specificity). We performed quality assessment according to QUADAS (Quality Assessment of Diagnostic Accuracy Studies) criteria. We used hierarchical summary ROC meta‐analytical methods or random‐effects logistic regression methods to analyse test performance and compare test accuracy as appropriate. Analyses of studies allowing direct and indirect comparisons between tests were undertaken. Main results We included 56 studies (reported in 68 publications) involving 204,759 pregnancies (including 2113 with Down's syndrome). Studies were generally of good quality, although differential verification was common with invasive testing of only high‐risk pregnancies. We evaluated 78 test combinations formed from combinations of 18 different tests, with or without maternal age; ADAM12 (a disintegrin and metalloprotease), AFP (alpha‐fetoprotein), inhibin, PAPP‐A (pregnancy‐associated plasma protein A, ITA (invasive trophoblast antigen), free βhCG (beta human chorionic gonadotrophin), PlGF (placental growth factor), SP1 (Schwangerschafts protein 1), total hCG, progesterone, uE3 (unconjugated oestriol), GHBP (growth hormone binding protein), PGH (placental growth hormone), hyperglycosylated hCG, ProMBP (proform of eosinophil major basic protein), hPL (human placental lactogen), (free αhCG, and free ßhCG to AFP ratio. Direct comparisons between two or more tests were made in 27 studies. Meta‐analysis of the nine best performing or frequently evaluated test combinations showed that a test strategy involving maternal age and a double marker combination of PAPP‐A and free ßhCG significantly outperformed the individual markers (with or without maternal age) detecting about seven out of every 10 Down's syndrome pregnancies at a 5% false positive rate (FPR). Limited evidence suggested that marker combinations involving PAPP‐A may be more sensitive than those without PAPP‐A. Authors' conclusions Tests involving two markers in combination with maternal age, specifically PAPP‐A, free βhCG and maternal age are significantly better than those involving single markers with and without age. They detect seven out of 10 Down's affected pregnancies for a fixed 5% FPR. The addition of further markers (triple tests) has not been shown to be statistically superior; the studies included are small with limited power to detect a difference. The screening blood tests themselves have no adverse effects for the woman, over and above the risks of a routine blood test. However some women who have a ‘high risk’ screening test result, and are given amniocentesis or chorionic villus sampling (CVS) have a risk of miscarrying a baby unaffected by Down’s. Parents will need to weigh up this risk when deciding whether or not to have an amniocentesis or CVS following a ‘high risk’ screening test result. Plain language summary Screening tests for Down’s syndrome in first three months of pregnancy Background   Down's syndrome (also known as Down's or Trisomy 21) is an incurable genetic disorder that causes significant physical and mental health problems, and disabilities. However, there is wide variation in how Down's affects people. Some individuals are severely affected whilst others have mild problems and are able to lead relatively normal lives. There is no way of predicting how badly a baby might be affected.    Expectant parents are given the choice to be tested for Down’s during pregnancy to assist them in making decisions. If a mother is carrying a baby with Down’s, then there is the decision about whether to terminate or continue with the pregnancy. The information offers parents the opportunity to plan for life with a Down’s child.    The most accurate tests for Down’s involve testing fluid from around the baby (amniocentesis) or tissue from the placenta (chorionic villus sampling (CVS)) for the abnormal chromosomes associated with Down’s. Both these tests involve inserting needles through the mother's abdomen and are known to increase the risk of miscarriage. Thus, the tests are not suitable for offering to all pregnant women. Rather, tests that measure markers in the mother’s blood, urine or on ultrasound scans of the baby are used for screening. These screening tests are not perfect, they can miss cases of Down’s and also give a ‘high risk’ test result to a number of women whose babies are not affected by Down’s. Thus, pregnancies identified as ‘high risk’ using these screening tests require further testing using amniocentesis or CVS to confirm a diagnosis of Down’s. What we did   The aim of this review was to find out which of the blood screening tests done during the first three months of pregnancy are the most accurate at predicting the risk of a pregnancy being affected by Down's. We looked at 18 different blood markers that can be used alone or in combination, taken before 14 weeks gestation, thus creating 78 screening tests fro Down’s. We found 56 studies, involving 204,759 pregnancies of which 2113 had pregnancies affected by Down's. What we found   For the first 14 weeks of pregnancy, the evidence supports the use of the double test of two blood markers; pregnancy‐associated plasma protein A (PAPP‐A) and free beta human chorionic gonadotrophin (βhCG), in combination with the mother's age. This test detects around seven out of every 10 (68%) pregnancies affected by Down's. It is common practice to offer amniocentesis or CVS to women with a high risk test result. About one in 20 women (5%) having this test will have a ‘high risk’ result but most of these women will not be carrying a baby with Down’s. We found for tests in the first 14 weeks of pregnancy, there is little evidence to support the use of serum tests made up of more than two blood markers.    Other important information to consider   The blood tests themselves have no adverse effects for the woman, over and above the risks of a routine blood test. However some women who have a ‘high risk’ screening test result, and are given amniocentesis or CVS have a risk of miscarrying a baby unaffected by Down’s. Parents will need to weigh up this risk when deciding whether or not to have an amniocentesis or CVS following a ‘high risk’ screening test result.","11","John Wiley & Sons, Ltd","1465-1858","*Maternal Age; ADAM Proteins [blood]; ADAM12 Protein; Biomarkers [*blood]; Chorionic Gonadotropin, beta Subunit, Human [blood]; Down Syndrome [*diagnosis]; Female; Humans; Membrane Proteins [blood]; Predictive Value of Tests; Pregnancy; Pregnancy Trimester, First [*blood]; Pregnancy‐Associated Plasma Protein‐A [analysis]; Prenatal Diagnosis [*methods]; alpha‐Fetoproteins [analysis]","10.1002/14651858.CD011975","http://dx.doi.org/10.1002/14651858.CD011975","Pregnancy and Childbirth"
"CD008803.PUB2","Michelessi, M; Lucenteforte, E; Oddone, F; Brazzelli, M; Parravano, M; Franchi, S; Ng, SM; Virgili, G","Optic nerve head and fibre layer imaging for diagnosing glaucoma","Cochrane Database of Systematic Reviews","2015","Abstract - Background The diagnosis of glaucoma is traditionally based on the finding of optic nerve head (ONH) damage assessed subjectively by ophthalmoscopy or photography or by corresponding damage to the visual field assessed by automated perimetry, or both. Diagnostic assessments are usually required when ophthalmologists or primary eye care professionals find elevated intraocular pressure (IOP) or a suspect appearance of the ONH. Imaging tests such as confocal scanning laser ophthalmoscopy (HRT), optical coherence tomography (OCT) and scanning laser polarimetry (SLP, as used by the GDx instrument), provide an objective measure of the structural changes of retinal nerve fibre layer (RNFL) thickness and ONH parameters occurring in glaucoma. Objectives To determine the diagnostic accuracy of HRT, OCT and GDx for diagnosing manifest glaucoma by detecting ONH and RNFL damage. Search methods We searched several databases for this review. The most recent searches were on 19 February 2015. Selection criteria We included prospective and retrospective cohort studies and case‐control studies that evaluated the accuracy of OCT, HRT or the GDx for diagnosing glaucoma. We excluded population‐based screening studies, since we planned to consider studies on self‐referred people or participants in whom a risk factor for glaucoma had already been identified in primary care, such as elevated IOP or a family history of glaucoma. We only considered recent commercial versions of the tests: spectral domain OCT, HRT III and GDx VCC or ECC. Data collection and analysis We adopted standard Cochrane methods. We fitted a hierarchical summary ROC (HSROC) model using the  METADAS  macro in SAS software. After studies were selected, we decided to use 2 x 2 data at 0.95 specificity or closer in meta‐analyses, since this was the most commonly‐reported level. Main results We included 106 studies in this review, which analysed 16,260 eyes (8353 cases, 7907 controls) in total. Forty studies (5574 participants) assessed GDx, 18 studies (3550 participants) HRT, and 63 (9390 participants) OCT, with 12 of these studies comparing two or three tests. Regarding study quality, a case‐control design in 103 studies raised concerns as it can overestimate accuracy and reduce the applicability of the results to daily practice. Twenty‐four studies were sponsored by the manufacturer, and in 15 the potential conflict of interest was unclear. Comparisons made within each test were more reliable than those between tests, as they were mostly based on direct comparisons within each study.The Nerve Fibre Indicator yielded the highest accuracy (estimate, 95% confidence interval (CI)) among GDx parameters (sensitivity: 0.67, 0.55 to 0.77; specificity: 0.94, 0.92 to 0.95). For HRT measures, the Vertical Cup/Disc (C/D) ratio (sensitivity: 0.72, 0.60 to 0.68; specificity: 0.94, 0.92 to 0.95) was no different from other parameters. With OCT, the accuracy of average RNFL retinal thickness was similar to the inferior sector (0.72, 0.65 to 0.77; specificity: 0.93, 0.92 to 0.95) and, in different studies, to the vertical C/D ratio. Comparing the parameters with the highest diagnostic odds ratio (DOR) for each device in a single HSROC model, the performance of GDx, HRT and OCT was remarkably similar. At a sensitivity of 0.70 and a high specificity close to 0.95 as in most of these studies, in 1000 people referred by primary eye care, of whom 200 have manifest glaucoma, such as in those who have already undergone some functional or anatomic testing by optometrists, the best measures of GDx, HRT and OCT would miss about 60 cases out of the 200 patients with glaucoma, and would incorrectly refer 50 out of 800 patients without glaucoma. If prevalence were 5%, e.g. such as in people referred only because of family history of glaucoma, the corresponding figures would be 15 patients missed out of 50 with manifest glaucoma, avoiding referral of about 890 out of 950 non‐glaucomatous people. Heterogeneity investigations found that sensitivity estimate was higher for studies with more severe glaucoma, expressed as worse average mean deviation (MD): 0.79 (0.74 to 0.83) for MD < ‐6 db versus 0.64 (0.60 to 0.69) for MD ≥ ‐6 db, at a similar summary specificity (0.93, 95% CI 0.92 to 0.94 and, respectively, 0.94; 95% CI 0.93 to 0.95; P < 0.0001 for the difference in relative DOR). Authors' conclusions The accuracy of imaging tests for detecting manifest glaucoma was variable across studies, but overall similar for different devices. Accuracy may have been overestimated due to the case‐control design, which is a serious limitation of the current evidence base. We recommend that further diagnostic accuracy studies are carried out on patients selected consecutively at a defined step of the clinical pathway, providing a description of risk factors leading to referral and bearing in mind the consequences of false positives and false negatives in the setting in which the diagnostic question is made. Future research should report accuracy for each threshold of these continuous measures, or publish raw data. Plain language summary Tests for imaging the optic nerve and its fibres for diagnosing glaucoma Review question  We reviewed the evidence about the accuracy of confocal scanning laser ophthalmoscopy (commercially available as the Heidelberg Retinal Tomogram (HRT)), optical coherence tomography (OCT) and scanning laser polarimetry (as used by the GDx device) for diagnosing glaucoma in people who are at risk. These tests can measure the structure of the optic nerve head or measure the thickness of the nerve's fibres, or both. Background  Glaucoma is a progressive neurodegenerative disease that affects the optic nerve, with corresponding damage to the visual field. The course of the disease can be slowed or halted by reducing intraocular pressure with eye drops or surgery. Study characteristics  We found 106 studies, mostly assessing a single device, which analysed 16,260 eyes (8353 cases, 7907 controls). Forty studies (5574 participants) assessed GDx, 18 studies (3550 participants) HRT, and 63 (9390 patients) OCT. Twenty‐four studies were sponsored by the manufacturer, and in 15 the study funding was unclear. The final diagnosis of glaucoma had to be confirmed by clinical examination, including visual field testing or clinical optic nerve examination or both. However, we could not find studies comparing two tests, the most robust way to test these instruments, and including a series of consecutive patients at risk as seen in routine care, as we had hoped. Rather, we found studies assessing the performance of a single test in people without glaucoma as opposed to its performance in people with a previous diagnosis of glaucoma. The study search is current to 19 February 2015. Key results  The performance of all devices was very variable across studies, but overall similar. In 1000 people referred by primary eye care, of whom 200 (20%) have manifest glaucoma, such as in those who have already undergone some functional or anatomic testing by optometrists, the best measures of GDx, HRT and OCT would miss about 60 cases out of the 200 patients with glaucoma (sensitivity 70%), and would incorrectly refer 50 out of 800 patients without glaucoma (at specificity 95%). If prevalence were 5%, for example, in people referred only because of family history of glaucoma, the corresponding figures would be 15 patients missed out of 50 with manifest glaucoma, avoiding referral of about 890 out of 950 non‐glaucomatous people. The tests were better at detecting more severe glaucoma compared to early glaucoma. Quality of the evidence  The selection of two well‐defined groups of healthy and glaucoma eyes in nearly all studies, rather than the use of these imaging tests in a series of patients at risk of glaucoma as in the real world, may overestimate the accuracy of these devices compared to what could be achieved in daily practice.","11","John Wiley & Sons, Ltd","1465-1858","Diagnostic Errors [statistics & numerical data]; Glaucoma [*diagnosis]; Humans; Nerve Fibers [*pathology]; Odds Ratio; Ophthalmoscopy [*standards]; Optic Disk [*pathology]; Prospective Studies; Retrospective Studies; Scanning Laser Polarimetry [*standards]; Sensitivity and Specificity; Tomography, Optical Coherence [*standards]; Visual Field Tests","10.1002/14651858.CD008803.pub2","http://dx.doi.org/10.1002/14651858.CD008803.pub2","Eyes and Vision"
"CD009263.PUB2","Bleeker, G; Tytgat, GAM; Adam, JA; Caron, HN; Kremer, LCM; Hooft, L; van Dalen, EC","123I‐MIBG scintigraphy and 18F‐FDG‐PET imaging for diagnosing neuroblastoma","Cochrane Database of Systematic Reviews","2015","Abstract - Background Neuroblastoma is an embryonic tumour of childhood that originates in the neural crest. It is the second most common extracranial malignant solid tumour of childhood. Neuroblastoma cells have the unique capacity to accumulate Iodine‐123‐metaiodobenzylguanidine (¹²³I‐MIBG), which can be used for imaging the tumour. Moreover, ¹²³I‐MIBG scintigraphy is not only important for the diagnosis of neuroblastoma, but also for staging and localization of skeletal lesions. If these are present, MIBG follow‐up scans are used to assess the patient's response to therapy. However, the sensitivity and specificity of ¹²³I‐MIBG scintigraphy to detect neuroblastoma varies according to the literature. Prognosis, treatment and response to therapy of patients with neuroblastoma are currently based on extension scoring of ¹²³I‐MIBG scans. Due to its clinical use and importance, it is necessary to determine the exact diagnostic accuracy of ¹²³I‐MIBG scintigraphy. In case the tumour is not MIBG avid, fluorine‐18‐fluorodeoxy‐glucose ( 18 F‐FDG) positron emission tomography (PET) is often used and the diagnostic accuracy of this test should also be assessed. Objectives Primary objectives: 1.1 To determine the diagnostic accuracy of ¹²³I‐MIBG (single photon emission computed tomography (SPECT), with or without computed tomography (CT)) scintigraphy for detecting a neuroblastoma and its metastases at first diagnosis or at recurrence in children from 0 to 18 years old. 1.2 To determine the diagnostic accuracy of negative ¹²³I‐MIBG scintigraphy in combination with  18 F‐FDG‐PET(‐CT) imaging for detecting a neuroblastoma and its metastases at first diagnosis or at recurrence in children from 0 to 18 years old, i.e. an add‐on test. Secondary objectives: 2.1 To determine the diagnostic accuracy of  18 F‐FDG‐PET(‐CT) imaging for detecting a neuroblastoma and its metastases at first diagnosis or at recurrence in children from 0 to 18 years old. 2.2 To compare the diagnostic accuracy of ¹²³I‐MIBG (SPECT‐CT) and  18 F‐FDG‐PET(‐CT) imaging for detecting a neuroblastoma and its metastases at first diagnosis or at recurrence in children from 0 to 18 years old. This was performed within and between included studies. ¹²³I‐MIBG (SPECT‐CT) scintigraphy was the comparator test in this case. Search methods We searched the databases of MEDLINE/PubMed (1945 to 11 September 2012) and EMBASE/Ovid (1980 to 11 September 2012) for potentially relevant articles. Also we checked the reference lists of relevant articles and review articles, scanned conference proceedings and searched for unpublished studies by contacting researchers involved in this area. Selection criteria We included studies of a cross‐sectional design or cases series of proven neuroblastoma, either retrospective or prospective, if they compared the results of ¹²³I‐MIBG (SPECT‐CT) scintigraphy or  18 F‐FDG‐PET(‐CT) imaging, or both, with the reference standards or with each other. Studies had to be primary diagnostic and report on children aged between 0 to 18 years old with a neuroblastoma of any stage at first diagnosis or at recurrence. Data collection and analysis One review author performed the initial screening of identified references. Two review authors independently performed the study selection, extracted data and assessed the methodological quality. We used data from two‐by‐two tables, describing at least the number of patients with a true positive test and the number of patients with a false negative test, to calculate the sensitivity, and if possible, the specificity for each included study. If possible, we generated forest plots showing estimates of sensitivity and specificity together with 95% confidence intervals. Main results Eleven studies met the inclusion criteria. Ten studies reported data on patient level: the scan was positive or negative. One study reported on all single lesions (lesion level). The sensitivity of ¹²³I‐MIBG (SPECT‐CT) scintigraphy (objective 1.1), determined in 608 of 621 eligible patients included in the 11 studies, varied from 67% to 100%. One study, that reported on a lesion level, provided data to calculate the specificity: 68% in 115 lesions in 22 patients. The sensitivity of ¹²³I‐MIBG scintigraphy for detecting metastases separately from the primary tumour in patients with all neuroblastoma stages ranged from 79% to 100% in three studies and the specificity ranged from 33% to 89% for two of these studies. One study reported on the diagnostic accuracy of  18 F‐FDG‐PET(‐CT) imaging (add‐on test) in patients with negative ¹²³I‐MIBG scintigraphy (objective 1.2). Two of the 24 eligible patients with proven neuroblastoma had a negative ¹²³I‐MIBG scan and a positive  18 F‐FDG‐PET(‐CT) scan. The sensitivity of  18 F‐FDG‐PET(‐CT) imaging as a single diagnostic test (objective 2.1) and compared to ¹²³I‐MIBG (SPECT‐CT) (objective 2.2) was only reported in one study. The sensitivity of  18 F‐FDG‐PET(‐CT) imaging was 100% versus 92% of ¹²³I‐MIBG (SPECT‐CT) scintigraphy. We could not calculate the specificity for both modalities. Authors' conclusions The reported sensitivities of ¹²³‐I MIBG scintigraphy for the detection of neuroblastoma and its metastases ranged from 67 to 100% in patients with histologically proven neuroblastoma. Only one study in this review reported on false positive findings. It is important to keep in mind that false positive findings can occur. For example, physiological uptake should be ruled out, by using SPECT‐CT scans, although more research is needed before definitive conclusions can be made. As described both in the literature and in this review, in about 10% of the patients with histologically proven neuroblastoma the tumour does not accumulate ¹²³I‐MIBG (false negative results). For these patients, it is advisable to perform an additional test for staging and assess response to therapy. Additional tests might for example be  18 F‐FDG‐PET(‐CT), but to be certain of its clinical value, more evidence is needed. The diagnostic accuracy of  18 F‐FDG‐PET(‐CT) imaging in case of a negative ¹²³I‐MIBG scintigraphy could not be calculated, because only very limited data were available. Also the detection of the diagnostic accuracy of index test  18 F‐FDG‐PET(‐CT) imaging for detecting a neuroblastoma tumour and its metastases, and to compare this to comparator test ¹²³I‐MIBG (SPECT‐CT) scintigraphy, could not be calculated because of the limited available data at time of this search. At the start of this project, we did not expect to find only very limited data on specificity. We now consider it would have been more appropriate to use the term ""the sensitivity to assess the presence of neuroblastoma"" instead of ""diagnostic accuracy"" for the objectives. Plain language summary ¹²³I‐MIBG‐ and  18 F‐FDG‐PET‐imaging, two nuclear imaging methods for diagnosing neuroblastoma tumours Background and rationale Neuroblastoma is a childhood tumour that can be visualized by a specific nuclear imaging compound, called metaiodobenzylguanidine (¹²³I ‐MIBG). ¹²³I‐MIBG‐imaging is not only important for the diagnosis of neuroblastoma, but also for localization of metastases (spread of the disease to other organs). Sometimes, the neuroblastoma does not take up ¹²³I‐MIBG and as a result the neuroblastoma is not visible on the scan. In that case, another type of nuclear imaging might be useful to visualize the neuroblastoma: fluoro‐deoxy‐glucose – positron emission tomography ( 18 F‐FDG‐PET)‐imaging. In the literature the ability to discriminate between neuroblastoma and non‐neuroblastoma lesions for these two types of nuclear imaging methods vary. Prognosis, treatment and response to therapy of patients with neuroblastoma are currently based on scoring the amount of metastases per body segment visible on ¹²³I‐MIBG scans. Therefore, it is important to determine the exact ability to discriminate between neuroblastoma and non‐neuroblastoma on ¹²³I‐MIBG‐imaging and  18 F‐FDG‐PET‐imaging. We reviewed the evidence about the accuracy of ¹²³I‐MIBG‐imaging and  18 F‐FDG‐PET‐imaging for the detection of a neuroblastoma in children suspected of this disease. Study characteristics We searched scientific databases for clinical studies comparing ¹²³I‐MIBG or  18 F‐FDG‐PET imaging, or both, with microscopic examination of tissue suspected of neuroblastoma (histopathology). The evidence is current up to 11 September 2012. We identified 11 eligible studies including 621 children that fulfilled our inclusion criteria: children < 18 years old with a neuroblastoma and ¹²³I‐MIBG or  18 F‐FDG‐PET imaging or both. All studies included proven neuroblastoma. Quality of the evidence All 11 included studies had methodological limitations. Only one included study provided data on specificity (the ability of a test to correctly classify an individual as 'disease‐free') and therefore we could not perform all of the planned analyses. Key results When compared to histopathological results the sensitivity (the ability of a test to correctly classify an individual person as 'diseased') of ¹²³I‐MIBG imaging varied from 67% to 100% in patients with histologically proven neuroblastoma. This means that in 100 children with proven neuroblastoma ¹²³I‐MIBG imaging will correctly identify 67 to 100 of the neuroblastoma cases. Only one study, that reported on a lesion level, provided data to calculate the specificity (the ability of a test to correctly classify an individual as 'disease‐free'): 68% in 115 lesions. This means that of 100 disease‐free lesions in patients with proven neuroblastoma ¹²³I‐MIBG imaging will correctly identify 68 lesions. So, in about 10% of the cases the neuroblastoma is not visible on ¹²³I‐MIBG imaging (false negative results). For these cases, it is advisable to perform an additional test like  18 F‐FDG‐PET imaging, but to be certain of its clinical value, more evidence is needed. Only one included study reported on false positive findings. This means that ¹²³I‐MIBG imaging and  18 F‐FDG‐PET imaging incorrectly identified neuroblastoma lesions in patients which might result in wrongly classifying a patient with metastatic disease. It is important to keep in mind that false positive findings can occur, although more research is needed before definitive conclusions can be made. We could not determine the diagnostic accuracy of  18 F‐FDG‐PET imaging, in case the neuroblastoma was incorrectly not identified with ¹²³I‐MIBG, due to limited data. Also, we could not calculate the diagnostic accuracy of  18 F‐FDG‐PET imaging for detecting a neuroblastoma and compare this to ¹²³I‐MIBG imaging because of the limited available data.","9","John Wiley & Sons, Ltd","1465-1858","*3‐Iodobenzylguanidine; *Positron‐Emission Tomography; *Tomography, Emission‐Computed, Single‐Photon; Adolescent; Child; Child, Preschool; Humans; Infant; Infant, Newborn; Neuroblastoma [*diagnostic imaging, pathology, secondary]; Sensitivity and Specificity","10.1002/14651858.CD009263.pub2","http://dx.doi.org/10.1002/14651858.CD009263.pub2","Childhood Cancer"
"CD011021.PUB2","Palaniyappan, L; Maayan, N; Bergman, H; Davenport, C; Adams, CE; Soares‐Weiser, K","Voxel‐based morphometry for separation of schizophrenia from other types of psychosis in first episode psychosis","Cochrane Database of Systematic Reviews","2015","Abstract - Background Schizophrenia is a psychiatric disorder which involves distortions in thought and perception, blunted affect, and behavioural disturbances. The longer psychosis goes unnoticed and untreated, the more severe the repercussions for relapse and recovery. There is some evidence that early intervention services can help, and diagnostic techniques that could contribute to early intervention may offer clinical utility in these situations. The index test being evaluated in this review is the structural magnetic resonance imaging (MRI) analysis technique known as voxel‐based morphometry (VBM) that estimates the distribution of grey matter tissue volume across several brain regions. This review is an exploratory examination of the diagnostic ‘potential’ of VBM for use as an additional tool in the clinical examination of patients with first episode psychosis to establish whether an individual will progress on to developing schizophrenia as opposed to other types of psychosis. Objectives To determine whether VBM applied to the brain can be used to differentiate schizophrenia from other types of psychosis in participants who have received a clinical diagnosis of first episode psychosis. Search methods In December 2013, we updated a previous search (May 2012) of MEDLINE, EMBASE, and PsycInfo using OvidSP . Selection criteria We included retrospective and prospective studies that consecutively or randomly selected adolescent and adult participants (< 45 years) with a first episode of psychosis; and that evaluated the diagnostic accuracy of VBM for differentiating schizophrenia from other psychoses compared with a clinical diagnosis made by a qualified mental health professional, with or without the use of standard operational criteria or symptom checklists. We excluded studies in children, and in adult participants with organic brain disorders or who were at high risk for schizophrenia, such as people with a genetic predisposition. Data collection and analysis Two review authors screened all references for inclusion. We assessed the quality of studies using the QUADAS‐2 instrument. Due to a lack of data, we were not able to extract 2 x 2 data tables for each study nor undertake any meta‐analysis. Main results We included four studies with a total of 275 participants with first episode psychosis. VBM was not used to diagnose schizophrenia in any of the studies, instead VBM was used to quantify the magnitude of differences in grey matter volume. Therefore, none of the included studies reported data that could be used in the analysis, and we summarised the findings narratively for each study. Authors' conclusions There is no evidence to currently support diagnosing schizophrenia (as opposed to other psychotic disorders) using the pattern of brain changes seen in VBM studies in patients with first episode psychosis. VBM has the potential to discriminate between diagnostic categories but the methods to do this reliably are currently in evolution. In addition, the lack of applicability of the use of VBM to clinical practice in the studies to date limits the usefulness of VBM as a diagnostic aid to differentiate schizophrenia from other types of psychotic presentations in people with first episode of psychosis. Plain language summary Brain imaging for diagnosing schizophrenia in people with first episode psychosis Background Schizophrenia is a psychiatric disorder which involves psychotic symptoms such as distortions in thought and perception, blunted affect, and behavioural disturbances. It is important for patients who have a first episode of psychosis to be correctly diagnosed as soon as possible. The earlier schizophrenia is diagnosed the better the treatment outcome. However, other diseases sometimes have similar psychotic symptoms as schizophrenia, for example bipolar disorder. This review looks at how accurate a type of brain imaging technique called voxel‐based morphology (VBM) is at diagnosing schizophrenia in people who have a first episode of psychosis. VBM is used to measure differences in the structure of the brain in people with different types of psychosis. These differences could be used to make a diagnosis. Study characteristics The evidence is current to December 2013. We found four studies that used VBM in 275 adolescents and adults with first episode psychosis. One study recruited participants from a hospital, one from an outpatient clinic and one from inpatient and outpatient psychiatric services, and the fourth study did not report setting. Participants' mean age ranged from 18.6 to 27.1 years. Only two studies reported on participants' gender and included both males and females. Quality of the evidence Study quality was found to be fairly good overall. In some instances it was unclear, mainly due to three of the studies that did not sufficiently describe method of VBM image processing. We were concerned about all of the studies' applicability because VBM was not used to diagnose schizophrenia in any of the studies, instead VBM was used to characterise differences in the brain's grey matter. Key results The four studies we identified used VBM on adolescents and adults with first episode psychosis, but VBM was used to describe brain structure and not to make a diagnosis. There is no evidence to support the use of VBM to diagnose schizophrenia in patients with first episode psychosis.","8","John Wiley & Sons, Ltd","1465-1858","Adolescent; Adult; Cohort Studies; Cross‐Sectional Studies; Diagnosis, Differential; Early Diagnosis; Female; Gray Matter [*pathology]; Humans; Magnetic Resonance Imaging [*methods]; Male; Psychotic Disorders [*pathology]; Schizophrenia [*pathology]","10.1002/14651858.CD011021.pub2","http://dx.doi.org/10.1002/14651858.CD011021.pub2","Schizophrenia"
"CD010896.PUB2","Archer, HA; Smailagic, N; John, C; Holmes, RB; Takwoingi, Y; Coulthard, EJ; Cullum, S","Regional Cerebral Blood Flow Single Photon Emission Computed Tomography for detection of Frontotemporal dementia in people with suspected dementia","Cochrane Database of Systematic Reviews","2015","Abstract - Background In the UK, dementia affects 5% of the population aged over 65 years and 25% of those over 85 years. Frontotemporal dementia (FTD) represents one subtype and is thought to account for up to 16% of all degenerative dementias. Although the core of the diagnostic process in dementia rests firmly on clinical and cognitive assessments, a wide range of investigations are available to aid diagnosis. Regional cerebral blood flow (rCBF) single‐photon emission computed tomography (SPECT) is an established clinical tool that uses an intravenously injected radiolabelled tracer to map blood flow in the brain. In FTD the characteristic pattern seen is hypoperfusion of the frontal and anterior temporal lobes. This pattern of blood flow is different to patterns seen in other subtypes of dementia and so can be used to differentiate FTD. It has been proposed that a diagnosis of FTD, (particularly early stage), should be made not only on the basis of clinical criteria but using a combination of other diagnostic findings, including rCBF SPECT. However, more extensive testing comes at a financial cost, and with a potential risk to patient safety and comfort. Objectives To determine the diagnostic accuracy of rCBF SPECT for diagnosing FTD in populations with suspected dementia in secondary/tertiary healthcare settings and in the differential diagnosis of FTD from other dementia subtypes. Search methods Our search strategy used two concepts: (a) the index test and (b) the condition of interest. We searched citation databases, including MEDLINE (Ovid SP), EMBASE (Ovid SP), BIOSIS (Ovid SP), Web of Science Core Collection (ISI Web of Science), PsycINFO (Ovid SP), CINAHL (EBSCOhost) and LILACS (Bireme), using structured search strategies appropriate for each database. In addition we searched specialised sources of diagnostic test accuracy studies and reviews including: MEDION (Universities of Maastricht and Leuven), DARE (Database of Abstracts of Reviews of Effects) and HTA (Health Technology Assessment) database. We requested a search of the Cochrane Register of Diagnostic Test Accuracy Studies and used the related articles feature in PubMed to search for additional studies. We tracked key studies in citation databases such as Science Citation Index and Scopus to ascertain any further relevant studies. We identified ‘grey’ literature, mainly in the form of conference abstracts, through the Web of Science Core Collection, including Conference Proceedings Citation Index and Embase. The most recent search for this review was run on the 1 June 2013. Following title and abstract screening of the search results, full‐text papers were obtained for each potentially eligible study. These papers were then independently evaluated for inclusion or exclusion. Selection criteria We included both case‐control and cohort (delayed verification of diagnosis) studies. Where studies used a case‐control design we included all participants who had a clinical diagnosis of FTD or other dementia subtype using standard clinical diagnostic criteria. For cohort studies, we included studies where all participants with suspected dementia were administered rCBF SPECT at baseline. We excluded studies of participants from selected populations (e.g. post‐stroke) and studies of participants with a secondary cause of cognitive impairment. Data collection and analysis Two review authors extracted information on study characteristics and data for the assessment of methodological quality and the investigation of heterogeneity. We assessed the methodological quality of each study using the QUADAS‐2 (Quality Assessment of Diagnostic Accuracy Studies) tool. We produced a narrative summary describing numbers of studies that were found to have high/low/unclear risk of bias as well as concerns regarding applicability. To produce 2 x 2 tables, we dichotomised the rCBF SPECT results (scan positive or negative for FTD) and cross‐tabulated them against the results for the reference standard. These tables were then used to calculate the sensitivity and specificity of the index test. Meta‐analysis was not performed due to the considerable between‐study variation in clinical and methodological characteristics. Main results Eleven studies (1117 participants) met our inclusion criteria. These consisted of six case‐control studies, two retrospective cohort studies and three prospective cohort studies. Three studies used single‐headed camera SPECT while the remaining eight used multiple‐headed camera SPECT. Study design and methods varied widely. Overall, participant selection was not well described and the studies were judged as having either high or unclear risk of bias. Often the threshold used to define a positive SPECT result was not predefined and the results were reported with knowledge of the reference standard. Concerns regarding applicability of the studies to the review question were generally low across all three domains (participant selection, index test and reference standard). Sensitivities and specificities for differentiating FTD from non‐FTD ranged from 0.73 to 1.00 and from 0.80 to 1.00, respectively, for the three multiple‐headed camera studies. Sensitivities were lower for the two single‐headed camera studies; one reported a sensitivity and specificity of 0.40 (95% confidence interval (CI) 0.05 to 0.85) and 0.95 (95% CI 0.90 to 0.98), respectively, and the other a sensitivity and specificity of 0.36 (95% CI 0.24 to 0.50) and 0.92 (95% CI 0.88 to 0.95), respectively. Eight of the 11 studies which used SPECT to differentiate FTD from Alzheimer's disease used multiple‐headed camera SPECT. Of these studies, five used a case‐control design and reported sensitivities of between 0.52 and 1.00, and specificities of between 0.41 and 0.86. The remaining three studies used a cohort design and reported sensitivities of between 0.73 and 1.00, and specificities of between 0.94 and 1.00. The three studies that used single‐headed camera SPECT reported sensitivities of between 0.40 and 0.80, and specificities of between 0.61 and 0.97. Authors' conclusions At present, we would not recommend the routine use of rCBF SPECT in clinical practice because there is insufficient evidence from the available literature to support this. Further research into the use of rCBF SPECT for differentiating FTD from other dementias is required. In particular, protocols should be standardised, study populations should be well described, the threshold for 'abnormal' scans predefined and clear details given on how scans are analysed. More prospective cohort studies that verify the presence or absence of FTD during a period of follow up should be undertaken. Plain language summary Regional Cerebral Blood Flow SPECT for detection of Frontotemporal dementia in people with suspected dementia Background This review focused on one type of dementia, frontotemporal dementia (FTD). This neurodegenerative disease affects the frontal and temporal lobes of the brain and accounts for up to 16% of all degenerative dementias. People who have this disease may develop changes in their behaviour, speech or ability to plan. It is important to identify people with FTD correctly as the disease course and response to treatment differs from other dementias such as Alzheimer’s disease. One test used by healthcare professionals to help make a diagnosis of FTD, is regional cerebral blood flow single photon emission computed tomography (rCBF SPECT). This investigation allows visualisation of blood flow within the brain. In FTD it is thought that the pattern of blood flow to the brain can be used to tell the difference between FTD and other dementias. However, it is not clear whether using rCBF SPECT in this way improves our ability to make an accurate diagnosis of FTD. As all investigations come with a financial cost, it is important that their benefit is known. Aim:  This review assessed the evidence regarding the accuracy of rCBF SPECT in detecting FTD in people with suspected dementia. Study characteristics We searched many databases for all papers with FTD and rCBF SPECT as their focus. These papers were reviewed independently by several researchers. After application of inclusion and exclusion criteria, eleven studies including 299 individuals with FTD were available for this review. The studies were published over a 21‐year period, with a study size ranging from 27 to 363 participants, mainly recruited from University clinics, tertiary referral centres or memory clinics. Of the 11 studies, three used single‐headed (single detector) gamma cameras, a method no longer used in clinical practice today. Evidence is current to June 2013. Quality of the evidence The majority of studies were at high risk of bias due to insufficient details on how participants were selected and how the rCBF SPECT scans were conducted and analysed. The main limitations of the review were poor reporting, variability of study design and a lack of standardisation of image interpretation between centres. Key findings Due to small study numbers and large variation in how the studies were carried out, we are unable at present to recommend the routine use of rCBF SPECT for diagnosing FTD in clinical practice.","6","John Wiley & Sons, Ltd","1465-1858","*Cerebrovascular Circulation; Case‐Control Studies; Cohort Studies; Dementia [diagnostic imaging]; Diagnosis, Differential; Frontal Lobe [blood supply]; Frontotemporal Dementia [*diagnostic imaging, physiopathology]; Humans; Sensitivity and Specificity; Temporal Lobe [blood supply]; Tomography, Emission‐Computed, Single‐Photon [*methods]","10.1002/14651858.CD010896.pub2","http://dx.doi.org/10.1002/14651858.CD010896.pub2","Dementia and Cognitive Improvement"
"CD010023.PUB2","Mallee, WH; Wang, J; Poolman, RW; Kloen, P; Maas, M; de Vet, HCW; Doornberg, JN","Computed tomography versus magnetic resonance imaging versus bone scintigraphy for clinically suspected scaphoid fractures in patients with negative plain radiographs","Cochrane Database of Systematic Reviews","2015","Abstract - Background In clinically suspected scaphoid fractures, early diagnosis reduces the risk of non‐union and minimises loss in productivity resulting from unnecessary cast immobilisation. Since initial radiographs do not exclude the possibility of a fracture, additional imaging is needed. Computed tomography (CT), magnetic resonance imaging (MRI) and bone scintigraphy (BS) are widely used to establish a definitive diagnosis, but there is uncertainty about the most appropriate method. Objectives The primary aim of this study is to identify the most suitable diagnostic imaging strategy for identifying clinically suspected fractures of the scaphoid bone in patients with normal radiographs. Therefore we looked at the diagnostic performance characteristics of the most used imaging modalities for this purpose: computed tomography, magnetic resonance imaging and bone scintigraphy. Search methods In July 2012, we searched the Cochrane Register of Diagnostic Test Accuracy Studies, MEDLINE, EMBASE, the Database of Abstracts of Reviews of Effects, the Cochrane Central Register of Controlled Trials, the NHS Economic Evaluation Database. In September 2012, we searched MEDION, ARIF, Current Controlled Trials, the World Health Organization (WHO) International Clinical Trials Registry Platform, conference proceedings and reference lists of all articles. Selection criteria We included all prospective or retrospective studies involving a consecutive series of patients of all ages that evaluated the accuracy of BS, CT or MRI, or any combination of these, for diagnosing suspected scaphoid fractures. We considered the use of one or two index tests or six‐week follow‐up radiographs as adequate reference standards. Data collection and analysis Two review authors independently screened titles and abstracts and assessed full‐text reports of potentially eligible studies. The same authors extracted data from full‐text reports and assessed methodological quality using the QUADAS checklist. For each index test, estimates of sensitivity and speciﬁcity from each study were plotted in ROC space; and forest plots were constructed for visual examination of variation in test accuracy. We performed meta‐analyses using the HSROC model to produce summary estimates of sensitivity and specificity. Main results We included 11 studies that looked at diagnostic accuracy of one or two index tests: four studies (277 suspected fractures) looked at CT, five studies (221 suspected fractures) looked at MRI and six studies (543 suspected fractures) looked at BS. Four of the studies made direct comparisons: two studies compared CT and MRI, one study compared CT and BS, and one study compared MRI and BS. Overall, the studies were of moderate to good quality, but relevant clinical information during evaluation of CT, MRI or BS was mostly unclear or unavailable. As few studies made direct comparisons between tests with the same participants, our results are based on data from indirect comparisons, which means that these results are more susceptible to bias due to confounding. Nonetheless, the direct comparisons showed similar patterns of differences in sensitivity and specificity as for the pooled indirect comparisons. Summary sensitivity and specificity of CT were 0.72 (95% confidence interval (CI) 0.36 to 0.92) and 0.99 (95% CI 0.71 to 1.00); for MRI, these were 0.88 (95% CI 0.64 to 0.97) and 1.00 (95% CI 0.38 to 1.00); for BS, these were 0.99 (95% CI 0.69 to 1.00) and 0.86 (95% CI 0.73 to 0.94). Indirect comparisons suggest that diagnostic accuracy of BS was significantly higher than CT and MRI; and CT and MRI have comparable diagnostic accuracy. The low prevalence of a true fracture among suspected fractures (median = 20%) means the lower specificity for BS is problematic. For example, in a cohort of 1000 patients, 112 will be over‐treated when BS is used for diagnosis. If CT is used, only 8 will receive unnecessary treatment. In terms of missed fractures, BS will miss 2 fractures and CT will miss 56 fractures. Authors' conclusions Although quality of the included studies is moderate to good, findings are based on only 11 studies and the confidence intervals for the summary estimates are wide for all three tests. Well‐designed direct comparison studies including CT, MRI and BS could give valuable additional information. Bone scintigraphy is statistically the best diagnostic modality to establish a definitive diagnosis in clinically suspected fractures when radiographs appear normal. However, physicians must keep in mind that BS is more invasive than the other modalities, with safety issues due to level of radiation exposure, as well as diagnostic delay of at least 72 hours. The number of overtreated patients is substantially lower with CT and MRI. Prior to performing comparative studies, there is a need to raise the initially detected prevalence of true fractures in order to reduce the effect of the relatively low specificity in daily practice. This can be achieved by improving clinical evaluation and initial radiographical assessment. Plain language summary Comparing different types of scan (CT, MRI, bone scan) for diagnosis of clinically suspected scaphoid fractures, when initial radiographs are negative This summary of a Cochrane review presents what we know from research about the accuracy of imaging tests to detect true scaphoid fractures among suspected fractures. When a patient presents to the emergency department with wrist injury and clinical signs of a scaphoid fracture, normal initial radiographs do not exclude a fracture. Approximately 20% of them do have a true scaphoid fracture and need additional imaging to establish a definitive diagnosis. Because of the low healing potential of the scaphoid bone, adequate diagnosis and treatment is vital to prevent complications such as non‐union. If a patient is clinically suspected for a scaphoid fracture, their wrist will be immobilised in a cast until definitive diagnosis is obtained. This fear of under‐treatment results in a large amount of over‐treated wrist injuries. Computed tomography (CT), magnetic resonance imaging (MRI) and bone scintigraphy (BS; bone scan) are all imaging modalities that can be chosen at this stage. The aim of this systematic review was to establish which is the superior technique for identifying a true fracture and preventing unnecessary treatment. A high sensitivity reduces the risk of missing fractures; a low specificity increases the number of unnecessary treatments. We conducted a thorough search of electronic databases, trial registers and conference proceedings up to July 2012. We included 11 studies in our analysis. The studies were moderate to good quality. Four studies (277 suspected fractures) looked at CT, five studies (221 suspected fractures) looked at MRI and six studies (543 suspected fractures) looked at BS. Four of these studies directly compared two modalities, such as both CT and MRI. When we compared the pooled data for the different imaging tests from all studies, we found that BS has the highest sensitivity, but specificity was lower than CT and MRI. All three imaging tests were found to be highly accurate for definitive diagnosis. CT and MRI were comparable in diagnostic accuracy (the correct diagnosis is made). Although BS had significantly better accuracy than CT and MRI, it could lead to more people receiving unnecessary treatment. Moreover, BS is an invasive technique and is believed to be inappropriate for use in some populations, especially children. Future studies should focus on improving clinical evaluation to raise the prevalence of true fractures. In addition, more direct comparison studies could add valuable data to determine which modality is superior in diagnosis of suspected scaphoid fractures.","6","John Wiley & Sons, Ltd","1465-1858","*Magnetic Resonance Imaging; *Radionuclide Imaging; *Tomography, X‐Ray Computed; Fractures, Bone [*diagnosis, diagnostic imaging]; Humans; Prospective Studies; Retrospective Studies; Scaphoid Bone [diagnostic imaging, *injuries]; Sensitivity and Specificity","10.1002/14651858.CD010023.pub2","http://dx.doi.org/10.1002/14651858.CD010023.pub2","Bone, Joint and Muscle Trauma"
"CD009647.PUB2","Hooper, L; Abdelhamid, A; Attreed, NJ; Campbell, WW; Channell, AM; Chassagne, P; Culp, KR; Fletcher, SJ; Fortes, MB; Fuller, N; Gaspar, PM; Gilbert, DJ; Heathcote, AC; Kafri, MW; Kajii, F; Lindner, G; Mack, GW; Mentes, JC; Merlani, P; Needham, RA; Olde Rikkert, MGM; Perren, A; Powers, J; Ranson, SC; Ritz, P; Rowat, AM; Sjöstrand, F; Smith, AC; Stookey, JJD; Stotts, NA; Thomas, DR; Vivanti, A; Wakefield, BJ; Waldréus, N; Walsh, NP; Ward, S; Potter, JF; Hunter, P","Clinical symptoms, signs and tests for identification of impending and current water‐loss dehydration in older people","Cochrane Database of Systematic Reviews","2015","Abstract - Background There is evidence that water‐loss dehydration is common in older people and associated with many causes of morbidity and mortality. However, it is unclear what clinical symptoms, signs and tests may be used to identify early dehydration in older people, so that support can be mobilised to improve hydration before health and well‐being are compromised. Objectives To determine the diagnostic accuracy of state (one time), minimally invasive clinical symptoms, signs and tests to be used as screening tests for detecting water‐loss dehydration in older people by systematically reviewing studies that have measured a reference standard and at least one index test in people aged 65 years and over. Water‐loss dehydration was defined primarily as including everyone with either impending or current water‐loss dehydration (including all those with serum osmolality ≥ 295 mOsm/kg as being dehydrated). Search methods Structured search strategies were developed for MEDLINE (OvidSP), EMBASE (OvidSP), CINAHL, LILACS, DARE and HTA databases ( The Cochrane Library ), and the International Clinical Trials Registry Platform (ICTRP). Reference lists of included studies and identified relevant reviews were checked. Authors of included studies were contacted for details of further studies. Selection criteria Titles and abstracts were scanned and all potentially relevant studies obtained in full text. Inclusion of full text studies was assessed independently in duplicate, and disagreements resolved by a third author. We wrote to authors of all studies that appeared to have collected data on at least one reference standard and at least one index test, and in at least 10 people aged ≥ 65 years, even where no comparative analysis has been published, requesting original dataset so we could create 2 x 2 tables. Data collection and analysis Diagnostic accuracy of each test was assessed against the best available reference standard for water‐loss dehydration (serum or plasma osmolality cut‐off ≥ 295 mOsm/kg, serum osmolarity or weight change) within each study. For each index test study data were presented in forest plots of sensitivity and specificity. The primary target condition was water‐loss dehydration (including either impending or current water‐loss dehydration). Secondary target conditions were intended as current (> 300 mOsm/kg) and impending (295 to 300 mOsm/kg) water‐loss dehydration, but restricted to current dehydration in the final review. We conducted bivariate random‐effects meta‐analyses (Stata/IC, StataCorp) for index tests where there were at least four studies and study datasets could be pooled to construct sensitivity and specificity summary estimates. We assigned the same approach for index tests with continuous outcome data for each of three pre‐specified cut‐off points investigated. Pre‐set minimum sensitivity of a useful test was 60%, minimum specificity 75%. As pre‐specifying three cut‐offs for each continuous test may have led to missing a cut‐off with useful sensitivity and specificity, we conducted post‐hoc exploratory analyses to create receiver operating characteristic (ROC) curves where there appeared some possibility of a useful cut‐off missed by the original three. These analyses enabled assessment of which tests may be worth assessing in further research. A further exploratory analysis assessed the value of combining the best two index tests where each had some individual predictive ability. Main results There were few published studies of the diagnostic accuracy of state (one time), minimally invasive clinical symptoms, signs or tests to be used as screening tests for detecting water‐loss dehydration in older people. Therefore, to complete this review we sought, analysed and included raw datasets that included a reference standard and an index test in people aged ≥ 65 years. We included three studies with published diagnostic accuracy data and a further 21 studies provided datasets that we analysed. We assessed 67 tests (at three cut‐offs for each continuous outcome) for diagnostic accuracy of water‐loss dehydration (primary target condition) and of current dehydration (secondary target condition). Only three tests showed any ability to diagnose water‐loss dehydration (including both impending and current water‐loss dehydration) as stand‐alone tests: expressing fatigue (sensitivity 0.71 (95% CI 0.29 to 0.96), specificity 0.75 (95% CI 0.63 to 0.85), in one study with 71 participants, but two additional studies had lower sensitivity); missing drinks between meals (sensitivity 1.00 (95% CI 0.59 to 1.00), specificity 0.77 (95% CI 0.64 to 0.86), in one study with 71 participants) and BIA resistance at 50 kHz (sensitivities 1.00 (95% CI 0.48 to 1.00) and 0.71 (95% CI 0.44 to 0.90) and specificities of 1.00 (95% CI 0.69 to 1.00) and 0.80 (95% CI 0.28 to 0.99) in 15 and 22 people respectively for two studies, but with sensitivities of 0.54 (95% CI 0.25 to 0.81) and 0.69 (95% CI 0.56 to 0.79) and specificities of 0.50 (95% CI 0.16 to 0.84) and 0.19 (95% CI 0.17 to 0.21) in 21 and 1947 people respectively in two other studies). In post‐hoc ROC plots drinks intake, urine osmolality and axillial moisture also showed limited diagnostic accuracy. No test was consistently useful in more than one study. Combining two tests so that an individual both missed some drinks between meals and expressed fatigue was sensitive at 0.71 (95% CI 0.29 to 0.96) and specific at 0.92 (95% CI 0.83 to 0.97). There was sufficient evidence to suggest that several stand‐alone tests often used to assess dehydration in older people (including fluid intake, urine specific gravity, urine colour, urine volume, heart rate, dry mouth, feeling thirsty and BIA assessment of intracellular water or extracellular water) are not useful, and should not be relied on individually as ways of assessing presence or absence of dehydration in older people. No tests were found consistently useful in diagnosing current water‐loss dehydration. Authors' conclusions There is limited evidence of the diagnostic utility of any individual clinical symptom, sign or test or combination of tests to indicate water‐loss dehydration in older people. Individual tests should not be used in this population to indicate dehydration; they miss a high proportion of people with dehydration, and wrongly label those who are adequately hydrated. Promising tests identified by this review need to be further assessed, as do new methods in development. Combining several tests may improve diagnostic accuracy. Plain language summary Clinical symptoms, signs and tests for identification of impending and current water‐loss dehydration in older people Water‐loss dehydration results from drinking too little fluid. It is common in older people and associated with increased risk of many health problems. We wanted to find out whether simple tests (like skin turgor, dry mouth, urine colour and bioelectrical impedance) can usefully tell us whether an older person (aged at least 65 years) is drinking enough. Within the review we assessed 67 different tests, but no tests were consistently useful in telling us whether older people are drinking enough, or are dehydrated. Some tests did appear useful in some studies, and these promising tests should be re‐checked to see whether they are useful in specific older populations. There was sufficient evidence to suggest that some tests should not be used to indicate dehydration. Tests that should not be used include dry mouth, feeling thirsty, heart rate, urine colour, and urine volume.","4","John Wiley & Sons, Ltd","1465-1858","Aged; Dehydration [blood, *diagnosis]; Drinking Water [*administration & dosage]; Electric Impedance; Female; Humans; Male; Mouth Diseases [diagnosis]; Osmolar Concentration; Sensitivity and Specificity; Skin Physiological Phenomena; Symptom Assessment [methods]; Urine","10.1002/14651858.CD009647.pub2","http://dx.doi.org/10.1002/14651858.CD009647.pub2","Kidney and Transplant"
"CD009579.PUB2","Ochodo, EA; Gopalakrishna, G; Spek, B; Reitsma, JB; van Lieshout, L; Polman, K; Lamberton, P; Bossuyt, PMM; Leeflang, MMG","Circulating antigen tests and urine reagent strips for diagnosis of active schistosomiasis in endemic areas","Cochrane Database of Systematic Reviews","2015","Abstract - Background Point‐of‐care (POC) tests for diagnosing schistosomiasis include tests based on circulating antigen detection and urine reagent strip tests. If they had sufficient diagnostic accuracy they could replace conventional microscopy as they provide a quicker answer and are easier to use. Objectives To summarise the diagnostic accuracy of: a) urine reagent strip tests in detecting active  Schistosoma haematobium  infection, with microscopy as the reference standard; and b) circulating antigen tests for detecting active  Schistosoma  infection in geographical regions endemic for  Schistosoma mansoni  or  S. haematobium  or both ,  with microscopy as the reference standard. Search methods We searched the electronic databases MEDLINE, EMBASE, BIOSIS, MEDION, and Health Technology Assessment (HTA) without language restriction up to 30 June 2014. Selection criteria We included studies that used microscopy as the reference standard: for  S. haematobium , microscopy of urine prepared by filtration, centrifugation, or sedimentation methods; and for  S. mansoni , microscopy of stool by Kato‐Katz thick smear. We included studies on participants residing in endemic areas only. Data collection and analysis Two review authors independently extracted data, assessed quality of the data using QUADAS‐2, and performed meta‐analysis where appropriate. Using the variability of test thresholds, we used the hierarchical summary receiver operating characteristic (HSROC) model for all eligible tests (except the circulating cathodic antigen (CCA) POC for  S. mansoni , where the bivariate random‐effects model was more appropriate). We investigated heterogeneity, and carried out indirect comparisons where data were sufficient. Results for sensitivity and specificity are presented as percentages with 95% confidence intervals (CI). Main results We included 90 studies; 88 from field settings in Africa. The median  S. haematobium  infection prevalence was 41% (range 1% to 89%) and 36% for  S. mansoni  (range 8% to 95%). Study design and conduct were poorly reported against current standards. Tests for  S. haematobium Urine reagent test strips versus microscopy Compared to microscopy, the detection of microhaematuria on test strips had the highest sensitivity and specificity (sensitivity 75%, 95% CI 71% to 79%; specificity 87%, 95% CI 84% to 90%; 74 studies, 102,447 participants). For proteinuria, sensitivity was 61% and specificity was 82% (82,113 participants); and for leukocyturia, sensitivity was 58% and specificity 61% (1532 participants). However, the difference in overall test accuracy between the urine reagent strips for microhaematuria and proteinuria was not found to be different when we compared separate populations (P = 0.25), or when direct comparisons within the same individuals were performed (paired studies; P = 0.21). When tests were evaluated against the higher quality reference standard (when multiple samples were analysed), sensitivity was marginally lower for microhaematuria (71% vs 75%) and for proteinuria (49% vs 61%). The specificity of these tests was comparable. Antigen assay Compared to microscopy, the CCA test showed considerable heterogeneity; meta‐analytic sensitivity estimate was 39%, 95% CI 6% to 73%; specificity 78%, 95% CI 55% to 100% (four studies, 901 participants). Tests for  S. mansoni Compared to microscopy, the CCA test meta‐analytic estimates for detecting  S. mansoni  at a single threshold of trace positive were: sensitivity 89% (95% CI 86% to 92%); and specificity 55% (95% CI 46% to 65%; 15 studies, 6091 participants) Against a higher quality reference standard, the sensitivity results were comparable (89% vs 88%) but specificity was higher (66% vs 55%). For the CAA test, sensitivity ranged from 47% to 94%, and specificity from 8% to 100% (four studies, 1583 participants). Authors' conclusions Among the evaluated tests for  S. haematobium  infection, microhaematuria correctly detected the largest proportions of infections and non‐infections identified by microscopy. The CCA POC test for  S. mansoni  detects a very large proportion of infections identified by microscopy, but it misclassifies a large proportion of microscopy negatives as positives in endemic areas with a moderate to high prevalence of infection, possibly because the test is potentially more sensitive than microscopy. 23 April 2019 No update planned Other Reliable evidence with clear conclusions. All eligible published studies found in the last search (30 Jun, 2014) were included. Plain language summary How well do point‐of‐care tests detect  Schistosoma  infections in people living inendemic areas? Schistosomiasis, also known as bilharzia, is a parasitic disease common in the tropical and subtropics. Point‐of‐care tests and urine reagent strip tests are quicker and easier to use than microscopy. We estimate how well these point‐of‐care tests are able to detect schistosomiasis infections compared with microscopy. We searched for studies published in any language up to 30 June 2014, and we considered the study’s risk of providing biased results. What do the results say? We included 90 studies involving almost 200,000 people, with 88 of these studies carried out in Africa in field settings. Study design and conduct were poorly reported against current expectations. Based on our statistical model, we found: • Among the urine strips for detecting urinary schistosomiasis, the strips for detecting blood were better than those detecting protein or white cells (sensitivity and specificity for blood 75% and 87%; for protein 61% and 82%; and for white cells 58% and 61%, respectively).  • For urinary schistosomiasis, the parasite antigen test performance was worse (sensitivity, 39% and specificity, 78%) than urine strips for detecting blood.  • For intestinal schistosomiasis, the parasite antigen urine test, detected many infections identified by microscopy but wrongly labelled many uninfected people as sick (sensitivity, 89% and specificity, 55%). What are the consequences of using these tests? If we take 1000 people, of which 410 have urinary schistosomiasis on microscopy testing, then using the strip detecting blood in the urine would misclassify 77 uninfected people as infected, and thus may receive unnecessary treatment; and it would wrongly classify 102 infected people as uninfected, who thus may not receive treatment. If we take 1000 people, of which 360 have intestinal schistosomiasis on microscopy testing, then the antigen test would misclassify 288 uninfected people as infected. These people may be given unnecessary treatment. This test also would wrongly classify 40 infected people as uninfected who thus may not receive treatment. Conclusion of review For urinary schistosomiasis, the urine strip for detecting blood leads to some infected people being missed and some non‐infected people being diagnosed with the condition, but is better than the protein or white cell tests. The parasite antigen test is not accurate. For intestinal schistosomiasis, the parasite antigen urine test classifies many microscopy negative people as being infected. This finding may be explained by the low sensitivity of microscopy.","3","John Wiley & Sons, Ltd","1465-1858","*Reagent Strips; *Schistosoma haematobium [immunology]; *Schistosoma mansoni [immunology]; Adult; Animals; Antigens, Helminth [blood]; Child; Cross‐Sectional Studies; Female; Hematuria [diagnosis]; Humans; Male; Microscopy; Prevalence; Proteinuria [diagnosis]; Reference Standards; Schistosomiasis haematobia [blood, *diagnosis, immunology, urine]; Schistosomiasis mansoni [blood, *diagnosis, immunology, urine]; Sensitivity and Specificity","10.1002/14651858.CD009579.pub2","http://dx.doi.org/10.1002/14651858.CD009579.pub2","Infectious Diseases"
"CD011548","Gurusamy, KS; Giljaca, V; Takwoingi, Y; Higgie, D; Poropat, G; Štimac, D; Davidson, BR","Ultrasound versus liver function tests for diagnosis of common bile duct stones","Cochrane Database of Systematic Reviews","2015","Abstract - Background Ultrasound and liver function tests (serum bilirubin and serum alkaline phosphatase) are used as screening tests for the diagnosis of common bile duct stones in people suspected of having common bile duct stones. There has been no systematic review of the diagnostic accuracy of ultrasound and liver function tests. Objectives To determine and compare the accuracy of ultrasound versus liver function tests for the diagnosis of common bile duct stones. Search methods We searched MEDLINE, EMBASE, Science Citation Index Expanded, BIOSIS, and Clinicaltrials.gov to September 2012. We searched the references of included studies to identify further studies and systematic reviews identified from various databases (Database of Abstracts of Reviews of Effects, Health Technology Assessment, Medion, and ARIF (Aggressive Research Intelligence Facility)). We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. Selection criteria We included studies that provided the number of true positives, false positives, false negatives, and true negatives for ultrasound, serum bilirubin, or serum alkaline phosphatase. We only accepted studies that confirmed the presence of common bile duct stones by extraction of the stones (irrespective of whether this was done by surgical or endoscopic methods) for a positive test result, and absence of common bile duct stones by surgical or endoscopic negative exploration of the common bile duct, or symptom‐free follow‐up for at least six months for a negative test result as the reference standard in people suspected of having common bile duct stones. We included participants with or without prior diagnosis of cholelithiasis; with or without symptoms and complications of common bile duct stones, with or without prior treatment for common bile duct stones; and before or after cholecystectomy. At least two authors screened abstracts and selected studies for inclusion independently. Data collection and analysis Two authors independently collected data from each study. Where meta‐analysis was possible, we used the bivariate model to summarise sensitivity and specificity. Main results Five studies including 523 participants reported the diagnostic accuracy of ultrasound. One studies (262 participants) compared the accuracy of ultrasound, serum bilirubin and serum alkaline phosphatase in the same participants. All the studies included people with symptoms. One study included only participants without previous cholecystectomy but this information was not available from the remaining studies. All the studies were of poor methodological quality. The sensitivities for ultrasound ranged from 0.32 to 1.00, and the specificities ranged from 0.77 to 0.97. The summary sensitivity was 0.73 (95% CI 0.44 to 0.90) and the specificity was 0.91 (95% CI 0.84 to 0.95). At the median pre‐test probability of common bile duct stones of 0.408, the post‐test probability (95% CI) associated with positive ultrasound tests was 0.85 (95% CI 0.75 to 0.91), and negative ultrasound tests was 0.17 (95% CI 0.08 to 0.33). The single study of liver function tests reported diagnostic accuracy at two cut‐offs for bilirubin (greater than 22.23 μmol/L and greater than twice the normal limit) and two cut‐offs for alkaline phosphatase (greater than 125 IU/L and greater than twice the normal limit). This study also assessed ultrasound and reported higher sensitivities for bilirubin and alkaline phosphatase at both cut‐offs but the specificities of the markers were higher at only the greater than twice the normal limit cut‐off. The sensitivity for ultrasound was 0.32 (95% CI 0.15 to 0.54), bilirubin (cut‐off greater than 22.23 μmol/L) was 0.84 (95% CI 0.64 to 0.95), and alkaline phosphatase (cut‐off greater than 125 IU/L) was 0.92 (95% CI 0.74 to 0.99). The specificity for ultrasound was 0.95 (95% CI 0.91 to 0.97), bilirubin (cut‐off greater than 22.23 μmol/L) was 0.91 (95% CI 0.86 to 0.94), and alkaline phosphatase (cut‐off greater than 125 IU/L) was 0.79 (95% CI 0.74 to 0.84). No study reported the diagnostic accuracy of a combination of bilirubin and alkaline phosphatase, or combinations with ultrasound. Authors' conclusions Many people may have common bile duct stones in spite of having a negative ultrasound or liver function test. Such people may have to be re‐tested with other modalities if the clinical suspicion of common bile duct stones is very high because of their symptoms. False‐positive results are also possible and further non‐invasive testing is recommended to confirm common bile duct stones to avoid the risks of invasive testing. It should be noted that these results were based on few studies of poor methodological quality and the results for ultrasound varied considerably between studies. Therefore, the results should be interpreted with caution. Further studies of high methodological quality are necessary to determine the diagnostic accuracy of ultrasound and liver function tests. Plain language summary Ultrasound and liver function tests for the diagnosis of common bile duct stones Background Bile, produced in the liver and stored temporarily in the gallbladder, is released into the small bowel on eating fatty food. The common bile duct is the tube through which bile flows from the gallbladder to the small bowel. Stones in the common bile duct (common bile duct stones), usually formed in the gallbladder before migration into the bile duct, can obstruct the flow of bile leading to jaundice (yellowish discolouration of skin, white of the eyes, and dark urine); infection of the bile (cholangitis); and inflammation of the pancreas (pancreatitis), which can be life threatening. Various diagnostic tests can be performed for the diagnosis of common bile duct stones. Depending upon the availability of resources, these stones are removed endoscopically (usually the case) or may be removed as a part of the operation performed to remove the gallbladder (it is important to remove the gallbladder since the stones continue to form in the gallbladder and can cause recurrent problems). Non‐invasive tests such as ultrasound (use of sound waves higher than audible range to differentiate tissues based on how they reflect the sound waves) and blood markers of bile flow obstruction such as serum bilirubin and serum alkaline phosphatase are used to identify people at high risk of having common bile duct stones. Using non‐invasive tests means that only those people at high risk can be subjected to further tests. We reviewed the evidence on the accuracy of ultrasound and liver function tests for detection of common bile duct stones. The evidence is current to September 2012. Study characteristics We identified five studies including 523 participants that reported the diagnostic test accuracy of ultrasound. One of these studies, involving 262 participants, also reported the diagnostic test accuracy of serum bilirubin and serum alkaline phosphatase. All the studies included people with symptoms. One study included only participants who had not undergone previous cholecystectomy (removal of gallbladder). This information was not available from the remaining studies. Key results Based on an average sensitivity of 73% for ultrasound, we would expect that on average 73 out of 100 people with common bile duct stones will be detected while the remaining 27 people will be missed and will not receive appropriate treatment. The average number of people with common bile duct stones detected using ultrasound may vary between 44 and 90 out of 100 people. Based on an average specificity of 91% for ultrasound, we would expect that on average 91 out of 100 people without common bile duct stones would be identified as not having common bile duct stones; 9 out of 100 would be false positives and not receive appropriate treatment. The average number of false positives could vary between 5 and 16 out of 100 people. Evidence from one study suggested that using a level of serum alkaline phosphatase higher than 125 units to distinguish between people who have and people who do not have common bile duct stones gave better diagnostic accuracy than using a level twice the normal limit (which usually ranges between 0 and 40). The study also showed better accuracy for serum alkaline phosphatase compared to serum bilirubin. The sensitivity of serum alkaline phosphatase at the 125 units cut‐off was 92%, which means that 92 out of 100 people with common bile duct stones would be detected but 8 out of 100 people will be missed. The number detected could vary between 74 and 99 out of 100 people. Based on the specificity of 79%, 79 out of 100 people without common bile duct stones will be correctly identified as not having common bile duct stones while the remaining 21 people will be false positives. The number of false positives could vary between 16 and 26 out of 100 people. This suggests that further non‐invasive tests may be useful to diagnose common bile duct stones prior to the use of invasive tests. Quality of evidence All the studies were of low methodological quality, which may undermine the validity of our findings. Future research Further studies of high methodological quality are necessary.","2","John Wiley & Sons, Ltd","1465-1858","*Liver Function Tests; Alkaline Phosphatase [blood]; Bilirubin [blood]; Biomarkers [blood]; Choledocholithiasis [*diagnosis, *diagnostic imaging]; Humans; Ultrasonography","10.1002/14651858.CD011548","http://dx.doi.org/10.1002/14651858.CD011548","Hepato-Biliary"
"CD011549","Giljaca, V; Gurusamy, KS; Takwoingi, Y; Higgie, D; Poropat, G; Štimac, D; Davidson, BR","Endoscopic ultrasound versus magnetic resonance cholangiopancreatography for common bile duct stones","Cochrane Database of Systematic Reviews","2015","Abstract - Background Endoscopic ultrasound (EUS) and magnetic resonance cholangiopancreatography (MRCP) are tests used in the diagnosis of common bile duct stones in patients suspected of having common bile duct stones prior to undergoing invasive treatment. There has been no systematic review of the accuracy of EUS and MRCP in the diagnosis of common bile duct stones using appropriate reference standards. Objectives To determine and compare the accuracy of EUS and MRCP for the diagnosis of common bile duct stones. Search methods We searched MEDLINE, EMBASE, Science Citation Index Expanded, BIOSIS, and Clinicaltrials.gov until September 2012. We searched the references of included studies to identify further studies and of systematic reviews identified from various databases (Database of Abstracts of Reviews of Effects (DARE), Health Technology Assessment (HTA), Medion, and ARIF (Aggressive Research Intelligence Facility)). We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. Selection criteria We included studies that provided the number of true positives, false positives, false negatives, and true negatives for EUS or MRCP. We only accepted studies that confirmed the presence of common bile duct stones by extraction of the stones (irrespective of whether this was done by surgical or endoscopic methods) for a positive test, and absence of common bile duct stones by surgical or endoscopic negative exploration of the common bile duct or symptom free follow‐up for at least six months for a negative test, as the reference standard in people suspected of having common bile duct stones. We included participants with or without prior diagnosis of cholelithiasis; with or without symptoms and complications of common bile duct stones, with or without prior treatment for common bile duct stones; and before or after cholecystectomy. At least two authors independently screened abstracts and selected studies for inclusion. Data collection and analysis Two authors independently collected the data from each study. We used the bivariate model to obtain pooled estimates of sensitivity and specificity. Main results We included a total of 18 studies involving 2366 participants (976 participants with common bile duct stones and 1390 participants without common bile duct stones). Eleven studies evaluated EUS alone, and five studies evaluated MRCP alone. Two studies evaluated both tests. Most studies included patients who were suspected of having common bile duct stones based on abnormal liver function tests; abnormal transabdominal ultrasound; symptoms such as obstructive jaundice, cholangitis, or pancreatitis; or a combination of the above. The proportion of participants who had undergone cholecystectomy varied across studies. Not one of the studies was of high methodological quality. For EUS, the sensitivities ranged between 0.75 and 1.00 and the specificities ranged between 0.85 and 1.00. The summary sensitivity (95% confidence interval (CI)) and specificity (95% CI) of the 13 studies that evaluated EUS (1537 participants; 686 cases and 851 participants without common bile duct stones) were 0.95 (95% CI 0.91 to 0.97) and 0.97 (95% CI 0.94 to 0.99). For MRCP, the sensitivities ranged between 0.77 and 1.00 and the specificities ranged between 0.73 and 0.99. The summary sensitivity and specificity of the seven studies that evaluated MRCP (996 participants; 361 cases and 635 participants without common bile duct stones) were 0.93 (95% CI 0.87 to 0.96) and 0.96 (95% CI 0.90 to 0.98). There was no evidence of a difference in sensitivity or specificity between EUS and MRCP (P value = 0.5). From the included studies, at the median pre‐test probability of common bile duct stones of 41% the post‐test probabilities (with 95% CI) associated with positive and negative EUS test results were 0.96 (95% CI 0.92 to 0.98) and 0.03 (95% CI 0.02 to 0.06). At the same pre‐test probability, the post‐test probabilities associated with positive and negative MRCP test results were 0.94 (95% CI 0.87 to 0.97) and 0.05 (95% CI 0.03 to 0.09). Authors' conclusions Both EUS and MRCP have high diagnostic accuracy for detection of common bile duct stones. People with positive EUS or MRCP should undergo endoscopic or surgical extraction of common bile duct stones and those with negative EUS or MRCP do not need further invasive tests. However, if the symptoms persist, further investigations will be indicated. The two tests are similar in terms of diagnostic accuracy and the choice of which test to use will be informed by availability and contra‐indications to each test. However, it should be noted that the results are based on studies of poor methodological quality and so the results should be interpreted with caution. Further studies that are of high methodological quality are necessary to determine the diagnostic accuracy of EUS and MRCP for the diagnosis of common bile duct stones. Plain language summary Endoscopic ultrasound versus magnetic resonance cholangiopancreatography for the diagnosis of common bile duct stones Background Bile, produced in the liver and stored temporarily in the gallbladder, is released into the small bowel on eating fatty food. The common bile duct (CBD) is the tube through which bile flows from the gallbladder to the small bowel. Stones in the CBD (CBD stones) are usually formed in the gallbladder before migration into the bile duct. They can obstruct the flow of bile leading to jaundice (yellowish discolouration of skin, whites of the eyes, and dark urine), infection of the bile (cholangitis), and inflammation of the pancreas (pancreatitis), which can be life threatening. Various diagnostic tests can be performed for the diagnosis of CBD stones. Depending upon the availability of resources, these stones are removed endoscopically (usually the case) or may be removed as part of the operation performed to remove the gallbladder (it is important to remove the gallbladder since the stones continue to form in the gallbladder and can cause recurrent problems). Prior to removal, invasive tests such as endoscopic retrograde cholangiopancreatography (ERCP) or intraoperative cholangiography (IOC) can be performed to detect CBD stones. However, before performing such invasive tests to diagnose CBD stones, non‐invasive tests such as endoscopic ultrasound (EUS) (using ultrasound attached to the endoscope) and magnetic resonance cholangiopancreatography (MRCP) are used to identify people at high risk of having CBD stones so that only those at high risk can be subjected to further tests. Study characteristics We performed a thorough search for studies that reported the accuracy of EUS or MRCP in the diagnosis of CBD stones. We included a total of 18 studies involving 2532 participants. Eleven studies evaluated EUS alone, five studies evaluated MRCP alone, and two studies evaluated both tests. A total of 1537 participants were included in the 13 studies that evaluated EUS and 995 participants were included in the seven studies that evaluated MRCP. Most studies included patients who were suspected of having CBD stones based on abnormal blood tests, abnormal ultrasound, or symptoms such as jaundice or pancreatitis, or a combination of the above. The proportion of participants who had undergone previous gallbladder removal varied across studies. Key results Based on an average sensitivity of 95% for EUS, on average 95 out of 100 people with CBD stones will be detected while the remaining 5 people will be missed and will not receive appropriate treatment. The average number of people with CBD stones detected using EUS may vary between 91 and 97 out of 100 people. The average specificity of 97% for EUS means that on average 97 out of 100 people without CBD stones will be identified as not having CBD stones; 3 out of 100 would be false positives and would not receive appropriate treatment. The average number of false positives could vary between 1 and 6 out of 100 people. For MRCP, an average sensitivity of 93% means that on average 93 out of 100 people with CBD stones will be detected while the remaining 7 people will be missed and will not receive appropriate treatment. The average number of people with CBD stones detected using MRCP may vary between 87 and 96 out of 100 people. With an average specificity of 96% for MRCP, 96 out of 100 people without CBD stones will be identified as not having CBD stones; 4 out of 100 would be false positives and would not receive appropriate treatment. The average number of false positives could vary between 2 and 10 out of 100 people. This means that some people with CBD stones can be missed by EUS and MRCP. Although most people with a negative EUS or MRCP do not need to undergo further invasive tests, in the presence of persistent symptoms further testing with MRCP if the patient had undergone EUS or EUS if the patient had undergone MRCP, ERCP, or IOC may be indicated. There is little to choose between EUS and MRCP in terms of diagnostic accuracy. Quality of evidence All the studies were of low methodological quality, which may undermine the validity of our findings. Future research Further studies of high methodological quality are necessary.","2","John Wiley & Sons, Ltd","1465-1858","*Cholangiopancreatography, Magnetic Resonance [standards]; *Endosonography [standards]; Choledocholithiasis [*diagnosis, *diagnostic imaging]; Humans; Sensitivity and Specificity","10.1002/14651858.CD011549","http://dx.doi.org/10.1002/14651858.CD011549","Hepato-Biliary"
"CD010339.PUB2","Gurusamy, KS; Giljaca, V; Takwoingi, Y; Higgie, D; Poropat, G; Štimac, D; Davidson, BR","Endoscopic retrograde cholangiopancreatography versus intraoperative cholangiography for diagnosis of common bile duct stones","Cochrane Database of Systematic Reviews","2015","Abstract - Background Endoscopic retrograde cholangiopancreatography (ERCP) and intraoperative cholangiography (IOC) are tests used in the diagnosis of common bile duct stones in people suspected of having common bile duct stones. There has been no systematic review of the diagnostic accuracy of ERCP and IOC. Objectives To determine and compare the accuracy of ERCP and IOC for the diagnosis of common bile duct stones. Search methods We searched MEDLINE, EMBASE, Science Citation Index Expanded, BIOSIS, and Clinicaltrials.gov to September 2012. To identify additional studies, we searched the references of included studies and systematic reviews identified from various databases (Database of Abstracts of Reviews of Effects (DARE)), Health Technology Assessment (HTA), Medion, and ARIF (Aggressive Research Intelligence Facility)). We did not restrict studies based on language or publication status, or whether data were collected prospectively or retrospectively. Selection criteria We included studies that provided the number of true positives, false positives, false negatives, and true negatives for ERCP or IOC. We only accepted studies that confirmed the presence of common bile duct stones by extraction of the stones (irrespective of whether this was done by surgical or endoscopic methods) for a positive test, and absence of common bile duct stones by surgical or endoscopic negative exploration of the common bile duct, or symptom‐free follow‐up for at least six months for a negative test as the reference standard in people suspected of having common bile duct stones. We included participants with or without prior diagnosis of cholelithiasis; with or without symptoms and complications of common bile duct stones; with or without prior treatment for common bile duct stones; and before or after cholecystectomy. At least two authors screened abstracts and selected studies for inclusion independently. Data collection and analysis Two authors independently collected data from each study. We used the bivariate model to summarise the sensitivity and specificity of the tests. Main results We identified five studies including 318 participants (180 participants with and 138 participants without common bile duct stones) that reported the diagnostic accuracy of ERCP and five studies including 654 participants (125 participants with and 529 participants without common bile duct stones) that reported the diagnostic accuracy of IOC. Most studies included people with symptoms (participants with jaundice or pancreatitis) suspected of having common bile duct stones based on blood tests, ultrasound, or both, prior to the performance of ERCP or IOC. Most studies included participants who had not previously undergone removal of the gallbladder (cholecystectomy). None of the included studies was of high methodological quality as evaluated by the QUADAS‐2 tool (quality assessment tool for diagnostic accuracy studies). The sensitivities of ERCP ranged between 0.67 and 0.94 and the specificities ranged between 0.92 and 1.00. For ERCP, the summary sensitivity was 0.83 (95% confidence interval (CI) 0.72 to 0.90) and specificity was 0.99 (95% CI 0.94 to 1.00). The sensitivities of IOC ranged between 0.75 and 1.00 and the specificities ranged between 0.96 and 1.00. For IOC, the summary sensitivity was 0.99 (95% CI 0.83 to 1.00) and specificity was 0.99 (95% CI 0.95 to 1.00). For ERCP, at the median pre‐test probability of common bile duct stones of 0.35 estimated from the included studies (i.e., 35% of people suspected of having common bile duct stones were confirmed to have gallstones by the reference standard), the post‐test probabilities associated with positive test results was 0.97 (95% CI 0.88 to 0.99) and negative test results was 0.09 (95% CI 0.05 to 0.14). For IOC, at the median pre‐test probability of common bile duct stones of 0.35, the post‐test probabilities associated with positive test results was 0.98 (95% CI 0.85 to 1.00) and negative test results was 0.01 (95% CI 0.00 to 0.10). There was weak evidence of a difference in sensitivity (P value = 0.05) with IOC showing higher sensitivity than ERCP. There was no evidence of a difference in specificity (P value = 0.7) with both tests having similar specificity. Authors' conclusions Although the sensitivity of IOC appeared to be better than that of ERCP, this finding may be unreliable because none of the studies compared both tests in the same study populations and most of the studies were methodologically flawed. It appears that both tests were fairly accurate in guiding further invasive treatment as most people diagnosed with common bile duct stones by these tests had common bile duct stones. Some people may have common bile duct stones in spite of having a negative ERCP or IOC result. Such people may have to be re‐tested if the clinical suspicion of common bile duct stones is very high because of their symptoms or persistently abnormal liver function tests. However, the results should be interpreted with caution given the limited quantity and quality of the evidence. Plain language summary Endoscopic retrograde cholangiopancreatography versus intraoperative cholangiography for the diagnosis of common bile duct stones Background The liver has various functions. Production of bile is one of these functions. The common bile duct (CBD) is the tube through which bile flows from the gallbladder (where bile is temporarily stored) into the small bowel. Stones in the CBD (CBD stones) can obstruct the flow of bile from the liver into the small bowel. Usually such stones are formed in the gallbladder and migrate into the CBD. Obstruction of the flow of bile can lead to jaundice (yellowish discolouration of skin and white of the eyes, and dark urine), infection of the bile duct (cholangitis), and inflammation of the pancreas (pancreatitis), which can be life threatening. Various diagnostic tests can be performed to diagnose CBD stones. Depending upon the availability of resources, these stones are removed endoscopically (a tube inserted into the stomach and upper part of small bowel through mouth; usually the case), or may be removed as part of the laparoscopic operation (key hole surgery) or open operation performed to remove the gallbladder (cholecystectomy; it is important to remove the gallbladder since the stones continue to form in the gallbladder and can cause recurrent health problems). If the stones are removed endoscopically, presence of stones is confirmed by endoscopic retrograde cholangiopancreatography (ERCP) (injection of dye into the CBD using an endoscope) before endoscopic removal of CBD stones. Alternatively, intraoperative cholangiography (IOC) (injection of dye into the biliary tree during an operation to remove the CBD stones, usually combined with an operation to remove gallstones) can be performed to detect CBD stones prior to operative removal of the stones. We performed a thorough search for studies that reported the accuracy of ERCP or IOC for the diagnosis of CBD stones. The evidence is current to September 2012. Study characteristics We identified five studies including 318 participants that reported the diagnostic test accuracy of ERCP and five studies including 654 participants that reported the diagnostic test accuracy of IOC. Most studies included people with symptoms (participants with jaundice or pancreatitis) who were suspected of having CBD stones based on blood tests, ultrasound (use of sound waves higher than audible range to differentiate tissues based on how they reflect the sound waves), or both, prior to the having ERCP or IOC. Most studies included participants who had not previously undergone cholecystectomy. Key results Given an average sensitivity of 83% for ERCP, we would expect that on average 83 out of 100 people (this may vary between 72 and 90 out of 100 people) with CBD stones would be detected while the remaining 17 people would be missed and would not receive appropriate treatment. Based on an average specificity of 99% for ERCP, we would expect that on average 99 out of 100 people without CBD stones would be identified as not having CBD stones; 1 out of 100 (this could vary between 0 and 17 out of 100 people) would be false positive and would not receive appropriate treatment. For IOC, an average sensitivity of 99% means that on average 99 out of 100 people (this may vary between 83 and 100 out of 100 people) with CBD stones would be detected while only one person would be missed and would not receive appropriate treatment. In terms of specificity, an average of 99% for IOC means that 99 out of 100 people without CBD stones would be identified as not having CBD stones with only one false positive (this could vary between 0 and 5 out of 100 people) who would not receive appropriate treatment. It appears that both tests are fairly accurate in guiding further invasive treatment as most people diagnosed with CBD stones by these tests have CBD stones. However, some people may have CBD stones in spite of having a negative ERCP or IOC test result. Such people may have to be re‐tested if the clinical suspicion of CBD stones is very high because of their symptoms. Quality of evidence All the studies were of low methodological quality, which may question the validity of our findings. Future research Further studies of high methodological quality are necessary.","2","John Wiley & Sons, Ltd","1465-1858","*Cholangiography; *Cholangiopancreatography, Endoscopic Retrograde; Choledocholithiasis [*diagnostic imaging]; Humans; Intraoperative Period; Randomized Controlled Trials as Topic; Sensitivity and Specificity","10.1002/14651858.CD010339.pub2","http://dx.doi.org/10.1002/14651858.CD010339.pub2","Hepato-Biliary"
"CD010438.PUB2","Hunt, H; Stanworth, S; Curry, N; Woolley, T; Cooper, C; Ukoumunne, O; Zhelev, Z; Hyde, C","Thromboelastography (TEG) and rotational thromboelastometry (ROTEM) for trauma‑induced coagulopathy in adult trauma patients with bleeding","Cochrane Database of Systematic Reviews","2015","Abstract - Background Trauma‐induced coagulopathy (TIC) is a disorder of the blood clotting process that occurs soon after trauma injury. A diagnosis of TIC on admission is associated with increased mortality rates, increased burdens of transfusion, greater risks of complications and longer stays in critical care. Current diagnostic testing follows local hospital processes and normally involves conventional coagulation tests including prothrombin time ratio/international normalized ratio (PTr/INR), activated partial prothrombin time and full blood count. In some centres, thromboelastography (TEG) and rotational thromboelastometry (ROTEM) are standard tests, but in the UK they are more commonly used in research settings. Objectives The objective was to determine the diagnostic accuracy of thromboelastography (TEG) and rotational thromboelastometry (ROTEM) for TIC in adult trauma patients with bleeding, using a reference standard of prothrombin time ratio and/or the international normalized ratio. Search methods We ran the search on 4 March 2013. Searches ran from 1970 to current. We searched The Cochrane Library, MEDLINE (OvidSP), EMBASE Classic and EMBASE, eleven other databases, the web, and clinical trials registers. The Cochrane Injuries Group's specialised register was not searched for this review as it does not contain diagnostic test accuracy studies. We also screened reference lists, conducted forward citation searches and contacted authors. Selection criteria We included all cross‐sectional studies investigating the diagnostic test accuracy of TEG and ROTEM in patients with clinically suspected TIC, as well as case‐control studies. Participants were adult trauma patients in both military and civilian settings. TIC was defined as a PTr/INR reading of 1.2 or greater, or 1.5 or greater. Data collection and analysis We piloted and performed all review stages in duplicate, including quality assessment using the QUADAS‐2 tool, adhering to guidance in the Cochrane Handbook for Diagnostic Test Accuracy Reviews. We analysed sensitivity and specificity of included studies narratively as there were insufficient studies to perform a meta‐analysis. Main results Three studies were included in the final analysis. All three studies used ROTEM as the test of global haemostatic function, and none of the studies used TEG. Tissue factor‐activated assay EXTEM clot amplitude (CA) was the focus of the accuracy measurements in blood samples taken near to the point of admission. These CAs were not taken at a uniform time after the start of the coagulopathic trace; the time varied from five minutes, to ten minutes and fifteen minutes. The three included studies were conducted in the UK, France and Afghanistan in both civilian and military trauma settings. In two studies, median Injury Severity Scores were 12, inter‐quartile range (IQR) 4 to 24; and 22, IQR 12 to 34; and in one study the median New Injury Severity Score was 34, IQR 17 to 43. There were insufficient included studies examining each of the three ROTEM CAs at 5, 10 and 15 minutes to make meta‐analysis and investigation of heterogeneity valid. The results of the included studies are thus reported narratively and illustrated by a forest plot and results plotted on the receiver operating characteristic (ROC) plane. For CA5 the accuracy results were sensitivity 70% (95% CI 47% to 87%) and specificity 86% (95% CI 82% to 90%) for one study, and sensitivity 96% (95% CI 88% to 100%) and specificity 58% (95% CI 44% to 72%) for the other. For CA10 the accuracy results were sensitivity 100% (95% CI 94% to 100%) and specificity 70% (95% CI 56% to 82%). For CA15 the accuracy results were sensitivity 88% (95% CI 69% to 97%) and specificity 100% (95% CI 94% to 100%). No uninterpretable ROTEM study results were mentioned in any of the included studies. Risk of bias and concerns around applicability of findings was low across all studies for the patient and flow and timing domains. However, risk of bias and concerns around applicability of findings for the index test domain was either high or unclear, and the risk of bias for the reference standard domain was high. This raised concerns around the interpretation of the sensitivity and specificity results of the included studies, which may be misleading. Authors' conclusions We found no evidence on the accuracy of TEG and very little evidence on the accuracy of ROTEM. The value of accuracy estimates are considerably undermined by the small number of included studies, and concerns about risk of bias relating to the index test and the reference standard. We recognise that the reference standards of PT and INR are imperfect, but in the absence of embedded clinical consensus these are judged to be the best reflection of current clinical practice. We are unable to offer advice on the use of global measures of haemostatic function for trauma based on the evidence on test accuracy identified in this systematic review. This evidence strongly suggests that at present these tests should only be used for research. We consider more thoroughly what this research could be in the Discussion section. Plain language summary TEG and ROTEM for diagnosing trauma‑induced coagulopathy (disorder of the clotting system) in adult trauma patients with bleeding What is 'trauma‐induced coagulopathy'? Trauma‐induced coagulopathy (TIC) is a disorder of the blood clotting process that can occur soon after trauma injury that can lead to the patient bleeding to death. A diagnosis of TIC on admission to hospital is associated with increases in death rates, blood transfusions, risks of complications and length of stay in hospital. How is TIC diagnosed? Current testing for TIC normally involves coagulation tests on the patient's blood. What are thromboelastography (TEG) and rotational thromboelastometry (ROTEM)? Thromboelastography (TEG) and rotational thromboelastometry (ROTEM) are tests which involve a group of assessments that can be used to diagnose TIC. In some centres TEG and ROTEM are used routinely to test patients' blood, but in the UK their use is usually restricted to experimental and research settings. The purpose of this research The purpose of this research was to determine how good the TEG and ROTEM assessments are at diagnosing TIC in adult trauma patients who are bleeding. The accuracy of TEG and ROTEM was compared against another test that is currently used (the reference standard), which was the prothrombin time/international normalized ratio (PTr/INR). What we discovered We identified 3 studies (with 300, 90 and 40 participants; 430 in total) that compared the diagnostic test accuracy of TEG or ROTEM for identifying TIC in bleeding adult trauma patients within the emergency setting against PTr/INR. We recognise that the reference standards of PT and INR are imperfect, but in the absence of embedded clinical consensus these are judged to be the best reflection of current clinical practice. Readers should note that the assessment of test accuracy was not the single purpose of any of these 3 included studies. None of the 3 studies investigated the accuracy of the TEG assessment; they all investigated the ROTEM assessment. The 3 studies provided very little evidence on the accuracy of ROTEM, and provided results for only one potential indicator of TIC (clot amplitude (CA) at 5, 10 and 15 minutes (CA5, CA10 and CA15)), although other indicators could have been used. The overall reliability of the estimates of accuracy for CA was undermined by the low number of studies (2 for CA5 measurements and 1 each for CA10 and CA15 measurements), as well as concerns that the studies might be subject to bias concerning aspects of the ROTEM test and the PTr/INR test being used as the reference standard. There was not enough research available on the test accuracy of TEG or ROTEM for the researchers to determine whether these assessments provide a good test for diagnosing TIC in bleeding adult trauma patients. This evidence strongly suggests that at the moment these tests should only be used for research. The review emphasises that it is not enough to define the index test solely in terms of the device (TEG and ROTEM). Both ROTEM and TEG offer a number of measures: time to initiate clotting; time of clot formation; alpha angle; clot amplitude; maximum strength of clot; time to maximum clot strength; time to lysis of different degrees. These are illustrated in  Figure 7 . In addition, the protocol for initiating clotting also needs to be specified e.g. INTEM, EXTEM or FIBTEM in the case of ROTEM. Greater clarity is needed on which of these measures is most reliable and which is most relevant for particular clinical tasks; there may be more than one. Finally, different test evaluations may help in assessing these various aspects of the tests. Evaluations of predictive studies may shed light on the link between test result and patient outcome, and provide insight into the best treatment strategies for this condition and patient group. The authors of this review are currently conducting a review of such predictive studies, and this is registered on the International Prospective Register of Systematic Reviews (PROSPERO).","2","John Wiley & Sons, Ltd","1465-1858","Adult; Blood Coagulation Disorders [*diagnosis, etiology]; Cross‐Sectional Studies; Hemorrhage [*blood, etiology]; Humans; Thrombelastography [*methods, standards]; Wounds and Injuries [blood, *complications]","10.1002/14651858.CD010438.pub2","http://dx.doi.org/10.1002/14651858.CD010438.pub2","Injuries"
"CD009944.PUB2","Mocellin, S; Pasquali, S","Diagnostic accuracy of endoscopic ultrasonography (EUS) for the preoperative locoregional staging of primary gastric cancer","Cochrane Database of Systematic Reviews","2015","Abstract - Background Endoscopic ultrasound (EUS) is proposed as an accurate diagnostic device for the locoregional staging of gastric cancer, which is crucial to developing a correct therapeutic strategy and ultimately to providing patients with the best chance of cure. However, despite a number of studies addressing this issue, there is no consensus on the role of EUS in routine clinical practice. Objectives To provide both a comprehensive overview and a quantitative analysis of the published data regarding the ability of EUS to preoperatively define the locoregional disease spread (i.e., primary tumor depth (T‐stage) and regional lymph node status (N‐stage)) in people with primary gastric carcinoma.  Search methods We performed a systematic search to identify articles that examined the diagnostic accuracy of EUS (the index test) in the evaluation of primary gastric cancer depth of invasion (T‐stage, according to the AJCC/UICC TNM staging system categories T1, T2, T3 and T4) and regional lymph node status (N‐stage, disease‐free (N0) versus metastatic (N+)) using histopathology as the reference standard. To this end, we searched the following databases: the Cochrane Library  (the Cochrane Central Register of Controlled Trials (CENTRAL)), MEDLINE, EMBASE ,  NIHR Prospero Register, MEDION, Aggressive Research Intelligence Facility (ARIF), ClinicalTrials.gov, Current Controlled Trials MetaRegister, and World Health Organization International Clinical Trials Registry Platform (WHO ICTRP), from 1988 to January 2015. Selection criteria We included studies that met the following main inclusion criteria: 1) a minimum sample size of 10 patients with histologically‐proven primary carcinoma of the stomach (target condition); 2) comparison of EUS (index test) with pathology evaluation (reference standard) in terms of primary tumor (T‐stage) and regional lymph nodes (N‐stage). We excluded reports with possible overlap with the selected studies. Data collection and analysis For each study, two review authors extracted a standard set of data, using a dedicated data extraction form. We assessed data quality using a standard procedure according to the Quality Assessment of Diagnostic Accuracy Studies (QUADAS‐2) criteria. We performed diagnostic accuracy meta‐analysis using the hierarchical bivariate method. Main results We identified 66 articles (published between 1988 and 2012) that were eligible according to the inclusion criteria. We collected the data on 7747 patients with gastric cancer who were staged with EUS. Overall the quality of the included studies was good: in particular, only five studies presented a high risk of index test interpretation bias and two studies presented a high risk of selection bias. For primary tumor (T) stage, results were stratified according to the depth of invasion of the gastric wall. The meta‐analysis of 50 studies (n = 4397) showed that the summary sensitivity and specificity of EUS in discriminating T1 to T2 (superficial) versus T3 to T4 (advanced) gastric carcinomas were 0.86 (95% confidence interval (CI) 0.81 to 0.90) and 0.90 (95% CI 0.87 to 0.93) respectively. For the diagnostic capacity of EUS to distinguish T1 (early gastric cancer, EGC) versus T2 (muscle‐infiltrating) tumors, the meta‐analysis of 46 studies (n = 2742) showed that the summary sensitivity and specificity were 0.85 (95% CI 0.78 to 0.91) and 0.90 (95% CI 0.85 to 0.93) respectively. When we addressed the capacity of EUS to distinguish between T1a (mucosal) versus T1b (submucosal) cancers the meta‐analysis of 20 studies (n = 3321) showed that the summary sensitivity and specificity were 0.87 (95% CI 0.81 to 0.92) and 0.75 (95% CI 0.62 to 0.84) respectively. Finally, for the metastatic involvement of lymph nodes (N‐stage), the meta‐analysis of 44 studies (n = 3573) showed that the summary sensitivity and specificity were 0.83 (95% CI 0.79 to 0.87) and 0.67 (95% CI 0.61 to 0.72), respectively. Overall, as demonstrated also by the Bayesian nomograms, which enable readers to calculate post‐test probabilities for any target condition prevalence, the EUS accuracy can be considered clinically useful to guide physicians in the locoregional staging of people with gastric cancer. However, it should be noted that between‐study heterogeneity was not negligible: unfortunately, we could not identify any consistent source of the observed heterogeneity. Therefore, all accuracy measures reported in the present work and summarizing the available evidence should be interpreted cautiously. Moreover, we must emphasize that the analysis of positive and negative likelihood values revealed that EUS diagnostic performance cannot be considered optimal either for disease confirmation or for exclusion, especially for the ability of EUS to distinguish T1a (mucosal) versus T1b (submucosal) cancers and positive versus negative lymph node status. Authors' conclusions By analyzing the data from the largest series ever considered, we found that the diagnostic accuracy of EUS might be considered clinically useful to guide physicians in the locoregional staging of people with gastric carcinoma. However, the heterogeneity of the results warrants special caution, as well as further investigation for the identification of factors influencing the outcome of this diagnostic tool. Moreover, physicians should be warned that EUS performance is lower in diagnosing superficial tumors (T1a versus T1b) and lymph node status (positive versus negative). Overall, we observed large heterogeneity and its source needs to be understood before any definitive conclusion can be drawn about the use of EUS can be proposed in routine clinical settings. Plain language summary Ultrasound for determining the spread of stomach cancer Review question There is much debate on the diagnostic performance of endoscopic ultrasound (EUS) in the preoperative staging of gastric cancer. The aim of this review was to collect the available evidence and then to calculate how well EUS stages stomach cancer. Background EUS is a diagnostic test that can be used to determine how far (stage) cancer of the stomach reaches prior to surgery. It consists of an endoscope coupled with an ultrasound device capable of scanning the stomach wall, which shows the different layers of the stomach. Changes from the normal ultrasonographic patterns due to the tumor growth can be used to determine the extent of cancer in the stomach wall (T‐stage) and the lymph nodes related to the stomach (N‐stage). Since the correct staging of the tumor enables physicians to personalize cancer treatment, it is important to understand the reliability of staging devices. Study characteristics We conducted a meta‐analysis according to the most recent methods for diagnostic tests. The last literature search was performed in January 2015. We included 66 studies (of 7747 patients) in the review. Key results We found that EUS can distinguish between superficial (T1 ‐ T2) and advanced (T3 ‐ T4) primary tumors with a sensitivity and a specificity greater than 85%. This performance is maintained for the discrimination between T1 and T2 superficial tumors. However, EUS diagnostic accuracy is lower when it comes to distinguishing between the different types of early tumors (T1a versus T1b) and between tumors with versus those without lymph node disease. Quality of the evidence Overall, EUS provides physicians with some helpful information on the stage of gastric cancer. Nevertheless, in the light of the variability of the results reported in the international medical literature, its limitations in terms of performance must be kept in mind in order to make the most out of the diagnostic potential of this tool. Finally, more work is needed to assess whether some technical improvements and the combination with other staging instruments may increase our ability to correctly stage the disease and thus optimize patient treatment.","2","John Wiley & Sons, Ltd","1465-1858","Endosonography [*standards]; Humans; Lymphatic Metastasis; Neoplasm Staging [methods]; Preoperative Care; Randomized Controlled Trials as Topic; Stomach Neoplasms [*diagnostic imaging, pathology]","10.1002/14651858.CD009944.pub2","http://dx.doi.org/10.1002/14651858.CD009944.pub2","Gut"
"CD010633.PUB2","McCleery, J; Morgan, S; Bradley, KM; Noel‐Storr, AH; Ansorge, O; Hyde, C","Dopamine transporter imaging for the diagnosis of dementia with Lewy bodies","Cochrane Database of Systematic Reviews","2015","Abstract - Background Dementia with Lewy bodies (DLB) is a common cause of neurodegenerative dementia of old age. Its accurate recognition can be important in clinical management and is essential for the development of disease‐modifying treatments. The current clinical diagnostic criteria are limited particularly by relatively poor sensitivity. Dopamine transporter (DAT) imaging using single‐photon emission computed tomography (SPECT) is the most highly developed supplementary test for DLB, and is now incorporated as a suggestive feature in the consensus diagnostic criteria. However, there is uncertainty about its accuracy and its place in clinical practice. It is most commonly used in people who are already suspected of having DLB. Objectives We had two objectives in this review: (A) to estimate the accuracy of DAT imaging for the diagnosis of DLB in people with dementia in secondary care (specialist dementia services), and (B) to estimate the accuracy of DAT imaging for the diagnosis of DLB in people with dementia in secondary care who are already suspected of having DLB on the basis of a prior clinical work‐up. Search methods We searched MEDLINE (1946 to February 2013), Embase (1980 to February 2013), BIOSIS Previews (1926 to February 2013), PsycINFO (1806 to February 2013), CINAHL (1982 to February 2013), LILACS (February 2013) and Web of Science and Conference Proceedings (ISI Web of Science) (1945 to February 2013). Several of these sources contain conference abstracts. We also searched four specialised databases containing diagnostic reviews: Meta‐analyses van Diagnostisch Onderzoek (MEDION; February 2013), Database of Abstracts of Reviews of Effects (DARE; February 2013), Health Technology Assessment Database (HTA; February 2013), and Aggressive Research Intelligence Facility (ARIF; February 2013). We checked reference lists of relevant studies and reviews for potential additional studies. Terms for electronic database searching were devised in conjunction with the team at the Cochrane Dementia and Cognitive Improvement Group. Selection criteria Study design:  We included test accuracy studies with delayed verification, diagnostic case‐control studies, and two‐gate studies with alternative diagnosis controls.  Participants:  (A) participants with dementia in secondary care, (B) participants in secondary care meeting consensus clinical criteria (other than the DAT imaging criterion) for possible or probable DLB, or both.  Index test:  SPECT or positron emission tomography (PET) imaging of brain dopamine transporters.  Reference standard:  Neuropathological diagnosis at autopsy. Data collection and analysis Two review authors independently selected studies for inclusion and extracted data. We extracted results into a 2x2 table, showing the binary test results cross‐classified with the binary reference standard. We used this data to calculate sensitivities, specificities, and their 95% confidence intervals. We used the QUADAS‐2 tool plus some additional items to assess methodological quality. Main results We included one study that was applicable to our first objective (A). It reported data on 22 participants who met consensus clinical criteria for DLB or National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer’s Disease and Related Disorders Association (NINCDS‐ADRDA) criteria for Alzheimer's disease, or both (a two‐gate design with alternative diagnosis controls). The index test was SPECT scanning using the ligand  123 I‐FP‐CIT. We considered the study to be at high risk of bias in the participant selection and index test domains (QUADAS‐2).  123 I‐FP‐CIT SPECT analysed semiquantitatively had a sensitivity of 1.00 (95% confidence interval (CI) 0.66 to 1.00) and a specificity of 0.92 (95% CI 0.64 to 1.00) for the diagnosis of DLB (n = 22, 1 study). Analysed visually, the sensitivity was 0.86 (95% CI 0.42 to 1.00) and the specificity was 0.83 (95% CI 0.52 to 0.98) (n = 19, 1 study). We considered that the study also provided the best available data to address our second objective (B). At baseline, 15 participants were clinically suspected of having DLB. In this group,  123 I‐FP‐CIT SPECT scanning analysed semiquantitatively had a sensitivity of 1.00 (95% CI 0.63 to 1.00) and a specificity of 1.00 (95% CI 0.59 to 1.00) for the diagnosis of DLB (n = 15, 1 study). Analysed visually, accuracy in this group was lower with a sensitivity of 0.83 (95% CI 0.36 to 1.00) and a specificity of 0.71 (95% CI 0.29 to 0.96) (n = 13, 1 study). Authors' conclusions Only one study has used a neuropathological reference standard to assess the accuracy of DAT imaging for the diagnosis of DLB. The small size of the included study means that sensitivity and specificity estimates are imprecise. However, data from this study suggest that DAT imaging is more accurate than clinical diagnosis. Clinical diagnosis is therefore unsuitable to use as a reference standard for assessing the accuracy of DAT imaging. No studies using a neuropathological reference standard have directly addressed the common clinical scenario where the use of DAT imaging is considered as a diagnostic test in a person with possible DLB, or assessed the accuracy of DAT imaging in people with mild dementia. However, the data from the included study suggest that, where there is moderately severe dementia and a strong pre‐existing suspicion of DLB (probable DLB), then a normal (123)I‐FP‐CIT SPECT scan may be an accurate means of excluding the diagnosis. Semiquantitative ratings of  123 I‐FP‐CIT SPECT scans appeared to be more accurate than visual ratings in all analyses. Plain language summary Dopamine transporter imaging for the diagnosis of dementia with Lewy bodies Dementia with Lewy bodies (DLB) is one of the more common causes of dementia in old age. At present, it is diagnosed on the basis of the presence of characteristic symptoms, of which the most important are visual hallucinations, fluctuations in cognition, and Parkinsonism (movement symptoms like those seen in Parkinson's disease). However, many people who have DLB do not have all of these symptoms, and it can be hard to distinguish from other causes of dementia, especially Alzheimer's disease (AD). It is important to diagnose it accurately because people with DLB can have particularly severe side effects if given antipsychotic medication and, in the long term, so that treatments can be improved. Diagnosis of DLB using clinical symptoms alone has proven in some studies not to be very sensitive, that is cases of DLB are often missed. People with DLB have reduced levels of the dopamine transporter protein (DAT) in a part of the brain known as the corpus striatum. It is possible, using tracers that bind to the dopamine transporter, to identify this reduction on certain brain scans (PET or SPECT scans). This is known as DAT imaging. It has been suggested that DAT imaging may help in the accurate diagnosis of DLB. In this review, we aimed to assess how accurately DAT imaging can identify DLB among all people with dementia seen in specialist dementia services (secondary care). Because DAT scans are expensive, they are usually used in people in whom a doctor already suspects the presence of DLB, so we also aimed to see how accurately DAT imaging identified DLB among these people. We only looked for studies in which the diagnosis was confirmed by examining the person's brain after death. We found only one study of 22 participants to include in the review. The participants had been selected for the study because they had been diagnosed clinically with DLB or AD. This method of recruiting participants tends to exaggerate the accuracy of a test. The participants in the study had moderately severe dementia when they had the DAT imaging. Overall, the DAT imaging correctly classified 100% of the participants who had DLB and 92% of the participants who did not have DLB at postmortem. When we looked at only the 15 participants whom the doctors thought had DLB at the start of the study, then the DAT imaging correctly classified 100% of the participants who had DLB and 100% of the participants who did not have DLB at postmortem. The small size of the included study meant that we could not be very confident in these estimates of accuracy, and it is still possible that the test is considerably less accurate than this would suggest. We can conclude that DAT imaging is a promising test for diagnosing DLB, but it is important to note that we did not find any studies of participants in whom diagnosis may be more difficult, such as those with only mild dementia or those who have only one symptom to raise a suspicion of DLB.","1","John Wiley & Sons, Ltd","1465-1858","*Dopamine Plasma Membrane Transport Proteins; *Iodine Radioisotopes; *Tomography, Emission‐Computed, Single‐Photon; *Tropanes; Alzheimer Disease [*diagnostic imaging]; Brain [*diagnostic imaging, pathology]; Diagnosis, Differential; Humans; Lewy Body Disease [*diagnostic imaging, pathology]; Sensitivity and Specificity","10.1002/14651858.CD010633.pub2","http://dx.doi.org/10.1002/14651858.CD010633.pub2","Dementia and Cognitive Improvement"
"CD010632.PUB2","Smailagic, N; Vacante, M; Hyde, C; Martin, S; Ukoumunne, O; Sachpekidis, C","18F‐FDG PET for the early diagnosis of Alzheimer’s disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2015","Abstract - Background ¹⁸F‐FDFG uptake by brain tissue as measured by positron emission tomography (PET) is a well‐established method for assessment of brain function in people with dementia. Certain findings on brain PET scans can potentially predict the decline of mild cognitive Impairment (MCI) to Alzheimer’s disease dementia or other dementias. Objectives To determine the diagnostic accuracy of the ¹⁸F‐FDG PET index test for detecting people with MCI at baseline who would clinically convert to Alzheimer’s disease dementia or other forms of dementia at follow‐up. Search methods We searched the Cochrane Register of Diagnostic Test Accuracy Studies, MEDLINE, EMBASE, Science Citation Index, PsycINFO, BIOSIS previews, LILACS, MEDION, (Meta‐analyses van Diagnostisch Onderzoek), DARE (Database of Abstracts of Reviews of Effects), HTA (Health Technology Assessment Database), ARIF (Aggressive Research Intelligence Facility) and C‐EBLM (International Federation of Clinical Chemistry and Laboratory Medicine Committee for Evidence‐based Laboratory Medicine) databases to January 2013. We checked the reference lists of any relevant studies and systematic reviews for additional studies. Selection criteria We included studies that evaluated the diagnostic accuracy of ¹⁸F‐FDG PET to determine the conversion from MCI to Alzheimer’s disease dementia or to other forms of dementia, i.e. any or all of vascular dementia, dementia with Lewy bodies, and fronto‐temporal dementia. These studies necessarily employ delayed verification of conversion to dementia and are sometimes labelled as ‘delayed verification cross‐sectional studies’. Data collection and analysis Two blinded review authors independently extracted data, resolving disagreement by discussion, with the option to involve a third review author as arbiter if necessary. We extracted and summarised graphically the data for two‐by‐two tables. We conducted exploratory analyses by plotting estimates of sensitivity and specificity from each study on forest plots and in receiver operating characteristic (ROC) space. When studies had mixed thresholds, we derived estimates of sensitivity and likelihood ratios at fixed values (lower quartile, median and upper quartile) of specificity from the hierarchical summary ROC (HSROC) models. Main results We included 14 studies (421 participants) in the analysis. The sensitivities for conversion from MCI to Alzheimer's disease dementia were between 25% and 100% while the specificities were between 15% and 100%. From the summary ROC curve we fitted we estimated that the sensitivity was 76% (95% confidence interval (CI): 53.8 to 89.7) at the included study median specificity of 82%. This equates to a positive likelihood ratio of 4.03 (95% CI: 2.97 to 5.47), and a negative likelihood ratio of 0.34 (95% CI: 0.15 to 0.75). Three studies recruited participants from the same Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort but only the largest ADNI study ( Herholz 2011 ) is included in the meta‐analysis. In order to demonstrate whether the choice of ADNI study or discriminating brain region ( Chételat 2003 ) or reader assessment ( Pardo 2010 ) make a difference to the pooled estimate, we performed five additional analyses. At the median specificity of 82%, the estimated sensitivity was between 74% and 76%. There was no impact on our findings. In addition to evaluating Alzheimer's disease dementia, five studies evaluated the accuracy of ¹⁸F‐FDG PET for all types of dementia. The sensitivities were between 46% and 95% while the specificities were between 29% and 100%; however, we did not conduct a meta‐analysis because of too few studies, and those studies which we had found recruited small numbers of participants. Our findings are based on studies with poor reporting, and the majority of included studies had an unclear risk of bias, mainly for the reference standard and participant selection domains. According to the assessment of Index test domain, more than 50% of studies were of poor methodological quality. Authors' conclusions It is difficult to determine to what extent the findings from the meta‐analysis can be applied to clinical practice. Given the considerable variability of specificity values and lack of defined thresholds for determination of test positivity in the included studies, the current evidence does not support the routine use of ¹⁸F‐FDG PET scans in clinical practice in people with MCI. The ¹⁸F‐FDG PET scan is a high‐cost investigation, and it is therefore important to clearly demonstrate its accuracy and to standardise the process of ¹⁸F‐FDG PET diagnostic modality prior to its being widely used. Future studies with more uniform approaches to thresholds, analysis and study conduct may provide a more homogeneous estimate than the one available from the included studies we have identified. Plain language summary ¹⁸F‐FDG PET scan for early prediction of developing Alzheimer’s disease or other dementia in people with mild cognitive impairment (MCI) Background The numbers of people with dementia and other cognitive problems are increasing globally. A diagnosis of dementia at early stage is recommended but there is no agreement on the best approach. A range of tests have been developed which healthcare professionals can use to assess people with poor memory or cognitive impairment. In this review we have focused on the ¹⁸F‐FDG PET test. Aim We aimed to see how accurately the ¹⁸F‐FDG PET scan identified those people with MCI who would clinically convert to Alzheimer’s disease dementia or other types of dementia over a period of time. Study characteristics The evidence is current to January 2013. We included 16 studies covering 697 participants with MCI. The studies have been published over a 14‐year period (1999 to 2013). Study sizes were small and ranged from 19 to 94 participants. Five papers have a mean age of less than 70 years. The age range in the youngest sample was 55 to 73 years and in the oldest sample was 71 to 86 years. Participants were mainly recruited from university departments, clinics or research centres. The percentage of participants with positive ¹⁸F‐FDG PET scans at baseline ranged in the included studies from 10.5% to 74% and the percentage of those participants who converted to Alzheimer’s disease dementia over a period of time ranged from 22% to 50%. Included studies reported a range of different cut‐off values used for identifying their participants with positive ¹⁸F‐FDG PET scans. Quality of the evidence Our findings are based on studies with poor reporting. The majority of included studies had an unclear risk of bias, mainly because they did not describe in sufficient details how participants were selected and how the clinical diagnosis of Alzheimer’s disease dementia was justified. According to the assessment of the ¹⁸F‐FDG PET test domain, more than 50% of studies were of poor methodological quality. The main limitations of the review are poor reporting in the included studies, a lack of a widely‐accepted cut‐off value of the ¹⁸F‐FDG PET scan in people with MCI, and the marked variation in test accuracy between the included studies. Key findings In this review, we have found that the ¹⁸F‐FDG PET scan, as a single test, lacks the accuracy to identify those people with MCI who would develop Alzheimer’s disease dementia or other forms of dementia over a period of time. Assuming a typical conversion rate of MCI to Alzheimer’s disease dementia of 38%, the findings indicate that for every 1000 ¹⁸F‐FDG PET scans, 174 cases with a negative scan will progress to Alzheimer's disease dementia and 285 with a positive scan will not. Therefore, a positive ¹⁸F‐FDG PET scan in people with MCI is of no clinical value in early prediction of developing Alzheimer's disease dementia.","1","John Wiley & Sons, Ltd","1465-1858","*Fluorodeoxyglucose F18; *Radiopharmaceuticals; Aged; Alzheimer Disease [*diagnostic imaging]; Brain [diagnostic imaging]; Cognitive Dysfunction [complications, *diagnostic imaging]; Dementia [*diagnostic imaging]; Disease Progression; Early Diagnosis; Humans; Middle Aged; Positron‐Emission Tomography; Sensitivity and Specificity","10.1002/14651858.CD010632.pub2","http://dx.doi.org/10.1002/14651858.CD010632.pub2","Dementia and Cognitive Improvement"
"CD010653.PUB2","Soares‐Weiser, K; Maayan, N; Bergman, H; Davenport, C; Kirkham, AJ; Grabowski, S; Adams, CE","First rank symptoms for schizophrenia","Cochrane Database of Systematic Reviews","2015","Abstract - Background Early and accurate diagnosis and treatment of schizophrenia may have long‐term advantages for the patient; the longer psychosis goes untreated the more severe the repercussions for relapse and recovery. If the correct diagnosis is not schizophrenia, but another psychotic disorder with some symptoms similar to schizophrenia, appropriate treatment might be delayed, with possible severe repercussions for the person involved and their family. There is widespread uncertainty about the diagnostic accuracy of First Rank Symptoms (FRS); we examined whether they are a useful diagnostic tool to differentiate schizophrenia from other psychotic disorders. Objectives To determine the diagnostic accuracy of one or multiple FRS for diagnosing schizophrenia, verified by clinical history and examination by a qualified professional (e.g. psychiatrists, nurses, social workers), with or without the use of operational criteria and checklists, in people thought to have non‐organic psychotic symptoms. Search methods We conducted searches in MEDLINE, EMBASE, and PsycInfo using  OvidSP  in April, June, July 2011 and December 2012. We also searched MEDION in December 2013. Selection criteria We selected studies that consecutively enrolled or randomly selected adults and adolescents with symptoms of psychosis, and assessed the diagnostic accuracy of FRS for schizophrenia compared to history and clinical examination performed by a qualified professional, which may or may not involve the use of symptom checklists or based on operational criteria such as ICD and DSM. Data collection and analysis Two review authors independently screened all references for inclusion. Risk of bias in included studies were assessed using the QUADAS‐2 instrument. We recorded the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for constructing a 2 x 2 table for each study or derived 2 x 2 data from reported summary statistics such as sensitivity, specificity, and/or likelihood ratios. Main results We included 21 studies with a total of 6253 participants (5515 were included in the analysis). Studies were conducted from 1974 to 2011, with 80% of the studies conducted in the 1970's, 1980's or 1990's. Most studies did not report study methods sufficiently and many had high applicability concerns. In 20 studies, FRS differentiated schizophrenia from all other diagnoses with a sensitivity of 57% (50.4% to 63.3%), and a specificity of 81.4% (74% to 87.1%) In seven studies, FRS differentiated schizophrenia from non‐psychotic mental health disorders with a sensitivity of 61.8% (51.7% to 71%) and a specificity of 94.1% (88% to 97.2%). In sixteen studies, FRS differentiated schizophrenia from other types of psychosis with a sensitivity of 58% (50.3% to 65.3%) and a specificity of 74.7% (65.2% to 82.3%). Authors' conclusions The synthesis of old studies of limited quality in this review indicates that FRS correctly identifies people with schizophrenia 75% to 95% of the time. The use of FRS to diagnose schizophrenia in triage will incorrectly diagnose around five to 19 people in every 100 who have FRS as having schizophrenia and specialists will not agree with this diagnosis. These people will still merit specialist assessment and help due to the severity of disturbance in their behaviour and mental state. Again, with a sensitivity of FRS of 60%, reliance on FRS to diagnose schizophrenia in triage will not correctly diagnose around 40% of people that specialists will consider to have schizophrenia. Some of these people may experience a delay in getting appropriate treatment. Others, whom specialists will consider to have schizophrenia, could be prematurely discharged from care, if triage relies on the presence of FRS to diagnose schizophrenia. Empathetic, considerate use of FRS as a diagnostic aid ‐ with known limitations ‐ should avoid a good proportion of these errors. We hope that newer tests ‐ to be included in future Cochrane reviews ‐ will show better results. However, symptoms of first rank can still be helpful where newer tests are not available ‐ a situation which applies to the initial screening of most people with suspected schizophrenia. FRS remain a simple, quick and useful clinical indicator for an illness of enormous clinical variability. Plain language summary First rank symptoms for schizophrenia It is important for patients with psychosis to be correctly diagnosed as soon as possible. The earlier schizophrenia is diagnosed the better the treatment outcome. However, other diseases sometimes have similar psychotic symptoms as schizophrenia, for example bipolar disorder. This review looks at how accurate First Rank Symptoms (FRS) are at diagnosing schizophrenia. FRS are symptoms that people with psychosis may experience, for example hallucinations, hearing voices and thinking that other people can hear their thoughts. We found 21 studies, with 6253 participants, that looked at how good FRS are at diagnosing schizophrenia when compared to a diagnosis made by a psychiatrist. These studies showed that for people who actually have schizophrenia, FRS would only correctly diagnose just over half of them as schizophrenic. For people who do not have schizophrenia, almost 20% would be incorrectly diagnosed with schizophrenia. Therefore, if a person is experiencing a FRS, schizophrenia is a possible diagnosis, but there is also a chance that it is another mental health disorder. We do not recommend that FRS alone can be used to diagnose schizophrenia. However, FRS could be useful to triage patients who need to be assessed by a psychiatrist.","1","John Wiley & Sons, Ltd","1465-1858","Adolescent; Adult; Diagnosis, Differential; Early Diagnosis; Humans; Mental Disorders [diagnosis]; Prospective Studies; Psychotic Disorders [*diagnosis]; Retrospective Studies; Schizophrenia [complications, *diagnosis]; Sensitivity and Specificity; Symptom Assessment [*methods]","10.1002/14651858.CD010653.pub2","http://dx.doi.org/10.1002/14651858.CD010653.pub2","Schizophrenia"
"CD010542.PUB2","Pavlov, CS; Casazza, G; Nikolova, D; Tsochatzis, E; Burroughs, AK; Ivashkin, VT; Gluud, C","Transient elastography for diagnosis of stages of hepatic fibrosis and cirrhosis in people with alcoholic liver disease","Cochrane Database of Systematic Reviews","2015","Abstract - Background The presence and progression of hepatic (liver) fibrosis into cirrhosis is a prognostic variable having impact on survival in people with alcoholic liver disease. Liver biopsy, although an invasive method, is the recommended 'reference standard' for diagnosis and staging of hepatic fibrosis in people with liver diseases. Transient elastography is a non‐invasive method for assessing and staging hepatic fibrosis. Objectives To determine the diagnostic accuracy of transient elastography for diagnosis and staging hepatic fibrosis in people with alcoholic liver disease when compared with liver biopsy. To identify the optimal cut‐off values for differentiating the five stages of hepatic fibrosis. Search methods The Cochrane Hepato‐Biliary Group Controlled and Diagnostic Test Accuracy Studies Registers,  The Cochrane Library , MEDLINE (OvidSP), EMBASE (OvidSP), and the Science Citation Index Expanded (last search August 2014). Selection criteria Diagnostic cohort and diagnostic case‐control study designs that assessed hepatic fibrosis in participants with alcoholic liver disease with transient elastography and liver biopsy, irrespective of language or publication status. The study participants could be of any sex and ethnic origin, above 16 years old, hospitalised or managed as outpatients. We excluded participants with viral hepatitis, autoimmunity, metabolic diseases, and toxins. Data collection and analysis We followed the guidelines in the draft  Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy. Main results Five retrospective and nine prospective cohort studies with 834 participants provided data for the review analyses. Authors of seven of those studies sent us individual participant data. The risk of bias in the included studies was high in all but three studies. We could identify no serious concerns regarding the applicability of the studies in answering the main study question of our review, namely to use transient elastography to diagnose hepatic fibrosis. We could not identify the optimal cut‐off values for the fibrosis stages. The definition of the diagnosis of alcoholic liver disease was not provided in one study and was not clearly defined in two studies, but it was clear in the remaining 11 studies. The study authors used different liver stiffness cut‐off values of transient elastography for the hepatic fibrosis stages. There was only one study (103 participants) with data on hepatic fibrosis stage F1 or worse, with a cut‐off of 5.9 kPa, and reporting sensitivity of 0.83 (95% confidence interval (CI) 0.74 to 0.90) and specificity of 0.88 (95% CI 0.47 to 1.00). The summary sensitivity and specificity of transient elastography for F2 or worse (seven studies with 338 participants and with cut‐offs around 7.5 kPa (range 7.00 to 7.8 kPa)) were 0.94 and 0.89 with LR+ 8.2 and LR‐ 0.07, which suggests that transient elastography could be useful to rule out the presence of significant hepatic fibrosis, thus avoiding liver biopsy. Due to the wide range of cut‐off values (from 8.0 to 17.0 kPa) found in the 10 studies with 760 participants with hepatic fibrosis F3 or worse, we fitted a hierarchical summary receiver operating characteristic (HSROC) model and estimated a summary ROC (SROC) curve. The sensitivity of the 10 studies varied from 72% to 100% and the specificity from 59% to 89%. We performed an additional analysis by including the studies with a cut‐off value of around and equal to 9.5 kPa (range 8.0 to 11.0 kPa). The summary sensitivity and specificity of transient elastography (eight studies with 564 participants) were 0.92 and 0.70 with LR+ 3.1 and LR‐ 0.11, which suggests that transient elastography could also be useful to rule out the presence of severe hepatic fibrosis (F3 or worse), avoiding liver biopsy. We carried out a sensitivity analysis by considering only the studies with a cut‐off value equal to 9.5 kPa and the result did not differ. We performed an HSROC analysis and reported an SROC curve for hepatic fibrosis stage F4 (cirrhosis). The HSROC analysis suggested that when the cut‐off value changes, there is a wide variation in specificity and a more limited variation in sensitivity. We performed an additional analysis with the studies with the most commonly used cut‐off value of 12.5 kPa. The summary sensitivity and specificity of transient elastography (seven studies with 330 participants) were 0.95 and 0.71 with LR+ 3.3 and LR‐ 0.07, which again suggests that transient elastography could be useful to rule out the presence of cirrhosis, avoiding liver biopsy. Authors' conclusions We identified a small number of studies with a few participants and were unable to include several studies, which raises the risk of outcome reporting bias. With these caveats in mind, transient elastography may be used as a diagnostic method to rule out liver cirrhosis (F4) in people with alcoholic liver disease when the pre‐test probability is about 51% (range 15% to 79%). Transient elastography may also help in ruling out severe fibrosis (F3 or worse). Liver biopsy investigation remains an option if the certainty to rule in or rule out the stage of hepatic fibrosis or cirrhosis remains insufficient after a clinical follow‐up or any other non‐invasive test considered useful by the clinician. The proposed cut‐off values for the different stages of hepatic fibrosis may be used in clinical practice, but caution is needed, as those values reported in this review are only the most common cut‐off values used by the study authors. The best cut‐off values for hepatic fibrosis in people with alcoholic liver disease could not be established yet. In order to diagnose correctly the stage of hepatic fibrosis in people with alcoholic liver disease using transient elastography assessment, the studies should consider a single aetiology. Hepatic fibrosis should be diagnosed with both transient elastography and liver biopsy and in this sequence, and transient elastography cut‐off values should be pre‐specified and validated. The time interval between the two investigations should not exceed three months, which is the interval mainly valid for people without cirrhosis, and assessment of results should be properly blinded. Only studies with low risk of bias, fulfilling the Standards for Reporting of Diagnostic Accuracy may answer the review question. Plain language summary Transient elastography for measurement of liver fibrosis and cirrhosis in people with alcoholic liver disease Background Liver fibrosis is a change in the microscopic structure of the liver because of liver inflammation. After many years of excessive alcohol consumption, liver fibrosis progresses to cirrhosis. Abstaining from alcohol may stop the fibrosis from further progression into significant or severe fibrosis and cirrhosis. The latter lead to complications of underlying diseases, including cancer. Measurement of the amount of fibrosis is called staging. There are five stages (F0: no scarring (no fibrosis); F1: minimal scarring; F2: scarring has occurred and extends outside the liver area (significant fibrosis); F3: fibrosis spreading and forming bridges with other fibrotic liver areas (severe fibrosis); F4: cirrhosis or advanced scarring). Cut‐off values may distinguish between the different stages of fibrosis, but in people with alcoholic liver disease, the best cut‐off values have not been determined yet. Rationale Liver biopsy is where a sample of tissue is taken from the liver using a small needle. It is the standard method of detecting and measuring fibrosis. Transient elastography measures stiffening of the liver caused by progressive scarring, but it has not been validated in people with alcoholic liver disease. Aims To find out how well transient elastography may determine the presence or absence of fibrosis and if it can stage fibrosis in people with alcoholic liver disease when compared with liver biopsy. Methods Using Cochrane methods and searching the literature (August 2014), the review authors obtained results from 14 studies (834 participants), out of which only seven included people with only alcoholic liver disease. Participants underwent both transient elastography (the index test) and liver biopsy (the standard test). Findings and conclusions The number of studies and participants was small and the participants had different severity of liver fibrosis. Only four studies were judged good quality. Transient elastography fibrosis stage F2 or worse (significant fibrosis) There were seven studies with 338 participants: 81% of people had significant fibrosis. Out of 1000 people, 810 will have significant fibrosis. Of these 810 people, 49 people would be missed even though they had significant fibrosis. A clinical follow‐up could provide physicians with knowledge for the next diagnostic step. The remaining 190 people would not have significant fibrosis; 21 people would have unnecessary worries about their liver fibrosis stage. Transient elastography fibrosis stage F3 or worse (severe fibrosis) There were eight studies with 564 participants: 61% of people had severe fibrosis. Out of 1000 people, 610 would have severe fibrosis. Of these 610 people, 49 people would be missed even though they had severe fibrosis. A clinical follow‐up could provide physicians with knowledge for the next diagnostic step. The remaining 390 people would not have severe fibrosis; 117 people would have unnecessary worries about their liver fibrosis stage. Transient elastography fibrosis stage F4 (cirrhosis) There were seven studies with 330 participants: 51% of people had cirrhosis. Out of 1000 people, 510 will have cirrhosis. Of these 510 people, 26 people would be missed even though they had cirrhosis. A clinical follow‐up could provide physicians with knowledge for the next diagnostic step. The remaining 490 people would not have cirrhosis; 143 people would have unnecessary worries about their liver fibrosis stage. Transient elastography may be used as a diagnostic tool to rule out liver cirrhosis and may also help in ruling out severe fibrosis in people with alcoholic liver disease. Liver biopsy investigation still remains an option if the certainty to rule in or rule out the stage of hepatic fibrosis or cirrhosis remains insufficient after a clinical follow‐up or any other non‐invasive test considered useful by the clinician. The best cut‐off values for differentiating between the five liver fibrosis stages could still not be established. Future studies should include only people with alcoholic liver disease. Hepatic fibrosis should be diagnosed with transient elastography followed by liver biopsy and the transient elastography cut‐off values of liver stiffness for the different stages of liver fibrosis should be decided before the test occurs. The time interval between the two tests should not exceed three months, an interval that is mainly valid for people without cirrhosis. Assessors of results should be unaware of the treatment given.","1","John Wiley & Sons, Ltd","1465-1858","Disease Progression; Elasticity Imaging Techniques [*methods]; Humans; Liver Cirrhosis [*diagnostic imaging, pathology]; Liver Diseases, Alcoholic [*complications]; Prospective Studies; Retrospective Studies; Sensitivity and Specificity","10.1002/14651858.CD010542.pub2","http://dx.doi.org/10.1002/14651858.CD010542.pub2","Hepato-Biliary"
"CD008081.PUB3","Virgili, G; Menchini, F; Casazza, G; Hogg, R; Das, RR; Wang, X; Michelessi, M","Optical coherence tomography (OCT) for detection of macular oedema in patients with diabetic retinopathy","Cochrane Database of Systematic Reviews","2015","Abstract - Background Diabetic macular oedema (DMO) is a thickening of the central retina, or the macula, and is associated with long‐term visual loss in people with diabetic retinopathy (DR). Clinically significant macular oedema (CSMO) is the most severe form of DMO. Almost 30 years ago, the Early Treatment Diabetic Retinopathy Study (ETDRS) found that CSMO, diagnosed by means of stereoscopic fundus photography, leads to moderate visual loss in one of four people within three years. It also showed that grid or focal laser photocoagulation to the macula halves this risk. Recently, intravitreal injection of antiangiogenic drugs has also been used to try to improve vision in people with macular oedema due to DR. Optical coherence tomography (OCT) is based on optical reflectivity and is able to image retinal thickness and structure producing cross‐sectional and three‐dimensional images of the central retina. It is widely used because it provides objective and quantitative assessment of macular oedema, unlike the subjectivity of fundus biomicroscopic assessment which is routinely used by ophthalmologists instead of photography. Optical coherence tomography is also used for quantitative follow‐up of the effects of treatment of CSMO. Objectives To determine the diagnostic accuracy of OCT for detecting DMO and CSMO, defined according to ETDRS in 1985, in patients referred to ophthalmologists after DR is detected. In the update of this review we also aimed to assess whether OCT might be considered the new reference standard for detecting DMO. Search methods We searched the Cochrane Database of Systematic Reviews (CDSR), the Database of Abstracts of Reviews of Effects (DARE), the Health Technology Assessment Database (HTA) and the NHS Economic Evaluation Database (NHSEED) ( The Cochrane Library  2013, Issue 5) ,  Ovid MEDLINE, Ovid MEDLINE In‐Process and Other Non‐Indexed Citations, Ovid MEDLINE Daily, Ovid OLDMEDLINE (January 1946 to June 2013), EMBASE (January 1950 to June 2013), Web of Science Conference Proceedings Citation Index ‐ Science (CPCI‐S) (January 1990 to June 2013), BIOSIS Previews (January 1969 to June 2013), MEDION and the Aggressive Research Intelligence Facility database (ARIF). We did not use any date or language restrictions in the electronic searches for trials. We last searched the electronic databases on 25 June 2013. We checked bibliographies of relevant studies for additional references. Selection criteria We selected studies that assessed the diagnostic accuracy of any OCT model for detecting DMO or CSMO in patients with DR who were referred to eye clinics. Diabetic macular oedema and CSMO were diagnosed by means of fundus biomicroscopy by ophthalmologists or stereophotography by ophthalmologists or other trained personnel. Data collection and analysis Three authors independently extracted data on study characteristics and measures of accuracy. We assessed data using random‐effects hierarchical sROC meta‐analysis models. Main results We included 10 studies (830 participants, 1387 eyes), published between 1998 and 2012. Prevalence of CSMO was 19% to 65% (median 50%) in nine studies with CSMO as the target condition. Study quality was often unclear or at high risk of bias for QUADAS 2 items, specifically regarding study population selection and the exclusion of participants with poor quality images. Applicablity was unclear in all studies since professionals referring patients and results of prior testing were not reported. There was a specific 'unit of analysis' issue because both eyes of the majority of participants were included in the analyses as if they were independent. In nine studies providing data on CSMO (759 participants, 1303 eyes), pooled sensitivity was 0.78 (95% confidence interval (CI) 0.72 to 0.83) and specificity was 0.86 (95% CI 0.76 to 0.93). The median central retinal thickness cut‐off we selected for data extraction was 250 µm (range 230 µm to 300 µm). Central CSMO was the target condition in all but two studies and thus our results cannot be applied to non‐central CSMO. Data from three studies reporting accuracy for detection of DMO (180 participants, 343 eyes) were not pooled. Sensitivities and specificities were about 0.80 in two studies and were both 1.00 in the third study. Since this review was conceived, the role of OCT has changed and has become a key ingredient of decision‐making at all levels of ophthalmic care in this field. Moreover, disagreements between OCT and fundus examination are informative, especially false positives which are referred to as subclinical DMO and are at higher risk of developing clinical CSMO. Authors' conclusions Using retinal thickness thresholds lower than 300 µm and ophthalmologist's fundus assessment as reference standard, central retinal thickness measured with OCT was not sufficiently accurate to diagnose the central type of CSMO in patients with DR referred to retina clinics. However, at least OCT false positives are generally cases of subclinical DMO that cannot be detected clinically but still suffer from increased risk of disease progression. Therefore, the increasing availability of OCT devices, together with their precision and the ability to inform on retinal layer structure, now make OCT widely recognised as the new reference standard for assessment of DMO, even in some screening settings. Thus, this review will not be updated further. Plain language summary Optical coherence tomography measurement of central retinal thickness to diagnose diabetic macular oedema Background  Diabetic macular oedema (DMO) is a thickening of the central part of the retina, the macula, that may affect people with diabetic retinopathy (DR). Diabetic retinopathy is a complication of diabetes in which the retina (a layer of tissue at the back of the eye) becomes progressively damaged. Diabetic macular oedema is detected by means of visual examination by an ophthalmologist. The most severe form of DMO ‐ clinically significant macular oedema (CSMO) ‐ is associated with sight loss in the long‐term. This condition is treatable. Laser photocoagulation (where a laser is used to burn off blood vessels) has been used for many years to reduce the risk of visual loss. More recently, antiangiogenic therapy (which prevents fluid leakage from retinal vessels) has been approved to try to improve vision. Review question  Optical coherence tomography (OCT) is based on how light is reflected. It can be used to measure retinal thickness. We originally aimed to assess the accuracy of OCT for diagnosing diabetic macular oedema (DMO), as well as to investigate differences in diagnostic performance. However, the role of OCT is expanding so in the update of this review we also aimed to assess whether OCT might be considered the new standard for diagnosing DMO. Search date  This review is updated as of June 2013. Study characteristics  Our review included 10 studies (830 participants, 1387 eyes) published between 1998 and 2012. Nine of these studies investigated the ability of OCT to diagnose CSMO. Study funding sources  There were no overt declarations of potential conflicts of interest in terms of the manufacturer of the OCT device being involved in funding the research. Key results  We found that OCT retinal thickness measurement is not sufficiently accurate to detect CSMO, involving the centre of the macula, using clinical fundus examination as the reference standard. Of 10 patients with diabetic retinopathy, 5 of whom have CSMO, 1 of 5 with no CSMO would be wrongly diagnosed as having CSMO, and about 1 of 5 with CSMO would be missed. However, researchers have found that disagreements between OCT and clinical examination occur because OCT can detect early, subclinical retinal thickening in people without CSMO and more advanced retinopathy. They suggested that such cases of subclinical macular oedema are followed more closely, since they are at increased risk of progression to CSMO. Furthermore, OCT is an essential tool to manage antiangiogenic therapy in patients with DMO and is believed by many to be a new reference standard for its diagnosis. Quality of the evidence  Study quality was often unclear because of incomplete reporting or because it was at risk of bias. Specifically, this concerned how patients were selected in the study, who referred them and how, and exclusion of those for whom poor quality images were obtained. Furthermore, many studies included both patient's eyes, which is a problem in data analyses.","1","John Wiley & Sons, Ltd","1465-1858","Diabetic Retinopathy [*complications]; Diagnostic Errors; Humans; Macular Edema [*diagnosis, etiology, pathology]; Randomized Controlled Trials as Topic; Retina [pathology]; Selection Bias; Sensitivity and Specificity; Tomography, Optical Coherence [*methods]","10.1002/14651858.CD008081.pub3","http://dx.doi.org/10.1002/14651858.CD008081.pub3","Eyes and Vision"
"CD011431","Abba, K; Kirkham, AJ; Olliaro, PL; Deeks, JJ; Donegan, S; Garner, P; Takwoingi, Y","Rapid diagnostic tests for diagnosing uncomplicated non‐falciparum or   Plasmodium vivax malaria in endemic countries","Cochrane Database of Systematic Reviews","2014","Abstract - Background In settings where both  Plasmodium vivax  and  Plasmodium falciparum  infection cause malaria, rapid diagnostic tests (RDTs) need to distinguish which species is causing the patients' symptoms, as different treatments are required. Older RDTs incorporated two test lines to distinguish malaria due to  P. falciparum,  from malaria due to any other  Plasmodium  species (non‐falciparum). These RDTs can be classified according to which antibodies they use: Type 2 RDTs use HRP‐2 (for  P. falciparum ) and aldolase (all species); Type 3 RDTs use HRP‐2 (for  P. falciparum ) and pLDH (all species); Type 4 use pLDH (from P. falciparum ) and pLDH (all species). More recently, RDTs have been developed to distinguish  P. vivax  parasitaemia by utilizing a pLDH antibody specific to  P. vivax . Objectives To assess the diagnostic accuracy of RDTs for detecting non‐falciparum or  P. vivax  parasitaemia in people living in malaria‐endemic areas who present to ambulatory healthcare facilities with symptoms suggestive of malaria, and to identify which types and brands of commercial test best detect non‐falciparum and  P. vivax  malaria. Search methods We undertook a comprehensive search of the following databases up to 31 December 2013: Cochrane Infectious Diseases Group Specialized Register; MEDLINE; EMBASE; MEDION; Science Citation Index; Web of Knowledge; African Index Medicus; LILACS; and IndMED. Selection criteria Studies comparing RDTs with a reference standard (microscopy or polymerase chain reaction) in blood samples from a random or consecutive series of patients attending ambulatory health facilities with symptoms suggestive of malaria in non‐falciparum endemic areas. Data collection and analysis For each study, two review authors independently extracted a standard set of data using a tailored data extraction form. We grouped comparisons by type of RDT (defined by the combinations of antibodies used), and combined in meta‐analysis where appropriate. Average sensitivities and specificities are presented alongside 95% confidence intervals (95% CI). Main results We included 47 studies enrolling 22,862 participants. Patient characteristics, sampling methods and reference standard methods were poorly reported in most studies. RDTs detecting 'non‐falciparum' parasitaemia Eleven studies evaluated Type 2 tests compared with microscopy, 25 evaluated Type 3 tests, and 11 evaluated Type 4 tests. In meta‐analyses, average sensitivities and specificities were 78% (95% CI 73% to 82%) and 99% (95% CI 97% to 99%) for Type 2 tests, 78% (95% CI 69% to 84%) and 99% (95% CI 98% to 99%) for Type 3 tests, and 89% (95% CI 79% to 95%) and 98% (95% CI 97% to 99%) for Type 4 tests, respectively. Type 4 tests were more sensitive than both Type 2 (P = 0.01) and Type 3 tests (P = 0.03). Five studies compared Type 3 tests with PCR; in meta‐analysis, the average sensitivity and specificity were 81% (95% CI 72% to 88%) and 99% (95% CI 97% to 99%) respectively. RDTs detecting P.vivax parasitaemia Eight studies compared pLDH tests to microscopy; the average sensitivity and specificity were 95% (95% CI 86% to 99%) and 99% (95% CI 99% to 100%), respectively. Authors' conclusions   RDTs designed to detect  P. vivax  specifically, whether alone or as part of a mixed infection, appear to be more accurate than older tests designed to distinguish  P. falciparum  malaria from non‐falciparum malaria. Compared to microscopy, these tests fail to detect around 5% of P. vivax  cases. This Cochrane Review, in combination with other published information about in vitro test performance and stability in the field, can assist policy‐makers to choose between the available RDTs. 12 April 2019 No update planned Review superseded This Cochrane Review has been superseded by Choi 2019 https://doi.org/10.1002/14651858.CD013218 Plain language summary Rapid tests for diagnosing malaria caused by  Plasmodium vivax  or other less common parasites This review summarises trials evaluating the accuracy of rapid diagnostic tests (RDTs) for diagnosing malaria due to  Plasmodium vivax  or other non‐falciparum species. After searching for relevant studies up to December 2013, we included 47 studies, enrolling 22,862 adults and children. What are rapid tests and why do they need to be able to distinguish Plasmodium vivax malaria RDTs are simple to use, point of care tests, suitable for use in rural settings by primary healthcare workers. RDTs work by using antibodies to detect malaria antigens in the patient's blood. A drop of blood is placed on the test strip where the antibodies and antigen combine to create a distinct line indicating a positive test. Malaria can be caused any one of five species of  Plasmodium  parasite, but  P. falciparum  and  P. vivax  are the most common. In some areas, RDTs need to be able to distinguish which species is causing the malaria symptoms as different species may require different treatments. Unlike  P. falciparum ,  P. vivax  has a liver stage which can cause repeated illness every few months unless it is treated with primaquine. The most common types of RDTs for  P. vivax  use two test lines in combination; one line specific to  P. falciparum,  and one line which can detect any species of Plasmodium. If the  P. falciparum  line is negative and the 'any species' line is positive, the illness is presumed to be due to  P. vivax  (but could also be caused by  P. malariae, or P. ovale ) .  More recently, RDTs have been developed which specifically test for  P. vivax. What does the research say RDTs testing for non‐falciparum malaria were very specific (range 98% to 100%) meaning that only 1% to 2% of patients who test positive would actually not have the disease. However, they were less sensitive (range 78% to 89%), meaning between 11% and 22% of people with non‐falciparum malaria would actually get a negative test result. RDTs which specifically tested for  P. vivax  were more accurate with a specificity of 99% and a sensitivity of 95%, meaning that only 5% of people with  P. vivax  malaria would have a negative test result.","12","John Wiley & Sons, Ltd","1465-1858","Antigens, Protozoan [*analysis]; Cohort Studies; Humans; Malaria [*diagnosis, immunology, parasitology]; Malaria, Vivax [*diagnosis, immunology]; Microscopy; Parasitemia [diagnosis]; Plasmodium [*immunology]; Plasmodium vivax [immunology]; Polymerase Chain Reaction; Reagent Kits, Diagnostic [*parasitology]; Sensitivity and Specificity; Species Specificity","10.1002/14651858.CD011431","http://dx.doi.org/10.1002/14651858.CD011431","Infectious Diseases"
"CD009519.PUB2","Schmidt‐Hansen, M; Baldwin, DR; Hasler, E; Zamora, J; Abraira, V; Roqué i Figuls, M","PET‐CT for assessing mediastinal lymph node involvement in patients with suspected resectable non‐small cell lung cancer","Cochrane Database of Systematic Reviews","2014","Abstract - Background A major determinant of treatment offered to patients with non‐small cell lung cancer (NSCLC) is their intrathoracic (mediastinal) nodal status. If the disease has not spread to the ipsilateral mediastinal nodes, subcarinal (N2) nodes, or both, and the patient is otherwise considered fit for surgery, resection is often the treatment of choice. Planning the optimal treatment is therefore critically dependent on accurate staging of the disease. PET‐CT (positron emission tomography–computed tomography) is a non‐invasive staging method of the mediastinum, which is increasingly available and used by lung cancer multidisciplinary teams. Although the non‐invasive nature of PET‐CT constitutes one of its major advantages, PET‐CT may be suboptimal in detecting malignancy in normal‐sized lymph nodes and in ruling out malignancy in patients with coexisting inflammatory or infectious diseases. Objectives To determine the diagnostic accuracy of integrated PET‐CT for mediastinal staging of patients with suspected or confirmed NSCLC that is potentially suitable for treatment with curative intent. Search methods We searched the following databases up to 30 April 2013:  The Cochrane Library , MEDLINE via OvidSP (from 1946), Embase via OvidSP (from 1974), PreMEDLINE via OvidSP, OpenGrey, ProQuest Dissertations & Theses, and the trials register  www.clinicaltrials.gov . There were no language or publication status restrictions on the search. We also contacted researchers in the field, checked reference lists, and conducted citation searches (with an end‐date of 9 July 2013) of relevant studies. Selection criteria Prospective or retrospective cross‐sectional studies that assessed the diagnostic accuracy of integrated PET‐CT for diagnosing N2 disease in patients with suspected resectable NSCLC. The studies must have used pathology as the reference standard and reported participants as the unit of analysis. Data collection and analysis Two authors independently extracted data pertaining to the study characteristics and the number of true and false positives and true and false negatives for the index test, and they independently assessed the quality of the included studies using QUADAS‐2. We calculated sensitivity and specificity with 95% confidence intervals (CI) for each study and performed two main analyses based on the criteria for test positivity employed:  Activity > background  or  SUVmax ≥ 2.5  (SUVmax = maximum standardised uptake value), where we fitted a summary receiver operating characteristic (ROC) curve using a hierarchical summary ROC (HSROC) model for each subset of studies. We identified the average operating point on the SROC curve and computed the average sensitivities and specificities. We checked for heterogeneity and examined the robustness of the meta‐analyses through sensitivity analyses. Main results We included 45 studies, and based on the criteria for PET‐CT positivity, we categorised the included studies into three groups:  Activity > background  (18 studies, N = 2823, prevalence of N2 and N3 nodes = 679/2328),  SUVmax ≥ 2.5  (12 studies, N = 1656, prevalence of N2 and N3 nodes = 465/1656), and  Other/mixed  (15 studies, N = 1616, prevalence of N2 to N3 nodes = 400/1616). None of the studies reported (any) adverse events. Under‐reporting generally hampered the quality assessment of the studies, and in 30/45 studies, the applicability of the study populations was of high or unclear concern. The summary sensitivity and specificity estimates for the ' Activity > background  PET‐CT positivity criterion were 77.4% (95% CI 65.3 to 86.1) and 90.1% (95% CI 85.3 to 93.5), respectively, but the accuracy estimates of these studies in ROC space showed a wide prediction region. This indicated high between‐study heterogeneity and a relatively large 95% confidence region around the summary value of sensitivity and specificity, denoting a lack of precision. Sensitivity analyses suggested that the overall estimate of sensitivity was especially susceptible to selection bias; reference standard bias; clear definition of test positivity; and to a lesser extent, index test bias and commercial funding bias, with lower combined estimates of sensitivity observed for all the low 'Risk of bias' studies compared with the full analysis. The summary sensitivity and specificity estimates for the  SUVmax ≥ 2.5  PET‐CT positivity criterion were 81.3% (95% CI 70.2 to 88.9) and 79.4% (95% CI 70 to 86.5), respectively.In this group, the accuracy estimates of these studies in ROC space also showed a very wide prediction region. This indicated very high between‐study heterogeneity, and there was a relatively large 95% confidence region around the summary value of sensitivity and specificity, denoting a clear lack of precision. Sensitivity analyses suggested that both overall accuracy estimates were marginally sensitive to flow and timing bias and commercial funding bias, which both lead to slightly lower estimates of sensitivity and specificity. Heterogeneity analyses showed that the accuracy estimates were significantly influenced by country of study origin, percentage of participants with adenocarcinoma, (¹⁸F)‐2‐fluoro‐deoxy‐D‐glucose (FDG) dose, type of PET‐CT scanner, and study size, but not by study design, consecutive recruitment, attenuation correction, year of publication, or tuberculosis incidence rate per 100,000 population. Authors' conclusions This review has shown that accuracy of PET‐CT is insufficient to allow management based on PET‐CT alone. The findings therefore support National Institute for Health and Care (formally 'clinical') Excellence (NICE) guidance on this topic, where PET‐CT is used to guide clinicians in the next step: either a biopsy or where negative and nodes are small, directly to surgery. The apparent difference between the two main makes of PET‐CT scanner is important and may influence the treatment decision in some circumstances. The differences in PET‐CT accuracy estimates between scanner makes, NSCLC subtypes, FDG dose, and country of study origin, along with the general variability of results, suggest that all large centres should actively monitor their accuracy. This is so that they can make reliable decisions based on their own results and identify the populations in which PET‐CT is of most use or potentially little value. Plain language summary PET‐CT scanning to assess the spread of non‐small cell lung cancer within the chest In the absence of distant metastasis, treatment options for non‐small cell lung cancer depend on how much the disease has spread to the different lymph nodes within the chest, that is, the stage of the disease. If the cancer has not spread beyond the nearest (N1) lymph nodes, surgery is often the treatment of choice. Other treatment options for these patients include treatment with either radiotherapy, chemotherapy, or both. Planning the optimal treatment is therefore critically dependent on accurate staging of the disease. PET‐CT scanning is a non‐invasive method of establishing the spread of NSCLC within the chest and elsewhere in the body, which is increasingly available and used by lung cancer multi‐disciplinary teams. Although the non‐invasive nature of PET‐CT constitutes one of the major advantages of the test, PET‐CT may be suboptimal in detecting malignancy in normal‐sized lymph nodes and in ruling out malignancy in patients with coexisting inflammatory or infectious diseases. We examined the accuracy of PET‐CT scanning in establishing the spread of cancer in patients with suspected or confirmed NSCLC that is potentially suitable for surgical treatment with curative intent. We included 45 studies, and based on the criteria for a positive PET‐CT scan, we performed two main analyses. In the 18 studies (2823 participants) in the  Activity > background  group, PET‐CT was found to accurately identify 77.4% (95% CI 65.3 to 86.1) of the participants with NSCLC spread beyond the N1 nodes and 90.1% (95% CI 85.3 to 93.5) of the participants without spread beyond the N1 nodes. In the 12 studies (1656 participants) in the  SUVmax of ≥ 2.5  group, PET‐CT accurately identified 81.3% (95% CI 70.2 to 88.9) of the participants with spread beyond the N1 nodes and 79.4% (95% CI 70 to 86.5) of the participants without spread beyond the N1 nodes. However, the results varied a lot between the studies in each analysis, and the quality and size of the studies themselves, country of study origin, percentage of participants with adenocarcinoma, FDG dose, and type of PET‐CT scanner influenced the results. We believe that the results of this review show that the accuracy of PET‐CT is insufficient to allow management based on PET‐CT alone.","11","John Wiley & Sons, Ltd","1465-1858","Carcinoma, Non‐Small‐Cell Lung [*diagnostic imaging, pathology]; Cross‐Sectional Studies; Humans; Lung Neoplasms [*diagnostic imaging, pathology]; Lymph Nodes [*diagnostic imaging, pathology]; Lymphatic Metastasis; Mediastinum [diagnostic imaging]; Multimodal Imaging [*methods]; Positron‐Emission Tomography [instrumentation, *methods]; Prospective Studies; Retrospective Studies; Tomography, X‐Ray Computed [instrumentation, methods]","10.1002/14651858.CD009519.pub2","http://dx.doi.org/10.1002/14651858.CD009519.pub2","Lung Cancer"
"CD008760.PUB2","Colli, A; Gana, JC; Turner, D; Yap, J; Adams‐Webber, T; Ling, SC; Casazza, G","Capsule endoscopy for the diagnosis of oesophageal varices in people with chronic liver disease or portal vein thrombosis","Cochrane Database of Systematic Reviews","2014","Abstract - Background Current guidelines recommend performance of oesophago‐gastro‐duodenoscopy at the time of diagnosis of hepatic cirrhosis to screen for oesophageal varices. These guidelines require people to undergo an unpleasant invasive procedure repeatedly with its attendant risks, despite the fact that half of the people do not have identifiable oesophageal varices 10 years after the initial diagnosis of cirrhosis. Video capsule endoscopy is a non‐invasive test proposed as an alternative method for the diagnosis of oesophageal varices. Objectives To determine the diagnostic accuracy of capsule endoscopy for the diagnosis of oesophageal varices in children or adults with chronic liver disease or portal vein thrombosis, irrespective of the aetiology. To investigate the accuracy of capsule endoscopy as triage or replacement of oesophago‐gastro‐duodenoscopy. Search methods We searched the Cochrane Hepato‐Biliary Group Diagnostic Test Accuracy Studies Register (October 2013), MEDLINE (Ovid SP) (1950 to October 2013), EMBASE (Ovid SP) (1980 to October 2013), ACP Journal Club (Ovid SP) (1991 to October 2013), Database of Abstracts of Reviews of Effects (DARE) (Ovid SP) (third quarter), Health Technology Assessment (HTA) (Ovid SP) (third quarter), NHS Economic Evaluation Database (NHSEED) (Ovid SP) (third quarter), and Science Citation Index Expanded (SCI‐EXPANDED) (ISI Web of Knowledge) (1955 to October 2013). We applied no language or document type restrictions. Selection criteria Studies that evaluated the diagnostic accuracy of capsule endoscopy for the diagnosis of oesophageal varices using oesophago‐gastro‐duodenoscopy as the reference standard in children or adults of any age, with chronic liver disease or portal vein thrombosis. Data collection and analysis We followed the available guidelines provided in the  Cochrane Handbook for Diagnostic Test of Accuracy Reviews.  We calculated the pooled estimates of sensitivity and specificity using the bivariate model due to the absence of a negative correlation in the receiver operating characteristic (ROC) space and of a threshold effect. Main results The search identified 16 eligible studies, in which only adults with cirrhosis were included. In one study, people with portal thrombosis were also included. We classified most of the studies at high risk of bias for the 'Participants selection' and the 'Flow and timing' domains. One study assessed the accuracy of capsule endoscopy for the diagnosis of large (high‐risk) oesophageal varices. In the remaining15 studies that assessed the accuracy of capsule endoscopy for the diagnosis of oesophageal varices of any size in people with cirrhosis, 936 participants were included; the pooled estimate of sensitivity was 84.8% (95% confidence interval (CI) 77.3% to 90.2%) and of specificity 84.3% (95% CI 73.1% to 91.4%). Eight of these studies included people with suspected varices or people with already diagnosed or even treated varices, or both, introducing a selection bias. Seven studies including only people with suspected but unknown varices were at low risk of bias; the pooled estimate of sensitivity was 79.7% (95% CI 73.1% to 85.0%) and of specificity 86.1% (95% CI 64.5% to 95.5%). Six studies assessed the diagnostic accuracy of capsule endoscopy for the diagnosis of large oesophageal varices, associated with a higher risk of bleeding; the pooled sensitivity was 73.7% (95% CI 52.4% to 87.7%) and of specificity 90.5% (95% CI 84.1% to 94.4%). Two studies also evaluated the presence of red marks, which are another marker of high risk of bleeding; the estimates of sensitivity and specificity varied widely. Two studies obtained similar results with the use of a modified device as index test (string capsule). Due to the absence of data, we could not perform all planned subgroup analyses. Interobserver agreement in the interpretation of capsule endoscopy results and any adverse event attributable to capsule endoscopy were poorly assessed and reported. Only four studies evaluated the interobserver agreement in the interpretation of capsule endoscopy results: the concordance was moderate. The participants' preferences for capsule endoscopy or oesophago‐gastro‐duodenoscopy were reported differently but seemed in favour of capsule endoscopy in nine of 10 studies. In 10 studies, participants reported some minor discomfort on swallowing the capsule. Only one study identified other significant adverse events, including impaction of the capsule due to previously unidentified oesophageal strictures in two participants. No adverse events were reported as a consequence of the reference standard. Authors' conclusions We cannot support the use of capsule endoscopy as a triage test in adults with cirrhosis, administered before oesophago‐gastro‐duodenoscopy, despite the low incidence of adverse events and participant reports of being better tolerated. Thus, we cannot conclude that oesophago‐gastro‐duodenoscopy can be replaced by capsule endoscopy for the detection of oesophageal varices in adults with cirrhosis. We found no data assessing capsule endoscopy in children and in people with portal thrombosis. Plain language summary Capsule endoscopy for the diagnosis of oesophageal varices in people with chronic liver disease or portal vein thrombosis Background In cases of hepatic cirrhosis, whatever the cause, the changes in the structure of, and blood flow within, the liver increase the pressure in the portal vein (called portal vein hypertension), which is the vein that drains blood from the bowels to the liver. Portal hypertension induces dilation (opening) of veins within the wall of the oesophagus (food pipe or gullet), which often rupture (break) with severe bleeding. Thus, when liver cirrhosis is diagnosed, an oesophago‐gastro‐duodenoscopy (OGD) is recommended to detect the presence of oesophageal varices (areas of abnormal dilation of veins). During OGD, a small camera on the end of a tube is inserted down the oesophagus from the mouth. This relays pictures back to a screen. The presence of large varices or of red‐coloured signs on even small varices identifies high risk of rupture and bleeding. If high‐risk varices are found, treatment with beta‐blockers is effective in reducing the risk of bleeding. Capsule endoscopy is a less invasive test than OGD as participants have only to swallow a small device that is able to produce images of the oesophageal walls and could be able to detect the presence of dilated veins. Study characteristics We searched scientific databases for clinical studies comparing OGD to capsule endoscopy and reporting the size and appearance of varices in children or adults with chronic liver disease or portal vein thrombosis (narrowing of the portal vein). The evidence is current to October 2013. Key results We found 16 studies assessing the ability of capsule endoscopy to diagnose the presence of varices and grade the risk of bleeding and comparing it with OGD in adults with cirrhosis. Capsule endoscopy, even if more acceptable to participants, cannot replace OGD for the detection of oesophageal varices as about 15% are left undetected and 15% are not confirmed by endoscopy. Even the accuracy in detecting large varices or red marks on varices was very lower than endoscopy. Hence, in conclusion, capsule endoscopy is not sufficiently accurate to replace OGD for the detection of oesophageal varices in cirrhotic participants. Quality of the evidence In nine of the sixteen studies there were problems concerning participant selection and incompleteness of reported data which impair accuracy estimates and the transferability of the results.","10","John Wiley & Sons, Ltd","1465-1858","*Capsule Endoscopy; *Portal Vein; Adult; Endoscopy, Digestive System; Esophageal and Gastric Varices [*diagnosis]; Humans; Liver Diseases [*complications]; Randomized Controlled Trials as Topic; Venous Thrombosis [*complications]","10.1002/14651858.CD008760.pub2","http://dx.doi.org/10.1002/14651858.CD008760.pub2","Hepato-Biliary"
"CD009372.PUB2","Josephson, CB; White, PM; Krishan, A; Al‐Shahi Salman, R","Computed tomography angiography or magnetic resonance angiography for detection of intracranial vascular malformations in patients with intracerebral haemorrhage","Cochrane Database of Systematic Reviews","2014","Abstract - Background Intracranial vascular malformations (brain or pial/dural arteriovenous malformations/fistulae, and aneurysms) are the leading cause of intracerebral haemorrhage (ICH) in young adults. Early identification of the intracranial vascular malformation may improve outcome if treatment can prevent ICH recurrence. Catheter intra‐arterial digital subtraction angiography (IADSA) is considered the reference standard for the detection an intracranial vascular malformation as the cause of ICH. Computed tomography angiography (CTA) and magnetic resonance angiography (MRA) are less invasive than IADSA and may be as accurate for identifying some causes of ICH. Objectives To evaluate the diagnostic test accuracy of CTA and MRA versus IADSA for the detection of intracranial vascular malformations as a cause of ICH. Search methods We searched MEDLINE (1948 to August 2013), EMBASE (1980 to August 2013), MEDION (August 2013), the Database of Abstracts of Reviews of Effects (DARE; August 2013), the Health Technology Assessment Database (HTA; August 2013), ClinicalTrials.gov (August 2013), and WHO ICTRP (International Clinical Trials Register Portfolio; August 2013). We also performed a cited reference search for forward tracking of relevant articles on Google Scholar (http://scholar.google.com/), screened bibliographies, and contacted authors to identify additional studies. Selection criteria We selected studies reporting data that could be used to construct contingency tables that compared CTA or MRA, or both, with IADSA in the same patients for the detection of intracranial vascular malformations following ICH. Data collection and analysis Two authors (CBJ and RA‐SS) independently extracted data on study characteristics and measures of test accuracy. Two authors (CBJ and PMW) independently extracted data on test characteristics. We obtained data restricted to the subgroup undergoing IADSA in studies using multiple reference standards. We combined data using the bivariate model. We generated forest plots of the sensitivity and specificity of CTA and MRA and created a summary receiver operating characteristic plot. Main results Eleven studies (n = 927 participants) met our inclusion criteria. Eight studies compared CTA with IADSA (n = 526) and three studies compared MRA with IADSA (n = 401). Methodological quality varied considerably among studies, with partial verification bias in 7/11 (64%) and retrospective designs in 5/10 (50%). In studies of CTA, the pooled estimate of sensitivity was 0.95 (95% confidence interval (CI) 0.90 to 0.97) and specificity was 0.99 (95% CI 0.95 to 1.00). The results remained robust in a sensitivity analysis in which only studies evaluating adult patients (≥ 16 years of age) were included. In studies of MRA, the pooled estimate of sensitivity was 0.98 (95% CI 0.80 to 1.00) and specificity was 0.99 (95% CI 0.97 to 1.00). An indirect comparison of CTA and MRA using a bivariate model incorporating test type as one of the parameters failed to reveal a statistically significant difference in sensitivity or specificity between the two imaging modalities (P value = 0.6). Authors' conclusions CTA and MRA appear to have good sensitivity and specificity following ICH for the detection of intracranial vascular malformations, although several of the included studies had methodological shortcomings (retrospective designs and partial verification bias in particular) that may have increased apparent test accuracy. Plain language summary Computed tomography angiography or magnetic resonance angiography for detecting blood vessel abnormalities in patients with intracerebral haemorrhage Blood vessel abnormalities are the leading cause of bleeding in the brain (known as intracerebral haemorrhage) in young adults. Early detection of blood vessel abnormalities may improve outcome if treatment can prevent bleeding recurrence. This review looked at different tests used to identify blood vessel abnormalities in the brain. Intra‐arterial digital subtraction angiography (IADSA) is the standard test used and involves positioning a tube, introduced through a blood vessel in the groin, into blood vessels near the brain. Dye is directly injected into the brain's blood vessels using this tube. Computed tomographic angiography (CTA) and magnetic resonance angiography (MRA) are newer tests that may be done without any injections (MRA) or only through an injection into the arm (CTA and MRA). This review investigated the accuracy of CTA or MRA, or both, compared with IADSA after intracerebral haemorrhage. We found eight studies (involving 526 participants) that compared CTA with IADSA and three studies (involving 401 participants) that compared MRA with IADSA. Both CTA and MRA appear to have good accuracy when compared with IADSA. However, the studies were small and were limited in many cases by their design. Further research that looks at accuracy, practicality, and costs is needed.","9","John Wiley & Sons, Ltd","1465-1858","*Magnetic Resonance Angiography; *Tomography, X‐Ray Computed; Adolescent; Adult; Cerebral Angiography [*methods]; Cerebral Hemorrhage [*etiology]; Female; Humans; Intracranial Arteriovenous Malformations [*complications, diagnosis]; Male; Middle Aged; Randomized Controlled Trials as Topic; Sensitivity and Specificity","10.1002/14651858.CD009372.pub2","http://dx.doi.org/10.1002/14651858.CD009372.pub2","Stroke"
"CD010386.PUB2","Zhang, S; Smailagic, N; Hyde, C; Noel‐Storr, AH; Takwoingi, Y; McShane, R; Feng, J","11C‐PIB‐PET for the early diagnosis of Alzheimer’s disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2014","Abstract - Background According to the latest revised National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer's Disease and Related Disorders Association (now known as the Alzheimer's Association) (NINCDS‐ADRDA) diagnostic criteria for Alzheimer's disease dementia, the confidence in diagnosing mild cognitive impairment (MCI) due to Alzheimer's disease dementia is raised with the application of imaging biomarkers. These tests, added to core clinical criteria, might increase the sensitivity or specificity of a testing strategy. However, the accuracy of biomarkers in the diagnosis of Alzheimer’s disease dementia and other dementias has not yet been systematically evaluated. A formal systematic evaluation of the sensitivity, specificity, and other properties of positron emission tomography (PET) imaging with the  11 C‐labelled Pittsburgh Compound‐B ( 11 C‐PIB) ligand was performed. Objectives To determine the diagnostic accuracy of the  11 C‐ PIB‐PET scan for detecting participants with MCI at baseline who will clinically convert to Alzheimer’s disease dementia or other forms of dementia over a period of time. Search methods The most recent search for this review was performed on 12 January 2013. We searched MEDLINE (OvidSP), EMBASE (OvidSP), BIOSIS Previews (ISI Web of Knowledge), Web of Science and Conference Proceedings (ISI Web of Knowledge), PsycINFO (OvidSP), and LILACS (BIREME). We also requested a search of the Cochrane Register of Diagnostic Test Accuracy Studies (managed by the Cochrane Renal Group). No language or date restrictions were applied to the electronic searches and methodological filters were not used so as to maximise sensitivity. Selection criteria We selected studies that had prospectively defined cohorts with any accepted definition of MCI with baseline  11 C‐PIB‐PET scan. In addition, we only selected studies that applied a reference standard for Alzheimer's dementia diagnosis for example NINCDS‐ADRDA or Diagnostic and Statistical Manual of Mental Disorders‐IV (DSM‐IV) criteria. Data collection and analysis We screened all titles generated by electronic database searches. Two review authors independently assessed the abstracts of all potentially relevant studies. The identified full papers were assessed for eligibility and data were extracted to create two by two tables. Two independent assessors performed quality assessment using the QUADAS 2 tool. We used the hierarchical summary receiver operating characteristic (ROC) model to produce a summary ROC curve. Main results Conversion from MCI to Alzheimer's disease dementia was evaluated in nine studies. The quality of the evidence was limited. Of the 274 participants included in the meta‐analysis, 112 developed Alzheimer’s dementia. Based on the nine included studies, the median proportion converting was 34%. The studies varied markedly in how the PIB scans were done and interpreted. The sensitivities were between 83% and 100% while the specificities were between 46% and 88%. Because of the variation in thresholds and measures of  11 C‐PIB amyloid retention, we did not calculate summary sensitivity and specificity. Although subject to considerable uncertainty, to illustrate the potential strengths and weaknesses of  11 C‐PIB‐PET scans we estimated from the fitted summary ROC curve that the sensitivity was 96% (95% confidence interval (CI) 87 to 99) at the included study median specificity of 58%. This equated to a positive likelihood ratio of 2.3 and a negative likelihood ratio of 0.07. Assuming a typical conversion rate of MCI to Alzheimer’s dementia of 34%, for every 100 PIB scans one person with a negative scan would progress and 28 with a positive scan would not actually progress to Alzheimer’s dementia. There were limited data for formal investigation of heterogeneity. We performed two sensitivity analyses to assess the influence of type of reference standard and the use of a pre‐specified threshold. There was no effect on our findings. Authors' conclusions Although the good sensitivity achieved in some included studies is promising for the value of  11 C‐PIB‐PET, given the heterogeneity in the conduct and interpretation of the test and the lack of defined thresholds for determination of test positivity, we cannot recommend its routine use in clinical practice. 11 C‐PIB‐PET biomarker is a high cost investigation, therefore it is important to clearly demonstrate its accuracy and standardise the process of the  11 C‐PIB diagnostic modality prior to it being widely used. Plain language summary 11 C‐PIB‐PET scan for early prediction of developing Alzheimer’s disease or other dementia in people with mild cognitive impairment (MCI) The numbers of people with dementia and other cognitive problems are increasing globally. A diagnosis of the pre‐dementia phase of disease is recommended but there is no agreement on the best approach. A range of tests have been developed which healthcare professionals can use to assess people with poor memory or cognitive impairment. Based on the published data, we have found that the  11 C‐PIB‐PET scan as a single test lacks the accuracy to identify those patients with MCI who would develop Alzheimer's disease dementia or other forms of dementia. The findings indicate that for every 100 PIB scans, one person with a negative scan will progress to Alzheimer's disease dementia and 28 with a positive scan will not. Therefore, a positive PIB scan in patients with MCI is of no clinical value in the early prediction of Alzheimer's disease dementia developing.","7","John Wiley & Sons, Ltd","1465-1858","*Aniline Compounds; *Carbon Radioisotopes; *Thiazoles; Aged; Alzheimer Disease [*diagnostic imaging]; Cognitive Dysfunction [*diagnostic imaging]; Dementia [*diagnostic imaging]; Disease Progression; Early Diagnosis; Humans; Positron‐Emission Tomography [*methods]; Prospective Studies; Sensitivity and Specificity","10.1002/14651858.CD010386.pub2","http://dx.doi.org/10.1002/14651858.CD010386.pub2","Dementia and Cognitive Improvement"
"CD010409.PUB2","Lawrie, TA; Patel, A; Martin‐Hirsch, PPL; Bryant, A; Ratnavelu, NDG; Naik, R; Ralte, A","Sentinel node assessment for diagnosis of groin lymph node involvement in vulval cancer","Cochrane Database of Systematic Reviews","2014","Abstract - Background Vulval cancer is usually treated by wide local excision with removal of groin lymph nodes (inguinofemoral lymphadenectomy) from one or both sides, depending on the tumour location. However, this procedure is associated with significant morbidity. As lymph node metastasis occurs in about 30% of women with early vulval cancer, accurate prediction of lymph node metastases could reduce the extent of surgery in many women, thereby reducing morbidity. Sentinel node assessment is a diagnostic technique that uses traceable agents to identify the spread of cancer cells to the lymph nodes draining affected tissue. Once the sentinel nodes are identified, they are removed and submitted to histological examination. This technique has been found to be useful in diagnosing the nodal involvement of other types of tumours. Sentinel node assessment in vulval cancer has been evaluated with various tracing agents. It is unclear which tracing agent or combination of agents is most accurate. Objectives To assess the diagnostic test accuracy of various techniques using traceable agents for sentinel lymph node assessment to diagnose groin lymph node metastasis in women with FIGO stage IB or higher vulval cancer and to investigate sources of heterogeneity. Search methods We searched MEDLINE (1946 to February 2013), EMBASE (1974 to March 2013) and the relevant Cochrane trial registers. Selection criteria Studies that evaluated the diagnostic accuracy of traceable agents for sentinel node assessment (involving the identification of a sentinel node plus histological examination) compared with histological examination of removed groin lymph nodes following complete inguinofemoral lymphadenectomy (IFL) in women with vulval cancer, provided there were sufficient data for the construction of two‐by‐two tables. Data collection and analysis Two authors (TAL, AP) independently screened titles and abstracts for relevance, classified studies for inclusion/exclusion and extracted data. We assessed the methodological quality of studies using the QUADAS‐2 tool. We used univariate meta‐analytical methods to estimate pooled sensitivity estimates. Main results We included 34 studies evaluating 1614 women and approximately 2396 groins. The overall methodological quality of included studies was moderate. The studies included in this review used the following traceable techniques to identify sentinel nodes in their participants: blue dye only (three studies), technetium only (eight studies), blue dye plus technetium combined (combined tests; 13 studies) and various inconsistent combinations of these three techniques (mixed tests; 10 studies). For studies of mixed tests, we obtained separate test data where possible. Most studies used haematoxylin and eosin (H&E) stains for the histological examination. Additionally an immunohistochemical (IHC) stain with and without ultrastaging was employed by 14 and eight studies, respectively. One study used reverse transcriptase polymerase chain reaction analysis (CA9 RT‐PCR), whilst three studies did not describe the histological methods used. The pooled sensitivity estimate for studies using blue dye only was 0.94 (68 women; 95% confidence interval (CI) 0.69 to 0.99), for mixed tests was 0.91 (679 women; 95% CI 0.71 to 0.98), for technetium only was 0.93 (149 women; 95% CI 0.89 to 0.96) and for combined tests was 0.95 (390 women; 95% CI 0.89 to 0.97). Negative predictive values (NPVs) for all index tests were > 95%. Most studies also reported sentinel node detection rates (the ability of the test to identify a sentinel node) of the index test. The mean detection rate for blue dye alone was 82%, compared with 95%, 96% and 98% for mixed tests, technetium only and combined tests, respectively. We estimated the clinical consequences of the various tests for 100 women undergoing the sentinel node procedure, assuming the prevalence of groin metastases to be 30%. For the combined or technetium only tests, one and two women with groin metastases might be 'missed', respectively (95% CI 1 to 3); and for mixed tests, three women with groin metastases might be 'missed' (95% CI 1 to 9). The wide CIs associated with the pooled sensitivity estimates for blue dye and mixed tests increased the potential for these tests to 'miss' women with groin metastases. Authors' conclusions There is little difference in diagnostic test accuracy between the technetium and combined tests. The combined test may reduce the number of women with 'missed' groin node metastases compared with technetium only. Blue dye alone may be associated with more 'missed' cases compared with tests using technetium. Sentinel node assessment with technetium‐based tests will reduce the need for IFL by 70% in women with early vulval cancer. It is not yet clear how the survival of women with negative sentinel nodes compares to those undergoing standard surgery (IFL). A randomised controlled trial of sentinel node dissection and IFL has methodological and ethical issues, therefore more observational data on the survival of women with early vulval cancer are needed. Plain language summary Can tests used to identify the main groin lymph node/s in women with vulval cancer accurately predict whether the cancer has spread to the groin/s? The issue Women with vulval cancer that has spread to the groin lymph nodes need additional treatment. The standard treatment usually involves surgical removal of as many groin nodes as possible (known as complete inguinofemoral lymphadenectomy (IFL)). However, only about 30% of women with vulval cancer in whom lymph nodes are not obviously enlarged will have groin involvement; therefore, in about 70% of these women additional surgery is not necessary. As groin surgery often causes later swelling of the legs and other unpleasant side effects, it would be preferable not to undergo the surgery if it is not required; therefore, accurate screening tests to determine who should have surgery are needed. Sentinel node assessment involves identifying the main lymph node/s draining the tumour. After the main (sentinel) nodes are identified, they are removed and examined under a microscope to check for cancer cells. Additional surgery depends on the findings of the examination: if cancer cells are found in the nodes, additional surgery is necessary; if the nodes are cancer‐free, additional surgery can be avoided. Why is this review important? Several studies have been done using dyes or traceable agents to identify sentinel nodes. From these studies, it is not clear whether all of these agents are sufficiently accurate to predict which women have cancerous spread to the groin. This review summarises the evidence and produces overall estimates of the relative accuracies of the available tests. How was the review conducted? We included all studies that tested the accuracy of tracer agent/s against the standard method of identifying cancer in the groin nodes (removing all groin nodes (IFL) and examining them under a microscope). Women in these studies had vulval cancer of Federation of Gynecology and Obstetrics (FIGO) stage IB or higher without obvious signs of cancer in the groin (enlarged or palpable nodes). We only included studies of at least 10 women, and noted any concerns about the quality of studies.    What are the findings? We included 34 studies (1614 women) that evaluated three techniques: blue dye only, technetium (a radioactive substance) only, or blue dye and technetium combined. Ten studies used all three techniques during the course of the study (one technique per participant). There are two attributes to a test: the ability to identify or detect the sentinel node, and the ability to identify the cancer in the sentinel node. We found that all tests can identify cancer in the groin nodes with good accuracy (more than 90% of nodes with cancer will be accurately identified with any of the tests), although the combined test was the most accurate (95%). The ability of the tests to detect sentinel nodes varied, with the blue dye test only detecting sentinel nodes in 82% of women, compared with 98% for the combined test. If sentinel nodes are not detected, they cannot be examined for cancer cells; therefore, women in whom sentinel nodes are not detected will usually need to undergo IFL. What does this mean? The combined and technetium only tests are able to predict accurately which women have cancerous spread to the groin. For a group of 100 women undergoing assessment, the findings mean that approximately one or fewer women having the combined or technetium only tests will undergo an unnecessary IFL, compared with approximately 11 women having the blue dye only test. This is mainly because the blue dye only test is not as good as technetium in identifying sentinel nodes. Fewer women with spread to the groin will be missed with the combined or technetium only tests (1 to 3 out of 30) compared with the blue dye only test (1 to 8 out of 30). It is not clear whether women with negative sentinel nodes (i.e. no spread of cancer to the groin lymph nodes) who do not undergo IFL will live as long as those who undergo IFL. The current best data on survival come from a Dutch study that followed up 259 women with negative sentinel nodes and reported a three‐year survival of 97%.","6","John Wiley & Sons, Ltd","1465-1858","Coloring Agents; Female; Groin; Humans; Lymph Node Excision; Lymph Nodes [*pathology]; Lymphatic Metastasis; Neoplasm Staging; Randomized Controlled Trials as Topic; Sentinel Lymph Node Biopsy [*methods]; Technetium; Vulvar Neoplasms [*pathology]","10.1002/14651858.CD010409.pub2","http://dx.doi.org/10.1002/14651858.CD010409.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD009135.PUB2","Boelaert, M; Verdonck, K; Menten, J; Sunyoto, T; van Griensven, J; Chappuis, F; Rijal, S","Rapid tests for the diagnosis of visceral leishmaniasis in patients with suspected disease","Cochrane Database of Systematic Reviews","2014","Abstract - Background The diagnosis of visceral leishmaniasis (VL) in patients with fever and a large spleen relies on showing  Leishmania  parasites in tissue samples and on serological tests. Parasitological techniques are invasive, require sophisticated laboratories, consume time, or lack accuracy. Recently, rapid diagnostic tests that are easy to perform have become available. Objectives To determine the diagnostic accuracy of rapid tests for diagnosing VL in patients with suspected disease presenting at health services in endemic areas. Search methods We searched MEDLINE, EMBASE, LILACS, CIDG SR, CENTRAL, SCI‐expanded, Medion, Arif, CCT, and the WHO trials register on 3 December 2013, without applying language or date limits. Selection criteria This review includes original, phase III, diagnostic accuracy studies of rapid tests in patients clinically suspected to have VL. As reference standards, we accepted: (1) direct smear or culture of spleen aspirate; (2) composite reference standard based on one or more of the following: parasitology, serology, or response to treatment; and (3) latent class analysis. Data collection and analysis Two review authors independently extracted data and assessed quality of included studies using the QUADAS‐2 tool. Discrepancies were resolved by a third author. We carried out a meta‐analysis to estimate sensitivity and specificity of rapid tests, using a bivariate normal model with a complementary log‐log link function. We analysed each index test separately. As possible sources of heterogeneity, we explored: geographical area, commercial brand of index test, type of reference standard, disease prevalence, study size, and risk of bias (QUADAS‐2). We also undertook a sensitivity analysis to assess the influence of imperfect reference standards. Main results Twenty‐four studies containing information about five index tests (rK39 immunochromatographic test (ICT), KAtex latex agglutination test in urine, FAST agglutination test, rK26 ICT, and rKE16 ICT) recruiting 4271 participants (2605 with VL) were included. We carried out a meta‐analysis for the rK39 ICT (including 18 studies; 3622 participants) and the latex agglutination test (six studies; 1374 participants). The results showed considerable heterogeneity. For the rK39 ICT, the overall sensitivity was 91.9% (95% confidence interval (95% CI) 84.8 to 96.5) and the specificity 92.4% (95% CI 85.6 to 96.8). The sensitivity was lower in East Africa (85.3%; 95% CI 74.5 to 93.2) than in the Indian subcontinent (97.0%; 95% CI 90.0 to 99.5). For the latex agglutination test, overall sensitivity was 63.6% (95% CI 40.9 to 85.6) and specificity 92.9% (95% CI 76.7 to 99.2). Authors' conclusions The rK39 ICT shows high sensitivity and specificity for the diagnosis of visceral leishmaniasis in patients with febrile splenomegaly and no previous history of the disease, but the sensitivity is notably lower in east Africa than in the Indian subcontinent. Other rapid tests lack accuracy, validation, or both. 15 April 2019 Update pending Studies awaiting assessment The CIDG is currently examining a new search conducted in April 2019 for potentially relevant studies. These studies have not yet been incorporated into this Cochrane Review. Plain language summary Rapid diagnostic tests for visceral leishmaniasis Visceral leishmaniasis (or kala‐azar) is caused by a parasite, results in fever, a large spleen and other health problems, occuring in India, Bangladesh and Nepal, east Africa, the Mediterranean region and Brazil. Without treatment people die, and proper treatment can result in cure, so diagnosis is important. Many of the tests that are used to determine if a person has visceral leishmaniasis are complicated, costly, painful and sometimes dangerous for the patients. Now rapid diagnostic tests that are safe and easy to perform are available. This Cochrane review describes how accurate these rapid diagnostic tests are for diagnosing visceral leishmaniasis. We summarize those studies that evaluated the rapid tests in people who, according to their physicians, could have the disease. We only included studies in which the researchers had used established methods to distinguish the people with visceral leishmaniasis from those who did not have the disease. We found 24 studies, which contained information about five different rapid tests. A total of 4271 people participated in these studies. One of the rapid tests (called the rK39 immunochromatographic test) gave correct, positive results in 92% of the people with visceral leishmaniasis and it gave correct, negative results in 92% of the people who did not have the disease. This test worked better in India and Nepal than in east Africa. In India and Nepal, it gave correct, positive results in 97% of the people with the disease. In east Africa, it gave correct, positive results in only 85% of the people with the disease. A second rapid test (called latex agglutination test) gave correct, positive results in 64% of the people with the disease and it gave correct, negative results in 93% of the people without the disease. For the other rapid tests evaluated, there are too few studies to know how accurate they are.","6","John Wiley & Sons, Ltd","1465-1858","*Asymptomatic Infections; Africa, Eastern; Agglutination Tests [*methods]; Antigens, Protozoan [*analysis]; Biomarkers [analysis]; Chromatography, Affinity [*methods]; Clinical Trials, Phase III as Topic; Humans; India; Latex Fixation Tests [methods]; Leishmaniasis, Visceral [*diagnosis]; Nepal; Protozoan Proteins [*analysis]; Sensitivity and Specificity","10.1002/14651858.CD009135.pub2","http://dx.doi.org/10.1002/14651858.CD009135.pub2","Infectious Diseases"
"CD008782.PUB4","Ritchie, C; Smailagic, N; Noel‐Storr, AH; Takwoingi, Y; Flicker, L; Mason, SE; McShane, R","Plasma and cerebrospinal fluid amyloid beta for the diagnosis of Alzheimer's disease dementia and other dementias in people with mild cognitive impairment (MCI)","Cochrane Database of Systematic Reviews","2014","Abstract - Background According to the latest revised National Institute of Neurological and Communicative Disorders and Stroke and the Alzheimer's Disease and Related Disorders Association (now known as the Alzheimer's Association) (NINCDS‐ADRDA) diagnostic criteria for Alzheimer's disease dementia of the National Institute on Aging and Alzheimer Association, the confidence in diagnosing mild cognitive impairment (MCI) due to Alzheimer's disease dementia is raised with the application of biomarkers based on measures in the cerebrospinal fluid (CSF) or imaging. These tests, added to core clinical criteria, might increase the sensitivity or specificity of a testing strategy. However, the accuracy of biomarkers in the diagnosis of Alzheimer’s disease dementia and other dementias has not yet been systematically evaluated. A formal systematic evaluation of sensitivity, specificity, and other properties of plasma and CSF amyloid beta (Aß) biomarkers was performed. Objectives To determine the accuracy of plasma and CSF Aß levels for detecting those patients with MCI who would convert to Alzheimer's disease dementia or other forms of dementia over time. Search methods The most recent search for this review was performed on 3 December 2012. We searched MEDLINE (OvidSP), EMBASE (OvidSP), BIOSIS Previews (ISI Web of Knowledge), Web of Science and Conference Proceedings (ISI Web of Knowledge), PsycINFO (OvidSP), and LILACS (BIREME). We also requested a search of the Cochrane Register of Diagnostic Test Accuracy Studies (managed by the Cochrane Renal Group). No language or date restrictions were applied to the electronic searches and methodological filters were not used so as to maximise sensitivity. Selection criteria We selected those studies that had prospectively well defined cohorts with any accepted definition of cognitive decline, but no dementia, with baseline CSF or plasma Aß levels, or both, documented at or around the time the above diagnoses were made. We also included studies which looked at data from those cohorts retrospectively, and which contained sufficient data to construct two by two tables expressing plasma and CSF Aß biomarker results by disease status. Moreover, studies were only selected if they applied a reference standard for Alzheimer's dementia diagnosis, for example the NINCDS‐ADRDA or Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM‐IV) criteria. Data collection and analysis We screened all titles generated by the electronic database searches. Two review authors independently assessed the abstracts of all potentially relevant studies. We assessed the identified full papers for eligibility and extracted data to create standard two by two tables. Two independent assessors performed quality assessment using the QUADAS‐2 tool. Where data allowed, we derived estimates of sensitivity at fixed values of specificity from the model we fitted to produce the summary receiver operating characteristic (ROC) curve. Main results Alzheimer's disease dementia was evaluated in 14 studies using CSF Aß 42 . Of the 1349 participants included in the meta‐analysis, 436 developed Alzheimer’s dementia. Individual study estimates of sensitivity were between 36% and 100% while the specificities were between 29% and 91%. Because of the variation in assay thresholds, we did not estimate summary sensitivity and specificity. However, we derived estimates of sensitivity at fixed values of specificity from the model we fitted to produce the summary ROC curve. At the median specificity of 64%, the sensitivity was 81% (95% CI 72 to 87). This equated to a positive likelihood ratio (LR+) of 2.22 (95% CI 2.00 to 2.47) and a negative likelihood ratio (LR–) of 0.31 (95% CI 0.21 to 0.48). The accuracy of CSF Aß 42  for all forms of dementia was evaluated in four studies. Of the 464 participants examined, 188 developed a form of dementia (Alzheimer’s disease and other forms of dementia).The thresholds used were between 209 mg/ml and 512 ng/ml. The sensitivities were between 56% and 75% while the specificities were between 47% and 76%. At the median specificity of 75%, the sensitivity was estimated to be 63% (95% CI 22 to 91) from the meta‐analytic model. This equated to a LR+ of 2.51 (95% CI 1.30 to 4.86) and a LR– of 0.50 (95% CI 0.16 to 1.51). The accuracy of CSF Aß 42  for non‐Alzheimer's disease dementia was evaluated in three studies. Of the 385 participants examined, 61 developed non‐Alzheimer's disease dementia. Since there were very few studies and considerable variation between studies, the results were not meta‐analysed. The sensitivities were between 8% and 63% while the specificities were between 35% and 67%. Only one study examined the accuracy of plasma Aß 42  and the plasma Aß 42 /Aß 40  ratio for Alzheimer's disease dementia. The sensitivity of 86% (95% CI 81 to 90) was the same for both tests while the specificities were 50% (95% CI 44 to 55) and 70% (95% CI 64 to 75) for plasma Aß 42  and the plasma Aß 42 /Aß 40  ratio respectively. Of the 565 participants examined, 245 developed Alzheimer’s dementia and 87 non‐Alzheimer's disease dementia. There was substantial heterogeneity between studies. The accuracy of Aß 42  for the diagnosis of Alzheimer's disease dementia did not differ significantly (P = 0.8) between studies that pre‐specified the threshold for determining test positivity (n = 6) and those that only determined the threshold at follow‐up (n = 8). One study excluded a sample of MCI non‐Alzheimer's disease dementia converters from their analysis. In sensitivity analyses, the exclusion of this study had no impact on our findings. The exclusion of eight studies (950 patients) that were considered at high (n = 3) or unclear (n = 5) risk of bias for the patient selection domain also made no difference to our findings. Authors' conclusions The proposed diagnostic criteria for prodromal dementia and MCI due to Alzheimer's disease, although still being debated, would be fulfilled where there is both core clinical and cognitive criteria and a single biomarker abnormality. From our review, the measure of abnormally low CSF Aß levels has very little diagnostic benefit with likelihood ratios suggesting only marginal clinical utility. The quality of reports was also poor, and thresholds and length of follow‐up were inconsistent. We conclude that when applied to a population of patients with MCI, CSF Aß levels cannot be recommended as an accurate test for Alzheimer's disease. Plain language summary Proteins in blood and cerebrospinal fluids for early prediction of developing Alzheimer’s disease or other dementia in people with cognitive problems The numbers of people with dementia and other cognitive problems are increasing globally. A diagnosis of the pre‐dementia phase of disease is recommended but there is no agreement on the best approach. A range of tests have been developed which healthcare professionals can use to assess people with poor memory or cognitive impairment. In this review, however, we have found that measuring protein in cerebrospinal fluid (CSF amyloid beta (Aβ 40 ) or CSF Aβ 42 ), as a single test, lacks the accuracy to identify those patients with mild cognitive impairment who would develop Alzheimer's disease dementia or other forms of dementia.","6","John Wiley & Sons, Ltd","1465-1858","Alzheimer Disease [*diagnosis]; Amyloid beta‐Peptides [blood, *cerebrospinal fluid]; Biomarkers [blood, cerebrospinal fluid]; Cognitive Dysfunction [*diagnosis]; Dementia [diagnosis]; Disease Progression; Humans; Peptide Fragments [blood, *cerebrospinal fluid]; Sensitivity and Specificity","10.1002/14651858.CD008782.pub4","http://dx.doi.org/10.1002/14651858.CD008782.pub4","Dementia and Cognitive Improvement"
"CD009694.PUB2","Taylor, T; Dineen, RA; Gardiner, DC; Buss, CH; Howatson, A; Pace, NL","Computed tomography (CT) angiography for confirmation of the clinical diagnosis of brain death","Cochrane Database of Systematic Reviews","2014","Abstract - Background The diagnosis of death using neurological criteria (brain death) has profound social, legal and ethical implications. The diagnosis can be made using standard clinical tests examining for brain function, but in some patient populations and in some countries additional tests may be required. Computed tomography (CT) angiography, which is currently in wide clinical use, has been identified as one such test. Objectives To assess from the current literature the sensitivity of CT cerebral angiography as an additional confirmatory test for diagnosing death using neurological criteria, following satisfaction of clinical neurological criteria for brain death. Search methods We performed comprehensive literature searches to identify studies that would assess the diagnostic accuracy of CT angiography (the index test) in cohorts of adult patients, using the diagnosis of brain death according to neurological criteria as the target condition. We searched the Cochrane Central Register of Controlled Trials (CENTRAL) ( The Cochrane Library 2012 , Issue 5) and the following databases from January 1992 to August 2012: MEDLINE; EMBASE; BNI; CINAHL; ISI Web of Science; BioMed Central. We also conducted searches in regional electronic bibliographic databases and subject‐specific databases (MEDION; IndMed; African Index Medicus). A search was also conducted in Google Scholar where we reviewed the first 100 results only. We handsearched reference lists and conference proceedings to identify primary studies and review articles. Abstracts were identified by two authors. Methodological assessment of studies using the QUADAS‐2 tool and further data extraction for re‐analysis were performed by three authors. Selection criteria We included in this review all large case series and cohort studies that compared the results of CT angiography with the diagnosis of brain death according to neurological criteria. Uniquely, the reference standard was the same as the target condition in this review. Data collection and analysis We reviewed all included studies for methodological quality according to the QUADAS‐2 criteria. We encountered significant heterogeneity in methods used to interpret CT angiography studies and therefore, where possible, we re‐analysed the published data to conform to a standard radiological interpretation model. The majority of studies (with one exception) were not designed to include patients who were not brain dead, and therefore overall specificity was not estimable as part of a meta‐analysis. Sensitivity, confidence and prediction intervals were calculated for both as‐published data and as re‐analysed to a standardized interpretation model. Main results Ten studies were found including 366 patients in total. We included eight studies in the as‐published data analysis, comprising 337 patients . The methodological quality of the studies was overall satisfactory, however there was potential for introduction of significant bias in several specific areas relating to performance of the index test and to the timing of index versus reference tests. Results demonstrated a sensitivity estimate of 0.84 (95% confidence interval (CI) 0.69 to 0.93). The 95% approximate prediction interval was very wide (0.34 to 0.98). Data in three studies were available as a four‐vessel interpretation model and the data could be re‐analysed to a four‐vessel interpretation model in a further five studies, comprising 314 patient events. Results demonstrated a similar sensitivity estimate of 0.85 (95% CI 0.77 to 0.91) but with an improved 95% approximate prediction interval (0.56 to 0.96). Authors' conclusions The available evidence cannot support the use of CT angiography as a mandatory test, or as a complete replacement for neurological testing, in the management pathway of patients who are suspected to be clinically brain dead. CT angiography may be useful as a confirmatory or add‐on test following a clinical diagnosis of death, assuming that clinicians are aware of the relatively low overall sensitivity. Consensus on a standard radiological interpretation protocol for future published studies would facilitate further meta‐analysis. Plain language summary The use of computed tomography angiography (CTA) to confirm the clinical diagnosis of brain death This Cochrane diagnostic test accuracy review looked at the evidence for the radiology test computed tomography angiography (CTA), which demonstrates blood flow in the main vessels of the brain, to support the results of clinical tests of brain function performed in unconscious patients on mechanical breathing machines who are thought by their doctors to have died. Establishing a correct diagnosis is very important as the diagnosis confirms the death of the patient, which will have profound legal and societal implications including making organs available for transplantation. The diagnosis of death using neurological criteria, or brain death, is usually made by performing a highly specific set of clinical tests on the patient. However in some cases, for example when patients are anaesthetized or heavily sedated, performing these tests may not be possible and additional tests are required such as CTA. In some countries it is a statutory requirement for doctors to always carry out an additional test and recently some clinicians have called for these additional tests to be mandatory even when there is no statutory requirement. It is important for doctors to know how useful CTA is when compared to, or added to, the usual clinical tests. Ten studies were found, including 366 patients in total. Most of the studies were performed in intensive care departments but involved only small numbers of patients. In most studies it would be possible for the doctors performing the CTA test to already know the results of the clinical test. This might affect the study results, however this situation would also be the case in normal medical practice. Methods used to report the CTA study also varied from study to study and so the published results were re‐analysed to take this into account. When compared to clinical testing for brain death, the CTA test had a sensitivity of 0.85. This means that in 100 cases of patients satisfying the clinical tests for death, the CTA test will correctly identify 85 of the cases. The data also showed that this might be as few as 77 cases per 100 and as many as 91 cases per 100. Our review was unable to tell us how many patients the CTA might falsely give a diagnosis of death for, when the patient was not dead. Based on these results, it appears that CTA is not good enough to be a compulsory test.","3","John Wiley & Sons, Ltd","1465-1858","Brain Death [*diagnosis]; Cerebral Angiography [*methods]; Cohort Studies; Humans; Sensitivity and Specificity; Tomography, X‐Ray Computed [*methods]","10.1002/14651858.CD009694.pub2","http://dx.doi.org/10.1002/14651858.CD009694.pub2","Emergency and Critical Care"
"CD009020.PUB2","Lenza, M; Buchbinder, R; Takwoingi, Y; Johnston, RV; Hanchard, NCA; Faloppa, F","Magnetic resonance imaging, magnetic resonance arthrography and ultrasonography for assessing rotator cuff tears in people with shoulder pain for whom surgery is being considered","Cochrane Database of Systematic Reviews","2013","Abstract - Background Shoulder pain is a very common symptom. Disorders of the rotator cuff tendons due to wear or tear are among the most common causes of shoulder pain and disability. Magnetic resonance imaging (MRI), magnetic resonance arthrography (MRA) and ultrasound (US) are increasingly being used to assess the presence and size of rotator cuff tears to assist in planning surgical treatment. It is not known whether one imaging method is superior to any of the others. Objectives To compare the diagnostic test accuracy of MRI, MRA and US for detecting any rotator cuff tears (i.e. partial or full thickness) in people with suspected rotator cuff tears for whom surgery is being considered. Search methods We searched the Cochrane Register of Diagnostic Test Accuracy Studies, MEDLINE, EMBASE, and LILACS from inception to February 2011. We also searched trial registers, conference proceedings and reference lists of articles to identify additional studies. No language or publication restrictions were applied. Selection criteria We included all prospective diagnostic accuracy studies that assessed MRI, MRA or US against arthroscopy or open surgery as the reference standard, in people suspected of having a partial or full thickness rotator cuff tear. We excluded studies that selected a healthy control group, or participants who had been previously diagnosed with other specific causes of shoulder pain such as osteoarthritis or rheumatoid arthritis. Studies with an excessively long period (a year or longer) between the index and reference tests were also excluded. Data collection and analysis Two review authors independently extracted data on study characteristics and results of included studies, and performed quality assessment according to QUADAS criteria. Our unit of analysis was the shoulder. For each test, estimates of sensitivity and specificity from each study were plotted in ROC space and forest plots were constructed for visual examination of variation in test accuracy. Meta‐analyses were performed using the bivariate model to produce summary estimates of sensitivity and specificity. We were unable to formally investigate potential sources of heterogeneity because of the small number of studies. Main results We included 20 studies of people with suspected rotator cuff tears (1147 shoulders), of which six evaluated MRI and US (252 shoulders), or MRA and US (127 shoulders) in the same people. Many studies had design flaws, with the potential for bias, thus limiting the reliability of their findings. Overall, the methodological quality of the studies was judged to be low or unclear. For each test, we observed considerable heterogeneity in study results, especially between studies that evaluated US for the detection of full thickness tears and studies that evaluated MRA for the detection of partial thickness tears. The criteria for a positive diagnostic test (index tests and reference standard) varied between studies. Meta‐analyses were not possible for studies that assessed MRA for detection of any rotator cuff tears or partial thickness tears. We found no statistically significant differences in sensitivity or specificity between MRI and US for detecting any rotator cuff tears (P = 0.13), or for detecting partial thickness tears (P = 1.0). Similarly, for the comparison between MRI, MRA and US for detecting full thickness tears, there was no statistically significant difference in diagnostic performance (P = 0.7). For any rotator cuff tears, the summary sensitivity and specificity were 98% (95% CI 92% to 99%) and 79% (95% CI 68% to 87%) respectively for MRI (6 studies, 347 shoulders), and 91% (95% CI 83% to 95%) and 85% (95% CI 74% to 92%) respectively for US (13 studies, 854 shoulders). For full thickness tears, the summary sensitivity and specificity were 94% (95% CI 85% to 98%) and 93% (95% CI 83% to 97%) respectively for MRI (7 studies, 368 shoulders); 94% (95% CI 80% to 98%) and 92% (95% CI 83% to 97%) respectively for MRA (3 studies, 183 shoulders); and 92% (95% CI 82% to 96%) and 93% (95% CI 81% to 97%) respectively for US (10 studies, 729 shoulders). Because few studies were direct head‐to‐head comparisons, we could not perform meta‐analyses restricted to these studies. The test comparisons for each of the three classifications of the target condition were therefore based on indirect comparisons which may be prone to bias due to confounding. Authors' conclusions MRI, MRA and US have good diagnostic accuracy and any of these tests could equally be used for detection of full thickness tears in people with shoulder pain for whom surgery is being considered. The diagnostic performance of MRI and US may be similar for detection of any rotator cuff tears. However, both MRI and US may have poor sensitivity for detecting partial thickness tears, and the sensitivity of US may be much lower than that of MRI. The strength of evidence for all test comparisons is limited because most studies were small, heterogeneous and methodologically flawed, and there were few comparative studies. Well designed studies that directly compare MRI, MRA and US for detection of rotator cuff tears are needed. Plain language summary Diagnostic tests for assessing rotator cuff tears in people with shoulder pain for whom surgery is being considered This summary of a Cochrane review presents what we know from research about the accuracy of imaging tests to detect tears of the rotator cuff tendons in the shoulder. The rotator cuff is a group of tendons involved in the positioning and moving of the shoulder joint. The rotator cuff lets people lift their arm and reach overhead. In a lot of people, wear and tear of the rotator cuff tendons is a normal part of ageing and they may not have symptoms. However, many people will develop pain in their shoulder at some point as the tendons degenerate further and tears in the rotator cuff tendons develop. There may also be inflammation of the shoulder tendons or bursa (a sac with internal gliding surfaces that helps the shoulder to move). Often the pain is made worse by sleeping on the affected shoulder and moving the shoulder in certain directions. Often there will be pressure on the tendons by the overlying bone when lifting the arm up. This is called impingement. It may become difficult to use the shoulder in every day activities, sports or work. If the pain does not go away by itself or with treatments such as steroid injections or physiotherapy, surgery may be performed. Imaging tests such as magnetic resonance imaging (MRI), magnetic resonance arthrography (MRA) and ultrasound (US) are used to assess the presence and size of rotator cuff tears to assist in the planning of the surgery. Rotator cuff tears can be classified as full or partial thickness tears based on the extent or size of the tears. No test is 100% accurate in identifying tears or assessing their size. The accuracy of the tests is commonly assessed by the sensitivity of the test (the proportion of people who had a tear according to the test, among patients with tears), and specificity (the proportion of people without tears on the test, among patients with no tears). We searched electronic databases up to February 2011, as well as trial registers, conference proceedings and reference lists of articles, for studies comparing diagnostic tests for people with suspected rotator cuff tears. Our review included 20 studies (1147 shoulders). Many studies had design flaws, which limited the reliability of their findings. We found that MRI, MRA and US may have similar accuracy for detecting the presence of full thickness tears. For identifying any tears (no distinction between partial or full thickness) or identifying partial thickness tears, MRI and US may also have similar accuracy. However, it appears that compared with US, MRI may be more sensitive in identifying partial thickness tears. With these results we can conclude that all three imaging tests (MRI, MRA and US) may help decisions regarding referral for surgery for people with suspected full thickness tears. Information on adverse effects of using these tests was not reported by the included studies.","9","John Wiley & Sons, Ltd","1465-1858","*Arthroscopy; *Magnetic Resonance Imaging; *Rotator Cuff Injuries; *Ultrasonography; Arthrography [*methods]; Humans; Prospective Studies; Rotator Cuff [surgery]; Shoulder Pain [*etiology, surgery]","10.1002/14651858.CD009020.pub2","http://dx.doi.org/10.1002/14651858.CD009020.pub2","Bone, Joint and Muscle Trauma"
"CD007427.PUB2","Hanchard, NCA; Lenza, M; Handoll, HHG; Takwoingi, Y","Physical tests for shoulder impingements and local lesions of bursa, tendon or labrum that may accompany impingement","Cochrane Database of Systematic Reviews","2013","Abstract - Background Impingement is a common cause of shoulder pain. Impingement mechanisms may occur subacromially (under the coraco‐acromial arch) or internally (within the shoulder joint), and a number of secondary pathologies may be associated. These include subacromial‐subdeltoid bursitis (inflammation of the subacromial portion of the bursa, the subdeltoid portion, or both), tendinopathy or tears affecting the rotator cuff or the long head of biceps tendon, and glenoid labral damage. Accurate diagnosis based on physical tests would facilitate early optimisation of the clinical management approach. Most people with shoulder pain are diagnosed and managed in the primary care setting. Objectives To evaluate the diagnostic accuracy of physical tests for shoulder impingements (subacromial or internal) or local lesions of bursa, rotator cuff or labrum that may accompany impingement, in people whose symptoms and/or history suggest any of these disorders. Search methods We searched electronic databases for primary studies in two stages. In the first stage, we searched MEDLINE, EMBASE, CINAHL, AMED and DARE (all from inception to November 2005). In the second stage, we searched MEDLINE, EMBASE and AMED (2005 to 15 February 2010). Searches were delimited to articles written in English. Selection criteria We considered for inclusion diagnostic test accuracy studies that directly compared the accuracy of one or more physical index tests for shoulder impingement against a reference test in any clinical setting. We considered diagnostic test accuracy studies with cross‐sectional or cohort designs (retrospective or prospective), case‐control studies and randomised controlled trials. Data collection and analysis Two pairs of review authors independently performed study selection, assessed the study quality using QUADAS, and extracted data onto a purpose‐designed form, noting patient characteristics (including care setting), study design, index tests and reference standard, and the diagnostic 2 x 2 table. We presented information on sensitivities and specificities with 95% confidence intervals (95% CI) for the index tests. Meta‐analysis was not performed. Main results We included 33 studies involving 4002 shoulders in 3852 patients. Although 28 studies were prospective, study quality was still generally poor. Mainly reflecting the use of surgery as a reference test in most studies, all but two studies were judged as not meeting the criteria for having a representative spectrum of patients. However, even these two studies only partly recruited from primary care. The target conditions assessed in the 33 studies were grouped under five main categories: subacromial or internal impingement, rotator cuff tendinopathy or tears, long head of biceps tendinopathy or tears, glenoid labral lesions and multiple undifferentiated target conditions. The majority of studies used arthroscopic surgery as the reference standard. Eight studies utilised reference standards which were potentially applicable to primary care (local anaesthesia, one study; ultrasound, three studies) or the hospital outpatient setting (magnetic resonance imaging, four studies). One study used a variety of reference standards, some applicable to primary care or the hospital outpatient setting. In two of these studies the reference standard used was acceptable for identifying the target condition, but in six it was only partially so. The studies evaluated numerous standard, modified, or combination index tests and 14 novel index tests. There were 170 target condition/index test combinations, but only six instances of any index test being performed and interpreted similarly in two studies. Only two studies of a modified empty can test for full thickness tear of the rotator cuff, and two studies of a modified anterior slide test for type II superior labrum anterior to posterior (SLAP) lesions, were clinically homogenous. Due to the limited number of studies, meta‐analyses were considered inappropriate. Sensitivity and specificity estimates from each study are presented on forest plots for the 170 target condition/index test combinations grouped according to target condition. Authors' conclusions There is insufficient evidence upon which to base selection of physical tests for shoulder impingements, and local lesions of bursa, tendon or labrum that may accompany impingement, in primary care. The large body of literature revealed extreme diversity in the performance and interpretation of tests, which hinders synthesis of the evidence and/or clinical applicability. Plain language summary Physical tests for shoulder impingement in primary care Impingement (or pinching) of soft‐tissues in or around the shoulder is a common cause of pain and is often linked to tissue damage in and around the joint. If doctors and therapists could identify impingement and associated damage using simple, physical tests, it would help them to inform on the best treatment approach at an early stage. We were particularly interested in the primary (community) care setting, because this is where most shoulder pain is diagnosed and managed. We reviewed original research papers for evidence on the accuracy of physical tests for shoulder impingement or associated damage, in people whose symptoms and/or history suggest any of these disorders. To find the research papers, we searched the main electronic databases of medical and allied literature up to 2010. Two review authors screened assessed the quality of each research paper and extracted important information. If multiple research papers reported using the same test for the same condition, we intended to combine their results to gain a more precise estimate of the test's accuracy. We included 33 research papers. These related to studies of 4002 shoulders in 3852 patients. None of the studies exclusively looked at patients from primary care, though two recruited some of their patients from primary care. The majority of studies used arthroscopic surgery as the reference standard. There were 170 different target condition/index test combinations but only six instances where the same test was used in the same way, and for the same reason, in two studies. For this reason combining results was not appropriate. We concluded that there is insufficient evidence upon which to base selection of physical tests for shoulder impingement, and potentially related conditions, in primary care.","4","John Wiley & Sons, Ltd","1465-1858","Arthroscopy; Bursa, Synovial [injuries]; Bursitis [*diagnosis]; Glenoid Cavity; Humans; Joint Instability [diagnosis]; Physical Examination [*methods]; Prospective Studies; Randomized Controlled Trials as Topic; Rotator Cuff Injuries; Rupture [diagnosis]; Shoulder Impingement Syndrome [*diagnosis]; Tendinopathy [*diagnosis]","10.1002/14651858.CD007427.pub2","http://dx.doi.org/10.1002/14651858.CD007427.pub2","Bone, Joint and Muscle Trauma"
"CD008054.PUB2","Arbyn, M; Roelens, J; Simoens, C; Buntinx, F; Paraskevaidis, E; Martin-Hirsch, PPL; Prendiville, WJ","Human papillomavirus testing versus repeat cytology for triage of minor cytological cervical lesions","Cochrane Database of Systematic Reviews","2013","Abstract - Background Atypical squamous cells of undetermined significance (ASCUS) and low‐grade squamous intra‐epithelial lesions (LSIL) are minor lesions of the cervical epithelium, detectable by cytological examination of cells collected from the surface of the cervix of a woman. Usually, women with ASCUS and LSIL do not have cervical (pre‐) cancer, however a substantial proportion of them do have underlying high‐grade cervical intra‐epithelial neoplasia (CIN, grade 2 or 3) and so are at increased risk for developing cervical cancer. Therefore, accurate triage of women with ASCUS or LSIL is required to identify those who need further management. This review evaluates two ways to triage women with ASCUS or LSIL: repeating the cytological test, and DNA testing for high‐risk types of the human papillomavirus (hrHPV) ‐ the main causal factor of cervical cancer. Objectives Main objective To compare the accuracy of hrHPV testing with the Hybrid Capture 2 (HC2) assay against that of repeat cytology for detection of underlying cervical intraepithelial neoplasia of grade 2 or worse (CIN2+) or grade 3 or worse (CIN3+) in women with ASCUS or LSIL. For the HC2 assay, a positive result was defined as proposed by the manufacturer. For repeat cytology, different cut‐offs were used to define positivity: Atypical squamous cells of undetermined significance or worse (ASCUS+), low‐grade squamous intra‐epithelial lesions or worse (LSIL+) or high‐grade squamous intra‐epithelial lesions or worse (HSIL+). Secondary objective To assess the accuracy of the HC2 assay to detect CIN2+ or CIN3+ in women with ASCUS or LSIL in a larger group of reports of studies that applied hrHPV testing and the reference standard (coloscopy and biopsy), irrespective whether or not repeat cytology was done. Search methods We made a comprehensive literature search that included the Cochrane Register of Diagnostic Test Accuracy Studies; the Cochrane Central Register of Controlled Trials (CENTRAL) ( The Cochrane Library ), MEDLINE (through PubMed), and EMBASE (last search 6 January 2011).  Selected journals likely to contain relevant papers were handsearched from 1992 to 2010 (December). We also searched CERVIX, the bibliographic database of the Unit of Cancer Epidemiology at the Scientific Institute of Public Health (Brussels, Belgium) which contains more than 20,000 references on cervical cancer. More recent searches, up to December 2012, targeted reports on the accuracy of triage of ASCUS or LSIL with other HPV DNA assays, or HPV RNA assays and other molecular markers. These searches will be used for new Cochrane reviews as well as for updates of the current review. Selection criteria Studies eligible for inclusion in the review had to include: women presenting with a cervical cytology result of ASCUS or LSIL, who had undergone both HC2 testing and repeat cytology, or HC2 testing alone, and were subsequently subjected to reference standard verification with colposcopy and colposcopy‐directed biopsies for histologic verification. Data collection and analysis The review authors independently extracted data from the selected studies, and obtained additional data from report authors. Two groups of meta‐analyses were performed: group I concerned triage of women with ASCUS, group II concerned women with LSIL.   The bivariate model (METADAS‐macro in SAS) was used to assess the absolute accuracy of the triage tests in both groups as well as the differences in accuracy between the triage tests. Main results The pooled sensitivity of HC2 was significantly higher than that of repeat cytology at cut‐off ASCUS+ to detect CIN2+ in both triage of ASCUS and LSIL (relative sensitivity of 1.27 (95% CI 1.16 to 1.39; P value < 0.0001) and 1.23 (95% CI 1.06 to 1.4; P value 0.007), respectively. In ASCUS triage, the pooled specificity of the triage methods did not differ significantly from each other (relative specificity: 0.99 (95% CI 0.97 to 1.03; P value 0.98)). However, the specificity of HC2 was substantially, and significantly, lower than that of repeat cytology in the triage of LSIL (relative specificity: 0.66 (95% CI 0.58 to 0.75) P value < 0.0001). Authors' conclusions HPV‐triage with HC2 can be recommended to triage women with ASCUS because it has higher accuracy (significantly higher sensitivity, and similar specificity) than repeat cytology. When triaging women with LSIL, an HC2 test yields a significantly higher sensitivity, but a significantly lower specificity, compared to a repeat cytology. Therefore, practice recommendations for management of women with LSIL should be balanced, taking local circumstances into account.","3","John Wiley & Sons, Ltd","1465-1858","DNA, Viral [*isolation & purification]; Female; Humans; Papillomaviridae [*genetics]; Papillomavirus Infections [*diagnosis]; Sensitivity and Specificity; Triage [*methods]; Uterine Cervical Dysplasia [*diagnosis, virology]; Uterine Cervical Neoplasms [*diagnosis, virology]; Vaginal Smears [*methods]","10.1002/14651858.CD008054.pub2","http://dx.doi.org/10.1002/14651858.CD008054.pub2","Gynaecological, Neuro-oncology and Orphan Cancer"
"CD008686.PUB2","Henschke, N; Maher, CG; Ostelo, RWJG; de Vet, HCW; Macaskill, P; Irwig, L","Red flags to screen for malignancy in patients with low‐back pain","Cochrane Database of Systematic Reviews","2013","Abstract - Background The identification of serious pathologies, such as spinal malignancy, is one of the primary purposes of the clinical assessment of patients with low‐back pain (LBP). Clinical guidelines recommend awareness of ""red flag"" features from the patient's clinical history and physical examination to achieve this. However, there are limited empirical data on the diagnostic accuracy of these features and there remains very little information on how best to use them in clinical practice. Objectives To assess the diagnostic performance of clinical characteristics identified by taking a clinical history and conducting a physical examination (""red flags"") to screen for spinal malignancy in patients presenting with LBP. Search methods We searched electronic databases for primary studies (MEDLINE, EMBASE, and CINAHL) and systematic reviews (PubMed and Medion) from the earliest date until 1 April 2012. Forward and backward citation searching of eligible articles was also performed. Selection criteria We considered studies if they compared the results of history taking and physical examination on patients with LBP with those of diagnostic imaging (magnetic resonance imaging, computed tomography, myelography). Data collection and analysis Two review authors independently assessed the quality of each included study with the QUality Assessment of Diagnostic Accuracy Studies (QUADAS) tool and extracted details on patient characteristics, study design, index tests, and reference standard. Diagnostic accuracy data were presented as sensitivities and specificities with 95% confidence intervals for all index tests. Main results We included eight cohort studies of which six were performed in primary care (total number of patients; n = 6622), one study was from an accident and emergency setting (n = 482), and one study was from a secondary care setting (n = 257). In the six primary care studies, the prevalence of spinal malignancy ranged from 0% to 0.66%. Overall, data from 20 index tests were extracted and presented, however only seven of these were evaluated by more than one study. Because of the limited number of studies and clinical heterogeneity, statistical pooling of diagnostic accuracy data was not performed. There was some evidence from individual studies that having a previous history of cancer meaningfully increases the probability of malignancy. Most ""red flags"" such as insidious onset, age > 50, and failure to improve after one month have high false positive rates. All of the tests were evaluated in isolation and no study presented data on a combination of positive tests to identify spinal malignancy. Authors' conclusions For most ""red flags,"" there is insufficient evidence to provide recommendations regarding their diagnostic accuracy or usefulness for detecting spinal malignancy. The available evidence indicates that in patients with LBP, an indication of spinal malignancy should not be based on the results of one single ""red flag"" question. Further research to evaluate the performance of different combinations of tests is recommended. Plain language summary Physician use of red flags to screen for cancer in patients with new back pain This review describes the understanding of a common practice for checking for spinal injuries when patients come to a family practice doctor, back pain clinic or emergency room with new back pain.  Doctors usually ask a few questions and examine the back to check for the possibility of a spinal tumor.  The reason for this check for tumors is that the treatment is different for common back pain and tumors.  Tumors are usually diagnosed with an x‐ray, magnetic resonance imaging (MRI) or computed tomography (CT), then treated with surgery and/or chemotherapy. Common back pain is treated with exercise, spinal manipulation, and pain relievers; x‐rays, CT and MRI scans are not useful for diagnosis.  Tumors are rare, being the cause of back pain in approximately 1% of new back pain visits to family doctors.  Only about 10% of these cancers are new cases; 90% are recurrences of cancers from other parts of the body (metastases). Six family practice studies including over 6,600 back pain patients found 21 tumors (0.3%).  One study on back pain diagnosed in an emergency room and one on back pain in a spine clinic included 482 and 257 patients.  The family practice studies described 15 different questions and physical exam tests that have been used to screen for spinal tumors.  Most of the 15 were not accurate.  A previous history of cancer is a very useful indicator.  Other facts that may indicate cancer are age greater than 50, no prior history of back pain, and failure to improve after one month.  These are most likely useful when combined, or with other indicators such as a history of cancer.  By themselves, these three questions would result in over‐testing of patients without cancer. The worst effects of low quality red flag screening are overtreatment and undertreatment.  If the tests are not accurate, patients without a tumor may get an x‐ray, MRI, bone scan or CT scan that they don’t need—unnecessary exposure to x‐rays, extra worry for the patient and extra cost.  At the other extreme (and much less common), it might be possible to miss a real tumor, and cause the patient to have extra time without the best treatment. Most of the studies were of low or moderate quality and did not use an MRI, the most accurate imaging test, to confirm the presence or absence of a tumor, so more research is needed to identify the best combination of questions and examination methods.","2","John Wiley & Sons, Ltd","1465-1858","*Physical Examination; Cohort Studies; Confidence Intervals; Humans; Low Back Pain [*etiology]; Medical History Taking; Middle Aged; Sensitivity and Specificity; Spinal Neoplasms [complications, *diagnosis]","10.1002/14651858.CD008686.pub2","http://dx.doi.org/10.1002/14651858.CD008686.pub2","Back and Neck"
"CD009175.PUB2","Wang, K; Gill, P; Perera, R; Thomson, A; Mant, D; Harnden, A","Clinical symptoms and signs for the diagnosis of   Mycoplasma pneumoniae in children and adolescents with community‐acquired pneumonia","Cochrane Database of Systematic Reviews","2012","Abstract - Background Mycoplasma pneumoniae  ( M. pneumoniae ) is a significant cause of community‐acquired pneumonia in children and adolescents. Treatment with macrolide antibiotics is recommended. However,  M. pneumoniae  is difficult to diagnose based on clinical symptoms and signs. Diagnostic uncertainty can lead to inappropriate antibiotic prescribing, which may worsen clinical prognosis and increase antibiotic resistance. Objectives The objectives of this review are (i) to assess the diagnostic accuracy of symptoms and signs in the clinical recognition of  M. pneumoniae  in children and adolescents with community‐acquired pneumonia; and (ii) to assess the influence of potential sources of heterogeneity on the diagnostic accuracy of symptoms and signs in the clinical recognition of  M. pneumoniae . Search methods We searched MEDLINE (January 1950 to 26 June 2012) and EMBASE (January 1980 to 26 June 2012). We identified additional references by handsearching the reference lists of included articles and snowballing. We searched the reference lists of relevant systematic reviews identified by searching the Medion database, Database of Reviews of Effects 2012, Issue 6 (25 June 2012) and the Cochrane Register of Diagnostic Test Accuracy studies (2 July 2012). Experts in the field reviewed our list of included studies for any obvious omissions. Selection criteria We included peer‐reviewed published studies which prospectively and consecutively recruited children with community‐acquired pneumonia from any healthcare setting, confirmed the presence of  M. pneumoniae  using serology with or without other laboratory methods and reported data on clinical symptoms and signs in sufficient detail to construct 2 x 2 tables. Data collection and analysis One review author scanned titles to exclude obviously irrelevant articles. Two review authors independently scanned the remaining titles and abstracts, reviewed full‐text versions of potentially relevant articles, assessed the quality of included articles and extracted data on study characteristics and the following clinical features: cough, wheeze, coryza, crepitations, fever, rhonchi, shortness of breath, chest pain, diarrhea, myalgia and headache. We calculated study‐specific values for sensitivity, specificity and positive and negative likelihood ratios with 95% confidence intervals (CIs). We estimated the post‐test probability of  M. pneumoniae  based on the absence or presence of symptoms and signs. We calculated pooled sensitivities, specificities, positive and negative likelihood ratios with 95% CIs for symptoms and signs where data were reported by at least four included studies by fitting a bivariate normal model for the logit transforms of sensitivity and specificity. We explored potential sources of heterogeneity by fitting bivariate models with covariates using multi‐level mixed‐effects logistic regression. We performed sensitivity analyses excluding data from studies for which we were concerned about the representativeness of the study population and/or the acceptability of the reference standard. Main results Our search identified 8299 articles (excluding duplicates). We examined the titles and abstracts of 1125 articles and the full‐text versions of 97 articles. We included seven studies in our review, which reported data from 1491 children; all were conducted in hospital settings. Overall, study quality was moderate. In two studies the presence of chest pain more than doubled the probability of  M. pneumoniae . Wheeze was 12% more likely to be absent in children with  M. pneumoniae  (pooled positive likelihood ratio (LR+) 0.76, 95% CI 0.60 to 0.97; pooled negative likelihood ratio (LR‐) 1.12, 95% CI 1.02 to 1.23). Our sensitivity analysis showed that the presence of crepitations was associated with  M. pneumoniae , but this finding was of borderline statistical significance (pooled LR+ 1.10, 95% CI 0.99 to 1.23; pooled LR‐ 0.66, 95% CI 0.46 to 0.96). Authors' conclusions M. pneumoniae  cannot be reliably diagnosed in children and adolescents with community‐acquired pneumonia based on clinical symptoms and signs. Although the absence of wheeze is a statistically significant diagnostic indicator, it does not have sufficient diagnostic value to guide empirical macrolide treatment. Data from two studies suggest that the presence of chest pain more than doubles the probability of  M. pneumoniae . However, further research is needed to substantiate this finding. More high quality large‐scale studies in primary care settings are needed to help develop prediction rules based on epidemiological data as well as clinical and baseline patient characteristics. Plain language summary Clinical symptoms and signs for the diagnosis of  Mycoplasma pneumoniae  in children and adolescents with community‐acquired pneumonia Chest infections are among the commonest reasons why children and young people go to see their doctor or nurse. At the moment, it is difficult for doctors and nurses to tell patients what type of infection they have based on their symptoms and signs. This can result in antibiotics being prescribed or withheld inappropriately.  Mycoplasma pneumoniae  ( M. pneumoniae ) is an important bacterial cause of chest infections in children and adolescents. This review assesses the value of clinical symptoms and signs in helping doctors and nurses decide whether a child or young person might have a chest infection caused by  M. pneumoniae . We analysed data from seven studies including a total of 1491 children, all of which were conducted in hospital settings. We found that the presence of wheeze makes  M. pneumoniae  slightly less likely and the presence of crepitations ( i.e.  crackles heard on listening to the chest) makes  M. pneumoniae  slightly more likely. However, these clinical features are not sufficiently helpful to guide decisions about prescribing antibiotics for possible  M. pneumoniae  infections. Based on the results of two studies, the presence of chest pain doubles the likelihood of  M. pneumoniae . However, further research in this area is needed, particularly in general practice and outpatient populations.","10","John Wiley & Sons, Ltd","1465-1858","*Mycoplasma pneumoniae; Adolescent; Child; Community‐Acquired Infections [diagnosis, microbiology]; Humans; Pneumonia, Mycoplasma [*diagnosis]; Randomized Controlled Trials as Topic; Respiratory Sounds; Symptom Assessment [*methods]","10.1002/14651858.CD009175.pub2","http://dx.doi.org/10.1002/14651858.CD009175.pub2","Acute Respiratory Infections"
"CD009925","Alldred, SK; Deeks, JJ; Guo, B; Neilson, JP; Alfirevic, Z","Second trimester serum tests for Down's Syndrome screening","Cochrane Database of Systematic Reviews","2012","Abstract - Background Down's syndrome occurs when a person has three copies of chromosome 21 ‐ or the specific area of chromosome 21 implicated in causing Down's syndrome ‐ rather than two. It is the commonest congenital cause of mental retardation. Noninvasive screening based on biochemical analysis of maternal serum or urine, or fetal ultrasound measurements, allows estimates of the risk of a pregnancy being affected and provides information to guide decisions about definitive testing.   Objectives To estimate and compare the accuracy of second trimester serum markers for the detection of Down’s syndrome. Search methods We carried out a sensitive and comprehensive literature search of MEDLINE (1980 to May 2007), EMBASE (1980 to 18 May 2007), BIOSIS via EDINA (1985 to 18 May 2007), CINAHL via OVID (1982 to 18 May 2007), The Database of Abstracts of Reviews of Effectiveness ( The Cochrane Library  2007, Issue 1), MEDION (May 2007), The Database of Systematic Reviews and Meta‐Analyses in Laboratory Medicine (May 2007), The National Research Register (May 2007), Health Services Research Projects in Progress database (May 2007). We studied reference lists and published review articles.   Selection criteria Studies evaluating tests of maternal serum in women at 14‐24 weeks of gestation for Down's syndrome, compared with a reference standard, either chromosomal verification or macroscopic postnatal inspection. Data collection and analysis Data were extracted as test positive/test negative results for Down's and non‐Down's pregnancies allowing estimation of detection rates (sensitivity) and false positive rates (1‐specificity). We performed quality assessment according to QUADAS criteria. We used hierarchical summary ROC meta‐analytical methods to analyse test performance and compare test accuracy. Analysis of studies allowing direct comparison between tests was undertaken. We investigated the impact of maternal age on test performance in subgroup analyses. Main results Fifty‐nine studies involving 341,261 pregnancies (including 1,994 with Down's syndrome) were included. Studies were generally high quality, although differential verification was common with invasive testing of only high‐risk pregnancies. Seventeen studies made direct comparisons between tests. Fifty‐four test combinations were evaluated formed from combinations of 12 different tests and maternal age; alpha‐fetoprotein (AFP), unconjugated oestriol (uE3), total human chorionic gonadotrophin (hCG), free beta human chorionic gonadotrophin (βhCG), free alpha human chorionic gonadotrophin (αhCG), Inhibin A, SP2, CA125, troponin, pregnancy‐associated plasma protein A (PAPP‐A), placental growth factor (PGF) and proform of eosinophil major basic protein (ProMBP). Meta‐analysis of 12 best performing or frequently evaluated test combinations showed double and triple tests (involving AFP, uE3, total hCG, free βhCG) significantly outperform individual markers, detecting six to seven out of every 10 Down's syndrome pregnancies at a 5% false positive rate. Tests additionally involving inhibin performed best (eight out of every 10 Down's syndrome pregnancies) but were not shown to be significantly better than standard triple tests in direct comparisons. Significantly lower sensitivity occurred in women over the age of 35 years. Women who miscarried in the over 35 group were more likely to have been offered an invasive test to verify a negative screening results, whereas those under 35 were usually not offered invasive testing for a negative screening result. Pregnancy loss in women under 35 therefore leads to under ascertainment of screening results, potentially missing a proportion of affected pregnancies and affecting the accuracy of the sensitivity. Authors' conclusions Tests involving two or more markers in combination with maternal age are significantly more sensitive than those involving one marker. The value of combining four or more tests or including inhibin have not been proven to show statistically significant improvement. Further study is required to investigate reduced test performance in women aged over 35 and the impact of differential pregnancy loss on study findings. Plain language summary [Summary title] [Summary text]","6","John Wiley & Sons, Ltd","1465-1858","Adult; Biomarkers [*blood]; Down Syndrome [*diagnosis]; Female; Humans; Maternal Age; Pregnancy; Pregnancy Trimester, Second [blood]; Prenatal Diagnosis [*methods]; Randomized Controlled Trials as Topic","10.1002/14651858.CD009925","http://dx.doi.org/10.1002/14651858.CD009925","Pregnancy and Childbirth"
"CD008691.PUB2","Wang, LW; Fahim, MA; Hayen, A; Mitchell, RL; Baines, L; Lord, S; Craig, JC; Webster, AC","Cardiac testing for coronary artery disease in potential kidney transplant recipients","Cochrane Database of Systematic Reviews","2011","Abstract - Background Patients with chronic kidney disease (CKD) are at increased risk of coronary artery disease (CAD) and adverse cardiac events. Screening for CAD is therefore an important part of preoperative evaluation for kidney transplant candidates. There is significant interest in the role of non‐invasive cardiac investigations and their ability to identify patients at high risk of CAD.  Objectives We investigated the accuracy of non‐invasive cardiac screening tests compared with coronary angiography to detect CAD in patients who are potential kidney transplant recipients. Search methods MEDLINE and EMBASE searches (inception to November 2010) were performed to identify studies that assessed the diagnostic accuracy of non‐invasive screening tests, using coronary angiography as the reference standard. We also conducted citation tracking via Web of Science and handsearched reference lists of identified primary studies and review articles.   Selection criteria We included in this review all diagnostic cross sectional, cohort and randomised studies of test accuracy that compared the results of any cardiac test with coronary angiography (the reference standard) relating to patients considered as potential candidates for kidney transplantation or kidney‐pancreas transplantation at the time diagnostic tests were performed.  Data collection and analysis We used a hierarchical modelling strategy to produce summary receiver operating characteristic (SROC) curves, and pooled estimates of sensitivity and specificity. Sensitivity analyses to determine test accuracy were performed if only studies that had full verification or applied a threshold of ≥ 70% stenosis on coronary angiography for the diagnosis of significant CAD were included. Main results The following screening investigations included in the meta‐analysis were: dobutamine stress echocardiography (DSE) (13 studies), myocardial perfusion scintigraphy (MPS) (nine studies), echocardiography (three studies), exercise stress electrocardiography (two studies), resting electrocardiography (three studies), and one study each of electron beam computed tomography (EBCT), exercise ventriculography, carotid intimal media thickness (CIMT) and digital subtraction fluorography (DSF). Sufficient studies were present to allow hierarchical summary receiver operating characteristic (HSROC) analysis for DSE and MPS. When including all available studies, both DSE and MPS had moderate sensitivity and specificity in detecting coronary artery stenosis in patients who are kidney transplant candidates [DSE (13 studies) ‐ pooled sensitivity 0.79 (95% CI 0.67 to 0.88), pooled specificity 0.89 (95% CI 0.81 to 0.94); MPS (nine studies) ‐ pooled sensitivity 0.74 (95% CI 0.54 to 0.87), pooled specificity 0.70 (95% CI 0.51 to 0.84)]. When limiting to studies which defined coronary artery stenosis using a reference threshold of ≥ 70% stenosis on coronary angiography, there was little change in these pooled estimates of accuracy [DSE (9 studies) ‐ pooled sensitivity 0.76 (95% CI 0.60 to 0.87), specificity 0.88 (95% CI 0.78 to 0.94); MPS (7 studies) ‐ pooled sensitivity 0.67 (95% CI 0.48 to 0.82), pooled specificity 0.77 (95% CI 0.61 to 0.88)]. There was evidence that DSE had improved accuracy over MPS (P = 0.02) when all studies were included in the analysis, but this was not significant when we excluded studies which did not avoid partial verification or use a reference standard threshold of ≥70% stenosis (P = 0.09).   Authors' conclusions DSE may perform better than MPS but additional studies directly comparing these cardiac screening tests are needed. Absence of significant CAD may not necessarily correlate with cardiac‐event free survival following transplantation. Further research should focus on assessing the ability of functional tests to predict postoperative outcome. Plain language summary [Summary title] [Summary text]","12","John Wiley & Sons, Ltd","1465-1858","*Kidney Transplantation; Coronary Angiography [*standards]; Coronary Artery Disease [*diagnosis, etiology]; Heart Function Tests [*methods, standards]; Humans; Kidney Failure, Chronic [*complications]; Pancreas Transplantation; Reference Standards","10.1002/14651858.CD008691.pub2","http://dx.doi.org/10.1002/14651858.CD008691.pub2","Kidney and Transplant"
"CD008122.PUB2","Abba, K; Deeks, JJ; Olliaro, PL; Naing, CM; Jackson, SM; Takwoingi, Y; Donegan, S; Garner, P","Rapid diagnostic tests for diagnosing uncomplicated   P. falciparum malaria in endemic countries","Cochrane Database of Systematic Reviews","2011","Abstract - Background Rapid diagnostic tests (RDTs) for  Plasmodium falciparum  malaria use antibodies to detect either HRP‐2 antigen or pLDH antigen, and can improve access to diagnostics in developing countries. Objectives To assess the diagnostic accuracy of RDTs for detecting  P. falciparum  parasitaemia in persons living in endemic areas who present to ambulatory healthcare facilities with symptoms suggestive of malaria by type and brand. Search methods We undertook a comprehensive search of the following databases: Cochrane Infectious Diseases Group Specialized Register; MEDLINE; EMBASE; MEDION; Science Citation Index; Web of Knowledge; African Index Medicus; LILACS; IndMED; to January 14, 2010. Selection criteria Studies comparing RDTs with a reference standard (microscopy or polymerase chain reaction) in blood samples from a random or consecutive series of patients attending ambulatory health facilities with symptoms suggestive of malaria in  P. falciparum  endemic areas. Data collection and analysis For each study, a standard set of data was extracted independently by two authors, using a tailored data extraction form. Comparisons were grouped hierarchically by target antigen, and type and brand of RDT, and combined in meta‐analysis where appropriate. Main results We identified 74 unique studies as eligible for this review and categorized them according to the antigens they detected. Types 1 to 3 include HRP‐2 (from P. falciparum)  either by itself or with other antigens. Types 4 and 5 included pLDH (from  P. falciparum)  either by itself or with other antigens. In comparisons with microscopy, we identified 71 evaluations of Type 1 tests, eight evaluations of Type 2 tests and five evaluations of Type 3 tests. In meta‐analyses, average sensitivities and specificities (95% CI) were 94.8% (93.1% to 96.1%) and 95.2% (93.2% to 96.7%) for Type 1 tests, 96.0% (94.0% to 97.3%) and 95.3% (87.3% to 98.3%) for Type 2 tests, and 99.5% (71.0% to 100.0%) and 90.6% (80.5% to 95.7%) for Type 3 tests, respectively.  Overall for HRP‐2, the meta‐analytical average sensitivity and specificity (95% CI) were 95.0% (93.5% to 96.2%) and 95.2% (93.4% to 99.4%), respectively.  For pLDH antibody‐based RDTs verified with microscopy, we identified 17 evaluations of Type 4 RDTs and three evaluations of Type 5 RDTs. In meta‐analyses, average sensitivity for Type 4 tests was 91.5% (84.7% to 95.3%) and average specificity was 98.7% (96.9% to 99.5%). For Type 5 tests, average sensitivity was 98.4% (95.1% to 99.5%) and average specificity was 97.5% (93.5% to 99.1%).  Overall for pLDH, the meta‐analytical average sensitivity and specificity (95% CI) were 93.2% (88.0% to 96.2%) and 98.5% (96.7% to 99.4%), respectively.  For both categories of test, there was substantial heterogeneity in study results. Quality of the microscopy reference standard could only be assessed in 40% of studies due to inadequate reporting, but results did not seem to be influenced by the reporting quality. Overall, HRP‐2 antibody‐based tests (such as the Type 1 tests) tended to be more sensitive and were significantly less specific than pLDH‐based tests (such as the Type 4 tests). If the point estimates for Type 1 and Type 4 tests are applied to a hypothetical cohort of 1000 patients where 30% of those presenting with symptoms have  P. falciparum , Type 1 tests will miss 16 cases, and Type 4 tests will miss 26 cases. The number of people wrongly diagnosed with  P. falciparum  would be 34 with Type 1 tests, and nine with Type 4 tests. Authors' conclusions The sensitivity and specificity of all RDTs is such that they can replace or extend the access of diagnostic services for uncomplicated  P. falciparum  malaria. HRP‐2 antibody types may be more sensitive but are less specific than pLDH antibody‐based tests, but the differences are small. The HRP‐2 antigen persists even after effective treatment and so is not useful for detecting treatment failures.   23 April 2019 No update planned Other Good evidence of benefit and further research is unlikely to change our confidence in the estimates of test accuracy. All eligible published studies found in the last search (14 Jan, 2010) were included. Plain language summary Rapid diagnostic tests for diagnosing malaria Fever is common in malarial areas, and getting the diagnosis right (ie deciding if it is due to malaria or other causes) and treating correctly helps save lives, particularly in children. The World Health Organization (WHO) now recommends that all patients with fever suspected of being malaria are properly diagnosed before any treatment begins. This ensures that highly effective antimalarial drugs such as artemisinin‐based combination treatments (ACTs) are properly used to prevent unnecessary treatments, untoward risks, and resistance developing. Standard diagnosis of malaria in the past has depended on blood microscopy, but this requires a technician and a laboratory, and is often not feasible for basic health services in many areas. Sometimes in research studies, another technique called polymerase chain reaction (PCR) is used, but again this requires equipment and trained staff, and cannot be used routinely. Technological advances have led to rapid diagnostic tests (RDTs) for malaria. These detect parasite‐specific antigens in the blood, are simple to use, and can give results as a simple positive or negative result, within 15 minutes. This review evaluates the accuracy of RDTs compared with microscopy and PCR for detecting  Plasmodium falciparum  parasites in the blood. It includes 74 studies, giving a total of 111 RDT evaluations (of which 104 compared RDTs with microscopy), reporting a total of 60,396 RDT results. Results are presented by type of test, classified by the malaria antigen that they are designed to detect (either histidine‐rich protein‐2 (HRP‐2), or plasmodium lactate dehydrogenase (pLDH)). The results indicate that RDTs can be very accurate compared to microscopy and PCR. The performance of RDT types varied but the differences were not large. HRP‐2‐based tests tended to be more sensitive (ie they identified more true cases of malaria) and less specific (ie they wrongly identified more malaria that was not present) than pLDH‐based tests. Choice will depend on prevalence of malaria, and we provide data in this review to assist these decisions, although policy makers will also take into account other factors relating to cost and test stability.","7","John Wiley & Sons, Ltd","1465-1858","*Endemic Diseases; *Plasmodium falciparum [enzymology, immunology]; Antigens, Protozoan [analysis]; Biomarkers [analysis]; Diagnostic Tests, Routine [*methods]; Humans; L-Lactate Dehydrogenase [analysis]; Malaria, Falciparum [*diagnosis, epidemiology, immunology]; Parasitemia [*diagnosis, epidemiology, immunology]; Protozoan Proteins [analysis]; Sensitivity and Specificity","10.1002/14651858.CD008122.pub2","http://dx.doi.org/10.1002/14651858.CD008122.pub2","Infectious Diseases"
"CD007431.PUB2","van der Windt, DAWM; Simons, E; Riphagen, II; Ammendolia, C; Verhagen, AP; Laslett, M; Devillé, W; Deyo, RA; Bouter, LM; de Vet, HCW; Aertgeerts, B","Physical examination for lumbar radiculopathy due to disc herniation in patients with low‐back pain","Cochrane Database of Systematic Reviews","2010","Abstract - Background Low‐back pain with leg pain (sciatica) may be caused by a herniated intervertebral disc exerting pressure on the nerve root. Most patients will respond to conservative treatment, but in carefully selected patients, surgical discectomy may provide faster relief of symptoms. Primary care clinicians use patient history and physical examination to evaluate the likelihood of disc herniation and select patients for further imaging and possible surgery. Objectives (1) To assess the performance of tests performed during physical examination (alone or in combination) to identify radiculopathy due to lower lumbar disc herniation in patients with low‐back pain and sciatica; (2) To assess the influence of sources of heterogeneity on diagnostic performance. Search methods We searched electronic databases for primary studies: PubMed (includes MEDLINE), EMBASE, and CINAHL, and (systematic) reviews: PubMed and Medion (all from earliest until 30 April 2008), and checked references of retrieved articles. Selection criteria We considered studies if they compared the results of tests performed during physical examination on patients with back pain with those of diagnostic imaging (MRI, CT, myelography) or findings at surgery. Data collection and analysis Two review authors assessed the quality of each publication with the QUADAS tool, and extracted details on patient and study design characteristics, index tests and reference standard, and the diagnostic two‐by‐two table. We presented information on sensitivities and specificities with 95% confidence intervals (95% CI) for all aspects of physical examination. Pooled estimates of sensitivity and specificity were computed for subsets of studies showing sufficient clinical and statistical homogeneity. Main results We included 16 cohort studies (median N = 126, range 71 to 2504) and three case control studies (38 to100 cases). Only one study was carried out in a primary care population. When used in isolation, diagnostic performance of most physical tests (scoliosis, paresis or muscle weakness, muscle wasting, impaired reflexes, sensory deficits) was poor. Some tests (forward flexion, hyper‐extension test, and slump test) performed slightly better, but the number of studies was small. In the one primary care study, most tests showed higher specificity and lower sensitivity compared to other settings. Most studies assessed the Straight Leg Raising (SLR) test. In surgical populations, characterized by a high prevalence of disc herniation (58% to 98%), the SLR showed high sensitivity (pooled estimate 0.92, 95% CI: 0.87 to 0.95) with widely varying specificity (0.10 to 1.00, pooled estimate 0.28, 95% CI: 0.18 to 0.40). Results of studies using imaging showed more heterogeneity and poorer sensitivity. The crossed SLR showed high specificity (pooled estimate 0.90, 95% CI: 0.85 to 0.94) with consistently low sensitivity (pooled estimate 0.28, 95% CI: 0.22 to 0.35). Combining positive test results increased the specificity of physical tests, but few studies presented data on test combinations. Authors' conclusions When used in isolation, current evidence indicates poor diagnostic performance of most physical tests used to identify lumbar disc herniation. However, most findings arise from surgical populations and may not apply to primary care or non‐selected populations. Better performance may be obtained when tests are combined. Plain language summary Physical examination for the diagnosis of lumbar radiculopathy due to disc herniation in patients with low‐back pain and sciatica: a systematic review. Of all patients with back pain, less than 2% will undergo surgery for a herniated disc in the lumbar spine. In back pain patients who also have leg pain (sciatica), doctors and therapists use a physical examination to estimate the probability that the pain is caused by a disc herniation, and to assist the selection of patients for imaging and surgery. We conducted a systematic review to summarize available information on the diagnostic value of different aspects of physical examination. We included 19 different studies in which a wide variety of tests were investigated, such as the straight leg raising test, absence of tendon reflexes, or muscle weakness. The results show that most individual tests carried out during physical examination are not very accurate in discriminating between patients who have, or do not have a herniated disc with sciatica. However, most of the studies were conducted in highly selected patients who had already been referred for surgery, and only one study was carried out in a primary care population. Furthermore, better diagnostic performance of physical examination may be expected when combinations of tests are used, including information from both the patient history and physical examination. However, more research is needed to investigate the performance of such test combinations.","2","John Wiley & Sons, Ltd","1465-1858","Humans; Intervertebral Disc Displacement [complications, *diagnosis]; Low Back Pain [*etiology]; Lumbosacral Region; Physical Examination [*methods, standards]; Radiculopathy [*etiology]; Sciatica [etiology]","10.1002/14651858.CD007431.pub2","http://dx.doi.org/10.1002/14651858.CD007431.pub2","Back and Neck"
"CD007424.PUB2","Brazzelli, M; Sandercock, PAG; Chappell, FM; Celani, MG; Righetti, E; Arestis, N; Wardlaw, JM; Deeks, JJ","Magnetic resonance imaging versus computed tomography for detection of acute vascular lesions in patients presenting with stroke symptoms","Cochrane Database of Systematic Reviews","2009","Abstract - Background Magnetic resonance imaging (MRI) is increasingly used for the diagnosis of acute ischaemic stroke but its sensitivity for the early detection of intracerebral haemorrhage has been debated. Computed tomography (CT) is extensively used in the clinical management of acute stroke, especially for the rapid exclusion of intracerebral haemorrhage. Objectives To compare the diagnostic accuracy of diffusion‐weighted MRI (DWI) and CT for acute ischaemic stroke, and to estimate the diagnostic accuracy of MRI for acute haemorrhagic stroke. Search methods We searched MEDLINE and EMBASE (January 1995 to March 2009) and perused bibliographies of relevant studies for additional references. Selection criteria We selected studies that either compared DWI and CT in the same patients for detection of ischaemic stroke or examined the utility of MRI for detection of haemorrhagic stroke, had imaging performed within 12 hours of stroke onset, and presented sufficient data to allow construction of contingency tables. Data collection and analysis Three authors independently extracted data on study characteristics and measures of accuracy. We assessed data on ischaemic stroke using random‐effects and fixed‐effect meta‐analyses. Main results Eight studies with a total of 308 participants met our inclusion criteria. Seven studies contributed to the assessment of ischaemic stroke and two studies to the assessment of haemorrhagic stroke. The spectrum of patients was relatively narrow in all studies, sample sizes were small, there was substantial incorporation bias, and blinding procedures were often incomplete. Amongst the patients subsequently confirmed to have acute ischaemic stroke (161/226), the summary estimates for DWI were: sensitivity 0.99 (95% CI 0.23 to 1.00), specificity 0.92 (95% CI 0.83 to 0.97). The summary estimates for CT were: sensitivity 0.39 (95% CI 0.16 to 0.69), specificity 1.00 (95% CI 0.94 to 1.00). The two studies on haemorrhagic stroke reported high estimates for diffusion‐weighted and gradient‐echo sequences but had inconsistent reference standards. We did not calculate overall estimates for these two studies. We were not able to assess practicality or cost‐effectiveness issues. Authors' conclusions DWI appears to be more sensitive than CT for the early detection of ischaemic stroke in highly selected patients. However, the variability in the quality of included studies and the presence of spectrum and incorporation biases render the reliability and generalisability of observed results questionable. Further well‐designed studies without methodological biases, in more representative patient samples, with practicality and cost estimates are now needed to determine which patients should undergo MRI and which CT in suspected acute stroke. Plain language summary [Plain language title] [Summary text]","4","John Wiley & Sons, Ltd","1465-1858","*Diffusion Magnetic Resonance Imaging; *Tomography, X‐Ray Computed; Cerebral Hemorrhage [*diagnosis]; Humans; Randomized Controlled Trials as Topic; Sensitivity and Specificity; Stroke [*diagnosis]","10.1002/14651858.CD007424.pub2","http://dx.doi.org/10.1002/14651858.CD007424.pub2","Stroke"
