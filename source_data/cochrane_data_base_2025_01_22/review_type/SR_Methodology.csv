Cochrane Review ID,Author(s),Title,Source,Year,Abstract,Issue,Publisher,ISSN,Keywords,DOI,URL,Cochrane Review Group Code
"MR000011.PUB3","Showell, MG; Cole, S; Clarke, MJ; DeVito, NJ; Farquhar, C; Jordan, V","Time to publication for results of clinical trials","Cochrane Database of Systematic Reviews","2024","Abstract - Background Researchers conducting trials have a responsibility to publish the results of their work in a peer‐reviewed journal, and failure to do so may introduce bias that affects the accuracy of available evidence. Moreover, failure to publish results constitutes research waste. Objectives To systematically review research reports that followed clinical trials from their inception and their investigated publication rates and time to publication. We also aimed to assess whether certain factors influenced publication and time to publication. Search methods We identified studies by searching MEDLINE, Embase, Epistemonikos, the Cochrane Methodology Register (CMR) and the database of the US Agency for Healthcare Research and Quality (AHRQ), from inception to 23 August 2023. We also checked reference lists of relevant studies and contacted experts in the field for any additional studies. Selection criteria Studies were eligible if they tracked the publication of a cohort of clinical trials and contained analyses of any aspect of the publication rate or time to publication of these trials. Data collection and analysis Two review authors performed data extraction independently. We extracted data on the prevalence of publication and the time from the trial start date or completion date to publication. We also extracted data from the clinical trials included in the research reports, including country of the study's first author, area of health care, means by which the publication status of these trials were sought and the risk of bias in the trials. Main results A total of 204 research reports tracking 165,135 trials met the inclusion criteria. Just over half (53%) of these trials were published in full. The median time to publication was approximately 4.8 years from the enrolment of the first trial participant and 2.1 years from the trial completion date. Trials with positive results (i.e. statistically significant results favouring the experimental arm) were more likely to be published than those with negative or null results (OR 2.69, 95% CI 2.02 to 3.60; 19 studies), and they were published in a shorter time (adjusted HR 1.92, 95% CI 1.51 to 2.45; 4 studies). On average, trials with positive results took 2 years to publish, whereas trials with negative or null results took 2.6 years. Large trials were more likely to be published than smaller ones (adjusted OR 1.92, 95% CI 1.33 to 2.77; 11 studies), and they were published in a shorter time (adjusted HR 1.41, 95% CI 1.18 to 1.68; 7 studies). Multicentre trials were more likely to be published than single‐centre trials (adjusted OR 1.20, 95% CI 1.03 to 1.40; 2 studies). We found no difference between multicentre and single‐centre trials in time to publication. Trials funded by non‐industry sources (e.g.governments or universities) were more likely to be published than trials funded by industry (e.g. pharmaceutical companies or for‐profit organisations) (adjusted OR 2.13, 95% CI 1.82 to 2.49; 14 studies); they were also published in a shorter time (adjusted HR 1.46, 95% CI 1.15 to 1.86; 7 studies). Authors' conclusions Our updated review shows that trial publication is poor, with only half of all trials that are conducted being published. Factors that may make publication more likely and lead to faster publication are positive results, large sample size and being funded by non‐industry sources. Differences in publication rates result in publication bias and time‐lag bias that may influence findings and therefore ultimately affect treatment decisions. Systematic review authors should consider the possibility of time‐lag bias when conducting a systematic review, especially when updating their review. Funding This Cochrane review had no dedicated funding. Registration This review combines and updates two earlier Cochrane reviews. The two protocols and previous versions of the two updated reviews are available via 10.1002/14651858.MR000006 and 10.1002/14651858.MR000006.pub3 and 10.1002/14651858.MR000011 and 10.1002/14651858.MR000011.pub2. Plain language summary How long do clinical trials take to publish their results? What questions does this review address? How many of the clinical trials (studies) carried out to examine health interventions are published in journals, and how long does it take for these trials to be published? Are publication rates and the time taken to publication influenced by the nature of trial results, the number of participants and study centres, or the funding source for the trial? Key messages Nearly half (47%) of all clinical trials remain unpublished. Whether a trial is published and how long it takes is influenced by whether there are positive results, how large the trial is and if it is single‐ or multi‐centred, and which type of organisation has funded the trial. What are publication and publication bias? There are several steps in the publication of clinical trial results. The process begins with a clinical trial being carried out, then a summary document (manuscript) is written that contains the aims, methods and results of the trial. The manuscript is sent to a journal editor, then checked and commented on by the editor and colleagues, and by peer reviewers who are experts on the topic of the manuscript but were not involved in the trial. If they assess the manuscript as correct and suitable, it will be made available in an online or printed journal to be read by people with an interest in the findings. Sometimes, a subscription must be paid to access the manuscript, but often it will be 'open access', which means it is freely available. If the decision whether to publish a trial's results is influenced by the nature of those results, i.e. whether they are favourable to the intervention, this is referred to as 'publication bias'. Publication bias is a problem because it means that the information available to people making important health‐related decisions for themselves, their relatives or their patients is not complete and may even be misleading. For example, if negative results have not been published, there is a danger that the decision‐makers may not be aware of possible harms linked to the intervention. Publication bias in trials can be compounded in systematic reviews, which collect all the evidence on a topic. This is a concern as systematic reviews are heavily depended upon to make healthcare decisions and policies. Systematic reviews combine the results of multiple trials, allowing people to make decisions based on all available data, but if some trials cannot be included as they have not been published, the evidence is incomplete and may well be inaccurate. How can we avoid publication bias? The results of all clinical trials should be published, and the decision to publish should not be determined by the nature of the results or any other factors. What did we want to find out? We wanted to find out how many trials are being published. We also wanted to find out whether publication and the time taken to publish are influenced by the nature of the results, the size of the trial and the type of organisation responsible for providing the money to run the trial. What did we do? We searched multiple health‐related databases to find studies that examined the publication rate or the time to publication of results of clinical trials. We compared and summarised the results of the studies and rated our confidence in the evidence based on the study methods. What did we find? We found 204 studies, published between 1992 and 2023, which included 165,135 trials. The studies showed that just over half (53%) of all trials had published their results in a journal. The studies also showed that trials were more likely to be published if their results were positive, they were large (involving a lot of people), there were carried out in multiple places ('sites') and they were funded by a non‐commercial organisation. The time taken to publish was shorter for trials with positive results or a large sample size or if they were funded by a non‐commercial organisation. This evidence has important implications for when we should conduct systematic reviews and the best time to update them, especially if systematic reviews do not make efforts to include unpublished evidence. It is of particular concern for reviews containing only a small number of studies. What are the limitations of the evidence? The studies we included in this review varied in their methods, the type of trials they examined and their quality. In the review, we acknowledge and discuss these issues, and we provide separate analyses for trials of different types and quality. How up to date is this evidence? The evidence is based on searches carried out in August 2023.","11","John Wiley & Sons, Ltd","1465-1858","*Publishing [statistics & numerical data]; Clinical Trials as Topic [statistics & numerical data]; Humans; Publication Bias [statistics & numerical data]; Randomized Controlled Trials as Topic; Time Factors","10.1002/14651858.MR000011.pub3","http://dx.doi.org/10.1002/14651858.MR000011.pub3","Central Editorial Service"
"MR000034.PUB3","Toews, I; Anglemyer, A; Nyirenda, JLZ; Alsaid, D; Balduzzi, S; Grummich, K; Schwingshackl, L; Bero, L","Healthcare outcomes assessed with observational study designs compared with those assessed in randomized trials: a meta‐epidemiological study","Cochrane Database of Systematic Reviews","2024","Abstract - Background Researchers and decision‐makers often use evidence from randomised controlled trials (RCTs) to determine the efficacy or effectiveness of a treatment or intervention. Studies with observational designs are often used to measure the effectiveness of an intervention in 'real world' scenarios. Numerous study designs and their modifications (including both randomised and observational designs) are used for comparative effectiveness research in an attempt to give an unbiased estimate of whether one treatment is more effective or safer than another for a particular population. An up‐to‐date systematic analysis is needed to identify differences in effect estimates from RCTs and observational studies. This updated review summarises the results of methodological reviews that compared the effect estimates of observational studies with RCTs from evidence syntheses that addressed the same health research question. Objectives To assess and compare synthesised effect estimates by study type, contrasting RCTs with observational studies. To explore factors that might explain differences in synthesised effect estimates from RCTs versus observational studies (e.g. heterogeneity, type of observational study design, type of intervention, and use of propensity score adjustment). To identify gaps in the existing research comparing effect estimates across different study types. Search methods We searched MEDLINE, the Cochrane Database of Systematic Reviews, Web of Science databases, and Epistemonikos to May 2022. We checked references, conducted citation searches, and contacted review authors to identify additional reviews. Selection criteria We included systematic methodological reviews that compared quantitative effect estimates measuring the efficacy or effectiveness of interventions tested in RCTs versus in observational studies. The included reviews compared RCTs to observational studies (including retrospective and prospective cohort, case‐control and cross‐sectional designs). Reviews were not eligible if they compared RCTs with studies that had used some form of concurrent allocation. Data collection and analysis Using results from observational studies as the reference group, we examined the relative summary effect estimates (risk ratios (RRs), odds ratios (ORs), hazard ratios (HRs), mean differences (MDs), and standardised mean differences (SMDs)) to evaluate whether there was a relatively larger or smaller effect in the ratio of odds ratios (ROR) or ratio of risk ratios (RRR), ratio of hazard ratios (RHR), and difference in (standardised) mean differences (D(S)MD). If an included review did not provide an estimate comparing results from RCTs with observational studies, we generated one by pooling the estimates for observational studies and RCTs, respectively. Across all reviews, we synthesised these ratios to produce a pooled ratio of ratios comparing effect estimates from RCTs with those from observational studies. In overviews of reviews, we estimated the ROR or RRR for each overview using observational studies as the reference category. We appraised the risk of bias in the included reviews (using nine criteria in total). To receive an overall low risk of bias rating, an included review needed: explicit criteria for study selection, a complete sample of studies, and to have controlled for study methodological differences and study heterogeneity. We assessed reviews/overviews not meeting these four criteria as having an overall high risk of bias. We assessed the certainty of the evidence, consisting of multiple evidence syntheses, with the GRADE approach. Main results We included 39 systematic reviews and eight overviews of reviews, for a total of 47. Thirty‐four of these contributed data to our primary analysis. Based on the available data, we found that the reviews/overviews included 2869 RCTs involving 3,882,115 participants, and 3924 observational studies with 19,499,970 participants. We rated 11 reviews/overviews as having an overall low risk of bias, and 36 as having an unclear or high risk of bias. Our main concerns with the included reviews/overviews were that some did not assess the quality of their included studies, and some failed to account appropriately for differences between study designs – for example, they conducted aggregate analyses of all observational studies rather than separate analyses of cohort and case‐control studies. When pooling RORs and RRRs, the ratio of ratios indicated no difference or a very small difference between the effect estimates from RCTs versus from observational studies (ratio of ratios 1.08, 95% confidence interval (CI) 1.01 to 1.15). We rated the certainty of the evidence as low. Twenty‐three of 34 reviews reported effect estimates of RCTs and observational studies that were on average in agreement. In a number of subgroup analyses, small differences in the effect estimates were detected: ‐ pharmaceutical interventions only (ratio of ratios 1.12, 95% CI 1.04 to 1.21);  ‐ RCTs and observational studies with substantial or high heterogeneity; that is, I 2  ≥ 50% (ratio of ratios 1.11, 95% CI 1.04 to 1.18);  ‐ no use (ratio of ratios 1.07, 95% CI 1.03 to 1.11) or unclear use (ratio of ratios 1.13, 95% CI 1.03 to 1.25) of propensity score adjustment in observational studies; and  ‐ observational studies without further specification of the study design (ratio of ratios 1.06, 95% CI 0.96 to 1.18). We detected no clear difference in other subgroup analyses. Authors' conclusions We found no difference or a very small difference between effect estimates from RCTs and observational studies. These findings are largely consistent with findings from recently published research. Factors other than study design need to be considered when exploring reasons for a lack of agreement between results of RCTs and observational studies, such as differences in the population, intervention, comparator, and outcomes investigated in the respective studies. Our results underscore that it is important for review authors to consider not only study design, but the level of heterogeneity in meta‐analyses of RCTs or observational studies. A better understanding is needed of how these factors might yield estimates reflective of true effectiveness. Plain language summary How similar are estimates of treatment effectiveness derived from randomised controlled trials and observational studies? Key messages ‐ On average, the effect estimates of randomised controlled trials (RCTs) and observational studies differ only very slightly. Effect estimates are statistical constructs that describe the size of an intervention effect in terms of the difference between the outcomes of two groups of people in a clinical trial or study. ‐ We need more research with careful consideration of factors that might impact on the similarities and differences in effect estimates between different study types. What are RCTs and observational studies, and why do their effect estimates potentially differ? Randomised controlled trials (RCTs) are a type of healthcare experiment where participants are allocated at random to one of two (or more) treatment groups. One group is given an experimental treatment (also known as an 'intervention'); the other is the 'control' group, which is not given the intervention. RCTs test how effective and safe an experimental treatment is under ideal conditions. Observational studies try to measure the effectiveness of an intervention in non‐experimental, 'real world' scenarios. Case‐control (or retrospective) studies and cohort studies are two common types of observational study. Case‐control studies compare a group of people with a particular condition/disease to a group who do not have it but are otherwise similar. Cohort studies follow a group of people with a common characteristic over time to find out how many reach a certain health outcome of interest. Sometimes, the results of RCTs and observational studies addressing the same question may have different results. These types of study differ in how they are conducted and their susceptibility to systematic error. What did we want to find out? We wanted to assess the impact of study type (RCT versus observational studies) on the summary effect estimate and to explore methodological aspects that might explain any differences. What did we do? We searched databases for reviews that systematically compared the effect estimates reported in RCTs and observational studies that addressed the same health research question. We looked for reviews that included any healthcare outcomes, without restrictions on the language of publication. We searched for reviews/overviews published between 01 January 1990 and 12 May 2022. We then compared the results of the reviews, and summarised the evidence. We rated our confidence in this evidence, based on factors such as the methods used in the reviews and their size, and the consistency of findings across reviews. What did we find? We identified 47 relevant reviews; 34 contributed data to our main analysis. The reviews compared the effect estimates of RCTs to those of cohort studies, case‐control studies, or both. The reviews addressed a variety of health‐related topics. They were conducted in countries around the world, but most were done in the USA. Twelve reviews did not report any information on funding. In 8 reviews, the authors reported receiving no funding. In 23 reviews, the authors reported receiving public funding, such as governmental funding or funding from universities or foundations. Two reviews were funded by the European Union and two reviews reported receiving industry funding. Most funded reviews reported multiple sources of funding. Main results ‐ We found that the effect estimates of RCTs and observational studies may differ very little to not at all.  ‐ There may be small differences when we compare effect estimates of studies investigating only medicines (as opposed to other healthcare treatments, such as surgery or physical therapy). We also found little difference in the effect estimates that were based on data from: ‐ meta‐analysis of RCTs and observational studies that showed substantial statistical heterogeneity; that is, variability in the intervention effects being evaluated in the different studies; ‐ observational studies that either did not use or were unclear about how they used methods to account for population characteristics that can have an impact on the effectiveness of an intervention (propensity score adjustment);  ‐ observational studies that did not give sufficient information about their study design. What are the limitations of the evidence? We have little confidence in the evidence because the included reviews might be at risk for systematic errors because of how they were conducted. Moreover, the reviews were about different types of people and interventions, meaning that the individual findings amongst the reviews differed considerably. How up to date is this review? The evidence is current to May 2022.","1","John Wiley & Sons, Ltd","1465-1858","*Delivery of Health Care; Bias; Case-Control Studies; Humans; Observational Studies as Topic [methods]; Outcome Assessment, Health Care; Randomized Controlled Trials as Topic; Systematic Reviews as Topic","10.1002/14651858.MR000034.pub3","http://dx.doi.org/10.1002/14651858.MR000034.pub3","Methodology"
"MR000008.PUB5","Edwards, PJ; Roberts, I; Clarke, MJ; DiGuiseppi, C; Woolf, B; Perkins, C","Methods to increase response to postal and electronic questionnaires","Cochrane Database of Systematic Reviews","2023","Abstract - Background Self‐administered questionnaires are widely used to collect data in epidemiological research, but non‐response reduces the effective sample size and can introduce bias. Finding ways to increase response to postal and electronic questionnaires would improve the quality of epidemiological research. Objectives To identify effective strategies to increase response to postal and electronic questionnaires. Search methods We searched 14 electronic databases up to December 2021 and manually searched the reference lists of relevant trials and reviews. We contacted the authors of all trials or reviews to ask about unpublished trials; where necessary, we also contacted authors to confirm the methods of allocation used and to clarify results presented. Selection criteria Randomised trials of methods to increase response to postal or electronic questionnaires. We assessed the eligibility of each trial using pre‐defined criteria. Data collection and analysis We extracted data on the trial participants, the intervention, the number randomised to intervention and comparison groups and allocation concealment. For each strategy, we estimated pooled odds ratios (OR) and 95% confidence intervals (CI) in a random‐effects model. We assessed evidence for selection bias using Egger's weighted regression method and Begg's rank correlation test and funnel plot. We assessed heterogeneity amongst trial odds ratios using a Chi 2  test and quantified the degree of inconsistency between trial results using the I 2  statistic. Main results Postal We found 670 eligible trials that evaluated over 100 different strategies of increasing response to postal questionnaires. We found substantial heterogeneity amongst trial results in half of the strategies. The odds of response almost doubled when: using monetary incentives (odds ratio (OR) 1.86; 95% confidence interval (CI) 1.73 to 1.99; heterogeneity I 2  = 85%); using a telephone reminder (OR 1.96; 95% CI 1.03 to 3.74); and when clinical outcome questions were placed last (OR 2.05; 95% CI 1.00 to 4.24). The odds of response increased by about half when: using a shorter questionnaire (OR 1.58; 95% CI 1.40 to 1.78); contacting participants before sending questionnaires (OR 1.36; 95% CI 1.23 to 1.51; I 2  = 87%); incentives were given with questionnaires (i.e. unconditional) rather than when given only after participants had returned their questionnaire (i.e. conditional on response) (OR 1.53; 95% CI 1.35 to 1.74); using personalised SMS reminders (OR 1.53; 95% CI 0.97 to 2.42); using a special (recorded) delivery service (OR 1.68; 95% CI 1.36 to 2.08; I 2  = 87%); using electronic reminders (OR 1.60; 95% CI 1.10 to 2.33); using intensive follow‐up (OR 1.69; 95% CI 0.93 to 3.06); using a more interesting/salient questionnaire (OR 1.73; 95% CI 1.12 to 2.66); and when mentioning an obligation to respond (OR 1.61; 95% CI 1.16 to 2.22). The odds of response also increased with: non‐monetary incentives (OR 1.16; 95% CI 1.11 to 1.21; I 2  = 80%); a larger monetary incentive (OR 1.24; 95% CI 1.15 to 1.33); a larger non‐monetary incentive (OR 1.15; 95% CI 1.00 to 1.33); when a pen was included (OR 1.44; 95% CI 1.38 to 1.50); using personalised materials (OR 1.15; 95% CI 1.09 to 1.21; I 2  = 57%); using a single‐sided rather than a double‐sided questionnaire (OR 1.13; 95% CI 1.02 to 1.25); using stamped return envelopes rather than franked return envelopes (OR 1.23; 95% CI 1.13 to 1.33; I 2  = 69%), assuring confidentiality (OR 1.33; 95% CI 1.24 to 1.42); using first‐class outward mailing (OR 1.11; 95% CI 1.02 to 1.21); and when questionnaires originated from a university (OR 1.32; 95% CI 1.13 to 1.54). The odds of response were reduced when the questionnaire included questions of a sensitive nature (OR 0.94; 95% CI 0.88 to 1.00). Electronic We found 88 eligible trials that evaluated over 30 different ways of increasing response to electronic questionnaires. We found substantial heterogeneity amongst trial results in half of the strategies. The odds of response tripled when: using a brief letter rather than a detailed letter (OR 3.26; 95% CI 1.79 to 5.94); and when a picture was included in an email (OR 3.05; 95% CI 1.84 to 5.06; I 2  = 19%). The odds of response almost doubled when: using monetary incentives (OR 1.88; 95% CI 1.31 to 2.71; I 2  = 79%); and using a more interesting topic (OR 1.85; 95% CI 1.52 to 2.26). The odds of response increased by half when: using non‐monetary incentives (OR 1.60; 95% CI 1.25 to 2.05); using shorter e‐questionnaires (OR 1.51; 95% CI 1.06 to 2.16; I 2  = 94%); and using a more interesting e‐questionnaire (OR 1.85; 95% CI 1.52 to 2.26). The odds of response increased by a third when: offering survey results as an incentive (OR 1.36; 95% CI 1.16 to 1.59); using a white background (OR 1.31; 95% CI 1.10 to 1.56); and when stressing the benefits to society of response (OR 1.38; 95% CI 1.07 to 1.78; I 2  = 41%). The odds of response also increased with: personalised e‐questionnaires (OR 1.24; 95% CI 1.17 to 1.32; I 2  = 41%); using a simple header (OR 1.23; 95% CI 1.03 to 1.48); giving a deadline (OR 1.18; 95% CI 1.03 to 1.34); and by giving a longer time estimate for completion (OR 1.25; 95% CI 0.96 to 1.64). The odds of response were reduced when: ""Survey"" was mentioned in the e‐mail subject (OR 0.81; 95% CI 0.67 to 0.97); when the email or the e‐questionnaire was from a male investigator, or it included a male signature (OR 0.55; 95% CI 0.38 to 0.80); and by using university sponsorship (OR 0.84; 95%CI 0.69 to 1.01). The odds of response using a postal questionnaire were over twice those using an e‐questionnaire (OR 2.33; 95% CI 2.25 to 2.42; I 2  = 98%). Response also increased when: providing a choice of response mode (electronic or postal) rather than electronic only (OR 1.76 95% CI 1.67 to 1.85; I 2  = 97%); and when administering the e‐questionnaire by computer rather than by smartphone (OR 1.62 95% CI 1.36 to 1.94). Authors' conclusions Researchers using postal and electronic questionnaires can increase response using the strategies shown to be effective in this Cochrane review. Plain language summary How can response to postal or web questionnaires be increased? Key messages Response to questionnaires can be increased by contacting people before they are sent a questionnaire; Response to questionnaires can be increased by making questionnaires, letters, and emails more personal, and preferably kept short; Response to questionnaires can be increased by giving an incentive, for example, a small amount of money, or a non‐monetary incentive such as a pen. Why is response to questionnaires important? Postal and electronic questionnaires are a relatively inexpensive way to collect information from people for research purposes. If people do not reply (so called ’non‐responders’), the research results will tend to be less accurate. What did we want to find out? We wanted to find effective ways to increase response to postal and electronic questionnaires. What did we do?   We searched for studies that examined any way of increasing questionnaire response. We summarised the results of the studies. What did we find? A very large amount of research has been done to try to identify ways to increase response, and we have included 758 studies in this Cochrane methodology review update. The studies included a wide range of people asked to complete a questionnaire, from patients, doctors, university students, and professors, to marketing managers, accountants, and grocery store managers. We found that response will be increased by contacting people before they are sent a questionnaire. We also found that response to postal questionnaires will be increased if they are sent by a university. Response can also be increased by giving an incentive, for example, a small amount of money, or a non‐monetary incentive such as a pen. Response may be higher using a postal questionnaire rather than an electronic one, or by providing a choice of response modes (electronic or postal). Response can be increased by making questionnaires, letters, and emails more personal, and preferably kept short. What are the limitations of the evidence? We had to exclude some studies because we could not confirm that they were free from bias. How up‐to‐date is this evidence? This review updates our previous review. The evidence is up‐to‐date to December 2021.","11","John Wiley & Sons, Ltd","1465-1858","*Reminder Systems; *Smartphone; Electronics; Humans; Male; Sample Size; Surveys and Questionnaires","10.1002/14651858.MR000008.pub5","http://dx.doi.org/10.1002/14651858.MR000008.pub5","Methodology"
"MR000056.PUB2","Hesselberg, J-O; Dalsbø, TK; Stromme, H; Svege, I; Fretheim, A","Reviewer training for improving grant and journal peer review","Cochrane Database of Systematic Reviews","2023","Abstract - Background Funders and scientific journals use peer review to decide which projects to fund or articles to publish. Reviewer training is an intervention to improve the quality of peer review. However, studies on the effects of such training yield inconsistent results, and there are no up‐to‐date systematic reviews addressing this question. Objectives To evaluate the effect of peer reviewer training on the quality of grant and journal peer review. Search methods We used standard, extensive Cochrane search methods. The latest search date was 27 April 2022. Selection criteria We included randomized controlled trials (RCTs; including cluster‐RCTs) that evaluated peer review with training interventions versus usual processes, no training interventions, or other interventions to improve the quality of peer review. Data collection and analysis We used standard Cochrane methods. Our primary outcomes were 1. completeness of reporting and 2. peer review detection of errors. Our secondary outcomes were 1. bibliometric scores, 2. stakeholders' assessment of peer review quality, 3. inter‐reviewer agreement, 4. process‐centred outcomes, 5. peer reviewer satisfaction, and 6. completion rate and speed of funded projects. We used the first version of the Cochrane risk of bias tool to assess the risk of bias, and we used GRADE to assess the certainty of evidence. Main results We included 10 RCTs with a total of 1213 units of analysis. The unit of analysis was the individual reviewer in seven studies (722 reviewers in total), and the reviewed manuscript in three studies (491 manuscripts in total). In eight RCTs, participants were journal peer reviewers. In two studies, the participants were grant peer reviewers. The training interventions can be broadly divided into dialogue‐based interventions (interactive workshop, face‐to‐face training, mentoring) and one‐way communication (written information, video course, checklist, written feedback). Most studies were small. We found moderate‐certainty evidence that emails reminding peer reviewers to check items of reporting checklists, compared with standard journal practice, have little or no effect on the completeness of reporting, measured as the proportion of items (from 0.00 to 1.00) that were adequately reported (mean difference (MD) 0.02, 95% confidence interval (CI) −0.02 to 0.06; 2 RCTs, 421 manuscripts). There was low‐certainty evidence that reviewer training, compared with standard journal practice, slightly improves peer reviewer ability to detect errors (MD 0.55, 95% CI 0.20 to 0.90; 1 RCT, 418 reviewers). We found low‐certainty evidence that reviewer training, compared with standard journal practice, has little or no effect on stakeholders' assessment of review quality in journal peer review (standardized mean difference (SMD) 0.13 standard deviations (SDs), 95% CI −0.07 to 0.33; 1 RCT, 418 reviewers), or change in stakeholders' assessment of review quality in journal peer review (SMD −0.15 SDs, 95% CI −0.39 to 0.10; 5 RCTs, 258 reviewers). We found very low‐certainty evidence that a video course, compared with no video course, has little or no effect on inter‐reviewer agreement in grant peer review (MD 0.14 points, 95% CI −0.07 to 0.35; 1 RCT, 75 reviewers). There was low‐certainty evidence that structured individual feedback on scoring, compared with general information on scoring, has little or no effect on the change in inter‐reviewer agreement in grant peer review (MD 0.18 points, 95% CI −0.14 to 0.50; 1 RCT, 41 reviewers, low‐certainty evidence). Authors' conclusions Evidence from 10 RCTs suggests that training peer reviewers may lead to little or no improvement in the quality of peer review. There is a need for studies with more participants and a broader spectrum of valid and reliable outcome measures. Studies evaluating stakeholders' assessments of the quality of peer review should ensure that these instruments have sufficient levels of validity and reliability. Plain language summary What are the benefits of training peer reviewers? Key messages • Training peer reviewers may have little or no effect on the quality of peer review. • Larger, well‐designed studies are needed to give better estimates of the effect. What is a peer reviewer? A peer reviewer is a person who evaluates the research work done by another person. The peer reviewer is usually a researcher with skills similar to those needed to conduct the research they evaluate. What is peer review used for? Both funders and publishers of research can be uncertain if a research project or report is of good quality. Many use peer reviewers to evaluate the quality of a project or report. How could peer review quality be improved by training reviewers? Training peer reviewers might make them better at identifying strengths and weaknesses in the research they assess. What did we want to find out? We wanted to find out if training peer reviewers increased the quality of their work. What did we do? We searched for studies that looked at training for peer reviewers compared with no training, different types of training, or standard journal or funder practice. We extracted information and summarized the results of all the relevant studies. We rated our confidence in the evidence based on factors such as study methods and sizes. What did we find? We found 10 studies that involved a total of 1213 units of analysis (722 reviewers and 491 manuscripts). Eight studies included only journal peer reviewers. The remaining two studies included grant peer reviewers. Main results Emails reminding peer reviewers to check items of reporting checklists, compared with standard journal practice, probably have little or no effect on the completeness of reporting (evidence from 2 studies with 421 manuscripts). Reviewer training, compared with standard journal practice, may slightly improve peer reviewer ability to detect errors (evidence from 1 study with 418 reviewers). The reviewers who received training identified 3.25 out of 9 errors on average, whereas the reviewers who received no training identified 2.7 out of 9 errors on average. Reviewer training, compared with standard journal practice, may have little or no effect on stakeholders' assessment of review quality (evidence from 6 studies with 616 reviewers and 60 manuscripts). We are unsure about the effect of a video course, compared with no video course, on agreement between reviewers (evidence from 1 study with 75 reviewers). Structured individual feedback on scoring, compared with general information on scoring, may have little or no effect on the change in agreement between reviewers (evidence from 1 study with 41 reviewers). What are the limitations of the evidence? We have little confidence in most of the evidence because most studies lacked important information and included too few reviewers. Additionally, it is unclear whether the studies measured peer review quality in a valid and reliable way. How up to date is this evidence? The evidence is up to date to April 2022.","11","John Wiley & Sons, Ltd","1465-1858","*Peer Review, Research; *Publishing; Bias; Checklist; Humans; Peer Review; Reproducibility of Results","10.1002/14651858.MR000056.pub2","http://dx.doi.org/10.1002/14651858.MR000056.pub2","Methodology"
"MR000054.PUB2","Escobar Liquitay, CM; Garegnani, L; Garrote, V; Solà, I; Franco, JVA","Search strategies (filters) to identify systematic reviews in MEDLINE and Embase","Cochrane Database of Systematic Reviews","2023","Abstract - Background Bibliographic databases provide access to an international body of scientific literature in health and medical sciences. Systematic reviews are an important source of evidence for clinicians, researchers, consumers, and policymakers as they address a specific health‐related question and use explicit methods to identify, appraise and synthesize evidence from which conclusions can be drawn and decisions made. Methodological search filters help database end‐users search the literature effectively with different levels of sensitivity and specificity. These filters have been developed for various study designs and have been found to be particularly useful for intervention studies. Other filters have been developed for finding systematic reviews. Considering the variety and number of available search filters for systematic reviews, there is a need for a review of them in order to provide evidence about their retrieval properties at the time they were developed. Objectives To review systematically empirical studies that report the development, evaluation, or comparison of search filters to retrieve reports of systematic reviews in MEDLINE and Embase. Search methods We searched the following databases from inception to January 2023: MEDLINE, Embase, PsycINFO; Library, Information Science & Technology Abstracts (LISTA) and Science Citation Index (Web of Science). Selection criteria We included studies if one of their primary objectives is the development, evaluation, or comparison of a search filter that could be used to retrieve systematic reviews on MEDLINE, Embase, or both. Data collection and analysis Two review authors independently extracted data using a pre‐specified and piloted data extraction form using InterTASC Information Specialist Subgroup (ISSG) Search Filter Evaluation Checklist. Main results We identified eight studies that developed filters for MEDLINE and three studies that developed filters for Embase. Most studies are very old and some were limited to systematic reviews in specific clinical areas. Six included studies reported the sensitivity of their developed filter. Seven studies reported precision and six studies reported specificity. Only one study reported the number needed to read and positive predictive value. None of the filters were designed to differentiate systematic reviews on the basis of their methodological quality. For MEDLINE, all filters showed similar sensitivity and precision, and one filter showed higher levels of specificity. For Embase, filters showed variable sensitivity and precision, with limited study reports that may affect accuracy assessments. The report of these studies had some limitations, and the assessments of their accuracy may suffer from indirectness, considering that they were mostly developed before the release of the PRISMA 2009 statement or due to their limited scope in the selection of systematic review topics. Search filters for MEDLINE Three studies produced filters with sensitivity > 90% with variable degrees of precision, and only one of them was developed and validated in a gold‐standard database, which allowed the calculation of specificity. The other two search filters had lower levels of sensitivity. One of these produced a filter with higher levels of specificity (> 90%). All filters showed similar sensitivity and precision in the external validation, except for one which was not externally validated and another one which was conceptually derived and only externally validated. Search filters for Embase We identified three studies that developed filters for this database. One of these studies developed filters with variable sensitivity and precision, including highly sensitive strategies (> 90%); however, it was not externally validated. The other study produced a filter with a lower sensitivity (72.7%) but high specificity (99.1%) with a similar performance in the external validation. Authors' conclusions Studies reporting the development, evaluation, or comparison of search filters to retrieve reports of systematic reviews in MEDLINE showed similar sensitivity and precision, with one filter showing higher levels of specificity. For Embase, filters showed variable sensitivity and precision, with limited information about how the filter was produced, which leaves us uncertain about their performance assessments. Newer filters had limitations in their methods or scope, including very focused subject topics for their gold standards, limiting their applicability across other topics. Our findings highlight that consensus guidance on the conduct of search filters and standardized reporting of search filters are needed, as we found highly heterogeneous development methods, accuracy assessments and outcome selection. New strategies adaptable across interfaces could enhance their usability. Moreover, the performance of existing filters needs to be evaluated in light of the impact of reporting guidelines, including the PRISMA 2009, on how systematic reviews are reported. Finally, future filter developments should also consider comparing the filters against a common reference set to establish comparative performance and assess the quality of systematic reviews retrieved by strategies. Plain language summary How can we best filter systematic reviews in MEDLINE and Embase? Key Messages A wide range of search filters to retrieve systematic reviews were evaluated. Although many had acceptable sensitivity (missed few relevant studies) and specificity (omitted irrelevant studies), no single filter can be recommended since most were derived from older sets of reviews that may not reflect current reporting characteristics and standards. What are search filters for systematic reviews? Search filters combine words and phrases to retrieve records with a common feature (e.g. study design, clinical topic) and are typically evaluated in terms of their sensitivity and precision. Systematic reviews summarise and synthesise scientific evidence and represent an important source of information for healthcare professionals. Databases provide access to them, and search filters can be used to retrieve systematic reviews pragmatically. What did we want to find out? We wanted to identify search filters for systematic reviews, assess their quality and retrieve data on their sensitivity, specificity and precision. What did we do? We searched for studies that developed, evaluated or compared a search filter that could be used to retrieve systematic reviews in MEDLINE, Embase, or both. We identified nine studies that developed filters for MEDLINE and three studies that developed filters for Embase. What did we find? For MEDLINE, all filters showed similar sensitivity and precision, and one filter showed higher levels of specificity. For Embase, filters showed variable sensitivity and precision, with limited study reports that may affect accuracy assessments. What are the limitations of the evidence? Some filters were developed for specific topics (e.g. public health), and most were developed using older studies, which may not reflect how systematic reviews are currently reported. Moreover, filters may not be able to discern between high‐ and low‐quality reviews. How up‐to‐date is the evidence? The evidence is up‐to‐date to January 2023.","9","John Wiley & Sons, Ltd","1465-1858","*Checklist; *Systematic Reviews as Topic; Databases, Bibliographic; Humans; MEDLINE","10.1002/14651858.MR000054.pub2","http://dx.doi.org/10.1002/14651858.MR000054.pub2","Methodology"
"MR000055.PUB2","Laursen, DRT; Nejstgaard, CH; Bjørkedal, E; Frost, AD; Hansen, MR; Paludan-Müller, AS; Prosenz, J; Werner, CP; Hróbjartsson, A","Impact of active placebo controls on estimated drug effects in randomised trials: a systematic review of trials with both active placebo and standard placebo","Cochrane Database of Systematic Reviews","2023","Abstract - Background An estimated 60% of pharmacological randomised trials use placebo control interventions to blind (i.e. mask) participants. However, standard placebos do not control for perceptible non‐therapeutic effects (i.e. side effects) of the experimental drug, which may unblind participants. Trials rarely use active placebo controls, which contain pharmacological compounds designed to mimic the non‐therapeutic experimental drug effects in order to reduce the risk of unblinding. A relevant improvement in the estimated effects of active placebo compared with standard placebo would imply that trials with standard placebo may overestimate experimental drug effects. Objectives We aimed to estimate the difference in drug effects when an experimental drug is compared with an active placebo versus a standard placebo control intervention, and to explore causes for heterogeneity. In the context of a randomised trial, this difference in drug effects can be estimated by directly comparing the effect difference between the active placebo and standard placebo intervention. Search methods We searched PubMed, CENTRAL, Embase, two other databases, and two trial registries up to October 2020. We also searched reference lists and citations and contacted trial authors. Selection criteria We included randomised trials that compared an active placebo versus a standard placebo intervention. We considered trials both with and without a matching experimental drug arm. Data collection and analysis We extracted data, assessed risk of bias, scored active placebos for adequacy and risk of unintended therapeutic effect, and categorised active placebos as unpleasant, neutral, or pleasant. We requested individual participant data from the authors of four cross‐over trials published after 1990 and one unpublished trial registered after 1990. Our primary inverse‐variance, random‐effects meta‐analysis used standardised mean differences (SMDs) of active versus standard placebo for participant‐reported outcomes at earliest post‐treatment assessment. A negative SMD favoured the active placebo. We stratified analyses by trial type (clinical or preclinical) and supplemented with sensitivity and subgroup analyses and meta‐regression. In secondary analyses, we investigated observer‐reported outcomes, harms, attrition, and co‐intervention outcomes. Main results We included 21 trials (1462 participants). We obtained individual participant data from four trials. Our primary analysis of participant‐reported outcomes at earliest post‐treatment assessment resulted in a pooled SMD of −0.08 (95% confidence interval (CI) −0.20 to 0.04; I 2  = 31%; 14 trials), with no clear difference between clinical and preclinical trials. Individual participant data contributed 43% of the weight of this analysis. Two of seven sensitivity analyses found more pronounced and statistically significant differences; for example, in the five trials with low overall risk of bias, the pooled SMD was −0.24 (95% CI −0.34 to −0.13). The pooled SMD of observer‐reported outcomes was similar to the primary analysis. The pooled odds ratio (OR) for harms was 3.08 (95% CI 1.56 to 6.07), and for attrition, 1.22 (95% CI 0.74 to 2.03). Co‐intervention data were limited. Meta‐regression found no statistically significant association with adequacy of the active placebo or risk of unintended therapeutic effect. Authors' conclusions We did not find a statistically significant difference between active and standard placebo control interventions in our primary analysis, but the result was imprecise and the CI compatible with a difference ranging from important to irrelevant. Furthermore, the result was not robust, because two sensitivity analyses produced a more pronounced and statistically significant difference. We suggest that trialists and users of information from trials carefully consider the type of placebo control intervention in trials with high risk of unblinding, such as those with pronounced non‐therapeutic effects and participant‐reported outcomes. Plain language summary Do treatment effects in randomised trials differ when using active placebo compared to standard placebo? Key messages 1. We found no clear difference in effect between active and standard placebos, but we are very uncertain about the results.  2. We suggest that researchers carefully consider the type of placebo when investigating medicines with clear side effects. What are standard placebos and active placebos? Blinding (or masking) is an important part of randomised trials and ensures that participants and healthcare providers are not influenced by knowing whether the participant is receiving the experimental treatment. If they had this knowledge, they might unintentionally overestimate (or underestimate) the effect of the treatment. One method of blinding is to give the non‐treated group of participants a placebo, which looks like the medicine (e.g. a tablet of similar shape, colour, smell, taste, and texture) but does not contain its active ingredients. Blinding with a placebo may not always be successful. The treatment may have side effects that distinguish it from the placebo so that the treatment effect is still not measured accurately. For this reason, some trials use active placebos, which mimic some of the side effects of the experimental medicine. However, it is unclear whether the choice of placebo type actually makes a difference to the treatment effects. What did we want to find out? We wanted to find out whether treatment effects in randomised trials differ when using active placebos compared to standard placebos. What did we do? We collected and analysed trials that directly compared the two types of placebo, or that compared both placebos with an experimental medicine. If people receiving active placebo experience better results than those receiving standard placebo, this difference might be due to them believing that they are receiving the experimental medicine. It would also mean that in trials that compare a medicine with standard placebo, the beneficial effect of the medicine is exaggerated. This is important if the measured benefits of the treatment are small or moderate and thus especially sensitive to changes in the methods used for the trial. What did we find? We included 21 trials in this review, covering subjects such as pain and psychiatry. We found no clear difference between the two types of placebo in participant‐reported outcomes (such as pain intensity). However, because the result was uncertain, the possible range of this result included both no difference and a potentially important difference in favour of active placebo. When we limited our analysis to higher‐quality trials, active placebos were more beneficial than standard placebos, but these trials were not typical clinical trials and might not be applicable to clinical scenarios.","3","John Wiley & Sons, Ltd","1465-1858","*Dietary Supplements; *Drug-Related Side Effects and Adverse Reactions; Emotions; Humans; Odds Ratio; Randomized Controlled Trials as Topic","10.1002/14651858.MR000055.pub2","http://dx.doi.org/10.1002/14651858.MR000055.pub2","Methodology"
"MR000050.PUB2","Faltinsen, E; Todorovac, A; Staxen Bruun, L; Hróbjartsson, A; Gluud, C; Kongerslev, MT; Simonsen, E; Storebø, OJ","Control interventions in randomised trials among people with mental health disorders","Cochrane Database of Systematic Reviews","2022","Abstract - Background Control interventions in randomised trials provide a frame of reference for the experimental interventions and enable estimations of causality. In the case of randomised trials assessing patients with mental health disorders, many different control interventions are used, and the choice of control intervention may have considerable impact on the estimated effects of the treatments being evaluated. Objectives To assess the benefits and harms of typical control interventions in randomised trials with patients with mental health disorders. The difference in effects between control interventions translates directly to the impact a control group has on the estimated effect of an experimental intervention. We aimed primarily to assess the difference in effects between (i) wait‐list versus no‐treatment, (ii) usual care versus wait‐list or no‐treatment, and (iii) placebo interventions (all placebo interventions combined or psychological, pharmacological, and physical placebos individually) versus wait‐list or no‐treatment. Wait‐list patients are offered the experimental intervention by the researchers after the trial has been finalised if it offers more benefits than harms, while no‐treatment participants are not offered the experimental intervention by the researchers. Search methods In March 2018, we searched MEDLINE, PsycInfo, Embase, CENTRAL, and seven other databases and six trials registers. Selection criteria We included randomised trials assessing patients with a mental health disorder that compared wait‐list, usual care, or placebo interventions with wait‐list or no‐treatment . Data collection and analysis Titles, abstracts, and full texts were reviewed for eligibility. Review authors independently extracted data and assessed risk of bias using Cochrane’s risk of bias tool. GRADE was used to assess the quality of the evidence. We contacted researchers working in the field to ask for data from additional published and unpublished trials. A pre‐planned decision hierarchy was used to select one benefit and one harm outcome from each trial. For the assessment of benefits, we summarised continuous data as standardised mean differences (SMDs) and dichotomous data as risk ratios (RRs). We used risk differences (RDs) for the assessment of adverse events. We used random‐effects models for all statistical analyses. We used subgroup analysis to explore potential causes for heterogeneity (e.g. type of placebo) and sensitivity analyses to explore the robustness of the primary analyses (e.g. fixed‐effect model). Main results We included 96 randomised trials (4200 participants), ranging from 8 to 393 participants in each trial. 83 trials (3614 participants) provided usable data. The trials included 15 different mental health disorders, the most common being anxiety (25 trials), depression (16 trials), and sleep‐wake disorders (11 trials). All 96 trials were assessed as high risk of bias partly because of the inability to blind participants and personnel in trials with two control interventions. The quality of evidence was rated low to very low, mostly due to risk of bias, imprecision in estimates, and heterogeneity. Only one trial compared wait‐list versus no‐treatment directly but the authors were not able to provide us with any usable data on the comparison. Five trials compared usual care versus wait‐list or no‐treatment and found a SMD –0.33 (95% CI ‐0.83 to 0.16, I² = 86%, 523 participants) on benefits. The difference between all placebo interventions combined versus wait‐list or no‐treatment was SMD −0.37 (95% CI −0.49 to −0.25, I² = 41%, 65 trials, 2446 participants) on benefits. There was evidence of some asymmetry in the funnel plot (Egger’s test P value of 0.087). Almost all the trials were small. Subgroup analysis found a moderate effect in favour of psychological placebos SMD ‐0.49 (95% CI −0.64 to −0.30; I² = 53%, 39 trials, 1656 participants). The effect of pharmacological placebos versus wait‐list or no‐treatment on benefits was SMD ‐0.14 (95% CI −0.39 to 0.11, 9 trials, 279 participants) and the effect of physical placebos was SMD −0.21 (95% CI −0.35 to −0.08, I² = 0%, 17 trials, 896 participants). We found large variations in effect sizes in the psychological and pharmacological placebo comparisons. For specific mental health disorders, we found significant differences in favour of all placebos for sleep‐wake disorders, major depressive disorder, and anxiety disorders, but the analyses were imprecise due to sparse data. We found no significant differences in harms for any of the comparisons but the analyses suffered from sparse data. When using a fixed‐effect model in a sensitivity analysis on the comparison for usual care versus wait‐list and no‐treatment, the results were significant with an SMD of –0.46 (95 % CI –0.64 to –0.28). We reported an alternative risk of bias model where we excluded the blinding domains seeing how issues with blinding may be seen as part of the review investigation itself. However, this did not markedly change the overall risk of bias profile as most of the trials still included one or more unclear bias domains. Authors' conclusions We found marked variations in effects between placebo versus no‐treatment and wait‐list and between subtypes of placebo with the same comparisons. Almost all the trials were small with considerable methodological and clinical variability in factors such as mental health population, contents of the included control interventions, and outcome domains. All trials were assessed as high risk of bias and the evidence quality was low to very low. When researchers decide to use placebos or usual care control interventions in trials with people with mental health disorders it will often lead to lower estimated effects of the experimental intervention than when using wait‐list or no‐treatment controls. The choice of a control intervention therefore has considerable impact on how effective a mental health treatment appears to be. Methodological guideline development is needed to reach a consensus on future standards for the design and reporting of control interventions in mental health intervention research. Plain language summary Control interventions in randomised trials for people with a mental health disorder This systematic review assesses the effects of different control interventions in randomised trials including patients with a mental health disorder. In randomised trials, patients are assigned by chance to one of two or more groups – usually an experimental intervention and a control intervention. There are many types of control interventions in mental health intervention research. Some of the most common are different types of placebos that lack what is assumed to be the active component in the experimental intervention, and usual care, where patients receive the standard treatment for their mental health disorder in the area where they live. Two other types of control interventions are wait‐list or no‐treatment where patients receive no trial‐related care during the study (although some patients may receive care outside the studies). Wait‐list patients are often offered the experimental intervention after the trial has been finalised if it is likely to provide more benefits than harms, while no‐treatment participants are not offered the experimental intervention by the researchers. We searched for randomised trials with patients with mental health disorders where wait‐list, usual care, or placebo interventions were compared with either wait‐list or no‐treatment. We looked at differences between all the types of control interventions on beneficial effects and whether they caused any adverse effects. We included 96 trials with a total of 4200 participants. Only 83 trials (3614 participants) provided usable data. Fifteen different mental health disorders were included. We found that all the trials were at high risk of bias in how they had been conducted, which reduced the interpretability of our findings. However, the risk of bias was mostly due to lack of blinding in the placebo studies, which may be seen as an aspect of the review's methodological question rather than a flaw with the review itself. We found no clinically important differences for usual care or wait‐list control interventions in the main analyses, however in our secondary analyses we found a clinically important favourable difference for usual care. In general, placebo control interventions tended to be favourable over no‐treatment or wait‐list control interventions across mental health disorders. We found no clinically important differences on adverse events. This review suggests that different control interventions have a tendency to yield very different estimates for the effects of the experimental intervention and that the choice of control intervention has a large impact on how effective a mental health treatment appears to be. Control interventions in trials with patients with mental health disorders are often poorly reported upon, and guidelines are needed to inform researchers on how to properly design, report, and interpret these trials.","4","John Wiley & Sons, Ltd","1465-1858","*Depressive Disorder, Major; *Mental Health; Anxiety; Anxiety Disorders; Humans; Psychotherapy; Randomized Controlled Trials as Topic","10.1002/14651858.MR000050.pub2","http://dx.doi.org/10.1002/14651858.MR000050.pub2","Methodology"
"MR000028.PUB3","Welch, V; Dewidar, O; Tanjong Ghogomu, E; Abdisalam, S; Al Ameer, A; Barbeau, VI; Brand, K; Kebedom, K; Benkhalti, M; Kristjansson, E; Madani, MT; Antequera Martín, AM; Mathew, CM; McGowan, J; McLeod, W; Park, HA; Petkovic, J; Riddle, A; Tugwell, P; Petticrew, M; Trawin, J; Wells, GA","How effects on health equity are assessed in systematic reviews of interventions","Cochrane Database of Systematic Reviews","2022","Abstract - Background Enhancing health equity is endorsed in the Sustainable Development Goals. The failure of systematic reviews to consider potential differences in effects across equity factors is cited by decision‐makers as a limitation to their ability to inform policy and program decisions.  Objectives To explore what methods systematic reviewers use to consider health equity in systematic reviews of effectiveness. Search methods We searched the following databases up to 26 February 2021: MEDLINE, PsycINFO, the Cochrane Methodology Register, CINAHL, Education Resources Information Center, Education Abstracts, Criminal Justice Abstracts, Hein Index to Foreign Legal Periodicals, PAIS International, Social Services Abstracts, Sociological Abstracts, Digital Dissertations and the Health Technology Assessment Database. We searched SCOPUS to identify articles that cited any of the included studies on 10 June 10 2021. We contacted authors and searched the reference lists of included studies to identify additional potentially relevant studies. Selection criteria We included empirical studies of cohorts of systematic reviews that assessed methods for measuring effects on health inequalities. We define health inequalities as unfair and avoidable differences across socially stratifying factors that limit opportunities for health. We operationalised this by assessing studies which evaluated differences in health across any component of the PROGRESS‐Plus acronym, which stands for Place of residence, Race/ethnicity/culture/language, Occupation, Gender or sex, Religion, Education, Socioeconomic status, Social capital. ""Plus"" stands for other factors associated with discrimination, exclusion, marginalisation or vulnerability such as personal characteristics (e.g. age, disability), relationships that limit opportunities for health (e.g. children in a household with parents who smoke) or environmental situations which provide limited control of opportunities for health (e.g. school food environment). Data collection and analysis Two review authors independently extracted data using a pre‐tested form. Risk of bias was appraised for included studies according to the potential for bias in selection and detection of systematic reviews.  Main results In total, 48,814 studies were identified and the titles and abstracts were screened in duplicate. In this updated review, we identified an additional 124 methodological studies published in the 10 years since the first version of this review, which included 34 studies. Thus, 158 methodological studies met our criteria for inclusion. The methods used by these studies focused on evidence relevant to populations experiencing health inequity (108 out of 158 studies), assess subgroup analysis across PROGRESS‐Plus (26 out of 158 studies), assess analysis of a gradient in effect across PROGRESS‐Plus (2 out of 158 studies) or use a combination of subgroup analysis and focused approaches (20 out of 158 studies). The most common PROGRESS‐Plus factors assessed were age (43 studies), socioeconomic status in 35 studies, low‐ and middle‐income countries in 24 studies, gender or sex in 22 studies, race or ethnicity in 17 studies, and four studies assessed multiple factors across which health inequity may exist. Only 16 studies provided a definition of health inequity. Five methodological approaches to consider health equity in systematic reviews of effectiveness were identified: 1) descriptive assessment of reporting and analysis in systematic reviews (140 of 158 studies used a type of descriptive method); 2) descriptive assessment of reporting and analysis in original trials (50 studies); 3) analytic approaches which assessed differential effects across one or more PROGRESS‐Plus factors (16 studies); 4) applicability assessment (25 studies) and 5) stakeholder engagement (28 studies), which is a new finding in this update and examines the appraisal of whether relevant stakeholders with lived experience of health inequity were included in the design of systematic reviews or design and delivery of interventions. Reporting for both approaches (analytic and applicability) lacked transparency and was insufficiently detailed to enable the assessment of credibility. Authors' conclusions There is a need for improvement in conceptual clarity about the definition of health equity, describing sufficient detail about analytic approaches (including subgroup analyses) and transparent reporting of judgments required for applicability assessments in order to consider health equity in systematic reviews of effectiveness. Plain language summary How effects on health equity are assessed in systematic reviews of effectiveness Key message We found five methodological approaches to consider health equity in systematic reviews of effectiveness but the most appropriate way to address any of these approaches is unclear. Review question We reviewed the methods that systematic reviewers use to consider health equity in systematic reviews of effectiveness. Background Reducing health inequities, avoidable and unfair differences in health, has achieved international political importance and global endorsement. Decision‐makers have cited a lack of equity considerations in systematic reviews, creating a need for guidance on the advantages and disadvantages of methods to assess effects on health equity in systematic reviews. Study characteristics We included empirical studies of collections of systematic reviews that assessed methods for measuring effects on health inequalities. We define health inequalities as unfair and avoidable differences across socially stratifying factors that limit opportunities for health. We evaluated differences in health across any component of the PROGRESS‐Plus acronym, which stands for Place of residence, Race/ethnicity/culture/language, Occupation, Gender or sex, Religion, Education, Socioeconomic status, Social capital. ""Plus"" stands for other factors associated with discrimination, exclusion, marginalisation or vulnerability such as personal characteristics (e.g. age, disability), relationships that limit opportunities for health (e.g. children in a household with smoking parents) or environmental situations which provide limited control of opportunities for health (e.g. school, food, environment). Key results This updated review includes 158 collections of systematic reviews: 108 focused on evidence relevant to populations experiencing inequity, 26 assessed subgroup analysis across PROGRESS‐Plus, two assessed analysis of a gradient in effect across PROGRESS‐Plus and 20 used a combination of subgroup analysis and focused approaches. The most common PROGRESS‐Plus factors assessed were age (43 studies), socioeconomic status (35 studies), low‐ and middle‐income countries (24 studies). Four studies assessed multiple factors across which health inequity may exist. We identified five methodological approaches to consider health equity in systematic reviews of effectiveness: 1) descriptive assessment in the reviews, 2) descriptive assessment of the studies included in the reviews, 3) analytic approaches, 4) applicability assessment, and 5) stakeholder engagement. However, the most appropriate way to address any of these approaches is unclear. Analysis of effects for specific populations need to be justified and reported appropriately to allow assessment of their credibility. Transparency of judgments about applicability and relevance to disadvantaged populations needs to be improved. Guidance on equity and specific populations is available in the Cochrane Handbook. Search date The evidence is up to date to February 2021.  ","1","John Wiley & Sons, Ltd","1465-1858","*Health Equity; Child; Humans; Parents; Research Design; Systematic Reviews as Topic","10.1002/14651858.MR000028.pub3","http://dx.doi.org/10.1002/14651858.MR000028.pub3","Methodology"
"MR000051.PUB2","Klatte, K; Pauli-Magnus, C; Love, SB; Sydes, MR; Benkert, P; Bruni, N; Ewald, H; Arnaiz Jimenez, P; Bonde, MM; Briel, M","Monitoring strategies for clinical intervention studies","Cochrane Database of Systematic Reviews","2021","Abstract - Background Trial monitoring is an important component of good clinical practice to ensure the safety and rights of study participants, confidentiality of personal information, and quality of data. However, the effectiveness of various existing monitoring approaches is unclear. Information to guide the choice of monitoring methods in clinical intervention studies may help trialists, support units, and monitors to effectively adjust their approaches to current knowledge and evidence. Objectives To evaluate the advantages and disadvantages of different monitoring strategies (including risk‐based strategies and others) for clinical intervention studies examined in prospective comparative studies of monitoring interventions. Search methods We systematically searched CENTRAL, PubMed, and Embase via Elsevier for relevant published literature up to March 2021. We searched the online 'Studies within A Trial' (SWAT) repository, grey literature, and trial registries for ongoing or unpublished studies. Selection criteria We included randomized or non‐randomized prospective, empirical evaluation studies of different monitoring strategies in one or more clinical intervention studies. We applied no restrictions for language or date of publication. Data collection and analysis We extracted data on the evaluated monitoring methods, countries involved, study population, study setting, randomization method, and numbers and proportions in each intervention group. Our primary outcome was critical and major monitoring findings in prospective intervention studies. Monitoring findings were classified according to different error domains (e.g. major eligibility violations) and the primary outcome measure was a composite of these domains. Secondary outcomes were individual error domains, participant recruitment and follow‐up, and resource use. If we identified more than one study for a comparison and outcome definitions were similar across identified studies, we quantitatively summarized effects in a meta‐analysis using a random‐effects model. Otherwise, we qualitatively summarized the results of eligible studies stratified by different comparisons of monitoring strategies. We used the GRADE approach to assess the certainty of the evidence for different groups of comparisons. Main results We identified eight eligible studies, which we grouped into five comparisons. 1.  Risk‐based versus extensive on‐site monitoring:  based on two large studies, we found moderate certainty of evidence for the combined primary outcome of major or critical findings that risk‐based monitoring is not inferior to extensive on‐site monitoring. Although the risk ratio was close to 'no difference' (1.03 with a 95% confidence interval [CI] of 0.81 to 1.33, below 1.0 in favor of the risk‐based strategy), the high imprecision in one study and the small number of eligible studies resulted in a wide CI of the summary estimate. Low certainty of evidence suggested that monitoring strategies with extensive on‐site monitoring were associated with considerably higher resource use and costs (up to a factor of 3.4). Data on recruitment or retention of trial participants were not available. 2.  Central monitoring with triggered on‐site visits versus regular on‐site visits:  combining the results of two eligible studies yielded low certainty of evidence with a risk ratio of 1.83 (95% CI 0.51 to 6.55) in favor of triggered monitoring intervention. Data on recruitment, retention, and resource use were not available. 3.  Central statistical monitoring and local monitoring performed by site staff with annual on‐site visits versus central statistical monitoring and local monitoring only:  based on one study, there was moderate certainty of evidence that a small number of major and critical findings were missed with the central monitoring approach without on‐site visits: 3.8% of participants in the group without on‐site visits and 6.4% in the group with on‐site visits had a major or critical monitoring finding (odds ratio 1.7, 95% CI 1.1 to 2.7; P = 0.03). The absolute number of monitoring findings was very low, probably because defined major and critical findings were very study specific and central monitoring was present in both intervention groups. Very low certainty of evidence did not suggest a relevant effect on participant retention, and very low‐quality evidence indicated an extra cost for on‐site visits of USD 2,035,392. There were no data on recruitment. 4.  Traditional 100% source data verification (SDV) versus targeted or remote SDV:  the two studies assessing targeted and remote SDV reported findings only related to source documents. Compared to the final database obtained using the full SDV monitoring process, only a small proportion of remaining errors on overall data were identified using the targeted SDV process in the MONITORING study (absolute difference 1.47%, 95% CI 1.41% to 1.53%). Targeted SDV was effective in the verification of source documents but increased the workload on data management. The other included study was a pilot study which compared traditional on‐site SDV versus remote SDV and found little difference in monitoring findings and the ability to locate data values despite marked differences in remote access in two clinical trial networks. There were no data on recruitment or retention. 5.  Systematic on‐site initiation visit versus on‐site initiation visit upon request:  very low certainty of evidence suggested no difference in retention and recruitment between the two approaches. There were no data on critical and major findings or on resource use. Authors' conclusions The evidence base is limited in terms of quantity and quality. Ideally, for each of the five identified comparisons, more prospective, comparative monitoring studies nested in clinical trials and measuring effects on all outcomes specified in this review are necessary to draw more reliable conclusions. However, the results suggesting risk‐based, targeted, and mainly central monitoring as an efficient strategy are promising. The development of reliable triggers for on‐site visits is ongoing; different triggers might be used in different settings. More evidence on risk indicators that identify sites with problems or the prognostic value of triggers is needed to further optimize central monitoring strategies. In particular, approaches with an initial assessment of trial‐specific risks that need to be closely monitored centrally during trial conduct with triggered on‐site visits should be evaluated in future research. Plain language summary New monitoring strategies for clinical trials Our question We reviewed the evidence on the effects of new monitoring strategies on monitoring findings, participant recruitment, participant follow‐up, and resource use in clinical trials. We also summarized the different components of tested strategies and qualitative evidence from process evaluations. Background Monitoring a clinical trial is important to ensure the safety of participants and the reliability of results. New methods have been developed for monitoring practices but further assessments of these new methods are needed to see if they do improve effectiveness without being inferior to established methods in terms of patient rights and safety, and quality assurance of trial results. We reviewed studies that examined this question within clinical trials, i.e. studies comparing different monitoring strategies used in clinical trials. Study characteristics We included eight studies which covered a variety of monitoring strategies in a wide range of clinical trials, including national and large international trials. They included primary (general), secondary (specialized), and tertiary (highly specialized) health care. The size of the studies ranged from 32 to 4371 participants at one to 196 sites. Key results We identified five comparisons. The first comparison of risk‐based monitoring versus extensive on‐site monitoring found no evidence that the risk‐based approach is inferior to extensive on‐site monitoring in terms of the proportion of participants with a critical or major monitoring finding not identified by the corresponding method, while resource use was three‐ to five‐fold higher with extensive on‐site monitoring. For the second comparison of central statistical monitoring with triggered on‐site visits versus regular (untriggered) on‐site visits, we found some evidence that central statistical monitoring can identify sites in need of support by an on‐site monitoring intervention. In the third comparison, the evaluation of adding an on‐site visit to local and central monitoring revealed a high percentage of participants with major or critical monitoring findings in the on‐site visit group, but low numbers of absolute monitoring findings in both groups. This means that without on‐site visits, some monitoring findings will be missed, but none of the missed findings had any serious impact on patient safety or the validity of the trial's results. In the fourth comparison, two studies assessed new source data verification processes, which are used to check that data recorded within the trial Case Report Form (CRF) match the primary source data (e.g. medical records), and reported little difference to full source data verification processes for the targeted as well as for the remote approach. In the fifth comparison, one study showed no difference in participant recruitment and participant follow‐up between a monitoring approach with systematic initiation visits versus an approach with initiation visits upon request by study sites. Certainty of evidence We are moderately certain that risk‐based monitoring is not inferior to extensive on‐site monitoring with respect to critical and major monitoring findings in clinical trials. For the remaining body of evidence, there is low or very low certainty in results due to imprecision, small number of studies, or high risk of bias. Ideally, for each of the five identified comparisons, more high‐quality monitoring studies that measure effects on all outcomes specified in this review are necessary to draw more reliable conclusions.","12","John Wiley & Sons, Ltd","1465-1858","*Prospective Studies; Humans; Pilot Projects","10.1002/14651858.MR000051.pub2","http://dx.doi.org/10.1002/14651858.MR000051.pub2","Methodology"
"MR000032.PUB3","Gillies, K; Kearney, A; Keenan, C; Treweek, S; Hudson, J; Brueton, VC; Conway, T; Hunter, A; Murphy, L; Carr, PJ; Rait, G; Manson, P; Aceves-Martins, M","Strategies to improve retention in randomised trials","Cochrane Database of Systematic Reviews","2021","Abstract - Background Poor retention of participants in randomised trials can lead to missing outcome data which can introduce bias and reduce study power, affecting the generalisability, validity and reliability of results. Many strategies are used to improve retention but few have been formally evaluated. Objectives To quantify the effect of strategies to improve retention of participants in randomised trials and to investigate if the effect varied by trial setting. Search methods We searched the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, Scopus, PsycINFO, CINAHL, Web of Science Core Collection (SCI‐expanded, SSCI, CPSI‐S, CPCI‐SSH and ESCI) either directly with a specified search strategy or indirectly through the ORRCA database. We also searched the SWAT repository to identify ongoing or recently completed retention trials. We did our most recent searches in January 2020. Selection criteria We included eligible randomised or quasi‐randomised trials of evaluations of strategies to increase retention that were embedded in 'host' randomised trials from all disease areas and healthcare settings. We excluded studies aiming to increase treatment compliance. Data collection and analysis We extracted data on: the retention strategy being evaluated; location of study; host trial setting; method of randomisation; numbers and proportions in each intervention and comparator group. We used a risk difference (RD) and 95% confidence interval (CI) to estimate the effectiveness of the strategies to improve retention. We assessed heterogeneity between trials. We applied GRADE to determine the certainty of the evidence within each comparison. Main results We identified 70 eligible papers that reported data from 81 retention trials. We included 69 studies with more than 100,000 participants in the final meta‐analyses, of which 67 studies evaluated interventions aimed at trial participants and two evaluated interventions aimed at trial staff involved in retention. All studies were in health care and most aimed to improve postal questionnaire response. Interventions were categorised into broad comparison groups: Data collection; Participants; Sites and site staff; Central study management; and Study design. These intervention groups consisted of 52 comparisons, none of which were supported by high‐certainty evidence as determined by GRADE assessment. There were four comparisons presenting moderate‐certainty evidence, three supporting retention (self‐sampling kits, monetary reward together with reminder or prenotification and giving a pen at recruitment) and one reducing retention (inclusion of a diary with usual follow‐up compared to usual follow‐up alone). Of the remaining studies, 20 presented GRADE low‐certainty evidence and 28 presented very low‐certainty evidence. Our findings do provide a priority list for future replication studies, especially with regard to comparisons that currently rely on a single study. Authors' conclusions Most of the interventions we identified aimed to improve retention in the form of postal questionnaire response. There were few evaluations of ways to improve participants returning to trial sites for trial follow‐up. None of the comparisons are supported by high‐certainty evidence. Comparisons in the review where the evidence certainty could be improved with the addition of well‐done studies should be the focus for future evaluations. Plain language summary Strategies that might help to encourage people to continue to participate in a randomised trial (a type of scientific study) Why is this review important? Randomised trials are a type of scientific study typically used to test new healthcare treatments. In a randomised trial, people who agree to take part are randomly (by chance) put into one of two or more treatment groups and then studied for a period of time. The research team try to keep in touch with them to collect information about how they are doing. This 'follow up' can last from days to years depending on the trial, but the longer the trial lasts, the more difficult it can be. This might be because people are too busy to reply, are unable to come to a clinic, or just do not want to participate any longer. Keeping people in a trial is called 'retention'. If retention is poor, it can make the trial results less certain but most trials do not get data from all the people who started out in the trial. The information gathered during follow‐up, sometimes called data, helps the trial team to determine which of the treatments being tested works the best. Often this information is collected directly from patients by asking them to complete a questionnaire or by asking them to come back for a clinic visit. There are many ways to collect data from people in trials. These include using letters, the internet, telephone calls, text messaging, face‐to‐face meetings or the return of medical test kits. Research teams use different methods to try to collect data and it's important to know which strategies are effective and worthwhile, which is why we did this review to compare the success of different strategies. How did we identify and evaluate the evidence? We searched scientific databases for studies that compared strategies that research teams use to improve trial retention against each other or against not using such a strategy.We looked for studies that included participants from any age, gender, ethnic, language or geographic group. We then compared the results of the studies, and summarised the evidence that we had found. Finally, we rated our confidence in this evidence, based on factors such as the methods used in the studies and their size, and the consistency of findings across studies. What did we find? We identified 70 relevant articles, which reported 81 retention studies involving more than 100,000 participants, that had investigated different ways of trying to encourage randomised trial participants to provide data and stay in the trial. We organised these into broad comparison groups but, unfortunately, we are not able to say with confidence that any of the results we found is a true effect and not caused by other factors, such as flaws with the design of the studies. As such, the effect of ways to encourage people to stay involved in trials is still not clear and more research is needed to see if these retention methods really do work. How certain is the evidence and how up‐to‐date is this review? The strategies we identified were tested in randomised trials run in many different disease areas and settings but, in some cases, were tested in only one trial. None of the comparisons we made provided high quality evidence and more studies are needed to help provide more confidence for the results we did find. The evidence in this Cochrane Review is current to January 2020.","3","John Wiley & Sons, Ltd","1465-1858","Case Management; Correspondence as Topic; Humans; Patient Compliance [psychology, *statistics & numerical data]; Patient Dropouts [statistics & numerical data]; Patient Selection; Randomized Controlled Trials as Topic [*statistics & numerical data]; Reward; Surveys and Questionnaires","10.1002/14651858.MR000032.pub3","http://dx.doi.org/10.1002/14651858.MR000032.pub3","Methodology"
"MR000040.PUB3","Nejstgaard, CH; Bero, L; Hróbjartsson, A; Jørgensen, AW; Jørgensen, KJ; Le, M; Lundh, A","Conflicts of interest in clinical guidelines, advisory committee reports, opinion pieces, and narrative reviews: associations with recommendations","Cochrane Database of Systematic Reviews","2020","Abstract - Background Treatment and diagnostic recommendations are often made in clinical guidelines, reports from advisory committee meetings, opinion pieces such as editorials, and narrative reviews. Quite often, the authors or members of advisory committees have industry ties or particular specialty interests which may impact on which interventions are recommended. Similarly, clinical guidelines and narrative reviews may be funded by industry sources resulting in conflicts of interest. Objectives To investigate to what degree financial and non‐financial conflicts of interest are associated with favourable recommendations in clinical guidelines, advisory committee reports, opinion pieces, and narrative reviews. Search methods We searched PubMed, Embase, and the Cochrane Methodology Register for studies published up to February 2020. We also searched reference lists of included studies, Web of Science for studies citing the included studies, and grey literature sources. Selection criteria We included studies comparing the association between conflicts of interest and favourable recommendations of drugs or devices (e.g. recommending a particular drug) in clinical guidelines, advisory committee reports, opinion pieces, or narrative reviews. Data collection and analysis Two review authors independently included studies, extracted data, and assessed risk of bias. When a meta‐analysis was considered meaningful to synthesise our findings, we used random‐effects models to estimate risk ratios (RRs) with 95% confidence intervals (CIs), with RR > 1 indicating that documents (e.g. clinical guidelines) with conflicts of interest more often had favourable recommendations. We analysed associations for financial and non‐financial conflicts of interest separately, and analysed the four types of documents both separately (pre‐planned analyses) and combined (post hoc analysis). Main results We included 21 studies analysing 106 clinical guidelines, 1809 advisory committee reports, 340 opinion pieces, and 497 narrative reviews. We received unpublished data from 11 studies; eight full data sets and three summary data sets. Fifteen studies had a risk of confounding, as they compared documents that may differ in other aspects than conflicts of interest (e.g. documents on different drugs used for different populations). The associations between financial conflicts of interest and favourable recommendations were: clinical guidelines, RR: 1.26, 95% CI: 0.93 to 1.69 (four studies of 86 clinical guidelines); advisory committee reports, RR: 1.20, 95% CI: 0.99 to 1.45 (four studies of 629 advisory committee reports); opinion pieces, RR: 2.62, 95% CI: 0.91 to 7.55 (four studies of 284 opinion pieces); and narrative reviews, RR: 1.20, 95% CI: 0.97 to 1.49 (four studies of 457 narrative reviews). An analysis combining all four document types supported these findings (RR: 1.26, 95% CI: 1.09 to 1.44). One study investigating specialty interests found that the association between including radiologist guideline authors and recommending routine breast cancer screening was RR: 2.10, 95% CI: 0.92 to 4.77 (12 clinical guidelines). Authors' conclusions We interpret our findings to indicate that financial conflicts of interest are associated with favourable recommendations of drugs and devices in clinical guidelines, advisory committee reports, opinion pieces, and narrative reviews. However, we also stress risk of confounding in the included studies and the statistical imprecision of individual analyses of each document type. It is not certain whether non‐financial conflicts of interest impact on recommendations. Plain language summary Conflicts of interest and recommendations in clinical guidelines, advisory committee reports, opinion pieces, and narrative reviews Which treatments and diagnostic tests doctors offer to their patients are often based on recommendations expressed in a variety of documents. A common example is clinical guidelines, which are statements providing recommendations on how to diagnose and treat patients on the basis of the best available evidence. The treatments that may be offered to patients are also influenced by which drugs are recommended for approval by drug advisory committees at regulatory drug agencies such as the US Food and Drug Administration (FDA). Finally, doctors may also be influenced by recommendations expressed in opinion pieces, such as editorials, or in narrative review papers in medical journals. Quite often, publications expressing clinical recommendations are written by authors with conflicts of interest related to a specific product, for example when the author acts as a consultant for the company producing the treatment of interest. Such conflicts of interest may impact on the recommendations made. Similarly, authors may have so‐called non‐financial conflicts of interest such as belonging to a specific profession, for example being an orthopaedic surgeon, which may influence whether a specific intervention is preferred over another. This Cochrane Methodology Review investigated how financial and non‐financial conflicts of interest are associated with the recommendations made in clinical guidelines, advisory committee reports, opinion pieces, and narrative reviews. We included 21 studies and we interpreted our findings to indicate that financial conflicts of interest are associated with favourable recommendations in these documents, although there is some uncertainty around the size of the effect. This means that when such publications are written by authors with financial conflicts of interest, they more often have favourable recommendations than publications written by authors without conflicts of interest. Only a single study investigated the impact of non‐financial conflicts of interest in clinical guidelines and the results were uncertain, but indicated a similar direction of effect. We suggest that patients, doctors, and healthcare decision makers primarily use clinical guidelines, opinion pieces, and narrative reviews that have been written by authors without financial conflicts of interest. If that is not possible, users should read and interpret the publications with caution. Furthermore, our findings suggest that if committee members are asked to vote on the recommendation of a drug, they may be more likely to vote in favour of the drug when they have financial conflicts of interest.","12","John Wiley & Sons, Ltd","1465-1858","*Conflict of Interest [economics]; *Practice Guidelines as Topic; Advisory Committees [*ethics, statistics & numerical data]; Authorship; Bias; Consultants; Datasets as Topic [*ethics, statistics & numerical data]; Drug Industry [ethics]; Editorial Policies; Equipment and Supplies [ethics]; Humans; Publications [*ethics]; Radiologists; Review Literature as Topic","10.1002/14651858.MR000040.pub3","http://dx.doi.org/10.1002/14651858.MR000040.pub3","Methodology"
"MR000047.PUB2","Hansen, C; Lundh, A; Rasmussen, K; Hróbjartsson, A","Financial conflicts of interest in systematic reviews: associations with results, conclusions, and methodological quality","Cochrane Database of Systematic Reviews","2019","Abstract - Background Financial conflicts of interest in systematic reviews (e.g. funding by drug or device companies or authors' collaboration with such companies) may impact on how the reviews are conducted and reported. Objectives To investigate the degree to which financial conflicts of interest related to drug and device companies are associated with results, conclusions, and methodological quality of systematic reviews. Search methods We searched PubMed, Embase, and the Cochrane Methodology Register for studies published up to November 2016. We also read reference lists of included studies, searched grey literature sources, and Web of Science for studies citing the included studies. Selection criteria Eligible studies were studies that compared systematic reviews with and without financial conflicts of interest in order to investigate differences in results (estimated treatment effect and frequency of statistically favourable results), frequency of favourable conclusions, or measures of methodological quality of the review (e.g. as evaluated on the Oxman and Guyatt index). Data collection and analysis Two review authors independently determined the eligibility of studies, extracted data, and assessed risk of bias. We synthesised the results of each study relevant to each of our outcomes. For meta‐analyses, we used Mantel‐Haenszel random‐effects models to estimate risk ratios (RR) with 95% confidence intervals (CIs), with RR > 1 indicating that systematic reviews with financial conflicts of interest more frequently had statistically favourable results or favourable conclusions, and had lower methodological quality. When a quantitative synthesis was considered not meaningful, results from individual studies were summarised qualitatively. Main results Ten studies with a total of 995 systematic reviews of drug studies and 15 systematic reviews of device studies were included. We assessed two studies as low risk of bias and eight as high risk, primarily because of risk of confounding. The estimated treatment effect was not statistically significantly different for systematic reviews with and without financial conflicts of interest (Z‐score: 0.46, P value: 0.64; based on one study of 14 systematic reviews which had a matched design, comparing otherwise similar systematic reviews). We found no statistically significant difference in frequency of statistically favourable results for systematic reviews with and without financial conflicts of interest (RR: 0.84, 95% CI: 0.62 to 1.14; based on one study of 124 systematic reviews). An analysis adjusting for confounding due to methodological quality (i.e. score on the Oxman and Guyatt index) provided a similar result. Systematic reviews with financial conflicts of interest more often had favourable conclusions compared with systematic reviews without (RR: 1.98, 95% CI: 1.26 to 3.11; based on seven studies of 411 systematic reviews). Similar results were found in two studies with a matched design, which therefore had a reduced risk of confounding. Systematic reviews with financial conflicts of interest tended to have lower methodological quality compared with systematic reviews without financial conflicts of interest (RR for 11 dimensions of methodological quality spanned from 1.00 to 1.83). Similar results were found in analyses based on two studies with matched designs. Authors' conclusions Systematic reviews with financial conflicts of interest more often have favourable conclusions and tend to have lower methodological quality than systematic reviews without financial conflicts of interest. However, it is uncertain whether financial conflicts of interest are associated with the results of systematic reviews. We suggest that patients, clinicians, developers of clinical guidelines, and planners of further research could primarily use systematic reviews without financial conflicts of interest. If only systematic reviews with financial conflicts of interest are available, we suggest that users read the review conclusions with skepticism, critically appraise the methods applied, and interpret the review results with caution. Plain language summary Financial conflicts of interests and results, conclusions, and quality of systematic reviews Patient treatment practices are often based on clinical research. Systematic reviews are a core type of such clinical research. When several similar studies (i.e. studies investigating the same research questions using similar methods) have been conducted, these can be identified and analysed in a systematic review. Systematic reviews thereby summarise existing studies and provide an overview of a specific research field. Thus, systematic reviews may have a major influence on decisions about patient care and it is essential that such reviews are trustworthy. Sometimes, systematic reviews are funded by companies with a financial interest in the review's results and conclusions, for example because they produce a drug or device investigated in the review. At other times, systematic reviews are carried out by researchers with a personal financial interest in a specific result, for example when the researcher acts as a consultant for the company producing an intervention that is assessed in the review. These financial conflicts of interest may impact on how systematic reviews are conducted and reported. Our Cochrane Methodology Review focuses on financial conflicts of interest related to drug or device companies in systematic reviews. Our primary aim was to investigate the degree to which systematic reviews with financial conflicts of interest present review results and make conclusions that are more favourable than systematic reviews without such financial conflicts of interest. Our secondary aim was to investigate the degree to which systematic reviews with financial conflicts of interest differ in methodological quality from systematic reviews without such financial conflicts of interest. We found 10 studies comparing systematic reviews with and without financial conflicts of interest. Based on two of these studies, we found no evidence of a difference in review results between systematic reviews with and without financial conflicts of interest. Based on seven studies, we found that systematic reviews with financial conflicts of interest more often had conclusions favourable towards the experimental intervention (risk ratio (RR): 1.98, 95% confidence interval (CI): 1.26 to 3.11). Also, based on four studies, systematic reviews with financial conflicts of interest tended to have lower methodological quality (RR for 11 dimensions of methodological quality spanned from 1.00 to 1.83). Our analyses suggest that when systematic reviews have financial conflicts of interest related to drug or device companies, they are of lower methodological quality, and have more favourable conclusions. However, it is not clear whether this derives from actual differences in the review's results or the over‐interpretation of those results. Based on our findings, we suggest that people who use systematic reviews, including patients, clinicians, developers of clinical guidelines, and planners of future research, could primarily use systematic reviews without financial conflicts of interest. If such reviews are not available, we suggest that users are especially cautious when they read and interpret systematic reviews with financial conflicts of interest.","8","John Wiley & Sons, Ltd","1465-1858","*Conflict of Interest; *Nutritional Status; Humans","10.1002/14651858.MR000047.pub2","http://dx.doi.org/10.1002/14651858.MR000047.pub2","Methodology"
"MR000041.PUB2","Li, L; Smith, HE; Atun, R; Tudor Car, L","Search strategies to identify observational studies in MEDLINE and Embase","Cochrane Database of Systematic Reviews","2019","Abstract - Background Systematic reviews are essential for decision‐making. Systematic reviews on observational studies help answer research questions on aetiology, risk, prognosis, and frequency of rare outcomes or complications. However, identifying observational studies as part of systematic reviews efficiently is challenging due to poor and inconsistent indexing in literature databases. Search strategies that include a methodological filter focusing on study design of observational studies might be useful for improving the precision of the search performance. Objectives To assess the sensitivity and precision of a search strategy with a methodological filter to identify observational studies in MEDLINE and Embase. Search methods We searched MEDLINE (1946 to April 2018), Embase (1974 to April 2018), CINAHL (1937 to April 2018), the Cochrane Library (1992 to April 2018), Google Scholar and Open Grey in April 2018, and scanned reference lists of articles. Selection criteria Studies using a relative recall approach, i.e. comparing sensitivity or precision of a search strategy containing a methodological filter to identify observational studies in MEDLINE and Embase against a reference standard, or studies that compared two or more methodological filters. Data collection and analysis Two review authors independently screened articles, extracted relevant information and assessed the quality of the search strategies using the InterTASC Information Specialists' Sub‐Group (ISSG) Search Filter Appraisal Checklist. Main results We identified two eligible studies reporting 18 methodological filters. All methodological filters in these two studies were developed using terms from the reference standard records. The first study evaluated six filters for retrieving observational studies of surgical interventions. The study reported on six filters: one Precision Terms Filter (comprising terms with higher precision while maximum sensitivity was maintained) and one Specificity Terms Filter (comprising terms with higher specificity while maximum sensitivity was maintained), both of which were adapted for MEDLINE, for Embase, and for combined MEDLINE/Embase searches. The study reported one reference standard consisting of 217 articles from one systematic review of which 83.9% of the included studies were case series The second study reported on 12 filters for retrieving comparative non‐randomised studies (cNRSs) including cohort, case‐control, and cross‐sectional studies. This study reported on 12 filters using four different approaches: Fixed method A (comprising of a fixed set of controlled vocabulary (CV) words), Fixed method B (comprising a fixed set of CV words and text words (TW)), Progressive method (CV) (a random choice of study design‐related CV terms), and Progressive method (CV or TW) (a random choice of study design‐related CV terms, and title and abstracts‐based TWs). The study reported four reference standards consisting of 89 cNRSs from four systematic reviews. The six methodological filters developed from the first study reported sensitivity of 99.5% to 100% and precision of 16.7% to 21.1%. The Specificity Terms Filter for combined MEDLINE/Embase was preferred because it had higher precision and equal sensitivity to the Precision Terms Filter. The 12 filters from the second study reported lower sensitivity (48% to 100%) and much lower precision (0.09% to 4.47%). The Progressive method (CV or TW) had the highest sensitivity. There were methodological limitations in both included studies. The first study used one surgical intervention‐focused systematic review thus limiting the generalizability of findings. The second study used four systematic reviews but with less than 100 studies. The external validation was performed only on Specificity Terms Filter from the first study Both studies were published 10 years ago and labelling and indexing of observational studies has changed since then. Authors' conclusions We found 18 methodological filters across two eligible studies. Search strategies from the first study had higher sensitivity and precision, underwent external validation and targeted observational studies. Search strategies from the second study had lower sensitivity and precision, focused on cNRSs, and were not validated externally. Given this limited and heterogeneous evidence, and its methodological limitations, further research and better indexation are needed. Plain language summary Search strategies to identify observational studies in MEDLINE and Embase Background Systematic reviews collect, analyse and summarise research to answer defined research questions. The evidence from systematic reviews is considered as the most reliable evidence and is often used to inform healthcare‐related decision making. Systematic reviews on prognosis, cause, risk factors and complications of a disease include a specific type of study design: observational studies. Searching for literature usually involves the use of MEDLINE and Embase databases and can result in a large number of articles to check for the review. Therefore, in order to focus their search, researchers often use a set of terms relating to study designs, known as methodological filters. However, when using these, researchers may miss relevant studies but find many irrelevant articles. Performance of filters are evaluated by sensitivity, which is the ability of the search filter to retrieve all the relevant studies that exist, and precision, which is the ability of the search filter to retrieve only relevant studies. Given the importance of including evidence from observational studies in systematic reviews, we aimed to assess studies evaluating methodological strategies for identifying observational studies in the two main databases of health literature, MEDLINE and Embase. Study characteristics We found two eligible studies reporting on 18 methodological filters, including six MEDLINE, six Embase and six combined MEDLINE/Embase filters. The firsts study focused on filters on observational studies of surgical interventions. The second study focused on filters for a specific subtype of observational studies: comparative non‐randomised studies. Key results Six filters from the first study showed sensitivity of 99.5% to 100% and precision of 16.7% to 21.1%. One type of filter was evaluated by two additional systematic reviews (i.e. externally validated) and found that this retrieved 85.2% to 100% of the articles in the reference standard. Twelve filters from the second study had lower sensitivity (48% to 100%) and much lower precision (0.09% to 4.47%). Quality of evidence The included studies had several limitations. The first study used only one systematic review for search strategy development and focused on observational studies of surgical interventions, which might limit the generalizability of the findings to other literature searches. The reference standard in the second study, although encompassing four different systematic reviews, included a limited number of studies, which might affect the accuracy of the performance assessment. Both studies were published 10 years ago and labelling and indexing of observational studies has changed since then.","3","John Wiley & Sons, Ltd","1465-1858","*Databases, Bibliographic; *Non-Randomized Controlled Trials as Topic; *Observational Studies as Topic; Abstracting and Indexing [standards]; Information Storage and Retrieval [*methods]; MEDLINE; Sensitivity and Specificity; Systematic Reviews as Topic","10.1002/14651858.MR000041.pub2","http://dx.doi.org/10.1002/14651858.MR000041.pub2","Methodology"
"MR000005.PUB4","Scherer, RW; Meerpohl, JJ; Pfeifer, N; Schmucker, C; Schwarzer, G; von Elm, E","Full publication of results initially presented in abstracts","Cochrane Database of Systematic Reviews","2018","Abstract - Background Abstracts of presentations at scientific meetings are usually available only in conference proceedings. If subsequent full publication of results reported in these abstracts is based on the magnitude or direction of the results, publication bias may result. Publication bias creates problems for those conducting systematic reviews or relying on the published literature for evidence about health and social care. Objectives To systematically review reports of studies that have examined the proportion of meeting abstracts and other summaries that are subsequently published in full, the time between meeting presentation and full publication, and factors associated with full publication. Search methods We searched MEDLINE, Embase, the Cochrane Library, Science Citation Index, reference lists, and author files. The most recent search was done in February 2016 for this substantial update to our earlier Cochrane Methodology Review (published in 2007). Selection criteria We included reports of methodology research that examined the proportion of biomedical results initially presented as abstracts or in summary form that were subsequently published. Searches for full publications had to be at least two years after meeting presentation. Data collection and analysis Two review authors extracted data and assessed risk of bias. We calculated the proportion of abstracts published in full using a random‐effects model. Dichotomous variables were analyzed using risk ratio (RR), with multivariable models taking into account various characteristics of the reports. We assessed time to publication using Kaplan‐Meier survival analyses. Main results Combining data from 425 reports (307,028 abstracts) resulted in an overall full publication proportion of 37.3% (95% confidence interval (CI), 35.3% to 39.3%) with varying lengths of follow‐up. This is significantly lower than that found in our 2007 review (44.5%. 95% CI, 43.9% to 45.1%). Using a survival analyses to estimate the proportion of abstracts that would be published in full by 10 years produced proportions of 46.4% for all studies; 68.7% for randomized and controlled trials and 44.9% for other studies. Three hundred and fifty‐three reports were at high risk of bias on one or more items, but only 32 reports were considered at high risk of bias overall. Forty‐five reports (15,783 abstracts) with 'positive' results (defined as any 'significant' result) showed an association with full publication (RR = 1.31; 95% CI 1.23 to 1.40), as did 'positive' results defined as a result favoring the experimental treatment (RR =1.17; 95% CI 1.07 to 1.28) in 34 reports (8794 abstracts). Results emanating from randomized or controlled trials showed the same pattern for both definitions (RR = 1.21; 95% CI 1.10 to 1.32 (15 reports and 2616 abstracts) and RR = 1.17; 95% CI, 1.04 to 1.32 (13 reports and 2307 abstracts), respectively. Other factors associated with full publication include oral presentation (RR = 1.46; 95% CI 1.40 to 1.52; studied in 143 reports with 115,910 abstracts); acceptance for meeting presentation (RR = 1.65; 95% CI 1.48 to 1.85; 22 reports with 22,319 abstracts); randomized trial design (RR = 1.51; 95% CI 1.36 to 1.67; 47 reports with 28,928 abstracts); and basic research (RR = 0.78; 95% CI 0.74 to 0.82; 92 reports with 97,372 abstracts). Abstracts originating at an academic setting were associated with full publication (RR = 1.60; 95% CI 1.34 to 1.92; 34 reports with 16,913 abstracts), as were those considered to be of higher quality (RR = 1.46; 95% CI 1.23 to 1.73; 12 reports with 3364 abstracts), or having high impact (RR = 1.60; 95% CI 1.41 to 1.82; 11 reports with 6982 abstracts). Sensitivity analyses excluding reports that were abstracts themselves or classified as having a high risk of bias did not change these findings in any important way. In considering the reports of the methodology research that we included in this review, we found that reports published in English or from a native English‐speaking country found significantly higher proportions of studies published in full, but that there was no association with year of report publication. The findings correspond to a proportion of abstracts published in full of 31.9% for all reports, 40.5% for reports in English, 42.9% for reports from native English‐speaking countries, and 52.2% for both these covariates combined. Authors' conclusions More than half of results from abstracts, and almost a third of randomized trial results initially presented as abstracts fail to be published in full and this problem does not appear to be decreasing over time. Publication bias is present in that 'positive' results were more frequently published than 'not positive' results. Reports of methodology research written in English showed that a higher proportion of abstracts had been published in full, as did those from native English‐speaking countries, suggesting that studies from non‐native English‐speaking countries may be underrepresented in the scientific literature. After the considerable work involved in adding in the more than 300 additional studies found by the February 2016 searches, we chose not to update the search again because additional searches are unlikely to change these overall conclusions in any important way. Plain language summary Full publication of results initially presented in abstracts Key message Two important factors increase the probability that a study described in an abstract will subsequently be published in full, (1) the presence of 'positive' or statistically significant results in the abstract and (2) whether the team examining subsequent full publication were from an English‐speaking country or wrote their report in English. The consequence is that systematic reviews relying on fully published research may provide inaccurate or biased findings because of an over‐reliance on studies with positive results or from English‐speaking countries. Our question We reviewed the evidence about how often studies submitted as abstracts at a scientific meeting are published in full, usually as a journal article. We found 425 relevant reports, involving 307,028 abstracts. Background Investigators prepare and submit abstracts for presentation at scientific meetings. Abstracts selected for presentation are usually collated as conference proceedings, but these are not easily found. Thus, it is important to know whether the work submitted and presented is later published as a journal article, which can easily be identified and contains more study information than the abstract. It is also important to know if the publication of the study depends on the size or direction of results or other factors. If so, systematic reviews relying on the published literature for evidence about health and social care will have incomplete or unbalanced information, leading to inaccurate or biased estimates of the effects of the interventions studied. Study characteristics We included 425 research reports described in 551 articles, which had studied the subsequent full publication of 307,028 abstracts from a variety of biomedical and social sciences. Fifty‐four reports included data from abstracts describing randomized or controlled trials. Of the 425 reports, 376 were published in English, and 49 in other languages. Key results 1. Less than half of all studies, and about two‐thirds of randomized trials, initially presented as summaries or abstracts at meetings, are published as journal articles in the 10 years after presentation. 2. Studies with positive results are more likely to be published. 3. Studies with larger sample sizes are more likely to be published. 4. Studies with abstracts presented orally are more likely to be published than those presented as posters. 5. Studies accepted for presentation at a meeting are more likely to be published than those not accepted. 6. Studies describing basic science are more likely to be published that those describing clinical research. 7. Studies describing randomized trials are more likely to be published than those describing other types of studies. 8. Studies that took place in multiple centers are more likely to be published than those at a single center. 9. Studies classified as ‘high quality’ are more likely to be published than ‘low quality’ studies. 10. Studies with authors from an academic setting are more likely to be published than those with authors from other settings. 11. Studies considered by the report authors to have a high impact are more likely to be published than other studies. 12. Studies with funding source reported are more likely to be published than those not reporting funding. 13. Studies originating in North America or Europe are more likely to be published than those originating elsewhere. 14. Studies from English‐speaking countries are more likely to be published than studies originating elsewhere. Quality of the evidence We have confidence in our findings. We considered five criteria to constitute a risk of bias in the included reports, including methods to identify and match full publications to abstracts, and methods to determine whether a factor was associated with full publication. Overall, 7.5% (32/425) of the reports were scored as having an overall high risk of bias, 83.1% (353/425) had at least one criterion at high risk of bias, and 6.1% (26/425) had all criteria at low risk of bias. Search Date Our search updated our 2007 review and is current to February 2016. After the considerable work involved in including more than 300 additional studies from the February 2016 searches, we chose not to update the search again because additional searches are unlikely to change our overall conclusions in any important way.","11","John Wiley & Sons, Ltd","1465-1858","*Congresses as Topic; Abstracting and Indexing [*statistics & numerical data]; Controlled Clinical Trials as Topic [statistics & numerical data]; Publication Bias; Publishing [*statistics & numerical data]; Randomized Controlled Trials as Topic [statistics & numerical data]; Time Factors","10.1002/14651858.MR000005.pub4","http://dx.doi.org/10.1002/14651858.MR000005.pub4","Methodology"
"MR000013.PUB6","Treweek, S; Pitkethly, M; Cook, J; Fraser, C; Mitchell, E; Sullivan, F; Jackson, C; Taskila, TK; Gardner, H","Strategies to improve recruitment to randomised trials","Cochrane Database of Systematic Reviews","2018","Abstract - Background Recruiting participants to trials can be extremely difficult. Identifying strategies that improve trial recruitment would benefit both trialists and health research. Objectives To quantify the effects of strategies for improving recruitment of participants to randomised trials. A secondary objective is to assess the evidence for the effect of the research setting (e.g. primary care versus secondary care) on recruitment. Search methods We searched the Cochrane Methodology Review Group Specialised Register (CMR) in the Cochrane Library (July 2012, searched 11 February 2015); MEDLINE and MEDLINE In Process (OVID) (1946 to 10 February 2015); Embase (OVID) (1996 to 2015 Week 06); Science Citation Index & Social Science Citation Index (ISI) (2009 to 11 February 2015) and ERIC (EBSCO) (2009 to 11 February 2015). Selection criteria Randomised and quasi‐randomised trials of methods to increase recruitment to randomised trials. This includes non‐healthcare studies and studies recruiting to hypothetical trials. We excluded studies aiming to increase response rates to questionnaires or trial retention and those evaluating incentives and disincentives for clinicians to recruit participants. Data collection and analysis We extracted data on: the method evaluated; country in which the study was carried out; nature of the population; nature of the study setting; nature of the study to be recruited into; randomisation or quasi‐randomisation method; and numbers and proportions in each intervention group. We used a risk difference to estimate the absolute improvement and the 95% confidence interval (CI) to describe the effect in individual trials. We assessed heterogeneity between trial results. We used GRADE to judge the certainty we had in the evidence coming from each comparison. Main results We identified 68 eligible trials (24 new to this update) with more than 74,000 participants. There were 63 studies involving interventions aimed directly at trial participants, while five evaluated interventions aimed at people recruiting participants. All studies were in health care. We found 72 comparisons, but just three are supported by high‐certainty evidence according to GRADE. 1.  Open trials rather than blinded, placebo trials . The absolute improvement was 10% (95% CI 7% to 13%). 2.  Telephone reminders to people who do not respond to a postal invitation . The absolute improvement was 6% (95% CI 3% to 9%). This result applies to trials that have low underlying recruitment. We are less certain for trials that start out with moderately good recruitment (i.e. over 10%). 3.  Using a particular, bespoke, user‐testing approach to develop participant information leaflets . This method involved spending a lot of time working with the target population for recruitment to decide on the content, format and appearance of the participant information leaflet. This made little or no difference to recruitment: absolute improvement was 1% (95% CI −1% to 3%). We had moderate‐certainty evidence for eight other comparisons; our confidence was reduced for most of these because the results came from a single study. Three of the methods were changes to trial management, three were changes to how potential participants received information, one was aimed at recruiters, and the last was a test of financial incentives. All of these comparisons would benefit from other researchers replicating the evaluation. There were no evaluations in paediatric trials. We had much less confidence in the other 61 comparisons because the studies had design flaws, were single studies, had very uncertain results or were hypothetical (mock) trials rather than real ones. Authors' conclusions The literature on interventions to improve recruitment to trials has plenty of variety but little depth. Only 3 of 72 comparisons are supported by high‐certainty evidence according to GRADE: having an open trial and using telephone reminders to non‐responders to postal interventions both increase recruitment; a specialised way of developing participant information leaflets had little or no effect. The methodology research community should improve the evidence base by replicating evaluations of existing strategies, rather than developing and testing new ones. Plain language summary What improves trial recruitment? Key messages We had high‐certainty evidence for three methods to improve recruitment, two of which are effective: 1. Telling people what they are receiving in the trial rather than not telling them improves recruitment. 2. Phoning people who do not respond to a postal invitation is also effective (although we are not certain this works as well in all trials). 3. Using a tailored, user‐testing approach to develop participant information leaflets makes little or no difference to recruitment. Of the 72 strategies tested, only 7 involved more than one study. We need more studies to understand whether they work or not. Our question We reviewed the evidence about the effect of things trial teams do to try and improve recruitment to their trials. We found 68 studies involving more than 74,000 people. Background Finding participants for trials can be difficult, and trial teams try many things to improve recruitment. It is important to know whether these actually work. Our review looked for studies that examined this question using chance to allocate people to different recruitment strategies because this is the fairest way of seeing if one approach is better than another. Key results We found 68 studies including 72 comparisons. We have high certainty in what we found for only three of these. 1. Telling people what they are receiving in the trial rather than not telling them improves recruitment. Our best estimate is that if 100 people were told what they were receiving in a randomised trial, and 100 people were not, 10 more would take part n the group who knew. There is some uncertainty though: it could be as few as 7 more per hundred, or as many as 13 more. 2. Phoning people who do not respond to a postal invitation to take part is also effective. Our best estimate is that if investigators called 100 people who did not respond to a postal invitation, and did not call 100 others, 6 more would take part in the trial among the group who received a call. However, this number could be as few as 3 more per hundred, or as many as 9 more. 3. Using a tailored, user‐testing approach to develop participant information leaflets did not make much difference. The researchers who tested this method spent a lot of time working with people like those to be recruited to decide what should be in the participant information leaflet and what it should look like. Our best estimate is that if 100 people got the new leaflet, 1 more would take part in the trial compared to 100 who got the old leaflet. However, there is some uncertainty, and it could be 1 fewer (i.e. worse than the old leaflet) per hundred, or as many as 3 more. We had moderate certainty in what we found for eight other comparisons; our confidence was reduced for most of these because the method had been tested in only one study. We had much less confidence in the other 61 comparisons because the studies had design flaws, were the only studies to look at a particular method, had a very uncertain result or were mock trials rather than real ones. Study characteristics The 68 included studies covered a very wide range of disease areas, including antenatal care, cancer, home safety, hypertension, podiatry, smoking cessation and surgery. Primary, secondary and community care were included. The size of the studies ranged from 15 to 14,467 participants. Studies came from 12 countries; there was also one multinational study involving 19 countries. The USA and UK dominated with 25 and 22 studies, respectively. The next largest contribution came from Australia with eight studies. The small print Our search updated our 2010 review and is current to February 2015. We also identified six studies published after 2015 outside the search. The review includes 24 mock trials where the researchers asked people about whether they would take part in an imaginary trial. We have not presented or discussed their results because it is hard to see how the findings relate to real trial decisions.","2","John Wiley & Sons, Ltd","1465-1858","*Patient Selection; *Randomized Controlled Trials as Topic; *Reminder Systems; Humans; Patient Education as Topic; Sample Size; Telephone","10.1002/14651858.MR000013.pub6","http://dx.doi.org/10.1002/14651858.MR000013.pub6","Methodology"
"CD011450.PUB2","Mohammed Vashist, N; Samaan, M; Mosli, MH; Parker, CE; MacDonald, JK; Nelson, SA; Zou, GY; Feagan, BG; Khanna, R; Jairath, V","Endoscopic scoring indices for evaluation of disease activity in ulcerative colitis","Cochrane Database of Systematic Reviews","2018","Abstract - Background Endoscopic assessment of mucosal disease activity is routinely used to determine eligibility and response to therapy in clinical trials of ulcerative colitis. The operating properties of the existing endoscopic scoring indices are unclear. Objectives A systematic review was undertaken to evaluate the development and operating characteristics of endoscopic scoring indices for the evaluation of ulcerative colitis. Search methods We searched MEDLINE, Embase and CENTRAL from inception to 5 July 2016. We also searched references and conference proceedings (Digestive Disease Week, United European Gastroenterology Week, European Crohn’s and Colitis Organization). Selection criteria Any study design (e.g. randomized controlled trials, cohort studies, case series) that evaluated endoscopic indices for evaluation of ulcerative colitis disease activity were considered for inclusion. Eligible participants were adult patients (> 16 years), diagnosed with ulcerative colitis using conventional clinical, radiologic and endoscopic criteria. Data collection and analysis Two authors independently reviewed the studies identified from the literature search. These authors also independently extracted and recorded data on the number of patients enrolled; number of patients per treatment arm; patient characteristics including age and gender distribution; endoscopic index; and outcomes such as reliability (intra‐rater and inter‐rater), validity (content, construct, criterion), responsiveness and feasibility. Any disagreements regarding study inclusion or data extraction were resolved by discussion and consensus with a third author. Risk of bias was assessed by determining whether assessors were blinded to clinical information and whether assessors scored the endoscopic index independently. We also assessed the methodological quality of the validation studies using the COSMIN checklist Main results A total of 23 reports of 20 studies met the pre‐defined inclusion criteria and were included in the review. Of the 20 included validation studies, 19 endoscopic scoring indices were assessed, including the Azzolini Classification, Baron Score, Blackstone Endoscopic Interpretation, Chinese Grading System of Ulcerative Colitis, Endoscopic Activty Index, Jeroen Score, Magnifying Colonoscopy Grade, Matts Score, Mayo Clinic Endoscopic Subscore, Modified Baron Score, Modified Mayo Clinic Endoscopic Subscore, Osada Score, Rachmilewtiz Endoscopic Score, St. Mark's Index, Ulcerative Colitis Colonoscopic Index of Serverity (UCCIS), endoscopic component of the Ulcerative Colitis Disease Activity Index (UCDAI), Ulcerative Colitis Endoscopic Index of Severity (UCEIS), Witts Sigmoidoscopic Score and Watson Grade. The individuals who performed the endoscopic scoring were blinded to clinical and/or histologic information in ten of the included studies, not blinded to clinical and/or histologic information in one of the included studies, and it was unclear whether blinding occurred in the remaining nine included studies. Independent observation was confirmed in four of the included studies, unclear in five of the included studies, and non‐applicable (since inter‐rater reliability was not assessed) in the remaining eleven included studies. The methodological quality (COSMIN checklist) of most of the included studies was rated as 'good' or 'excellent'. One study that assessed responsiveness was rated as 'fair'. The inter‐rater reliability of nine endoscopic scoring indices including the Baron Score, Blackstone Endoscopic Interpretation, Endoscopic Activity Index, Matts Score, Mayo Clinic Endoscopic Subscore, Osada Score, UCCIS, UCEIS, Watson Grade was assessed in seven studies, with estimates of correlation, ƙ, ranging from 0.44 to 0.97. The iIntra‐rater reliability of seven endoscopic scoring indices including the Baron Score, Blackstone Endoscopic Interpretation, Matts Score, Mayo Clinic Endoscopic Subscore, Osada Score, UCCIS and UCEIS was assessed in three studies, with estimates of correlation, ƙ, ranging from 0.41 to 0.86. No studies assessed content validity. Three studies evaluated the criterion validity of three endoscopic scoring indices including the Rachmilewitz Endoscopic Score, Magnifying Colonoscopy Grade and the UCCIS. These indices were correlated with objective markers of disease activity including albumin, blood leukocytes, C‐reactive protein, fecal calprotectin, hemoglobin, mucosal interleukin‐8 concentration and platelet count. Correlation estimates ranged from r = ‐0.19 to 0.83. Thirteen endoscopic scoring indices were tested for construct validity in 13 studies. Estimates of correlation between the endoscopic scoring indices and other measures of disease activity ranged from r = 0.27 to 0.93. Two studies explored the responsiveness of four endoscopic scoring indices including the Mayo Endoscopic Subscore, Modified Baron Score, Modified Mayo Endoscopic Subscore and UCEIS. One study concluded that the Modified Baron Score, Modified Mayo Endoscopic Subscore and UCEIS had similar responsiveness for detecting disease change in ulcerative colitis. The other included study concluded that the UCEIS may be the most accurate endoscopic scoring tool. None of the included studies formally assessed feasibility. Authors' conclusions While the UCEIS, UCCIS and Mayo Clinic Endoscopic Subscore have undergone extensive validation, none of these instruments have been fully validated and only two studies assessed responsiveness. Further research on the operating properties of these indices is needed given the lack of a fully‐validated endoscopic scoring instrument for the evaluation of disease activity in ulcerative colitis. Plain language summary Endoscopic scoring indices for evaluation of disease activity in ulcerative colitis What is ulcerative colitis? Ulcerative colitis is an inflammatory bowel disease characterized by long‐term (chronic) inflammation and ulcers (sores) in the inner most lining of the large intestine and the rectum. Common symptoms include diarrhea, abdominal pain and cramping, weight loss and tiredness. What is an endoscopic scoring index? An endoscopic scoring index measures disease activity based on what a physician can see during an endoscopy procedure. An endoscopy is a non‐surgical procedure whereby a small camera is used to view the digestive tract. The physician who performs the endoscopy may rate disease activity using the index, or this may be done by another physician if the procedure was video recorded or photographs were taken. Commonly used endoscopic indices include the Baron Score, Rachmilewitz Index, Ulcerative Colitis Endoscopic Index of Severity, Mayo Clinic Endoscopic Subscore, and the Ulcerative Colitis Colonoscopic Index of Serverity. What did the researchers investigate? It is important for endoscopic indices to be valid, meaning that they accurately evaluate what they are intended to measure. The researchers investigated the validity of various endoscopic indices for assessing disease activity in ulcerative colitis. While the Ulcerative Colitis Endoscopic Index of Severity, Mayo Clinic Endoscopic Subscore, and the Ulcerative Colitis Colonoscopic Index of Serverity have undergone extensive validation compared to the other indices, none of these instruments have been fully validated, What did the researchers find? The researchers found that none of the currently used endoscopic indices have been fully validated. Further research on the operating properties of these indices is needed given the lack of a fully‐validated endoscopic scoring instrument for the evaluation of disease activity in ulcerative colitis.","1","John Wiley & Sons, Ltd","1465-1858","*Colonoscopy; Colitis, Ulcerative [*diagnosis, pathology]; Humans; Reproducibility of Results; Severity of Illness Index; Sigmoidoscopy","10.1002/14651858.CD011450.pub2","http://dx.doi.org/10.1002/14651858.CD011450.pub2","Gut"
"MR000039.PUB2","Allen, EN; Chandler, CIR; Mandimika, N; Leisegang, C; Barnes, K","Eliciting adverse effects data from participants in clinical trials","Cochrane Database of Systematic Reviews","2018","Abstract - Background Analysis of drug safety in clinical trials involves assessing adverse events (AEs) individually or by aggregate statistical synthesis to provide evidence of likely adverse drug reactions (ADR). While some AEs may be ascertained from physical examinations or tests, there is great reliance on reports from participants to detect subjective symptoms, where he/she is often the only source of information. There is no consensus on how these reports should be elicited, although it is known that questioning methods influence the extent and nature of data detected. This leaves room for measurement error and undermines comparisons between studies and pooled analyses. This review investigated comparisons of methods used in trials to elicit participant‐reported AEs. This should contribute to knowledge about the methodological challenges and possible solutions for achieving better, or more consistent, AE ascertainment in trials. Objectives To systematically review the research that has compared methods used within clinical drug trials (or methods that would be specific for such trials) to elicit information about AEs defined in the protocol or in the planning for the trial. Search methods Databases (searched to March 2015 unless indicated otherwise) included: Embase; MEDLINE; MEDLINE in Process and Other Non‐Indexed Citations; Cochrane Methodology Register (July 2012); Cochrane Central Register of Controlled Trials (February 2015); Cochrane Database of Systematic Reviews; Database of Abstracts of Reviews of Effects (January 2015); Health Technology Assessment database (January 2015); CINAHL; CAB Abstracts; BIOSIS (July 2013); Science Citation Index; Social Science Citation Index; Conference Proceedings Citation Index – Science. The search used thesaurus headings and synonyms for the following concepts: (A): Adverse events AND measurement; (B): Participants AND elicitation (also other synonyms for extraction of information about adverse effects from people); (C): Participants AND checklists (also other synonyms as for B). Pragmatic ways were used to limit the results whilst trying to maintain sensitivity. There were no date or sample size restrictions but only reports published in English were included fully, because of resource constraints as regards translation. Selection criteria Two types of studies were included: drug trials comparing two or more methods within‐ or between‐participants to elicit participant‐reported AEs, and research studies performed outside the context of a trial to compare methods which could be used in trials (evidenced by reference to such applicability). Primary outcome data included AEs elicited from participants taking part in any such clinical trial. We included any participant‐reported data relevant for an assessment of drug‐related harm, using the original authors' terminology (and definition, where available), with comment on whether the data were likely to be treatment‐emergent AEs or not. Data collection and analysis Titles and abstracts were independently reviewed for eligibility. Full texts of potentially eligible citations were independently reviewed for final eligibility. Relevant data were extracted and subjected to a 100% check. Disagreements were resolved by discussion, involving a third author. The risk of bias was independently assessed by two authors. The Cochrane 'Risk of bias' tool was used for reports comparing outcomes between participants, while for within‐participant comparisons, each study was critically evaluated in terms of potential impact of the design and conduct on findings using the framework of selection, performance, detection, attrition, reporting, and other biases. An attempt was made to contact authors to retrieve protocols or specific relevant missing information. Reports were not excluded on the basis of quality unless data for outcomes were impossible to compare (e.g. where denominators differed). A narrative synthesis was conducted because differences in study design and presentation meant that a quantitative meta‐analysis was not possible. Main results The 33 eligible studies largely compared open questions with checklist‐type questions or rating scales. Two included participant interviews. Despite different designs, populations and details of questioning methods, the narrative review showed that more specific questioning of participants led to more AEs detected compared to a more general enquiry. A subset of six studies suggested that more severe, bothersome, or otherwise clinically relevant AEs were reported when an initial open enquiry was used, while some less severe, bothersome, or clinically relevant AEs were only reported with a subsequent specific enquiry. However, two studies showed that quite severe or debilitating AEs were only detected by an interview, while other studies did not find a difference in the nature of AEs between elicitation methods. No conclusions could be made regarding the impact of question method on the ability to detect a statistically significant difference between study groups. There was no common statistical rubric, but we were able to represent some effect measures as a risk ratio of the proportion of participants with at least one AE. This showed a lower level of reporting for open questions (O) compared to checklists (CL), with a range for the risk ratios of 0.12 to 0.64. Authors' conclusions This review supports concerns that methods to elicit participant‐reported AEs influence the detection of these data. There was a risk for under‐detection of AEs in studies using a more general elicitation method compared to those using a comprehensive method. These AEs may be important from a clinical perspective or for patients. This under‐detection could compromise ability to pool AE data. However, the impact on the nature of the AE detected by different methods is unclear. The wide variety and low quality of methods to compare elicitation strategies limited this review. Future studies would be improved by using and reporting clear definitions and terminology for AEs (and other important variables), frequency and time period over which they were ascertained, how they were graded, assessed for a relationship to the study drug, coded, and tabulated/reported. While the many potential AE endpoints in a trial may preclude the development of general AE patient‐reported outcome measurement instruments, much could also be learnt from how these employ both quantitative and qualitative methods to better understand data elicited. Any chosen questioning method needs to be feasible for use by both staff and participants. Plain language summary Questioning clinical trial participants about their health in order to collect information on adverse effects of drugs Clinical drug trials or studies are usually conducted to assess how well the drug works but also whether it causes any harm (side effects or adverse effects). Adverse effects can be detected by the trial doctor examining participants or taking some blood samples or doing other kinds of tests. The trial staff can also ask participants about how they are feeling after taking the trial drug. However, the way participants are asked about their health can vary from trial to trial, or even within a trial. In some trials, participants may be asked a simple open question such as 'how have you been feeling?', while in other trials, participants may be asked about whether they have had any of a long list of possible symptoms (such as 'have you had a headache, stomach ache, or sore muscles?'). There has been concern that these different kinds of questions and how they are phrased will impact on what participants report about their health during a trial. This might then affect the trial's results and what we know about the side effects of drugs. We did this review to look at studies that compared different types of participant questioning methods in order to investigate these issues. We found 33 studies comparing mainly open questions with checklist‐type questions, but also some ratings scales and participant interviews. While the studies were all very different in terms of the types of disease, drugs, and patients studied, we found in general that, as would be expected, when a more specific type of question was asked (like a checklist), participants reported more symptoms. What is interesting is that, in those studies that looked more closely at the types of symptoms reported, it seems that an open question picks up the more severe or bothersome symptoms compared to a checklist‐type question. However, some studies found that even quite severe or bothersome symptoms were not reported when a participant is asked an open question and these severe symptoms will only be reported with the more specific question. This makes it difficult to say whether one method is better than any other and the different questioning methods may, in fact, be complementary and therefore should be used together. It is also difficult to say what a specific question should include, as it might take too long for a participant to have to answer a very long list. While more research is needed to resolve the remaining uncertainties, it is very important for trials to be clear about which kind of questioning was used when they publish their results. This will help readers understand the trial's findings about the side effects and make it easier to make accurate comparisons between trials.","1","John Wiley & Sons, Ltd","1465-1858","*Checklist; *Clinical Trials as Topic; *Drug‐Related Side Effects and Adverse Reactions; *Research Subjects; Humans","10.1002/14651858.MR000039.pub2","http://dx.doi.org/10.1002/14651858.MR000039.pub2","Methodology"
"CD012351.PUB2","Novak, G; Parker, CE; Pai, RK; MacDonald, JK; Feagan, BG; Sandborn, WJ; D'Haens, G; Jairath, V; Khanna, R","Histologic scoring indices for evaluation of disease activity in Crohn’s disease","Cochrane Database of Systematic Reviews","2017","Abstract - Background Histologic assessment of mucosal disease activity has been increasingly used in clinical trials of treatment for Crohn's disease. However, the operating properties of the currently existing histologic scoring indices remain unclear. Objectives A systematic review was undertaken to evaluate the development and operating characteristics of available histologic disease activity indices in Crohn's disease. Search methods Electronic searches of MEDLINE, EMBASE, PubMed, and the Cochrane Library (CENTRAL) databases from inception to 20 July 2016 were supplemented by manual reviews of bibliographies and abstracts submitted to major gastroenterology meetings (Digestive Disease Week, United European Gastroenterology Week, European Crohn's and Colitis Organisation). Selection criteria Any study design (e.g. randomised controlled trial, cohort study, case series) that evaluated a histologic disease activity index in patients with Crohn’s disease was considered for inclusion. Study participants included adult patients ( >  16 years), diagnosed with Crohn’s disease using conventional clinical, radiographic or endoscopic criteria. Data collection and analysis Two authors independently reviewed the titles and abstracts of the studies identified from the literature search. The full text of potentially relevant citations were reviewed for inclusion and the study investigators were contacted as needed for clarification. Any disagreements regarding study eligibility were resolved by discussion and consensus with a third author. Two authors independently extracted and recorded data using a standard form. The following data were recorded from each eligible study: number of patients enrolled; number of patients per treatment arm; patient characteristics: age and gender distribution; description of histologic disease activity index utilized; and outcomes such as content validity, construct validity, criterion validity, responsiveness, intra‐rater reliability, inter‐rater reliability, and feasibility. Main results Sixteen reports of 14 studies describing 14 different numerical histological indices fulfilled the inclusion criteria. Inter‐rater reliability was assessed in one study. For the Naini and Cortina Score, estimates of correlation were 'almost perfect', ranging from r = 0.94 to 0.96. The methodological quality of this study with respect to reliability was 'good'. With respect to validity, correlation estimates between various histological scoring systems and Crohn's disease activity as measured by objective markers of inflammation (including C‐reactive protein, erythrocyte sedimentation rate, fecal calprotectin and fecal lactoferrin); endoscopic disease activity scores; clinical disease activity scores; and quality of life questionnaires were reported. Comparisons between histologic scoring indices and endoscopic scoring indices ranged from no correlation to 'substantial' (r = 0.779). The methodological quality of the studies that explored validity ranged form 'poor' to 'good'. Responsiveness data were available in seven studies. After subjects were administered a treatment of known efficacy, statistically significant change in the index score was demonstrated in five studies with respect to six indices. Two studies failed to indicate whether there was statistically significant change in the index score post‐treatment. With regard to methodological quality, six of the studies were rated as 'poor' and one of the studies was rated as 'fair'. Feasibility was assessed by one study. The Naini and Cortina Score was shown to be simple to use and feasible for every given case. Authors' conclusions Currently there is no fully validated histological scoring index for evaluation of Crohn's disease activity. Development of a validated histological scoring index for Crohn’s disease is a clinical and research priority. Plain language summary Histologic measurement tools for evaluation of disease activity in Crohn’s disease What is Crohn's disease? Crohn’s disease is a life‐long (chronic), inflammatory disease of the gastrointestinal tract characterized by abdominal pain (cramping), rectal bleeding, diarrhoea, weight loss, and tiredness. The disease has a changing course with periods of symptoms (called 'active' disease or relapse) and periods without symptoms (called 'remission'). What is a histological scoring index? A histological scoring index measures disease activity based on the examination of biopsy specimens from the bowel (small pieces of tissue removed from the bowel) under a microscope. Biopsy specimens are removed from the bowel during colonoscopy (a non‐surgical procedure used to view the digestive tract using a camera) with biopsy forceps (instruments to grasp and remove pieces of tissue). Biopsies are then processed and assessed under microscope by a pathologist (a physician who interprets and diagnoses the changes caused by disease in tissues) who then rates disease activity using the index. What did the researchers investigate? It is important that histological scoring indices measure what they are supposed to measure (validity); that they detect change after treatment (responsiveness); that the scores are consistently reproducible (reliability); and that they can be easily utilized (feasibility). The researchers investigated whether studies have assessed the validity, responsiveness, reliability and feasibility of histological scoring indices. What did the researcher find? The researchers found that none of the existing histological scoring indices have been fully validated.","7","John Wiley & Sons, Ltd","1465-1858","Adult; Biopsy; Colon [pathology]; Colonoscopy; Crohn Disease [*pathology]; Histological Techniques [methods, standards]; Humans; Ileum [pathology]; Prospective Studies; Rectum [pathology]; Reproducibility of Results; Retrospective Studies","10.1002/14651858.CD012351.pub2","http://dx.doi.org/10.1002/14651858.CD012351.pub2","Gut"
"CD011256.PUB2","Mosli, MH; Parker, CE; Nelson, SA; Baker, KA; MacDonald, JK; Zou, GY; Feagan, BG; Khanna, R; Levesque, BG; Jairath, V","Histologic scoring indices for evaluation of disease activity in ulcerative colitis","Cochrane Database of Systematic Reviews","2017","Abstract - Background Disease activity can be determined using clinical, endoscopic or histologic criteria in patients with ulcerative colitis (UC). Persistent disease activity is associated with poor outcomes. Histologic disease activity has been shown to be associated with relapse, colectomy and colorectal cancer. The ability to objectively evaluate microscopic disease activity using histology is important for both clinical practice and clinical trials. However, the operating properties of the currently available histologic indices remain unclear. Objectives A systematic review was undertaken to identify and evaluate the development and operating characteristics of histologic disease activity indices used to assess disease activity in people with ulcerative colitis. Search methods We searched MEDLINE, EMBASE, PubMed, CENTRAL and the Cochrane IBD Review Group Specialized Trials Register from inception to 2 December 2016 for applicable studies. There were no language or document type restrictions. Selection criteria Any study design (e.g. randomized controlled trials, cohort studies, case series) that evaluated a histologic index in patients with UC were considered for inclusion. Eligible patients were adults ( >  18 years), diagnosed with UC using conventional clinical, radiographic, endoscopic and histologic criteria. Data collection and analysis Two authors (MHM and CEP) independently reviewed the titles and abstracts of the studies identified from the literature search. A standardized form was used to assess eligibility of trials for inclusion and for data extraction. Two authors (MHM and CEP) independently extracted and recorded data, which included the number of patients enrolled, number of patients per treatment arm, patient characteristics including age and gender distribution, and the name of the histologic index. Outcomes (i.e. intra‐rater reliability, inter‐rater reliability, internal consistency, content validity, criterion validity, construct validity, responsiveness, and feasibility) were recorded for each trial. Main results In total, 126 reports describing 30 scoring indices were identified through the screening process. Eleven of the 30 scoring indices have undergone some form of index validation. Intra‐rater reliability was assessed for eight scoring indices. Inter‐rater reliability was evaluated for all 11 of the scoring indices. Three of the indices underwent content validation. Two of the included scoring indices assessed criterion validity. Six of the included scoring indices explored content validity. Two of the included scoring indices were tested for responsiveness. Authors' conclusions The Nancy Index and the Robarts Histopathology Index have undergone the most validation in that four operating properties including reliability, content validity, construct validity (hypothesis testing) and criterion validity have been tested. However, none of the currently available histologic scoring indices have been fully validated. In order to determine the optimal endpoint for histologic healing in UC, more research is required. The optimal index would need to be fully validated. Plain language summary Histologic measurement tools for evaluating disease in ulcerative colitis patients What is ulcerative colitis? Ulcerative colitis is a life‐long (chronic) inflammatory bowel disease that causes inflammation and ulceration (sores) in the large intestine (colon). Patients with ulcerative colitis often experience diarrhoea, bloody stools, weight loss and abdominal pain. When patients experience symptoms, the disease is considered ""active"", whereas when symptoms are not present the disease is considered to be ""in remission"". What is a histologic scoring index? A histologic tissue sample (biopsy) can be taken from a patient's colon during colonoscopy. A colonoscopy is a non‐surgical procedure used to view the large intestine. Once the tissue sample is taken, it is placed (mounted) on a glass slide and looked at using a microscope. A histologic scoring index is a system used to assess the patient's disease severity using the tissue sample. What did the researchers investigate? It is important that histologic scoring indices are valid (i.e. they accurately measure what they are supposed to measure). The researchers identified histologic scoring indices that have been validated. What did the researchers find? The researchers found that 11 out of the 30 histologic scoring indices that exist have been partially validated. The Nancy Index and the Robarts Histopathology Index have undergone the most validation compared to the other nine indices. However, none of the currently available histologic scoring indices have been fully validated. In order to determine the ideal index to measure histologic healing in UC, more research is required. The ideal index would need to be fully validated.","5","John Wiley & Sons, Ltd","1465-1858","*Severity of Illness Index; Blood Sedimentation; C-Reactive Protein [analysis]; Colitis, Ulcerative [*pathology]; Feces [chemistry]; Humans; Lactoferrin [analysis]; Leukocyte Count; Leukocyte L1 Antigen Complex [analysis]; Observer Variation; Pancreatic Elastase [analysis]; Reproducibility of Results","10.1002/14651858.CD011256.pub2","http://dx.doi.org/10.1002/14651858.CD011256.pub2","Gut"
"MR000033.PUB3","Lundh, A; Lexchin, J; Mintzes, B; Schroll, JB; Bero, L","Industry sponsorship and research outcome","Cochrane Database of Systematic Reviews","2017","Abstract - Background Clinical research affecting how doctors practice medicine is increasingly sponsored by companies that make drugs and medical devices. Previous systematic reviews have found that pharmaceutical‐industry sponsored studies are more often favorable to the sponsor’s product compared with studies with other sources of sponsorship. A similar association between sponsorship and outcomes have been found for device studies, but the body of evidence is not as strong as for sponsorship of drug studies. This review is an update of a previous Cochrane review and includes empirical studies on the association between sponsorship and research outcome. Objectives To investigate whether industry sponsored drug and device studies have more favorable outcomes and differ in risk of bias, compared with studies having other sources of sponsorship. Search methods In this update we searched MEDLINE (2010 to February 2015), Embase (2010 to February 2015), the Cochrane Methodology Register (2015, Issue 2) and Web of Science (June 2015). In addition, we searched reference lists of included papers, previous systematic reviews and author files. Selection criteria Cross‐sectional studies, cohort studies, systematic reviews and meta‐analyses that quantitatively compared primary research studies of drugs or medical devices sponsored by industry with studies with other sources of sponsorship. We had no language restrictions. Data collection and analysis Two assessors screened abstracts and identified and included relevant papers. Two assessors extracted data, and we contacted authors of included papers for additional unpublished data. Outcomes included favorable results, favorable conclusions, effect size, risk of bias and whether the conclusions agreed with the study results. Two assessors assessed risk of bias of included papers. We calculated pooled risk ratios (RR) for dichotomous data (with 95% confidence intervals (CIs)). Main results Twenty‐seven new papers were included in this update and in total the review contains 75 included papers. Industry sponsored studies more often had favorable efficacy results, RR: 1.27 (95% CI: 1.17 to 1.37) (25 papers) (moderate quality evidence), similar harms results RR: 1.37 (95% CI: 0.64 to 2.93) (four papers) (very low quality evidence) and more often favorable conclusions RR: 1.34 (95% CI: 1.19 to 1.51) (29 papers) (low quality evidence) compared with non‐industry sponsored studies. Nineteen papers reported on sponsorship and efficacy effect size, but could not be pooled due to differences in their reporting of data and the results were heterogeneous. We did not find a difference between drug and device studies in the association between sponsorship and conclusions (test for interaction, P = 0.98) (four papers). Comparing industry and non‐industry sponsored studies, we did not find a difference in risk of bias from sequence generation, allocation concealment, follow‐up and selective outcome reporting. However, industry sponsored studies more often had low risk of bias from blinding, RR: 1.25 (95% CI: 1.05 to 1.50) (13 papers), compared with non‐industry sponsored studies. In industry sponsored studies, there was less agreement between the results and the conclusions than in non‐industry sponsored studies, RR: 0.83 (95% CI: 0.70 to 0.98) (six papers). Authors' conclusions Sponsorship of drug and device studies by the manufacturing company leads to more favorable efficacy results and conclusions than sponsorship by other sources. Our analyses suggest the existence of an industry bias that cannot be explained by standard 'Risk of bias' assessments. Plain language summary Industry sponsorship and research outcome Results from clinical studies on drugs and medical devices affect how doctors practice medicine and thereby the treatments offered to patients. However, clinical research is increasingly sponsored by companies that make these products, either because the companies directly perform the studies, or fully or partially fund them. Previous research has found that pharmaceutical industry sponsored studies tend to favor the sponsors’ drugs more than studies with any other sources of sponsorship. This suggests that industry sponsored studies are biased in favor of the sponsor’s products. This review is an update of a previous review that looked at sponsorship of drug and device studies. The primary aim of the review was to find out whether the published results and overall conclusions of industry sponsored drug and device studies were more likely to favor the sponsors’ products, compared with studies with other sources of sponsorship. The secondary aim was to find out whether such industry sponsored studies used methods that increase the risk of bias, again compared with studies with other sources of sponsorship. In this update, we carried out a comprehensive search of all relevant papers of empirical studies published from 2010 to February 2015 and included 27 new papers, yielding a total of 75 papers included in our review. Industry sponsored drug and device studies more often had efficacy results that were favorable to the sponsors' products, (risk ratio (RR): 1.27, 95% confidence interval (CI): 1.17 to 1.37), similar harms results (RR: 1.37, 95% CI: 0.64 to 2.93) and favorable overall conclusions (RR: 1.34, 95% CI: 1.19 to 1.51), compared with non‐industry sponsored drug and device studies. We did not find a difference between industry and non‐industry sponsored studies with respect to standard methodological factors that may increase the risk of bias, except for blinding: industry sponsored studies reported satisfactory blinding more often than non‐industry sponsored studies. In industry sponsored studies, there was less agreement between the results and the conclusions than in non‐industry sponsored studies, RR: 0.83 (95% CI: 0.70 to 0.98).We did not find a difference between drug and device studies in the association between sponsorship and conclusions. Our analysis suggests that industry sponsored drug and device studies are more often favorable to the sponsor’s products than non‐industry sponsored drug and device studies due to biases that cannot be explained by standard 'Risk of bias' assessment tools.","2","John Wiley & Sons, Ltd","1465-1858","*Conflict of Interest; *Equipment and Supplies; *Industry; Data Interpretation, Statistical; Drug Industry; Publication Bias; Research Report [*standards]; Research Support as Topic [*standards]; Treatment Outcome","10.1002/14651858.MR000033.pub3","http://dx.doi.org/10.1002/14651858.MR000033.pub3","Methodology"
"MR000007.PUB3","Tudur Smith, C; Marcucci, M; Nolan, SJ; Iorio, A; Sudell, M; Riley, R; Rovers, MM; Williamson, PR","Individual participant data meta‐analyses compared with meta‐analyses based on aggregate data","Cochrane Database of Systematic Reviews","2016","Abstract - Background Meta‐analyses based on individual participant data (IPD‐MAs) allow more powerful and uniformly consistent analyses as well as better characterisation of subgroups and outcomes, compared to those which are based on aggregate data (AD‐MAs) extracted from published trial reports. However, IPD‐MAs are a larger undertaking requiring greater resources than AD‐MAs. Researchers have compared results from IPD‐MA against results obtained from AD‐MA and reported conflicting findings. We present a methodology review to summarise this empirical evidence . Objectives To review systematically empirical comparisons of meta‐analyses of randomised trials based on IPD with those based on AD extracted from published reports, to evaluate the level of agreement between IPD‐MA and AD‐MA and whether agreement is affected by differences in type of effect measure, trials and participants included within the IPD‐MA and AD‐MA, and whether analyses were undertaken to explore the main effect of treatment or a treatment effect modifier. Search methods An electronic search of the Cochrane Library (includes  Cochrane Database of Systematic Reviews , Database of Abstracts of Reviews of Effectiveness, CENTRAL, Cochrane Methodology Register, HTA database, NHS Economic Evaluations Database), MEDLINE, and Embase was undertaken up to 7 January 2016. Potentially relevant articles that were known to any of the review authors and reference lists of retrieved articles were also checked. Selection criteria Studies reporting an empirical comparison of the results of meta‐analyses of randomised trials using IPD with those using AD. Studies were included if sufficient numerical data, comparing IPD‐MA and AD‐MA, were available in their reports. Data collection and analysis Two review authors screened the title and abstract of identified studies with full‐text publications retrieved for those identified as eligible or potentially eligible. A ‘quality’ assessment was done and data were extracted independently by two review authors with disagreements resolved by involving a third author. Data were summarised descriptively for comparisons where an estimate of effect measure and corresponding precision have been provided both for IPD‐MA and for AD‐MA in the study report. Comparisons have been classified according to whether identical effect measures, identical trials and patients had been used in the IPD‐MA and the AD‐MA, and whether the analyses were undertaken to explore the main effect of treatment, or to explore a potential treatment effect modifier. Effect measures were transformed to a standardised scale (z scores) and scatter plots generated to allow visual comparisons. For each comparison, we compared the statistical significance (at the 5% two‐sided level) of an IPD‐MA compared to the corresponding AD‐MA and calculated the number of discrepancies. We examined discrepancies by type of analysis (main effect or modifier) and according to whether identical trials, patients and effect measures had been used by the IPD‐MA and AD‐MA. We calculated the average of differences between IPD‐MA and AD‐MA (z scores, ratio effect estimates and standard errors (of ratio effects)) and 95% limits of agreement. Main results From the 9330 reports found by our searches, 39 studies were eligible for this review with effect estimate and measure of precision extracted for 190 comparisons of IPD‐MA and AD‐MA. We classified the quality of studies as ‘no important flaws’ (29 (74%) studies) or ‘possibly important flaws’ (10 (26%) studies). A median of 4 (interquartile range (IQR): 2 to 6) comparisons were made per study, with 6 (IQR 4 to 11) trials and 1225 (542 to 2641) participants in IPD‐MAs and 7 (4 to 11) and 1225 (705 to 2541) for the AD‐MAs. One hundred and forty‐four (76%) comparisons were made on the main treatment effect meta‐analysis and 46 (24%) made using results from analyses to explore treatment effect modifiers. There is agreement in statistical significance between the IPD‐MA and AD‐MA for 152 (80%) comparisons, 23 of which disagreed in direction of effect. There is disagreement in statistical significance for 38 (20%) comparisons with an excess proportion of IPD‐MA detecting a statistically significant result that was not confirmed with AD‐MA (28 (15%)), compared with 10 (5%) comparisons with a statistically significant AD‐MA that was not confirmed by IPD‐MA. This pattern of disagreement is consistent for the 144 main effect analyses but not for the 46 comparisons of treatment effect modifier analyses. Conclusions from some IPD‐MA and AD‐MA differed even when based on identical trials, participants (but not necessarily identical follow‐up) and treatment effect measures. The average difference between IPD‐MA and AD‐MA in z scores, ratio effect estimates and standard errors is small but limits of agreement are wide and include important differences in both directions. Discrepancies between IPD‐MA and AD‐MA do not appear to increase as the differences between trials and participants increase. Authors' conclusions IPD offers the potential to explore additional, more thorough, and potentially more appropriate analyses compared to those possible with AD. But in many cases, similar results and conclusions can be drawn from IPD‐MA and AD‐MA. Therefore, before embarking on a resource‐intensive IPD‐MA, an AD‐MA should initially be explored and researchers should carefully consider the potential added benefits of IPD. Plain language summary Meta‐analysis using individual participant data or summary aggregate data Meta‐analysis is a statistical technique to combine results from separate research studies. A meta‐analysis can be performed using summary data published in a study report, referred to as aggregate data (AD), or using data collected on each individual participant in the study, referred to as individual participant data (IPD). A meta‐analysis of individual participant data (IPD‐MA) can take longer and be more expensive than a meta‐analysis of aggregate data (AD‐MA), but the IPD‐MA can be more reliable and can answer much more detailed questions than an AD‐MA. We searched for studies, published up to 7 January 2016, that compared results of IPD‐MA with AD‐MA. We found that four times out of five, similar conclusions can be drawn, but in one out of five cases the two different types of meta‐analyses gave different results and conclusions. As we could not reliably identify when an IPD‐MA and AD‐MA will differ most using these studies, we recommend that an AD‐MA should be done first before doing an IPD‐MA. If there are shortcomings with the AD‐MA, researchers should then consider the possible benefits of IPD whilst remembering the extra work involved.","9","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.MR000007.pub3","http://dx.doi.org/10.1002/14651858.MR000007.pub3","Methodology"
"CD010642.PUB2","Khanna, R; Nelson, SA; Feagan, BG; D'Haens, G; Sandborn, WJ; Zou, GY; MacDonald, JK; Parker, CE; Jairath, V; Levesque, BG","Endoscopic scoring indices for evaluation of disease activity in Crohn’s disease","Cochrane Database of Systematic Reviews","2016","Abstract - Background Endoscopic assessment of mucosal disease activity is widely used to determine eligibility and response to therapy in clinical trials of treatment for Crohn’s disease. However, the operating properties of the currently available endoscopic indices remain unclear. Objectives A systematic review was undertaken to evaluate the development and operating characteristics of the Crohn’s Disease Endoscopic Index of Severity (CDEIS) and Simple Endoscopic Scale for Crohn’s Disease (SES‐CD). Search methods Electronic searches of the MEDLINE (1966 to December 2015), EMBASE (1980 to December 2015), and Cochrane CENTRAL Register of Controlled Trials (Issue 12, 2015) databases were supplemented by manual reviews of reference listings and conference proceedings (Digestive Disease Week, United European Gastroenterology Week, European Crohn’s and Colitis Organization). Selection criteria Any study design (e.g. randomized controlled trials, cohort studies, case series) that evaluated either or both the CDEIS or SES‐CD in patients with Crohn’s disease was considered for inclusion. Eligible participants were adult patients ( >  16 years), diagnosed with Crohn’s disease using conventional clinical, radiographic, and endoscopic criteria. Data collection and analysis Two authors (RK, JKM) independently reviewed the titles and abstracts of the studies identified from the literature search. The full texts of potentially relevant citations were reviewed for inclusion and the study investigators were contacted to clarify any unclear data. Any disagreements were resolved by discussion and consensus with a third author. A standardized form was used to assess eligibility of trials for inclusion in the study and for data extraction. Two authors independently extracted and recorded data (RK, SAN). The number of patients enrolled; number of patients per treatment arm; patient characteristics including age and gender distribution; endoscopic index; and outcomes such as intra‐rater reliability, inter‐rater reliability responsiveness, validity, feasibility, construct validity, and criterion validity were recorded for each trial. Main results Forty‐three reports of 30 studies fulfilled the inclusion criteria. For the SES‐CD, inter‐rater reliability was assessed in four studies. In the development study for the SES‐CD ( Daperno 2004 ), the overall ICC (0.9815, 95% CI 0.9705 to 0.9884) and the kappa for the regions is high; however the paired raters were in the same room which introduces the potential for bias. For the CDEIS, inter‐rater reliability was assessed in six studies.  Daperno 2014  reported that the ICC for the CDEIS was 0.985 (95% CI 0.939‐1.000) for average measures of video score and was 0.835 (95% CI 0.540‐0.995) for single measures of video score. With respect to validity, correlation between the CDEIS and clinical measures, including C‐reactive protein (CRP) and erythrocyte sedimentation rate (ESR), was also reported. The estimates of correlation with CRP were r = 0.521 ( Sipponen 2010b ), r = 0.553 ( Sipponen 2008a ) and r = 0.608 ( Sipponen 2008c ). For the SES‐CD, the corresponding values for correlation with CRP ranged from r = 0.46 ( Jones 2008 ) to r = 0.68 ( Green 2011 ). Responsiveness data for the CDEIS were available in nine studies. Seven studies demonstrated statistically significant decreases in the CDEIS score after administration of a treatment of known efficacy. Minimal responsiveness data were available for the SES‐CD.  Sipponen 2010a  and  Sipponen 2010b  demonstrated statistically significant changes in the SES‐CD score after subjects were administered a treatment of known efficacy. No studies were identified that explicitly evaluated the feasibility for either the SES‐CD or the CDEIS. The SES‐CD requires fewer calculations and may therefore be easier to use than the CDEIS. Authors' conclusions Although they are used in clinical trials, the CDEIS and SES‐CD remain incompletely validated. Future research is required to determine the operating properties and to define the optimal index. Plain language summary Endoscopic scoring indices for evaluation of disease activity in Crohn's disease What is Crohn's disease?   Crohn's disease is a long‐term (chronic) inflammatory bowel disease characterized by pain (abdominal cramping), tiredness, diarrhea and weight loss. When people with Crohn's disease are experiencing symptoms the disease is said to be ""active"" and when symptoms stop this is called ""remission"". What is an endoscopic scoring index?   An endoscopic scoring index measures disease activity based on what a doctor sees during endoscopy. An endoscopy is a non‐surgical procedure used to view the digestive tract using a camera. The doctor who performs the endoscopy can rate disease activity using the index, or this can be done by another off‐site doctor if a video of the procedure was recorded. The most commonly used endoscopic scoring indices are the Crohn's Disease Endoscopic Index of Severity (CDEIS) and the Simple Endoscopic Score for Crohn's Disease (SES‐CD). What did the researchers investigate?   It is important that endoscopic scoring indices are valid, meaning that they accurately measure what they are supposed to measure. The researchers investigated whether studies have assessed the validity of the CDEIS and/or the SES‐CD. What did the researchers find? The researchers found that neither the CDEIS nor the SES‐CD has been fully validated.","8","John Wiley & Sons, Ltd","1465-1858","*Endoscopy, Gastrointestinal; *Severity of Illness Index; Blood Sedimentation; Cohort Studies; Crohn Disease [*pathology]; C‐Reactive Protein [analysis]; Feces [chemistry]; Humans; Randomized Controlled Trials as Topic; Reproducibility of Results","10.1002/14651858.CD010642.pub2","http://dx.doi.org/10.1002/14651858.CD010642.pub2","Gut"
"MR000038.PUB2","Marusic, A; Wager, E; Utrobicic, A; Rothstein, HR; Sambunjak, D","Interventions to prevent misconduct and promote integrity in research and publication","Cochrane Database of Systematic Reviews","2016","Abstract - Background Improper practices and unprofessional conduct in clinical research have been shown to waste a significant portion of healthcare funds and harm public health. Objectives Our objective was to evaluate the effectiveness of educational or policy interventions in research integrity or responsible conduct of research on the behaviour and attitudes of researchers in health and other research areas. Search methods We searched the CENTRAL, MEDLINE, LILACS and CINAHL health research bibliographical databases, as well as the Academic Search Complete, AGRICOLA, GeoRef, PsycINFO, ERIC, SCOPUS and Web of Science databases. We performed the last search on 15 April 2015 and the search was limited to articles published between 1990 and 2014, inclusive. We also searched conference proceedings and abstracts from research integrity conferences and specialized websites. We handsearched 14 journals that regularly publish research integrity research. Selection criteria We included studies that measured the effects of one or more interventions, i.e. any direct or indirect procedure that may have an impact on research integrity and responsible conduct of research in its broadest sense, where participants were any stakeholders in research and publication processes, from students to policy makers. We included randomized and non‐randomized controlled trials, such as controlled before‐and‐after studies, with comparisons of outcomes in the intervention versus non‐intervention group or before versus after the intervention. Studies without a control group were not included in the review. Data collection and analysis We used the standard methodological procedures expected by Cochrane. To assess the risk of bias in non‐randomized studies, we used a modified Cochrane tool, in which we used four out of six original domains (blinding, incomplete outcome data, selective outcome reporting, other sources of bias) and two additional domains (comparability of groups and confounding factors). We categorized our primary outcome into the following levels: 1) organizational change attributable to intervention, 2) behavioural change, 3) acquisition of knowledge/skills and 4) modification of attitudes/perceptions. The secondary outcome was participants' reaction to the intervention. Main results Thirty‐one studies involving 9571 participants, described in 33 articles, met the inclusion criteria. All were published in English. Fifteen studies were randomized controlled trials, nine were controlled before‐and‐after studies, four were non‐equivalent controlled studies with a historical control, one was a non‐equivalent controlled study with a post‐test only and two were non‐equivalent controlled studies with pre‐ and post‐test findings for the intervention group and post‐test for the control group. Twenty‐one studies assessed the effects of interventions related to plagiarism and 10 studies assessed interventions in research integrity/ethics. Participants included undergraduates, postgraduates and academics from a range of research disciplines and countries, and the studies assessed different types of outcomes. We judged most of the included randomized controlled trials to have a high risk of bias in at least one of the assessed domains, and in the case of non‐randomized trials there were no attempts to alleviate the potential biases inherent in the non‐randomized designs. We identified a range of interventions aimed at reducing research misconduct. Most interventions involved some kind of training, but methods and content varied greatly and included face‐to‐face and online lectures, interactive online modules, discussion groups, homework and practical exercises. Most studies did not use standardized or validated outcome measures and it was impossible to synthesize findings from studies with such diverse interventions, outcomes and participants. Overall, there is very low quality evidence that various methods of training in research integrity had some effects on participants' attitudes to ethical issues but minimal (or short‐lived) effects on their knowledge. Training about plagiarism and paraphrasing had varying effects on participants' attitudes towards plagiarism and their confidence in avoiding it, but training that included practical exercises appeared to be more effective. Training on plagiarism had inconsistent effects on participants' knowledge about and ability to recognize plagiarism. Active training, particularly if it involved practical exercises or use of text‐matching software, generally decreased the occurrence of plagiarism although results were not consistent. The design of a journal's author contribution form affected the truthfulness of information supplied about individuals' contributions and the proportion of listed contributors who met authorship criteria. We identified no studies testing interventions for outcomes at the organizational level. The numbers of events and the magnitude of intervention effects were generally small, so the evidence is likely to be imprecise. No adverse effects were reported. Authors' conclusions The evidence base relating to interventions to improve research integrity is incomplete and the studies that have been done are heterogeneous, inappropriate for meta‐analyses and their applicability to other settings and population is uncertain. Many studies had a high risk of bias because of the choice of study design and interventions were often inadequately reported. Even when randomized designs were used, findings were difficult to generalize. Due to the very low quality of evidence, the effects of training in responsible conduct of research on reducing research misconduct are uncertain. Low quality evidence indicates that training about plagiarism, especially if it involves practical exercises and use of text‐matching software, may reduce the occurrence of plagiarism. Plain language summary Preventing misconduct and promoting integrity in research and publication Doctors and patients need to be able to trust reports of medical research because these are used to help them make decisions about treatments. It is therefore important to prevent false or misleading research. Problems with research include various types of misconduct such as altering results (falsification), making up results (fabrication) or copying other people's work (plagiarism). Good systems that produce reliable research are said to show 'research integrity'. We studied activities, such as training, designed to reduce research misconduct and encourage integrity. The effects of some of these activities on researchers' attitudes, knowledge and behaviour have been studied and we brought together the evidence from these studies. Some studies showed positive effects on researchers' attitudes to plagiarism. Practical training, such as using computer programs that can detect plagiarism, or writing exercises, sometimes decreased plagiarism by students but not all studies showed positive effects. We did not find any studies on fabrication or falsification. Two studies showed that the way in which journals ask authors for details about who did each part of a study can affect their responses. Many of the studies included in this review had problems such as small sample sizes or had used methods that might produce biased results. The training methods tested in the studies (which included online courses, lectures and discussion groups) were often not clearly described. Most studies tested effects over short time periods. Many studies involved university students rather than active researchers. In summary, the available evidence is of very low quality, so the effect of any intervention for preventing misconduct and promoting integrity in research and publication is uncertain. However, practical training about how to avoid plagiarism may be effective in reducing plagiarism by students, although we do not know whether it has long‐term effects.","4","John Wiley & Sons, Ltd","1465-1858","*Plagiarism; Attitude; Biomedical Research [*ethics]; Controlled Before‐After Studies [ethics, standards]; Controlled Clinical Trials as Topic [ethics, standards]; Humans; Publishing [ethics, standards]; Randomized Controlled Trials as Topic [ethics, standards]; Research Personnel [*ethics, standards]; Scientific Misconduct [*ethics]","10.1002/14651858.MR000038.pub2","http://dx.doi.org/10.1002/14651858.MR000038.pub2","Methodology"
"MR000043.PUB2","Ndounga Diakou, LA; Trinquart, L; Hróbjartsson, A; Barnes, C; Yavchitz, A; Ravaud, P; Boutron, I","Comparison of central adjudication of outcomes and onsite outcome assessment on treatment effect estimates","Cochrane Database of Systematic Reviews","2016","Abstract - Background Assessment of events by adjudication committees (ACs) is recommended in multicentre randomised controlled trials (RCTs). However, its usefulness has been questioned. Objectives The aim of this systematic review was to compare 1) treatment effect estimates of subjective clinical events assessed by onsite assessors versus by AC, and 2) treatment effect estimates according to the blinding status of the onsite assessor as well as the process used to select events to adjudicate. Search methods We searched Cochrane Central Register of Controlled Trials (CENTRAL), PubMed, EMBASE, PsycINFO, CINAHL and Google Scholar (25 August 2015 as the last updated search date), using a combination of terms to retrieve RCTs with commonly used terms to describe ACs. Selection criteria We included all reports of RCTs and the published RCTs included in reviews and meta‐analyses that reported the same subjective outcome event assessed by both an onsite assessor and an AC. Data collection and analysis We extracted the odds ratio (OR) from onsite assessment and the corresponding OR from AC assessment and calculated the ratio of the odds ratios (ROR). A ratio of odds ratios < 1 indicated that onsite assessors generated larger effect estimates in favour of the experimental treatment than ACs. Main results Data from 47 RCTs (275,078 patients) were used in the meta‐analysis. We excluded 11 RCTs because of incomplete outcome data to calculate the OR for onsite and AC assessments. On average, there was no difference in treatment effect estimates from onsite assessors and AC (combined ROR: 1.00, 95% confidence interval (CI) 0.97 to 1.04; I 2  = 0%, 47 RCTs). The combined ROR was 1.00 (95% CI 0.96 to 1.04; I 2  = 0%, 35 RCTs) when onsite assessors were blinded; 0.76 (95% CI 0.48 to 1.12, I 2  = 0%, two RCTs) when AC assessed events identified independently from unblinded onsite assessors; and 1.11 (95% CI 0.96 to 1.27, I 2  = 0%, 10 RCTs) when AC assessed events identified by unblinded onsite assessors. However, there was a statistically significant interaction between these subgroups (P = 0.03) Authors' conclusions On average, treatment effect estimates for subjective outcome events assessed by onsite assessors did not differ from those assessed by ACs. Results of subgroup analysis showed an interaction according to the blinded status of onsite assessors and the process used to submit data to AC. These results suggest that the use of ACs might be most important when onsite assessors are not blinded and the risk of misclassification is high. Furthermore, research is needed to explore the impact of the different procedures used to select events to adjudicate. Plain language summary Comparison of central adjudication of outcomes and onsite outcome assessment on treatment effect estimates It is widely recommended that multicentre randomised controlled trials (RCTs) should have a central process for assessing whether or not a patient has had an event, rather than relying solely on the outcomes reported by assessors at the relevant site where the decision might be subjective. These Adjudication Committees (ACs) are commonly used, especially in large trials. For example, the US Food and Drug Administration (FDA) and the European Medicine Agency (EMA) recommend assessment of events by such committees to harmonise and standardise outcome assessment across a trial. However, there is a need for evidence to justify the use of ACs and to decide on how central adjudication of clinical events should be conducted. This is the first large meta‐analysis across medical areas to evaluate the impact of central adjudication on the estimates for treatment effect produced by RCTs. We investigated whether using the event data from ACs produced different treatment effect estimates than the data from onsite for subjective outcomes in RCTs. We defined an AC as a committee of clinical experts in a specific medical area that seeks to harmonise and standardise the outcome assessment; whereas onsite assessors would be investigators, research nurses, data collectors, or patients themselves doing an onsite evaluation of the occurrence of the outcome during the RCT. Onsite assessors may, or may not, be blinded to the treatment assigned. We included all reports of RCTs and meta‐analyses of published RCTs that reported the same subjective binary clinical event outcome assessed by both an onsite assessor and an AC. We combined the findings of 47 RCTs (275,078 patients) in our systematic review and meta‐analysis in order to see if there is a difference between the results from ACs and from onsite assessment. Our results showed that treatment effect estimates of subjective clinical events did not differ, on average, from those assessed by ACs. When we divided the data into whether or not the onsite assessors knew the patient's allocated treatment in the RCT and the various ways of submitting data to ACs, we found that there might be important differences between onsite assessment and ACs depending on which methods are used. Our findings, which are up to date as of August 2015, raise important uncertainty about whether ACs are being used appropriately across all RCTs.","3","John Wiley & Sons, Ltd","1465-1858","*Advisory Committees; *Randomized Controlled Trials as Topic; Humans; Multicenter Studies as Topic; Odds Ratio; Outcome Assessment, Health Care [*methods, standards]; Treatment Outcome","10.1002/14651858.MR000043.pub2","http://dx.doi.org/10.1002/14651858.MR000043.pub2","Methodology"
"MR000036.PUB2","Preston, NJ; Farquhar, MC; Walshe, CE; Stevinson, C; Ewing, G; Calman, LA; Burden, S; Brown Wilson, C; Hopkinson, JB; Todd, C","Strategies designed to help healthcare professionals to recruit participants to research studies","Cochrane Database of Systematic Reviews","2016","Abstract - Background Identifying and approaching eligible participants for recruitment to research studies usually relies on healthcare professionals. This process is sometimes hampered by deliberate or inadvertent gatekeeping that can introduce bias into patient selection. Objectives Our primary objective was to identify and assess the effect of strategies designed to help healthcare professionals to recruit participants to research studies. Search methods We performed searches on 5 January 2015 in the following electronic databases: Cochrane Methodology Register, CENTRAL, MEDLINE, EMBASE, CINAHL, British Nursing Index, PsycINFO, ASSIA and Web of Science (SSCI, SCI‐EXPANDED) from 1985 onwards. We checked the reference lists of all included studies and relevant review articles and did citation tracking through Web of Science for all included studies. Selection criteria We selected all studies that evaluated a strategy to identify and recruit participants for research via healthcare professionals and provided pre‐post comparison data on recruitment rates. Data collection and analysis Two review authors independently screened search results for potential eligibility, read full papers, applied the selection criteria and extracted data. We calculated risk ratios for each study to indicate the effect of each strategy. Main results Eleven studies met our eligibility criteria and all were at medium or high risk of bias. Only five studies gave the total number of participants (totalling 7372 participants). Three studies used a randomised design, with the others using pre‐post comparisons. Several different strategies were investigated. Four studies examined the impact of additional visits or information for the study site, with no increases in recruitment demonstrated. Increased recruitment rates were reported in two studies that used a dedicated clinical recruiter, and five studies that introduced an automated alert system for identifying eligible participants. The studies were embedded into trials evaluating care in oncology mainly but also in emergency departments, diabetes and lower back pain. Authors' conclusions There is no strong evidence for any single strategy to help healthcare professionals to recruit participants in research studies. Additional visits or information did not appear to increase recruitment by healthcare professionals. The most promising strategies appear to be those with a dedicated resource (e.g. a clinical recruiter or automated alert system) for identifying suitable participants that reduced the demand on healthcare professionals, but these were assessed in studies at high risk of bias. Plain language summary Strategies designed to help healthcare professionals to recruit participants to research studies Introduction Most trials fail to recruit the number of participants they need within the time they had planned to conduct the study. Recruiting potential participants to research studies involves three stages: identifying, approaching and obtaining the consent of potential participants to join a study. Researchers often rely on healthcare staff, such as doctors and nurses, to identify and approach potential participants. This review examines what strategies could be used by researchers to improve recruitment to studies. Findings We found 11 studies that assessed recruitment strategies used with healthcare staff in search of the literature in January 2015. Five included the total number of participants (7372). There were three main strategies: 1. Using an alert system, either a computer system or member of staff to check patient records, to alert staff recruiting participants that someone might be suitable for the study (five studies). 2. Giving additional information about the study to the staff at hospitals or clinics who are recruiting people through visits from the researchers, educational seminars or leaflets (four studies). 3. Using a designated member of staff whose primary role was to recruit participants (two studies). All the studies identified were of quite low quality, so it is difficult to draw firm conclusions from them. Five studies examined the alert system to identify participants who might be suitable for a study. Alert systems showed some promising results but were not unanimous in their findings. The four studies that evaluated the provision of additional information, visits or education to the sites recruiting participants found that none of the tested strategies led to improved recruitment. The most promising strategy appears to be the employment of someone such as a clinical trials officer or research nurse with the specific task of recruiting participants to research studies. The two studies using this strategy showed improvement in recruitment rates but both were at high risk of bias. Conclusion More research is still needed to evaluate the role of a designated person to recruit to research studies.","2","John Wiley & Sons, Ltd","1465-1858",,"10.1002/14651858.MR000036.pub2","http://dx.doi.org/10.1002/14651858.MR000036.pub2","Methodology"
"MR000042.PUB2","Marcano Belisario, JS; Jamsek, J; Huckvale, K; O'Donoghue, J; Morrison, CP; Car, J","Comparison of self‐administered survey questionnaire responses collected using mobile apps versus other methods","Cochrane Database of Systematic Reviews","2015","Abstract - Background Self‐administered survey questionnaires are an important data collection tool in clinical practice, public health research and epidemiology. They are ideal for achieving a wide geographic coverage of the target population, dealing with sensitive topics and are less resource‐intensive than other data collection methods. These survey questionnaires can be delivered electronically, which can maximise the scalability and speed of data collection while reducing cost. In recent years, the use of apps running on consumer smart devices (i.e., smartphones and tablets) for this purpose has received considerable attention. However, variation in the mode of delivering a survey questionnaire could affect the quality of the responses collected. Objectives To assess the impact that smartphone and tablet apps as a delivery mode have on the quality of survey questionnaire responses compared to any other alternative delivery mode: paper, laptop computer, tablet computer (manufactured before 2007), short message service (SMS) and plastic objects. Search methods We searched MEDLINE, EMBASE, PsycINFO, IEEEXplore, Web of Science, CABI: CAB Abstracts, Current Contents Connect, ACM Digital, ERIC, Sociological Abstracts, Health Management Information Consortium, the Campbell Library and CENTRAL. We also searched registers of current and ongoing clinical trials such as ClinicalTrials.gov and the World Health Organization (WHO) International Clinical Trials Registry Platform. We also searched the grey literature in OpenGrey, Mobile Active and ProQuest Dissertation & Theses. Lastly, we searched Google Scholar and the reference lists of included studies and relevant systematic reviews. We performed all searches up to 12 and 13 April 2015. Selection criteria We included parallel randomised controlled trials (RCTs), crossover trials and paired repeated measures studies that compared the electronic delivery of self‐administered survey questionnaires via a smartphone or tablet app with any other delivery mode. We included data obtained from participants completing health‐related self‐administered survey questionnaire, both validated and non‐validated. We also included data offered by both healthy volunteers and by those with any clinical diagnosis. We included studies that reported any of the following outcomes: data equivalence; data accuracy; data completeness; response rates; differences in the time taken to complete a survey questionnaire; differences in respondent's adherence to the original sampling protocol; and acceptability to respondents of the delivery mode. We included studies that were published in 2007 or after, as devices that became available during this time are compatible with the mobile operating system (OS) framework that focuses on apps. Data collection and analysis Two review authors independently extracted data from the included studies using a standardised form created for this systematic review in REDCap. They then compared their forms to reach consensus. Through an initial systematic mapping on the included studies, we identified two settings in which survey completion took place: controlled and uncontrolled. These settings differed in terms of (i) the location where surveys were completed, (ii) the frequency and intensity of sampling protocols, and (iii) the level of control over potential confounders (e.g., type of technology, level of help offered to respondents). We conducted a narrative synthesis of the evidence because a meta‐analysis was not appropriate due to high levels of clinical and methodological diversity. We reported our findings for each outcome according to the setting in which the studies were conducted. Main results We included 14 studies (15 records) with a total of 2275 participants; although we included only 2272 participants in the final analyses as there were missing data for three participants from one included study. Regarding data equivalence, in both controlled and uncontrolled settings, the included studies found no significant differences in the mean overall scores between apps and other delivery modes, and that all correlation coefficients exceeded the recommended thresholds for data equivalence. Concerning the time taken to complete a survey questionnaire in a controlled setting, one study found that an app was faster than paper, whereas the other study did not find a significant difference between the two delivery modes. In an uncontrolled setting, one study found that an app was faster than SMS. Data completeness and adherence to sampling protocols were only reported in uncontrolled settings. Regarding the former, an app was found to result in more complete records than paper, and in significantly more data entries than an SMS‐based survey questionnaire. Regarding adherence to the sampling protocol, apps may be better than paper but no different from SMS. We identified multiple definitions of acceptability to respondents, with inconclusive results: preference; ease of use; willingness to use a delivery mode; satisfaction; effectiveness of the system informativeness; perceived time taken to complete the survey questionnaire; perceived benefit of a delivery mode; perceived usefulness of a delivery mode; perceived ability to complete a survey questionnaire; maximum length of time that participants would be willing to use a delivery mode; and reactivity to the delivery mode and its successful integration into respondents' daily routine. Finally, regardless of the study setting, none of the included studies reported data accuracy or response rates. Authors' conclusions Our results, based on a narrative synthesis of the evidence, suggest that apps might not affect data equivalence as long as the intended clinical application of the survey questionnaire, its intended frequency of administration and the setting in which it was validated remain unchanged. There were no data on data accuracy or response rates, and findings on the time taken to complete a self‐administered survey questionnaire were contradictory. Furthermore, although apps might improve data completeness, there is not enough evidence to assess their impact on adherence to sampling protocols. None of the included studies assessed how elements of user interaction design, survey questionnaire design and intervention design might influence mode effects. Those conducting research in public health and epidemiology should not assume that mode effects relevant to other delivery modes apply to apps running on consumer smart devices. Those conducting methodological research might wish to explore the issues highlighted by this systematic review. Plain language summary Can apps be used for the delivery of survey questionnaires in public health and clinical research? Background Survey questionnaires are important tools in public health and clinical research as they offer a convenient way of collecting data from a large number of respondents, dealing with sensitive topics, and are less resource intensive than other data collection techniques. The delivery of survey questionnaires via apps running on smartphones or tablets could maximise the scalability and speed of data collection offered by these tools, whilst reducing costs. However, before this technology becomes widely adopted, we need to understand how it could affect the quality of the responses collected. Particularly, if we consider the impact that data quality can have on the evidence base that supports many public health and healthcare decisions. Objective In this Cochrane review, we assessed the impact that using apps to deliver a survey can have on various aspects of the quality of responses. These include response rates, data accuracy, data completeness, time taken to complete a survey questionnaire, and acceptability to respondents. Methods and results We searched for studies published between January 2007 and April 2015. We included 14 studies and analysed data from 2272 participants. We did not conduct a meta‐analysis because of differences across the studies. Instead, we describe the results of each study. The studies took place in two types of setting: controlled and uncontrolled. The former refers to research or clinical environments in which healthcare practitioners or researchers were able to better control for potential confounders, such as the location and time of day in which surveys were completed, the type of technology used and the level of help available to respondents deal with technical difficulties. Uncontrolled settings refer to locations outside these research or clinical environments (e.g., the respondent's home). We found that apps may be equivalent to other delivery modes such as paper, laptops and SMS in both settings. It is unclear if apps could result in faster completion times than other delivery modes. Instead, our findings suggest that factors such as the characteristics of the clinical population, and survey and interface design could moderate the effect on this outcome. Data completeness and adherence to sampling protocols were only reported in uncontrolled settings. Our results indicate that apps may result in more complete datasets, and may improve adherence to sampling protocols compared to paper but not to SMS. There were multiple definitions of acceptability to respondents, which could not be standardised across the included studies. Lastly, none of the included studies reported on response rates or data accuracy. Conclusion Overall, there is not enough evidence to make clear recommendations about the impact that apps may have on survey questionnaire responses. Data equivalence may not be affected as long as the intended clinical application of a survey questionnaire and its intended frequency of administration is the same whether or not apps are used. Future research may need to consider how the design of the user interaction, survey questionnaire and intervention may affect data equivalence and the other outcomes evaluated in this review.","7","John Wiley & Sons, Ltd","1465-1858","Cell Phone [*statistics & numerical data]; Data Accuracy; Humans; Minicomputers [*statistics & numerical data]; Mobile Applications [*statistics & numerical data]; Surveys and Questionnaires [*standards]; Text Messaging [statistics & numerical data]; Time Factors","10.1002/14651858.MR000042.pub2","http://dx.doi.org/10.1002/14651858.MR000042.pub2","Methodology"
"MR000035.PUB2","Page, MJ; McKenzie, JE; Kirkham, J; Dwan, K; Kramer, S; Green, S; Forbes, A","Bias due to selective inclusion and reporting of outcomes and analyses in systematic reviews of randomised trials of healthcare interventions","Cochrane Database of Systematic Reviews","2014","Abstract - Background Systematic reviews may be compromised by selective inclusion and reporting of outcomes and analyses. Selective inclusion occurs when there are multiple effect estimates in a trial report that could be included in a particular meta‐analysis (e.g. from multiple measurement scales and time points) and the choice of effect estimate to include in the meta‐analysis is based on the results (e.g. statistical significance, magnitude or direction of effect). Selective reporting occurs when the reporting of a subset of outcomes and analyses in the systematic review is based on the results (e.g. a protocol‐defined outcome is omitted from the published systematic review). Objectives To summarise the characteristics and synthesise the results of empirical studies that have investigated the prevalence of selective inclusion or reporting in systematic reviews of randomised controlled trials (RCTs), investigated the factors (e.g. statistical significance or direction of effect) associated with the prevalence and quantified the bias. Search methods We searched the Cochrane Methodology Register (to July 2012), Ovid MEDLINE, Ovid EMBASE, Ovid PsycINFO and ISI Web of Science (each up to May 2013), and the US Agency for Healthcare Research and Quality (AHRQ) Effective Healthcare Program's Scientific Resource Center (SRC) Methods Library (to June 2013). We also searched the abstract books of the 2011 and 2012 Cochrane Colloquia and the article alerts for methodological work in research synthesis published from 2009 to 2011 and compiled in  Research Synthesis Methods . Selection criteria We included both published and unpublished empirical studies that investigated the prevalence and factors associated with selective inclusion or reporting, or both, in systematic reviews of RCTs of healthcare interventions. We included empirical studies assessing any type of selective inclusion or reporting, such as investigations of how frequently RCT outcome data is selectively included in systematic reviews based on the results, outcomes and analyses are discrepant between protocol and published review or non‐significant outcomes are partially reported in the full text or summary within systematic reviews. Data collection and analysis Two review authors independently selected empirical studies for inclusion, extracted the data and performed a risk of bias assessment. A third review author resolved any disagreements about inclusion or exclusion of empirical studies, data extraction and risk of bias. We contacted authors of included studies for additional unpublished data. Primary outcomes included overall prevalence of selective inclusion or reporting, association between selective inclusion or reporting and the statistical significance of the effect estimate, and association between selective inclusion or reporting and the direction of the effect estimate. We combined prevalence estimates and risk ratios (RRs) using a random‐effects meta‐analysis model. Main results Seven studies met the inclusion criteria. No studies had investigated selective inclusion of results in systematic reviews, or discrepancies in outcomes and analyses between systematic review registry entries and published systematic reviews. Based on a meta‐analysis of four studies (including 485 Cochrane Reviews), 38% (95% confidence interval (CI) 23% to 54%) of systematic reviews added, omitted, upgraded or downgraded at least one outcome between the protocol and published systematic review. The association between statistical significance and discrepant outcome reporting between protocol and published systematic review was uncertain. The meta‐analytic estimate suggested an increased risk of adding or upgrading (i.e. changing a secondary outcome to primary) when the outcome was statistically significant, although the 95% CI included no association and a decreased risk as plausible estimates (RR 1.43, 95% CI 0.71 to 2.85; two studies, n = 552 meta‐analyses). Also, the meta‐analytic estimate suggested an increased risk of downgrading (i.e. changing a primary outcome to secondary) when the outcome was statistically significant, although the 95% CI included no association and a decreased risk as plausible estimates (RR 1.26, 95% CI 0.60 to 2.62; two studies, n = 484 meta‐analyses). None of the included studies had investigated whether the association between statistical significance and adding, upgrading or downgrading of outcomes was modified by the type of comparison, direction of effect or type of outcome; or whether there is an association between direction of the effect estimate and discrepant outcome reporting. Several secondary outcomes were reported in the included studies. Two studies found that reasons for discrepant outcome reporting were infrequently reported in published systematic reviews (6% in one study and 22% in the other). One study (including 62 Cochrane Reviews) found that 32% (95% CI 21% to 45%) of systematic reviews did not report all primary outcomes in the abstract. Another study (including 64 Cochrane and 118 non‐Cochrane reviews) found that statistically significant primary outcomes were more likely to be completely reported in the systematic review abstract than non‐significant primary outcomes (RR 2.66, 95% CI 1.81 to 3.90). None of the studies included systematic reviews published after 2009 when reporting standards for systematic reviews (Preferred Reporting Items for Systematic reviews and Meta‐Analyses (PRISMA) Statement, and Methodological Expectations of Cochrane Intervention Reviews (MECIR)) were disseminated, so the results might not be generalisable to more recent systematic reviews. Authors' conclusions Discrepant outcome reporting between the protocol and published systematic review is fairly common, although the association between statistical significance and discrepant outcome reporting is uncertain. Complete reporting of outcomes in systematic review abstracts is associated with statistical significance of the results for those outcomes. Systematic review outcomes and analysis plans should be specified prior to seeing the results of included studies to minimise post‐hoc decisions that may be based on the observed results. Modifications that occur once the review has commenced, along with their justification, should be clearly reported. Effect estimates and CIs should be reported for all systematic review outcomes regardless of the results. The lack of research on selective inclusion of results in systematic reviews needs to be addressed and studies that avoid the methodological weaknesses of existing research are also needed. Plain language summary Bias due to selective inclusion and reporting of outcomes and analyses in systematic reviews of randomised trials of healthcare interventions A systematic review summarises evidence from multiple studies to answer a specific research question (e.g. what are the benefits and harms of a particular intervention for a particular health condition?). Often, there are many outcomes that systematic review authors could report to address their research question (e.g. pain, disability and quality of life for patients with musculoskeletal conditions) and many different results available for a particular outcome (e.g. a study might measure pain using three different scales at four time points). If the decision about which outcomes to investigate in a systematic review is made based on the results for those outcomes in the eligible studies, this may lead to bias. While, if the decision about which outcomes to report in a systematic review and the ways to report them is based on the results, this may mislead users of the systematic review. This methodology review summarises the findings of studies examining the inclusion of results and reporting of outcomes in systematic reviews. We searched for studies indexed in electronic bibliographic databases up to May 2013. We included seven studies and found that outcomes investigated and reported in systematic reviews were often changed between the protocol and published systematic review. We also found that it was unclear whether the decision to make these changes was related to how statistically convincing the treatment effect for that outcome was. More studies are needed to confirm if this relationship exists. Also, one study found that some systematic reviews did not report all of the most important outcomes in the abstract of the review. Another study found that outcomes with a more statistically convincing result were more likely to be completely reported in the abstract than other outcomes. The studies that we included were limited to systematic reviews published before 2009. New studies are needed to examine the inclusion of results and reporting of outcomes in more recent systematic reviews.","10","John Wiley & Sons, Ltd","1465-1858","*Meta‐Analysis as Topic; *Randomized Controlled Trials as Topic; *Review Literature as Topic; *Selection Bias; Treatment Outcome","10.1002/14651858.MR000035.pub2","http://dx.doi.org/10.1002/14651858.MR000035.pub2","Methodology"
"MR000022.PUB3","Beynon, R; Leeflang, MM; McDonald, S; Eisinga, A; Mitchell, RL; Whiting, P; Glanville, JM","Search strategies to identify diagnostic accuracy studies in MEDLINE and EMBASE","Cochrane Database of Systematic Reviews","2013","Abstract - Background A systematic and extensive search for as many eligible studies as possible is essential in any systematic review. When searching for diagnostic test accuracy (DTA) studies in bibliographic databases, it is recommended that terms for disease (target condition) are combined with terms for the diagnostic test (index test). Researchers have developed methodological filters to try to increase the precision of these searches. These consist of text words and database indexing terms and would be added to the target condition and index test searches. Efficiently identifying reports of DTA studies presents challenges because the methods are often not well reported in their titles and abstracts, suitable indexing terms may not be available and relevant indexing terms do not seem to be consistently assigned. A consequence of using search filters to identify records for diagnostic reviews is that relevant studies might be missed, while the number of irrelevant studies that need to be assessed may not be reduced. The current guidance for Cochrane DTA reviews recommends against the addition of a methodological search filter to target condition and index test search, as the only search approach. Objectives To systematically review empirical studies that report the development or evaluation, or both, of methodological search filters designed to retrieve DTA studies in MEDLINE and EMBASE. Search methods We searched MEDLINE (1950 to week 1 November 2012); EMBASE (1980 to 2012 Week 48); the Cochrane Methodology Register (Issue 3, 2012); ISI Web of Science (11 January 2013); PsycINFO (13 March 2013); Library and Information Science Abstracts (LISA) (31 May 2010); and Library, Information Science & Technology Abstracts (LISTA) (13 March 2013). We undertook citation searches on Web of Science, checked the reference lists of relevant studies, and searched the Search Filters Resource website of the InterTASC Information Specialists' Sub‐Group (ISSG). Selection criteria Studies reporting the development or evaluation, or both, of a MEDLINE or EMBASE search filter aimed at retrieving DTA studies, which reported a measure of the filter’s performance were eligible. Data collection and analysis The main outcome was a measure of filter performance, such as sensitivity or precision. We extracted data on the identification of the reference set (including the gold standard and, if used, the non‐gold standard records), how the reference set was used and any limitations, the identification and combination of the search terms in the filters, internal and external validity testing, the number of filters evaluated, the date the study was conducted, the date the searches were completed, and the databases and search interfaces used. Where 2 x 2 data were available on filter performance, we used these to calculate sensitivity, specificity, precision and Number Needed to Read (NNR), and 95% confidence intervals (CIs). We compared the performance of a filter as reported by the original development study and any subsequent studies that evaluated the same filter. Main results Ninteen studies were included, reporting on 57 MEDLINE filters and 13 EMBASE filters. Thirty MEDLINE and four EMBASE filters were tested in an evaluation study where the performance of one or more filters was tested against one or more gold standards. The reported outcome measures varied. Some studies reported specificity as well as sensitivity if a reference set containing non‐gold standard records in addition to gold standard records was used. In some cases, the original development study did not report any performance data on the filters. Original performance from the development study was not available for 17 filters that were subsequently tested in evaluation studies. All 19 studies reported the sensitivity of the filters that they developed or evaluated, nine studies reported the specificities and 14 studies reported the precision. No filter which had original performance data from its development study, and was subsequently tested in an evaluation study, had what we defined a priori as acceptable sensitivity (> 90%) and precision (> 10%). In studies that developed MEDLINE filters that were evaluated in another study (n = 13), the sensitivity ranged from 55% to 100% (median 86%) and specificity from 73% to 98% (median 95%). Estimates of performance were lower in eight studies that evaluated the same 13 MEDLINE filters, with sensitivities ranging from 14% to 100% (median 73%) and specificities ranging from 15% to 96% (median 81%). Precision ranged from 1.1% to 40% (median 9.5%) in studies that developed MEDLINE filters and from 0.2% to 16.7% (median 4%) in studies that evaluated these filters. A similar range of specificities and precision were reported amongst the evaluation studies for MEDLINE filters without an original performance measure. Sensitivities ranged from 31% to 100% (median 71%), specificity ranged from 13% to 90% (median 55.5%) and precision from 1.0% to 11.0% (median 3.35%). For the EMBASE filters, the original sensitivities reported in two development studies ranged from 74% to 100% (median 90%) for three filters, and precision ranged from 1.2% to 17.6% (median 3.7%). Evaluation studies of these filters had sensitivities from 72% to 97% (median 86%) and precision from 1.2% to 9% (median 3.7%). The performance of EMBASE search filters in development and evaluation studies were more alike than the performance of MEDLINE filters in development and evaluation studies. None of the EMBASE filters in either type of study had a sensitivity above 90% and precision above 10%. Authors' conclusions None of the current methodological filters designed to identify reports of primary DTA studies in MEDLINE or EMBASE combine sufficiently high sensitivity, required for systematic reviews, with a reasonable degree of precision. This finding supports the current recommendation in the  Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy  that the combination of methodological filter search terms with terms for the index test and target condition should not be used as the only approach when conducting formal searches to inform systematic reviews of DTA. Plain language summary Search strategies to identify diagnostic accuracy studies in MEDLINE and EMBASE A diagnostic test is any kind of medical test performed to help with the diagnosis or detection of a disease. A systematic review of a particular diagnostic test for a disease aims to bring together and assess all the available research evidence. Bibliographic databases are usually searched by combining terms for the disease with terms for the diagnostic test. However, depending on the topic area, the number of articles retrieved by such searches may be very large. Methodological filters consisting of text words and database indexing terms have been developed in the hope of improving the searches by increasing their precision when these filters are added to the search terms for the disease and diagnostic test. On the other hand, using filters to identify records for diagnostic reviews may miss relevant studies while at the same time not making a big difference to the number of studies that have to be assessed for inclusion. This review assessed the performance of 70 filters (reported in 19 studies) for identifying diagnostic studies in the two main bibliographic databases in health, MEDLINE and EMBASE. The results showed that search filters do not perform consistently, and should not be used as the only approach in formal searches to inform systematic reviews of diagnostic studies. None of the filters reached our minimum criteria of a sensitivity greater than 90% and a precision above 10%.","9","John Wiley & Sons, Ltd","1465-1858","*Diagnosis; *Subject Headings; Databases, Bibliographic; Information Storage and Retrieval [*methods, standards]; MEDLINE; Reference Standards; Review Literature as Topic; Search Engine; Sensitivity and Specificity","10.1002/14651858.MR000022.pub3","http://dx.doi.org/10.1002/14651858.MR000022.pub3","Methodology"
"MR000030.PUB2","Turner, L; Shamseer, L; Altman, DG; Weeks, L; Peters, J; Kober, T; Dias, S; Schulz, KF; Plint, AC; Moher, D","Consolidated standards of reporting trials (CONSORT) and the completeness of reporting of randomised controlled trials (RCTs) published in medical journals","Cochrane Database of Systematic Reviews","2012","Abstract - Background An overwhelming body of evidence stating that the completeness of reporting of randomised controlled trials (RCTs) is not optimal has accrued over time. In the mid‐1990s, in response to these concerns, an international group of clinical trialists, statisticians, epidemiologists, and biomedical journal editors developed the CONsolidated Standards Of Reporting Trials (CONSORT) Statement. The CONSORT Statement, most recently updated in March 2010, is an evidence‐based minimum set of recommendations including a checklist and flow diagram for reporting RCTs and is intended to facilitate the complete and transparent reporting of trials and aid their critical appraisal and interpretation. In 2006, a systematic review of eight studies evaluating the ""effectiveness of CONSORT in improving reporting quality in journals"" was published. Objectives To update the earlier systematic review assessing whether journal endorsement of the 1996 and 2001 CONSORT checklists influences the completeness of reporting of RCTs published in medical journals. Search methods We conducted electronic searches, known item searching, and reference list scans to identify reports of evaluations assessing the completeness of reporting of RCTs. The electronic search strategy was developed in MEDLINE and tailored to EMBASE. We searched the Cochrane Methodology Register and the Cochrane Database of Systematic Reviews using the Wiley interface. We searched the Science Citation Index, Social Science Citation Index, and Arts and Humanities Citation Index through the ISI Web of Knowledge interface. We conducted all searches to identify reports published between January 2005 and March 2010, inclusive. Selection criteria In addition to studies identified in the original systematic review on this topic, comparative studies evaluating the completeness of reporting of RCTs in any of the following comparison groups were eligible for inclusion in this review: 1) Completeness of reporting of RCTs published in journals that have and have not endorsed the CONSORT Statement; 2) Completeness of reporting of RCTs published in CONSORT‐endorsing journals before and after endorsement; or 3) Completeness of reporting of RCTs before and after the publication of the CONSORT Statement (1996 or 2001). We used a broad definition of CONSORT endorsement that includes any of the following: (a) requirement or recommendation in journal's 'Instructions to Authors' to follow CONSORT guidelines; (b) journal editorial statement endorsing the CONSORT Statement; or (c) editorial requirement for authors to submit a CONSORT checklist and/or flow diagram with their manuscript. We contacted authors of evaluations reporting data that could be included in any comparison group(s), but not presented as such in the published report and asked them to provide additional data in order to determine eligibility of their evaluation. Evaluations were not excluded due to language of publication or validity assessment. Data collection and analysis We completed screening and data extraction using standardised electronic forms, where conflicts, reasons for exclusion, and level of agreement were all automatically and centrally managed in web‐based management software, DistillerSR ® . One of two authors extracted general characteristics of included evaluations and all data were verified by a second author. Data describing completeness of reporting were extracted by one author using a pre‐specified form; a 10% random sample of evaluations was verified by a second author. Any discrepancies were discussed by both authors; we made no modifications to the extracted data. Validity assessments of included evaluations were conducted by one author and independently verified by one of three authors. We resolved all conflicts by consensus. For each comparison we collected data on 27 outcomes: 22 items of the CONSORT 2001 checklist, plus four items relating to the reporting of blinding, and one item of aggregate CONSORT scores. Where reported, we extracted and qualitatively synthesised data on the methodological quality of RCTs, by scale or score. Main results Fifty‐three publications reporting 50 evaluations were included. The total number of RCTs assessed within evaluations was 16,604 (median per evaluation 123 (interquartile range (IQR) 77 to 226) published in a median of six (IQR 3 to 26) journals. Characteristics of the included RCT populations were variable, resulting in heterogeneity between included evaluations. Validity assessments of included studies resulted in largely unclear judgements. The included evaluations are not RCTs and less than 8% (4/53) of the evaluations reported adjusting for potential confounding factors.    Twenty‐five of 27 outcomes assessing completeness of reporting in RCTs appeared to favour CONSORT‐endorsing journals over non‐endorsers, of which five were statistically significant. 'Allocation concealment' resulted in the largest effect, with risk ratio (RR) 1.81 (99% confidence interval (CI) 1.25 to 2.61), suggesting that 81% more RCTs published in CONSORT‐endorsing journals adequately describe allocation concealment compared to those published in non‐endorsing journals. Allocation concealment was reported adequately in 45% (393/876) of RCTs in CONSORT‐endorsing journals and in 22% (329/1520) of RCTs in non‐endorsing journals. Other outcomes with results that were significant include: scientific rationale and background in the 'Introduction' (RR 1.07, 99% CI 1.01 to 1.14); 'sample size' (RR 1.61, 99% CI 1.13 to 2.29); method used for 'sequence generation' (RR 1.59, 99% CI 1.38 to 1.84); and an aggregate score over reported CONSORT items, 'total sum score' (standardised mean difference (SMD) 0.68 (99% CI 0.38 to 0.98)). Authors' conclusions Evidence has accumulated to suggest that the reporting of RCTs remains sub‐optimal. This review updates a previous systematic review of eight evaluations. The findings of this review are similar to those from the original review and demonstrate that, despite the general inadequacies of reporting of RCTs, journal endorsement of the CONSORT Statement may beneficially influence the completeness of reporting of trials published in medical journals. Future prospective studies are needed to explore the influence of the CONSORT Statement dependent on the extent of editorial policies to ensure adherence to CONSORT guidance. Plain language summary CONsolidated Standards Of Reporting Trials (CONSORT) and the completeness of reporting of randomised controlled trials published in medical journals A group of experts has developed a checklist and flow diagram called the CONSORT Statement. The checklist is designed to help authors in the reporting of randomised controlled trials (RCTs). This systematic review aims to determine whether the CONSORT Statement has made a difference to the completeness of reporting of RCTs. Reporting of RCTs published in journals that encourage authors to use the CONSORT Statement with those that do not is compared. We found that some items in the CONSORT Statement were fully reported more often when journals encouraged the use of CONSORT. While the majority of items are reported more often when journals endorse CONSORT, the data only showed a statistically significant improvement in reporting for five of 27 items. No items suggest that CONSORT decreases the completeness of reporting of RCTs published in medical journals. None of the evaluations included in this review used experimental designs, and their methodological approaches were mostly poorly described and variable when they were described. Furthermore, evaluations assessed the completeness of reporting of RCTs within a wide range of medical fields and in journals with a wide variation in the enforcement of CONSORT endorsement. Our results do have some limitations, but given the number of included evaluations and the number of assessed RCTs, we conclude that while most RCTs are incompletely reported, the CONSORT Statement beneficially influences their reporting quality.","11","John Wiley & Sons, Ltd","1465-1858","Checklist [*standards]; Periodicals as Topic [*standards]; Publishing [*standards]; Randomized Controlled Trials as Topic [*standards]; Reference Standards","10.1002/14651858.MR000030.pub2","http://dx.doi.org/10.1002/14651858.MR000030.pub2","Methodology"
"MR000024.PUB3","Djulbegovic, B; Kumar, A; Glasziou, PP; Perera, R; Reljic, T; Dent, L; Raftery, J; Johansen, M; Di Tanna, GL; Miladinovic, B; Soares, HP; Vist, GE; Chalmers, I","New treatments compared to established treatments in randomized trials","Cochrane Database of Systematic Reviews","2012","Abstract - Background The proportion of proposed new treatments that are 'successful' is of ethical, scientific, and public importance. We investigated how often new, experimental treatments evaluated in randomized controlled trials (RCTs) are superior to established treatments. Objectives Our main question was: ""On average how often are new treatments more effective, equally effective or less effective than established treatments?"" Additionally, we wanted to explain the observed results, i.e. whether the observed distribution of outcomes is consistent with the 'uncertainty requirement' for enrollment in RCTs. We also investigated the effect of choice of comparator (active versus no treatment/placebo) on the observed results. Search methods We searched the Cochrane Methodology Register (CMR) 2010, Issue 1 in  The Cochrane Library  (searched 31 March 2010); MEDLINE Ovid 1950 to March Week 2 2010 (searched 24 March 2010); and EMBASE Ovid 1980 to 2010 Week 11 (searched 24 March 2010). Selection criteria Cohorts of studies were eligible for the analysis if they met all of the following criteria: (i) consecutive series of RCTs, (ii) registered at or before study onset, and (iii) compared new against established treatments in humans. Data collection and analysis RCTs from four cohorts of RCTs met all inclusion criteria and provided data from 743 RCTs involving 297,744 patients. All four cohorts consisted of publicly funded trials. Two cohorts involved evaluations of new treatments in cancer, one in neurological disorders, and one for mixed types of diseases. We employed kernel density estimation, meta‐analysis and meta‐regression to assess the probability of new treatments being superior to established treatments in their effect on primary outcomes and overall survival. Main results The distribution of effects seen was generally symmetrical in the size of difference between new versus established treatments. Meta‐analytic pooling indicated that, on average, new treatments were slightly more favorable both in terms of their effect on reducing the primary outcomes (hazard ratio (HR)/odds ratio (OR) 0.91, 99% confidence interval (CI) 0.88 to 0.95) and improving overall survival (HR 0.95, 99% CI 0.92 to 0.98). No heterogeneity was observed in the analysis based on primary outcomes or overall survival (I 2  = 0%). Kernel density analysis was consistent with the meta‐analysis, but showed a fairly symmetrical distribution of new versus established treatments indicating unpredictability in the results. This was consistent with the interpretation that new treatments are only slightly superior to established treatments when tested in RCTs. Additionally, meta‐regression demonstrated that results have remained stable over time and that the success rate of new treatments has not changed over the last half century of clinical trials. The results were not significantly affected by the choice of comparator (active versus placebo/no therapy). Authors' conclusions Society can expect that slightly more than half of new experimental treatments will prove to be better than established treatments when tested in RCTs, but few will be substantially better. This is an important finding for patients (as they contemplate participation in RCTs), researchers (as they plan design of the new trials), and funders (as they assess the 'return on investment'). Although we provide the current best evidence on the question of expected 'success rate' of new versus established treatments consistent with a priori theoretical predictions reflective of 'uncertainty or equipoise hypothesis', it should be noted that our sample represents less than 1% of all available randomized trials; therefore, one should exercise the appropriate caution in interpretation of our findings. In addition, our conclusion applies to publicly funded trials only, as we did not include studies funded by commercial sponsors in our analysis. Plain language summary New treatments versus established treatments in randomized trials Random allocation to different groups to compare the effects of treatments is used in fair tests to find out which among the treatment options is preferable. Random allocation is only ethical, however, if there is genuine uncertainty about which of the treatment options is preferable. If a patient or their healthcare provider is certain which of the treatments being compared is preferable they should not agree to random allocation, because this would involve the risk that they would be assigned to a treatment they believed to be inferior. Decisions about whether to participate in randomized trials are made more difficult because of the widespread belief that new treatments must inevitably be superior to existing (standard) treatments. Indeed, it is understandable that people hope that this will be the case. If this was actually so, however, the ethical precondition of uncertainty would often not apply. This Cochrane methodology review addresses this important question: ""What is the likelihood that new treatments being compared to established treatments in randomized trials will be shown to be superior?"" Four cohorts of consecutive, publicly funded, randomized trials, which altogether included 743 trials that enrolled 297,744 patients, met our inclusion criteria for this review. We found that, on average, new treatments were very slightly more likely to have favorable results than established treatments, both in terms of the primary outcomes targeted and overall survival. In other words, when new treatments are compared with established treatments in randomized trials we can expect slightly more than half will prove to be better, and slightly less than half will prove to be worse than established treatments. This conclusion applies to publicly funded trials as we did not include studies funded by commercial sponsors in our analysis.The results are consistent with the ethical preconditions for random allocation – when people are enrolled in randomized trials, the results cannot be predicted in advance as there is genuine uncertainty about which of the treatments being compared in randomized trials will prove to be superior. ","10","John Wiley & Sons, Ltd","1465-1858","*Treatment Outcome; Financing, Government; Humans; Randomized Controlled Trials as Topic [ethics, *standards]; Reference Standards; Therapies, Investigational [ethics, *standards]","10.1002/14651858.MR000024.pub3","http://dx.doi.org/10.1002/14651858.MR000024.pub3","Methodology"
"MR000027.PUB2","Young, T; Hopewell, S","Methods for obtaining unpublished data","Cochrane Database of Systematic Reviews","2011","Abstract - Background In order to minimise publication bias, authors of systematic reviews often spend considerable time trying to obtain unpublished data. These include data from studies conducted but not published (unpublished data), as either an abstract or full‐text paper, as well as missing data (data available to original researchers but not reported) in published abstracts or full‐text publications. The effectiveness of different methods used to obtain unpublished or missing data has not been systematically evaluated. Objectives To assess the effects of different methods for obtaining unpublished studies (data) and missing data from studies to be included in systematic reviews. Search methods We identified primary studies comparing different methods of obtaining unpublished studies (data) or missing data by searching the Cochrane Methodology Register (Issue 1, 2010), MEDLINE and EMBASE (1980 to 28 April 2010). We also checked references in relevant reports and contacted researchers who were known or who were thought likely to have carried out relevant studies. We used the Science Citation Index and PubMed 'related articles' feature to identify any additional studies identified by other sources (19 June 2009). Selection criteria Primary studies comparing different methods of obtaining unpublished studies (data) or missing data in the healthcare setting. Data collection and analysis The primary outcome measure was the proportion of unpublished studies (data) or missing data obtained, as defined and reported by the authors of the included studies. Two authors independently assessed the search results, extracted data and assessed risk of bias using a standardised data extraction form. We resolved any disagreements by discussion. Main results Six studies met the inclusion criteria; two were randomised studies and four were observational comparative studies evaluating different methods for obtaining missing data. Methods to obtain missing data Five studies, two randomised studies and three observational comparative studies, assessed methods for obtaining missing data (i.e. data available to the original researchers but not reported in the published study). Two studies found that correspondence with study authors by e‐mail resulted in the greatest response rate with the fewest attempts and shortest time to respond. The difference between the effect of a single request for missing information (by e‐mail or surface mail) versus a multistage approach (pre‐notification, request for missing information and active follow‐up) was not significant for response rate and completeness of information retrieved (one study). Requests for clarification of methods (one study) resulted in a greater response than requests for missing data. A well‐known signatory had no significant effect on the likelihood of authors responding to a request for unpublished information (one study). One study assessed the number of attempts made to obtain missing data and found that the number of items requested did not influence the probability of response. In addition, multiple attempts using the same methods did not increase the likelihood of response. Methods to obtain unpublished studies One observational comparative study assessed methods to obtain unpublished studies (i.e. data for studies that have never been published). Identifying unpublished studies ahead of time and then asking the drug industry to provide further specific detail proved to be more fruitful than sending of a non‐specific request. Authors' conclusions Those carrying out systematic reviews should continue to contact authors for missing data, recognising that this might not always be successful, particularly for older studies. Contacting authors by e‐mail results in the greatest response rate with the fewest number of attempts and the shortest time to respond. Plain language summary Methods for obtaining unpublished data This methodology review was conducted to assess the effects of different methods for obtaining unpublished studies (data) and missing data from studies to be included in systematic reviews. Six studies met the inclusion criteria, two were randomised studies and four were observational comparative studies evaluating different methods for obtaining missing data. Five studies assessed methods for obtaining missing data (i.e. data available to the original researchers but not reported in the published study). Two studies found that correspondence with study authors by e‐mail resulted in the greatest response rate with the fewest attempts and shortest time to respond. The difference between the effect of a single request for missing information (by e‐mail or surface mail) versus a multistage approach (pre‐notification, request for missing information and active follow‐up) was not significant for response rate and completeness of information retrieved (one study). Requests for clarification of methods (one study) resulted in a greater response than requests for missing data. A well‐known signatory had no significant effect on the likelihood of authors responding to a request for unpublished information (one study). One study assessed the number of attempts made to obtain missing data and found that the number of items requested did not influence the probability of response. In addition, multiple attempts using the same methods did not increase the likelihood of response. One study assessed methods to obtain unpublished studies (i.e. data for studies that have never been published). Identifying unpublished studies ahead of time and then asking the drug industry to provide further specific detail proved to be more fruitful than sending of a non‐specific request. Those carrying out systematic reviews should continue to contact authors for missing data, recognising that this might not always be successful, particularly for older studies. Contacting authors by e‐mail results in the greatest response rate with the fewest number of attempts and the shortest time to respond.","11","John Wiley & Sons, Ltd","1465-1858","*Access to Information; *Electronic Mail; Documentation [*methods]; Publishing; Randomized Controlled Trials as Topic","10.1002/14651858.MR000027.pub2","http://dx.doi.org/10.1002/14651858.MR000027.pub2","Methodology"
"MR000025.PUB2","Morissette, K; Tricco, AC; Horsley, T; Chen, MH; Moher, D","Blinded versus unblinded assessments of risk of bias in studies included in a systematic review","Cochrane Database of Systematic Reviews","2011","Abstract - Background The importance of appraising the risk of bias of studies included in systematic reviews is well‐established. However, uncertainty remains surrounding the method by which risk of bias assessments should be conducted. Specifically, no summary of evidence exists as to whether blinded (i.e. the assessor is unaware of the study author’s name, institution, sponsorship, journal, etc.) versus unblinded assessments of risk of bias yield systematically different assessments in a systematic review. Objectives To determine whether blinded versus unblinded assessments of risk of bias yield systematically different assessments in a systematic review. Search methods We searched MEDLINE (1966 to September week 4 2009), CINAHL (1982 to May week 3 2008), All EBM Reviews (inception to 6 October 2009), EMBASE (1980 to 2009 week 40) and HealthStar (1966 to September week 4 2009) (all Ovid interface). We applied no restrictions regarding language of publication, publication status or study design. We examined reference lists of included studies and contacted experts for potentially relevant literature. Selection criteria We included any study that examined blinded versus unblinded assessments of risk of bias included within a systematic review. Data collection and analysis We extracted information from each of the included studies using a pre‐specified 16‐item form. We summarized the level of agreement between blinded and unblinded assessments of risk of bias descriptively. We calculated the standardized mean difference whenever possible. Main results We included six randomized controlled trials (RCTs). Four studies had unclear risk of bias and two had high risk of bias. The results of these RCTs were not consistent; two demonstrated no differences between blinded and unblinded assessments, two found that blinded assessments had significantly lower quality scores, and another observed significantly higher quality scores for blinded assessments. The remaining study did not report the level of significance. We pooled five studies reporting sufficient information in a meta‐analysis. We observed no statistically significant difference in risk of bias assessments between blinded or unblinded assessments (standardized mean difference ‐0.13, 95% confidence interval ‐0.42 to 0.16). The mean difference might be slightly inaccurate, as we did not adjust for clustering in our meta‐analysis. We observed inconsistency of results visually and noted statistical heterogeneity. Authors' conclusions Our review highlights that discordance exists between studies examining blinded versus unblinded risk of bias assessments at the systematic review level. The best approach to risk of bias assessment remains unclear, however, given the increased time and resources required to conceal reports effectively, it may not be necessary for risk of bias assessments to be conducted under blinded conditions in a systematic review. Plain language summary Blinded versus unblinded assessments of risk of bias in studies included in a systematic review When researchers want to answer a question they can use an approach called a systematic review, which is intended to examine all of the studies that have been done in a particular area of interest. When examining and summarizing the literature, researchers are expected to determine which of the studies were well‐conducted (i.e. high quality) and those that were not. What we do not know enough about is  how  researchers should conduct the assessments to determine which studies were of high quality. This is important because if the researcher is aware of certain study characteristics (e.g. what journal the study was published in) they may inadvertently assess the study a certain way. For example, if the author of the study is well‐known to the assessor, they may be more likely to assume it is of 'high quality'. Our research examines whether blinding researchers to study characteristics makes a difference when the goal is to summarize the literature. We only found a few studies that reported data relevant to our question. The results from these studies were inconsistent, however, the results suggest that it may not make a difference if quality is appraised under blinded or unblinded conditions during a systematic review.","9","John Wiley & Sons, Ltd","1465-1858","*Publication Bias; *Review Literature as Topic; Randomized Controlled Trials as Topic; Risk Assessment","10.1002/14651858.MR000025.pub2","http://dx.doi.org/10.1002/14651858.MR000025.pub2","Methodology"
"MR000026.PUB2","Horsley, T; Dingwall, O; Sampson, M","Checking reference lists to find additional studies for systematic reviews","Cochrane Database of Systematic Reviews","2011","Abstract - Background Checking reference lists to identify relevant studies for systematic reviews is frequently recommended by systematic review manuals and is often undertaken by review authors. To date, no systematic review has explicitly examined the effectiveness of checking reference lists as a method to supplement electronic searching. Objectives To investigate the effectiveness of checking reference lists for the identification of additional, relevant studies for systematic reviews. Effectiveness is defined as the proportion of relevant studies identified by review authors solely by checking reference lists. Search methods We searched the databases of The Cochrane Library (Issue 3, 2008), Library and Information Science abstracts (LISA) (1969 to July 2008) and MEDLINE (1966 to July 2008). We contacted experts in systematic review methods and examined reference lists of articles. Selection criteria Studies of any design which examined checking reference lists as a search method for systematic reviews in any area. The primary outcome was the additional yield of relevant studies (i.e. studies not found through any other search methodologies); other outcomes were publication types identified and data pertaining to the costs (e.g. cost‐effectiveness, cost‐efficiency) of checking reference lists. Data collection and analysis We summarized data descriptively. Main results We included 12 studies (in 13 publications) in this review, but interpretability and generalizability of these studies is difficult and the study designs used were at high risk of bias. The additional yield (calculated by dividing the additional 'unique' yield identified by checking reference lists by the total number of studies found to be eligible within the study) of relevant studies identified through checking reference lists ranged from 2.5% to 42.7%. Only two studies reported yield information by publication type (dissertations and systematic reviews). No cost data were reported although one study commented that it was impossible to isolate the time spent on reference tracking since this was done in parallel with the critical appraisal of each paper, and for that particular study costs were not specifically estimated. Authors' conclusions There is some evidence to support the use of checking reference lists for locating studies in systematic reviews. However, this evidence is derived from weak study designs. In situations where the identification of all relevant studies through handsearching and database searching is difficult, it would seem prudent that authors of reviews check reference lists to supplement their searching. The challenge, therefore, is for review authors to recognize those situations. Plain language summary Examining reference lists to find relevant studies for systematic reviews Systematic reviews are summaries of the information that is available on one topic. The most common way to find information for a systematic review is to search electronic literature databases. To increase the chances of finding important information, researchers can also search the tables of contents of journals, and they can contact experts or organizations for more information on the topic of the review. Another way to find more information is to check through the reference lists of relevant studies to see if these references include reports of other studies that might be eligible for the review. It is important to determine whether or not checking reference lists is a good use of time and resources when conducting systematic reviews. We found 12 studies that explored whether or not checking reference lists was useful for systematic reviews. These studies reported a range of results, from identifying only a few additional studies (2.5%: 2 of 79 included studies) to identifying many additional studies (42.7%: 111 of 260 included studies) through checking reference lists. Unfortunately, none of the studies looked at how much time or money were spent on the process of checking reference lists, and it was suggested this would be almost impossible to estimate. Unfortunately our findings are based on weak information. The data do suggest that in situations where researchers may have difficulty locating information, checking through the reference lists may be an important way to reduce the risk of missing relevant information.","8","John Wiley & Sons, Ltd","1465-1858","*Bibliographies as Topic; *Review Literature as Topic","10.1002/14651858.MR000026.pub2","http://dx.doi.org/10.1002/14651858.MR000026.pub2","Methodology"
"MR000012.PUB3","Odgaard‐Jensen, J; Vist, GE; Timmer, A; Kunz, R; Akl, EA; Schünemann, H; Briel, M; Nordmann, AJ; Pregno, S; Oxman, AD","Randomisation to protect against selection bias in healthcare trials","Cochrane Database of Systematic Reviews","2011","Abstract - Background Randomised trials use the play of chance to assign participants to comparison groups. The unpredictability of the process, if not subverted, should prevent systematic differences between comparison groups (selection bias). Differences due to chance will still occur and these are minimised by randomising a sufficiently large number of people. Objectives To assess the effects of randomisation and concealment of allocation on the results of healthcare studies. Search methods We searched the Cochrane Methodology Register, MEDLINE, SciSearch and reference lists up to September 2009. In addition, we screened articles citing included studies (ISI Science Citation Index) and papers related to included studies (PubMed). Selection criteria Eligible study designs were cohorts of studies, systematic reviews or meta‐analyses of healthcare interventions that compared random allocation versus non‐random allocation or adequate versus inadequate/unclear concealment of allocation in randomised trials. Outcomes of interest were the magnitude and direction of estimates of effect and imbalances in prognostic factors. Data collection and analysis We retrieved and assessed studies that appeared to meet the inclusion criteria independently. At least two review authors independently appraised methodological quality and extracted information. We prepared tabular summaries of the results for each comparison and assessed the results across studies qualitatively to identify common trends or discrepancies. Main results A total of 18 studies (systematic reviews or meta‐analyses) met our inclusion criteria. Ten compared random allocation versus non‐random allocation and nine compared adequate versus inadequate or unclear concealment of allocation within controlled trials. All studies were at high risk of bias. For the comparison of randomised versus non‐randomised studies, four comparisons yielded inconclusive results (differed between outcomes or different modes of analysis); three comparisons showed similar results for random and non‐random allocation; two comparisons had larger estimates of effect in non‐randomised studies than in randomised trials; and two comparisons had larger estimates of effect in randomised than in non‐randomised studies. Five studies found larger estimates of effect in trials with inadequate concealment of allocation than in trials with adequate concealment. The four other studies did not find statistically significant differences. Authors' conclusions The results of randomised and non‐randomised studies sometimes differed. In some instances non‐randomised studies yielded larger estimates of effect and in other instances randomised trials yielded larger estimates of effect. The results of controlled trials with adequate and inadequate/unclear concealment of allocation sometimes differed. When differences occurred, most often trials with inadequate or unclear allocation concealment yielded larger estimates of effects relative to controlled trials with adequate allocation concealment. However, it is not generally possible to predict the magnitude, or even the direction, of possible selection biases and consequent distortions of treatment effects from studies with non‐random allocation or controlled trials with inadequate or unclear allocation concealment. Plain language summary Randomised controlled trials as a safeguard against biased estimates of treatment effects Randomised controlled trials (RCTs) use the play of chance to allocate participants to comparison groups to prevent selection bias. Other means of treatment allocation are more prone to bias because decisions about which treatment to use can be influenced by the preferences of the physician or patient. This review compares random allocation (allocated to treatment using a random method) versus non‐random allocation (allocated to treatment using a non‐random method, such as alternation or external, uncontrollable factors, with no clinical judgement involved) and controlled trials with adequate versus inadequate/unclear concealment of allocation. Concealed treatment allocation is best described in general terms as the process used to prevent foreknowledge of group assignment in a controlled trial (such as the use of sequentially numbered opaque, sealed envelopes). The results of randomised and non‐randomised studies sometimes differed. Sometimes non‐randomised studies yielded larger estimates of effect, and sometimes randomised trials yielded larger estimates of effect. On the other hand, not using concealed random allocation resulted in larger estimates of effect, but sometimes it resulted in similar estimates of effect (from harmful to beneficial or vice versa). It is a paradox that the unpredictability of random allocation is the best protection against the unpredictability of the extent to which non‐randomised studies may be biased.","4","John Wiley & Sons, Ltd","1465-1858","*Clinical Trials as Topic [methods, standards, statistics & numerical data]; *Random Allocation; *Selection Bias; Controlled Clinical Trials as Topic [methods, standards, statistics & numerical data]; Randomized Controlled Trials as Topic [methods, standards, statistics & numerical data]; Treatment Outcome","10.1002/14651858.MR000012.pub3","http://dx.doi.org/10.1002/14651858.MR000012.pub3","Methodology"
"MR000031.PUB2","Dwan, K; Altman, DG; Cresswell, L; Blundell, M; Gamble, CL; Williamson, PR","Comparison of protocols and registry entries to published reports for randomised controlled trials","Cochrane Database of Systematic Reviews","2011","Abstract - Background Publication of complete trial results is essential if people are to be able to make well‐informed decisions about health care. Selective reporting of randomised controlled trials (RCTs) is a common problem. Objectives To systematically review studies of cohorts of RCTs to compare the content of trial reports with the information contained in their protocols, or entries in a trial registry. Search methods We conducted electronic searches in Ovid MEDLINE (1950 to August 2010); Ovid EMBASE (1980 to August 2010); ISI Web of Science (1900 to August 2010) and the Cochrane Methodology Register (Issue 3, 2010), checked reference lists, and asked authors of eligible studies to identify further studies. Studies were not excluded based on language of publication or our assessment of their quality. Selection criteria Published or unpublished cohort studies comparing the content of protocols or trial registry entries with published trial reports. Data collection and analysis Data were extracted by two authors independently. Risk of bias in the cohort studies was assessed in relation to follow up and selective reporting of outcomes. Results are presented separately for the comparison of published reports to protocols and trial registry entries. Main results We included 16 studies assessing a median of 54 RCTs (range: 2 to 362). Twelve studies compared protocols to published reports and four compared trial registry entries to published reports. In two studies, eligibility criteria differed between the protocol and publication in 19% and 100% RCTs. In one study, 16% (9/58) of the reports included the same sample size calculation as the protocol. In one study, 6% (4/63) of protocol‐report pairs gave conflicting information regarding the method of allocation concealment, and 67% (49/73) of blinded studies reported discrepant information on who was blinded. In one study unacknowledged discrepancies were found for methods of handling protocol deviations (44%; 19/43), missing data (80%; 39/49), primary outcome analyses (60%; 25/42) and adjusted analyses (82%; 23/28). One study found that of 13 protocols specifying subgroup analyses, 12 of these 13 trials reported only some, or none, of these. Two studies found that statistically significant outcomes had a higher odds of being fully reported compared to nonsignificant outcomes (range of odds ratios: 2.4 to 4.7). Across the studies, at least one primary outcome was changed, introduced, or omitted in 4‐50% of trial reports. Authors' conclusions Discrepancies between protocols or trial registry entries and trial reports were common, although reasons for these were not discussed in the reports. Full transparency will be possible only when protocols are made publicly available or the quality and extent of information included in trial registries is improved, and trialists explain substantial changes in their reports. Plain language summary Comparison of protocols and registry entries to published reports for randomised controlled trials The non‐reporting of a piece of research and the selective reporting of only some of its findings has been identified as a problem for research studies such as randomised trials and systematic reviews of these. If the decision about what to report and what to keep unpublished is based on the results obtained in the trial, this will lead to bias and potentially misleading conclusions by users of the research. One way to see if there might be discrepancies between what was planned or done in a trial and what is eventually reported is to compare the protocol or entry in a trial registry for the trial with the content of its published report. This might reveal that changes were made between the registration and planning of the trial and its eventual analysis. Any such changes should be described in the published report, to reassure readers and others who will use the trial's results that the risk of bias has been kept low. This Cochrane methodology review examines the reporting of randomised trials by reviewing research done by others in which the information in protocols or trial registry entries were compared to that in the published reports for groups of trials, to see if this detected any inconsistencies for any aspects of the trials. We included 16 studies in this review and the results indicate that there are often discrepancies between the information provided in protocol and trial registry entries and that contained in the published reports for randomised trials. These discrepancies cover many aspects of the trials and are not explained or stated in the published reports.","1","John Wiley & Sons, Ltd","1465-1858","*Publication Bias; Clinical Protocols [standards]; Cohort Studies; Double‐Blind Method; Random Allocation; Randomized Controlled Trials as Topic [methods, *standards]; Registries [*standards]","10.1002/14651858.MR000031.pub2","http://dx.doi.org/10.1002/14651858.MR000031.pub2","Methodology"
"MR000006.PUB3","Hopewell, S; Loudon, K; Clarke, MJ; Oxman, AD; Dickersin, K","Publication bias in clinical trials due to statistical significance or direction of trial results","Cochrane Database of Systematic Reviews","2009","Abstract - Background The tendency for authors to submit, and of journals to accept, manuscripts for publication based on the direction or strength of the study findings has been termed publication bias. Objectives To assess the extent to which publication of a cohort of clinical trials is influenced by the statistical significance, perceived importance, or direction of their results. Search methods We searched the Cochrane Methodology Register ( The Cochrane Library [Online]  Issue 2, 2007), MEDLINE (1950 to March Week 2 2007), EMBASE (1980 to Week 11 2007) and  Ovid MEDLINE In‐Process & Other Non‐Indexed Citations  (March 21 2007). We also searched the Science Citation Index (April 2007), checked reference lists of relevant articles and contacted researchers to identify additional studies. Selection criteria Studies containing analyses of the association between publication and the statistical significance or direction of the results (trial findings), for a cohort of registered clinical trials. Data collection and analysis Two authors independently extracted data. We classified findings as either positive (defined as results classified by the investigators as statistically significant (P < 0.05), or perceived as striking or important, or showing a positive direction of effect) or negative (findings that were not statistically significant (P ≥ 0.05), or perceived as unimportant, or showing a negative or null direction in effect). We extracted information on other potential risk factors for failure to publish, when these data were available. Main results Five studies were included. Trials with positive findings were more likely to be published than trials with negative or null findings (odds ratio 3.90; 95% confidence interval 2.68 to 5.68). This corresponds to a risk ratio of 1.78 (95% CI 1.58 to 1.95), assuming that 41% of negative trials are published (the median among the included studies, range = 11% to 85%). In absolute terms, this means that if 41% of negative trials are published, we would expect that 73% of positive trials would be published. Two studies assessed time to publication and showed that trials with positive findings tended to be published after four to five years compared to those with negative findings, which were published after six to eight years. Three studies found no statistically significant association between sample size and publication. One study found no significant association between either funding mechanism, investigator rank, or sex and publication. Authors' conclusions Trials with positive findings are published more often, and more quickly, than trials with negative findings. Plain language summary Publication bias in clinical trials due to statistical significance or direction of trial results The validity of a systematic review depends on the methods used to conduct the review. If there is a systematic bias, such that studies with statistically significant or positive findings are more likely to be published and included in systematic reviews than trials with non‐significant findings, then the validity of a review's conclusions can be threatened. This methodology review identified five studies that investigated the extent to which the publication of clinical trials (such as those approved by an ethics review board) is influenced by the statistical significance or direction of a trial's results. These studies showed that trials with positive findings (defined either as those that were statistically significant (P < 0.05), or those findings perceived to be important or striking, or those indicating a positive direction of treatment effect), had nearly four times the odds of being published compared to findings that were not statistically significant (P ≥ 0.05), or perceived as unimportant, or showing a negative or null direction of treatment effect. This corresponds to a risk ratio of 1.78 (95% CI 1.58 to 1.95), assuming that 41% of negative trials are published.Two studies found that trials with positive findings also tended to be published more quickly than trials with negative findings. The size of the trial (assessed in three studies) and the source of funding, academic rank, and sex of the principal investigator (assessed in one study) did not appear to influence whether a trial was published. These results provide support for mandating that clinical trials are registered before recruiting participants so that review authors know about all potentially eligible studies, regardless of their findings. Those carrying out systematic reviews should ensure they assess the potential problems of publication bias in their review and consider methods for addressing this issue by ensuring a comprehensive search for both published and unpublished trials.","1","John Wiley & Sons, Ltd","1465-1858","*Publication Bias; Clinical Trials as Topic [*statistics & numerical data]","10.1002/14651858.MR000006.pub3","http://dx.doi.org/10.1002/14651858.MR000006.pub3","Central Editorial Service"
"MR000002.PUB3","Wager, E; Middleton, P","Technical editing of research reports in biomedical journals","Cochrane Database of Systematic Reviews","2008","Abstract - Background Most journals try to improve their articles by technical editing processes such as proof‐reading, editing to conform to 'house styles', grammatical conventions and checking accuracy of cited references. Despite the considerable resources devoted to technical editing, we do not know whether it improves the accessibility of biomedical research findings or the utility of articles. This is an update of a Cochrane methodology review first published in 2003. Objectives To assess the effects of technical editing on research reports in peer‐reviewed biomedical journals, and to assess the level of accuracy of references to these reports. Search methods We searched The Cochrane Library Issue 2, 2007; MEDLINE (last searched July 2006); EMBASE (last searched June 2007) and checked relevant articles for further references. We also searched the Internet and contacted researchers and experts in the field. Selection criteria Prospective or retrospective comparative studies of technical editing processes applied to original research articles in biomedical journals, as well as studies of reference accuracy. Data collection and analysis Two review authors independently assessed each study against the selection criteria and assessed the methodological quality of each study. One review author extracted the data, and the second review author repeated this. Main results We located 32 studies addressing technical editing and 66 surveys of reference accuracy. Only three of the studies were randomised controlled trials.     A 'package' of largely unspecified editorial processes applied between acceptance and publication was associated with improved readability in two studies and improved reporting quality in another two studies, while another study showed mixed results after stricter editorial policies were introduced. More intensive editorial processes were associated with fewer errors in abstracts and references. Providing instructions to authors was associated with improved reporting of ethics requirements in one study and fewer errors in references in two studies, but no difference was seen in the quality of abstracts in one randomised controlled trial. Structuring generally improved the quality of abstracts, but increased their length. The reference accuracy studies showed a median citation error rate of 38% and a median quotation error rate of 20%. Authors' conclusions Surprisingly few studies have evaluated the effects of technical editing rigorously. However there is some evidence that the 'package' of technical editing used by biomedical journals does improve papers. A substantial number of references in biomedical articles are cited or quoted inaccurately. Plain language summary Technical editing of articles before they are published in medical journals. Most journals try to improve articles before publication by editing them to make them fit a 'house‐style', and by other processes such as proof‐reading. We refer to all these processes as technical editing. We identified 32 studies of the effects of technical editing from a systematic review. There is some evidence that the overall 'package' of technical editing raises the quality of articles (suggested by 'before‐and‐after' studies) and that structuring abstracts makes them more useful, although longer. However, there has been little rigorous research to show which processes can improve accuracy or readability the most, or if any have harmful effects or disadvantages. Over one third of references cited in articles in medical journals have some inaccuracies and one‐fifth of quotations to references in these articles are not accurate","4","John Wiley & Sons, Ltd","1465-1858","Editorial Policies; Peer Review, Research [standards]; Periodicals as Topic [*standards]; Publishing [*standards]","10.1002/14651858.MR000002.pub3","http://dx.doi.org/10.1002/14651858.MR000002.pub3","Methodology"
"MR000009.PUB4","Vist, GE; Bryant, D; Somerville, L; Birminghem, T; Oxman, AD","Outcomes of patients who participate in randomized controlled trials compared to similar patients receiving similar interventions who do not participate","Cochrane Database of Systematic Reviews","2008","Abstract - Background Some people believe that patients who take part in randomised controlled trials (RCTs) face risks that they would not face if they opted for non‐trial treatment. Others think that trial participation is beneficial and the best way to ensure access to the most up‐to‐date physicians and treatments. This is an updated version of the original Cochrane review published in Issue 1, 2005. Objectives To assess the effects of patient participation in RCTs ('trial effects') independent both of the effects of the clinical treatments being compared ('treatment effects') and any differences between patients who participated in RCTs and those who did not. We aimed to compare similar patients receiving similar treatment inside and outside of RCTs. Search methods In March 2007, we searched The Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, EMBASE, The Cochrane Methodology Register, SciSearch and PsycINFO for potentially relevant studies. Our search yielded 7586 new references. In addition, we reviewed the reference lists of relevant articles. Selection criteria Randomized studies and cohort studies with data on clinical outcomes of RCT participants and similar patients who received similar treatment outside of RCTs. Data collection and analysis At least two review authors independently assessed studies for inclusion, assessed study quality and extracted data. Main results We identified 30 new non‐randomized cohort studies (45 comparisons): no new RCTs were found. This update now includes five RCTs (yielding 6 comparisons) and 80 non‐randomized cohort studies (130 comparisons), with 86,640 patients treated in RCTs and 57,205 patients treated outside RCTs. In the randomised studies, patients were invited to participate in an RCT or not; these comparisons provided limited information because of small sample sizes (a total of 412 patients) and the nature of the questions they addressed. When the results of RCTs and non‐randomized cohorts that reported dichotomous outcomes were combined, there were 98 comparisons; there was also heterogeneity (P < 0.00001, I 2  = 42.2%) between studies. No statistical significant differences were found for 85 of the 98 comparisons. Eight comparisons reported statistically significant better outcomes for patients treated within RCTs, and five comparisons reported statistically significant worse outcomes for patients treated within RCTs. There was significant heterogeneity (P < 0.00001, I 2  = 58.2%) among the 38 continuous outcome comparisons. No statistically significant differences were found for 30 of the 38 comparisons. Three comparisons reported statistically significant better outcomes for patients treated within RCTs, and five comparisons reported statistically significant worse outcomes for patients treated within RCTs. Authors' conclusions This review indicates that participation in RCTs is associated with similar outcomes to receiving the same treatment outside RCTs. These results challenge the assertion that the results of RCTs are not applicable to usual practice. Plain language summary Outcomes of patients who participate in randomized controlled trials compared to similar patients receiving similar interventions who do not participate This updated review assessed whether there were harmful or beneficial effects from participating in randomized controlled trials (RCTs). The outcomes of patients who participated in RCTs were compared with outcomes of patients who were eligible for the trial and received similar clinical interventions, but did not participate. Comparisons were included both of 'experimental' treatment inside and outside of RCT and of 'control' treatment comparisons. On average, the outcomes of patients participating and not participating in RCTs were similar, suggesting that participation in RCTs, independent of the effects of the clinical interventions being compared, is likely to be comparable.","3","John Wiley & Sons, Ltd","1465-1858","*Patient Acceptance of Health Care; *Process Assessment, Health Care; *Refusal to Participate; Cohort Studies; Humans; Randomized Controlled Trials as Topic [*adverse effects, mortality]; Risk Assessment; Treatment Outcome","10.1002/14651858.MR000009.pub4","http://dx.doi.org/10.1002/14651858.MR000009.pub4","Methodology"
"MR000023.PUB3","Moher, D; Tsertsvadze, A; Tricco, A; Eccles, M; Grimshaw, J; Sampson, M; Barrowman, N","When and how to update systematic reviews","Cochrane Database of Systematic Reviews","2008","Abstract - Background Systematic reviews are most helpful if they are up‐to‐date. We did a systematic review of strategies and methods describing when and how to update systematic reviews. Objectives To identify, describe and assess strategies and methods addressing: 1) when to update systematic reviews and 2) how to update systematic reviews. Search methods We searched MEDLINE (1966 to December 2005), PsycINFO, the Cochrane Methodology Register (Issue 1, 2006), and hand searched the 2005 Cochrane Colloquium proceedings. Selection criteria We included methodology reports, updated systematic reviews, commentaries, editorials, or other short reports describing the development, use, or comparison of strategies and methods for determining the need for updating or updating systematic reviews in healthcare. Data collection and analysis We abstracted information from each included report using a 15‐item questionnaire. The strategies and methods for updating systematic reviews were assessed and compared descriptively with respect to their usefulness, comprehensiveness, advantages, and disadvantages. Main results Four updating strategies, one technique, and two statistical methods were identified. Three strategies addressed steps for updating and one strategy presented a model for assessing the need to update. One technique discussed the use of the ""entry date"" field in bibliographic searching. Statistical methods were cumulative meta‐analysis and predicting when meta‐analyses are outdated. Authors' conclusions Little research has been conducted on when and how to update systematic reviews and the feasibility and efficiency of the identified approaches is uncertain. These shortcomings should be addressed in future research. Plain language summary When and how to update systematic reviews Systematic reviews are most useful when they are regularly updated. It is not clear, however, when updating should be done and which strategies and methods are most cost‐effectiveThis review identified strategies and methods used to update systematic reviews. Fifteen articles that documented 4 strategies and 2 statistical methods for updating systematic reviews were found. The 4 strategies have not been compared to one another so their practical performances are unclear. Little research has been conducted on when and how to update systematic reviews and the feasibility and efficiency of the identified approaches is uncertain. These shortcomings should be addressed in future research.","1","John Wiley & Sons, Ltd","1465-1858","*Meta‐Analysis as Topic; *Review Literature as Topic; Practice Guidelines as Topic; Time Factors","10.1002/14651858.MR000023.pub3","http://dx.doi.org/10.1002/14651858.MR000023.pub3","Methodology"
"MR000003.PUB2","Demicheli, V; Di Pietrantonj, C","Peer review for improving the quality of grant applications","Cochrane Database of Systematic Reviews","2007","Abstract - Background Grant giving relies heavily on peer review for the assessment of the quality of proposals but the evidence of effects of these procedures is scarce. Objectives To estimate the effect of grant giving peer review processes on importance, relevance, usefulness, soundness of methods, soundness of ethics, completeness and accuracy of funded research. Search methods Electronic database searches and citation searches; researchers in the field were contacted. Selection criteria Prospective or retrospective comparative studies with two or more comparison groups assessing different interventions or one intervention against doing nothing. Interventions may regard different ways of screening, assigning or masking submissions, different ways of eliciting opinions or different decision making procedures. Only original research proposals and quality outcome measures were considered. Data collection and analysis Studies were read, classified and described according to their design and study question. No quantitative analysis was performed. Main results Ten studies were included. Two studies assessed the effect of different ways of screening submissions, one study compared open versus blinded peer review and three studies assessed the effect of different decision making procedures. Four studies considered agreement of the results of peer review processes as the outcome measure. Screening procedures appear to have little effect on the result of the peer review process. Open peer reviewers behave differently from blinded ones. Studies on decision‐making procedures gave conflicting results. Agreement among reviewers and between different ways of assigning proposals or eliciting opinions was usually high. Authors' conclusions There is little empirical evidence on the effects of grant giving peer review. No studies assessing the impact of peer review on the quality of funded research are presently available. Experimental studies assessing the effects of grant giving peer review on importance, relevance, usefulness, soundness of methods, soundness of ethics, completeness and accuracy of funded research are urgently needed. Practices aimed to control and evaluate the potentially negative effects of peer review should be implemented meanwhile. Plain language summary Grant giving relies heavily on peer review for the assessment of the quality of proposals but the evidence of effects of these procedures is scarce This review was carried out in order to assess the effect of the various processes of peer review on the quality of funded research. Only ten studies were included and described in the review. We were unable to find comparative studies assessing the actual effect of peer review procedures on the quality of the funded researchThere is little empirical evidence on the effects of grant giving peer review. Experimental studies assessing the effects of grant giving peer‐review on importance, relevance, usefulness, soundness of methods, soundness of ethics, completeness and accuracy of funded research are urgently needed. Practices aimed to control and evaluate the potentially negative effects of peer review should be implemented meanwhile.","2","John Wiley & Sons, Ltd","1465-1858","*Financing, Organized; Peer Review, Research [*standards]; Quality Control","10.1002/14651858.MR000003.pub2","http://dx.doi.org/10.1002/14651858.MR000003.pub2","Methodology"
"MR000001.PUB2","Hopewell, S; Clarke, MJ; Lefebvre, C; Scherer, RW","Handsearching versus electronic searching to identify reports of randomized trials","Cochrane Database of Systematic Reviews","2007","Abstract - Background Systematic reviewers need to decide how best to reduce bias in identifying studies for their review. Even when journals are indexed in electronic databases, it can still be difficult to identify all relevant studies reported in these journals. Over 1700 journals have been or are being handsearched within The Cochrane Collaboration to identify reports of controlled trials in order to help address these problems. Objectives To review systematically empirical studies, which have compared the results of handsearching with the results of searching one or more electronic databases to identify reports of randomized trials. Search methods Studies were sought from The Cochrane Methodology Register (The Cochrane Library, Issue 2, 2002), MEDLINE (1966 to Week 1 July 2002), EMBASE (1980 to Week 25 2002), AMED (1985 to June 2002), BIOSIS (1985 to June 2002), CINAHL (1982 to June 2002), LISA (1969 to July 2002) and PsycINFO (1972 to May 2002). Researchers who may have carried out relevant studies were contacted. Selection criteria A research study was considered eligible for this review if it compared handsearching with searching one or more electronic databases to identify reports of randomized trials. Data collection and analysis The main outcome measure was the number of reports of randomized trials identified by handsearching as compared to electronic searching. Data were extracted on the electronic database searched, the complexity of electronic search strategy used, the characteristics of the journal reports identified, and the type of trial report identified. Main results Thirty‐four studies were included. Handsearching identified between 92% to 100% of the total number of reports of randomized trials found in the various comparisons in this review. Searching MEDLINE retrieved 55%, EMBASE 49% and PyscINFO 67%. The retrieval rate of the electronic database varied depending on the complexity of the search. The Cochrane Highly Sensitive Search Strategy (HSSS) identified 80% of the total number of reports of randomized trials found, searches categorised as 'complex' (including the Cochrane HSSS) found 65% and 'simple' found 42%. The retrieval rate for an electronic search was higher when the search was restricted to English language journals; 62% versus 39% for journals published in languages other than English. When the search was restricted to full reports of randomized trials, the retrieval rate for an electronic search improved: a complex search strategy (including the Cochrane HSSS) retrieved 82% of the total number of such reports of randomized trials. Authors' conclusions Handsearching still has a valuable role to play in identifying reports of randomized trials for inclusion in systematic reviews of health care interventions, particularly in identifying trials reported as abstracts, letters and those published in languages other than English, together with all reports published in journals not indexed in electronic databases. However, where time and resources are limited, searching an electronic database using a complex search (or the Cochrane HSSS) will identify the majority of trials published as full reports in English language journals, provided, of course, that the relevant journals have been indexed in the database. Plain language summary Handsearching versus electronic searching to identify reports of randomized trials This review shows that handsearching alone will miss a small proportion of studies and, that a combination of handsearching and electronic searching is the most comprehensive approach in identifying reports of randomized trials.","2","John Wiley & Sons, Ltd","1465-1858","*Randomized Controlled Trials as Topic; Abstracting and Indexing [standards]; Databases as Topic [standards]; Information Storage and Retrieval [*methods, standards]; Language","10.1002/14651858.MR000001.pub2","http://dx.doi.org/10.1002/14651858.MR000001.pub2","Methodology"
"MR000021.PUB3","Rendell, JM; Merritt, RK; Geddes, J","Incentives and disincentives to participation by clinicians in randomised controlled trials","Cochrane Database of Systematic Reviews","2007","Abstract - Background Patients and clinicians need reliable, up‐to‐date information from randomised controlled trials (RCTs) on the costs and benefits of treatments. Recruitment difficulties arise when clinicians do not invite patients to participate in trials. Objectives Primary: to assess the evidence for the effect of disincentives and incentives on the extent to which clinicians invite eligible patients to participate in RCTs of healthcare interventions.   Secondary: to assess the evidence in relation to stated willingness to invite participation. Search methods 1. The Cochrane Methodology Register and Cochrane Database of Methodology Reviews were searched in May 2006 and Cochrane Central Register of Controlled Trials, National Research Register and ClinicalTrialsGov in April 2005.   2. EMBASE, MEDLINE, CINAHL, PsycINFO and AMED were searched in April 2005.   3. Reference lists of included studies were checked. Selection criteria Studies exploring the effect of (dis)incentives on clinicians' views and recruitment‐related activity. Data collection and analysis The information about included studies was insufficient for a full assessment of quality. Data on (dis)incentives were extracted and association with recruitment tested. Main results No RCTs of interventions were identified. Eleven observational studies were included ‐ two medical records reviews, one matched pair study, one clinician interview study, two studies documenting clinicians' decisions and five postal surveys. Three measures of recruitment were used, invitation to participate, entry into RCT and reported entry to RCT. Five studies explored the effect of patient characteristics. The effect of age and prognosis varied between trials. Six studies considered the association between clinicians' views and recruitment. Clinicians who agreed to participate because they were acquainted with the researchers were less likely to participate than those otherwise motivated (1 study, 2‐sided p = 0.04 Fisher's exact test) and (Odds Ratio [OR] 0.4, 95% Confidence Interval [CI] 0.2 to 0.9, 1 study). Clinicians who had recruited were more likely to report some difficulties including ""trials involve extra work"" (OR 92.94, 95% CI 4.54 ‐ 1902.11; p ≤ 0.01, 1 study) and ""inviting patients to participate is embarrassing"" (chi‐square 15.55, df = 1, p < 0.0001, 1 study). The effect of the need to discuss clinical uncertainty was unclear but concern that the doctor‐patient relationship would be adversely affected by participation was a deterrent (chi‐square = 7.25, df = 1, p = 0.007, 1 study). Authors' conclusions The impact of factors varied across studies. Researchers need to be aware that aspects of the design and conduct of trials can affect clinicians' willingness to invite patients to participate. Further research is needed. Plain language summary Incentives and disincentives to participation by clinicians in randomised controlled trials Randomised controlled trials (RCTs) are needed to provide robust evidence of the relative efficacy and safety of treatments. In many RCTs, clinicians (i.e. healthcare professionals inviting patients to take part in an RCT in which they provide at least one of the interventions) only invite a small proportion of the people who are eligible for trials to take part. Observational studies have been conducted to explore reasons for this but the results do not identify any factors that appear to have a consistent impact on recruitment.","2","John Wiley & Sons, Ltd","1465-1858","*Attitude of Health Personnel; *Motivation; *Patient Selection; Humans; Randomized Controlled Trials as Topic [*psychology]; Research Personnel [*psychology]; Sample Size","10.1002/14651858.MR000021.pub3","http://dx.doi.org/10.1002/14651858.MR000021.pub3","Methodology"
"MR000016.PUB3","Jefferson, T; Rudin, M; Brodney Folse, S; Davidoff, F","Editorial peer review for improving the quality of reports of biomedical studies","Cochrane Database of Systematic Reviews","2007","Abstract - Background Scientific findings must withstand critical review if they are to be accepted as valid, and editorial peer review (critique, effort to disprove) is an essential element of the scientific process. We review the evidence of the editorial peer‐review process of original research studies submitted for paper or electronic publication in biomedical journals. Objectives To estimate the effect of processes in editorial peer review. Search methods The following databases were searched to June 2004: CINAHL, Ovid, Cochrane Methodology Register, Dissertation abstracts, EMBASE, Evidence Based Medicine Reviews: ACP Journal Club, MEDLINE, PsycINFO, PubMed. Selection criteria We included prospective or retrospective comparative studies with two or more comparison groups, generated by random or other appropriate methods, and reporting original research, regardless of publication status. We hoped to find studies identifying good submissions on the basis of: importance of the topic dealt with, relevance of the topic to the journal, usefulness of the topic, soundness of methods, soundness of ethics, completeness and accuracy of reporting. Data collection and analysis Because of the diversity of study questions, viewpoints, methods, and outcomes, we carried out a descriptive review of included studies grouping them by broad study question. Main results We included 28 studies. We found no clear‐cut evidence of effect of the well‐researched practice of reviewer and/or author concealment on the outcome of the quality assessment process (9 studies). Checklists and other standardisation media have some evidence to support their use (2 studies). There is no evidence that referees' training has any effect on the quality of the outcome (1 study). Different methods of communicating with reviewers and means of dissemination do not appear to have an effect on quality (3 studies). On the basis of one study, little can be said about the ability of the peer‐review process to detect bias against unconventional drugs. Validity of peer review was tested by only one small study in a specialist area. Editorial peer review appears to make papers more readable and improve the general quality of reporting (2 studies), but the evidence for this has very limited generalisability. Authors' conclusions At present, little empirical evidence is available to support the use of editorial peer review as a mechanism to ensure quality of biomedical research. However, the methodological problems in studying peer review are many and complex. At present, the absence of evidence on efficacy and effectiveness cannot be interpreted as evidence of their absence. A large, well‐funded programme of research on the effects of editorial peer review should be urgently launched. Plain language summary Editorial peer review for improving the quality of reports of biomedical studies Editorial peer review is used world‐wide as a tool to assess and improve the quality of submissions to paper and electronic biomedical journals. As the information revolution gathers pace, an empirically proven method of quality assurance is of paramount importance. The increasing availability of empirical research on the possible effects of peer review led us to carry out a review of current evidence on the efficacy of editorial peer review. We found few studies of reasonable quality, and most of these were concerned with the effects of blinding reviewers and/or authors to each others' identity. We could not identify any methodologically convincing studies assessing the core effects of peer review. Major research is urgently needed.","2","John Wiley & Sons, Ltd","1465-1858","Biomedical Research [*standards]; Peer Review, Research [*standards]","10.1002/14651858.MR000016.pub3","http://dx.doi.org/10.1002/14651858.MR000016.pub3","Methodology"
"MR000010.PUB3","Hopewell, S; McDonald, S; Clarke, MJ; Egger, M","Grey literature in meta‐analyses of randomized trials of health care interventions","Cochrane Database of Systematic Reviews","2007","Abstract - Background The inclusion of grey literature (i.e. literature that has not been formally published) in systematic reviews may help to overcome some of the problems of publication bias, which can arise due to the selective availability of data. Objectives To review systematically research studies, which have investigated the impact of grey literature in meta‐analyses of randomized trials of health care interventions. Search methods We searched the Cochrane Methodology Register ( The Cochrane Library  Issue 3, 2005), MEDLINE (1966 to 20 May 2005), the Science Citation Index (June 2005) and contacted researchers who may have carried out relevant studies. Selection criteria A study was considered eligible for this review if it compared the effect of the inclusion and exclusion of grey literature on the results of a cohort of meta‐analyses of randomized trials. Data collection and analysis Data were extracted from each report independently by two reviewers. The main outcome measure was an estimate of the impact of trials from the grey literature on the pooled effect estimates of the meta‐analyses. Information was also collected on the area of health care, the number of meta‐analyses, the number of trials, the number of trial participants, the year of publication of the trials, the language and country of publication of the trials, the number and type of grey and published literature, and methodological quality. Main results Five studies met the inclusion criteria. All five studies showed that published trials showed an overall greater treatment effect than grey trials. This difference was statistically significant in one of the five studies. Data could be combined for three of the five studies. This showed that, on average, published trials showed a 9% greater treatment effect than grey trials (ratio of odds ratios for grey versus published trials 1.09; 95% CI 1.03‐1.16). Overall there were more published trials included in the meta‐analyses than grey trials (median 224 (IQR 108‐365) versus 45(IQR 40‐102)). Published trials had more participants on average. The most common types of grey literature were abstracts (55%) and unpublished data (30%). There is limited evidence to show whether grey trials are of poorer methodological quality than published trials. Authors' conclusions This review shows that published trials tend to be larger and show an overall greater treatment effect than grey trials. This has important implications for reviewers who need to ensure they identify grey trials, in order to minimise the risk of introducing bias into their review. Plain language summary Grey literature in meta‐analyses of randomized trials of health care interventions This methodology review identified five studies which investigated the effect of including trials found in the grey literature in systematic reviews of health care interventions. They showed that trials found in the published literature tend to be larger and show larger effects of a health care intervention than those trials found in the grey literature. There was limited evidence to show whether grey trials are of poorer methodological quality than published trials. This means that those carrying out systematic reviews need to search for trials in both the published and grey literature in order to help minimise the effects of publication bias in their review.","2","John Wiley & Sons, Ltd","1465-1858","*Meta‐Analysis as Topic; *Publishing [standards, statistics & numerical data]; *Randomized Controlled Trials as Topic [standards]; Publication Bias; Treatment Outcome","10.1002/14651858.MR000010.pub3","http://dx.doi.org/10.1002/14651858.MR000010.pub3","Methodology"
