[
  {
    "objectID": "database_analysis.html",
    "href": "database_analysis.html",
    "title": "Database Analysis",
    "section": "",
    "text": "IDEE: on peut regarder versions qui ont changé de titre, d’auteurs…\nIDEE: temps entre 1er protocole et 1ere version\nhistogramme year of first publication vs histrogramm year of last update\nCode\nknitr::opts_chunk$set(warning=F, message=F, results=F, fig.align = \"center\",  dev='svg')\n# Load the function file\nsource(\"functions.R\")\n#included:\n#-function to save csv f_save_csv_files\n#-set the default scale_color and scale_fill to viridis theme\n#-loads the core tidyverse package\n\nlibrary(patchwork) # for multiple graphs with ggplot\nlibrary(ggbeeswarm)  # For geom_quasirandom\n\n# Set theme for graphs\ntheme_set(\n  theme_classic() +\n  theme(\n    panel.grid.major.y = element_line(), #no vertical lines by default\n    #text = element_text(family = \"Times New Roman\"), #default font\n    plot.title = element_text(face=\"bold\") #graphs titles in bolds\n    )\n  )\nCode\n# Load the original downloaded database of cochrane systematic reviews on interventions\n# gives the latest published version of each SR\ncochrane_dataset &lt;- read.csv(\"source_data/cochrane_data_base_2025_01_22/review_type/SR_Interventions.csv\")\n\n# database of all versions, not only the latest one\nversions &lt;- read_csv(\"output_data/versions/SR_versions.csv\")\n\n# only keep versions that are not protocols\nversions_without_protocols &lt;- versions %&gt;% \n  filter(version_stage!=\"Protocol\")\n\n# database of all version with also detailed updates events\nversions_with_events &lt;- read_csv(\"output_data/versions/SR_versions_with_events.csv\")\nversions_with_events$Published &lt;- as.Date(versions_with_events$Published, format = \"%Y %b %d\") # Transform date string to a date object"
  },
  {
    "objectID": "database_analysis.html#latest-published-version",
    "href": "database_analysis.html#latest-published-version",
    "title": "Database Analysis",
    "section": "Latest published version",
    "text": "Latest published version\nThere are almost 9,000 systematic reviews in the database. Of these, for a bit more than half (55%, or 5,000 reviews), the latest version is Version 1. So we can only study the evolution of the certainty of evidence on the remaining ~4,000 reviews, which have at least 2 distinct versions..\n\n\nCode\n# get only latest version for each systematic review\nDOIs_latest &lt;- unique(cochrane_dataset$DOI)\n\nversions_without_protocols %&gt;% \n  filter(Version %in% paste0(\"https://doi.org/\", DOIs_latest)) %&gt;%\n  count(version_stage) %&gt;%  # Count occurrences for each version_stage\n  mutate(percentage = n / sum(n) * 100) %&gt;%  # Calculate percentage for each version_stage\n  ggplot(aes(x = version_stage, y = n, fill = version_stage)) +\n  geom_bar(stat = \"identity\", alpha=.9) +\n  geom_text(aes(label = paste0(n, \"\\n(\", round(percentage, 1), \"%)\")), vjust = -0.5) +  # Add text labels with count and percentage\n  theme(legend.position = \"none\") +\n  labs(\n    title = paste0(\"Latest version of Cochrane Systematic Reviews (\", nrow(cochrane_dataset), \" reviews)\"),\n    subtitle = \"read: for ~5000 reviews, the latest published version is version 1\",\n    x = \"latest version\",\n    y = \"number of\\nSystematic Reviews\"\n  ) +\n  scale_y_continuous(limits = c(0, 6000), breaks=seq(0, 6000, 1000))"
  },
  {
    "objectID": "database_analysis.html#time-between-versions-updates",
    "href": "database_analysis.html#time-between-versions-updates",
    "title": "Database Analysis",
    "section": "Time between versions updates",
    "text": "Time between versions updates\nHere is plotted the distribution of time span before a systematic review update.\nTBD: REGLER PRBLEME DES 0, ET ECRIRE V1 TO V2, V2 TO V3…\n\n\nCode\n #Calculate duration between consecutive version stages for each DOI.unique\nversions_duration &lt;- versions_without_protocols %&gt;%\n  # Arrange the data by DOI.unique and Published date\n  arrange(DOI.unique, Published) %&gt;%\n  # Group by DOI.unique to calculate version durations per DOI\n  group_by(DOI.unique) %&gt;%\n  # Create a new column for version stages: lag of 'Stage' and 'Published'\n  mutate(\n    previous_version = lag(version_stage),         # Previous version stage\n    previous_date = lag(Published),                # Previous version date\n    duration_years = as.numeric(interval(previous_date, Published) / years(1))  # Duration in years\n  ) %&gt;%\n  # Filter rows where the version is not the first (since no previous version exists for V1)\n  filter(!is.na(previous_version)) %&gt;%\n  select(DOI.unique, version_stage, previous_version, duration_years)\n\n\n# View the result\nglimpse(versions_duration)\n\n\n\n\nCode\n# Plot of the duration between any 2 versions\nggplot(\n  data = versions_duration, \n  aes(y = duration_years, x = as.factor(\"\"))\n) +\n  # Quasirandom points (jittered for better visibility)\n  geom_quasirandom(\n    alpha = 0.1\n  ) +\n  # Boxplot (basic, no weights)\n  geom_boxplot(\n    color = \"black\",\n    width = 0.3,\n    outlier.shape = NA,\n  ) +\n  # Labels and theme\n  labs(\n    title = \"Distribution of Duration between 2 versions\", \n    subtitle = \"Each dot represents one duration between 2 versions; boxplot shows the distribution.\",\n    x = \"\", \n    y = \"Duration before an upadate\\n(Years)\",\n    caption = \"\"\n  ) +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() +\n  coord_flip() \n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot of the duration between specific versions (V1/V2, V3/V4///)\nggplot(\n  data = versions_duration, \n  aes(y = duration_years, x = version_stage)\n) +\n  # Quasirandom points (jittered for better visibility)\n  geom_quasirandom(\n    aes(color = version_stage),\n    alpha = 0.3\n  ) +\n  # Boxplot (basic, no weights)\n  geom_boxplot(\n    aes(fill = version_stage),\n    color = \"black\",\n    width = 0.3,\n    outlier.shape = NA,\n  ) +\n  # Labels and theme\n  labs(\n    title = \"Distribution of Duration by Version Stage\", \n    subtitle = \"Each dot represents one observation, and boxplot shows the distribution.\",\n    x = \"\", \n    y = \"Duration before an upadate\\n(Years)\",\n    caption = \"\"\n  ) +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() +\n  coord_flip() +\n  theme(axis.title.y = element_blank(), legend.position = \"none\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a project attempting to analyze the evolution of the “certainty of evidence” of outcomes assessed in Cochrane library."
  },
  {
    "objectID": "cochrane_updates.html",
    "href": "cochrane_updates.html",
    "title": "Get Cochrane Systematic Reviews Data",
    "section": "",
    "text": "This page presents the procedure and code to gather data from the Cochrane Systematic Reviews Database, mainly through webs crapping.\n\n\n\n\n\n\nCode\nknitr::opts_chunk$set(warning=F, message=F, results=F, fig.align = \"center\",  dev='svg')\n# Load the function file\nsource(\"functions.R\")\n#included:\n#-function to save csv f_save_csv_files\n#-function to identify URLS that have not yet been processed during scrapping f_get_urls_to_process\n#-function to load a web page, with given URL f_get_html_page\n#-set the default scale_color and scale_fill to viridis theme\n#-loads the core tidyverse package\n\nlibrary(openai)\nlibrary(DT) # for interactive table\n\n# Set theme for graphs\ntheme_set(\n  theme_classic() +\n  theme(\n    panel.grid.major.y = element_line(), #no vertical lines by default\n    #text = element_text(family = \"Times New Roman\"), #default font\n    plot.title = element_text(face=\"bold\") #graphs titles in bolds\n    )\n  )\n\n\n\nLoad Cochrane reviews dataset\nThis work focuses on systematic reviews in the Cochrane library. We just focus on Reviews on Interventions (8976 reviews), excluding reviews labelled Diagnostic (193), Overview (70), Methodology (45), Qualitative (30), Prognosis (21), Rapid (13), Prototype (10).\nThe reviews data were downloaded as csv file on Cochrane Database of Systematic Reviews webpage on January 22, 2025.\nJE PENSE ENLEVER ABSTRACT EST OK; AUSSI JE PENSE ON PEUT GARDER 1 SEUL ENTRE DOI/URL\n\n\nCode\n# Load the downloaded database of cochrane systematic reviews on interventions\ncochrane_dataset &lt;- read.csv(\"source_data/cochrane_data_base_2025_01_22/review_type/SR_Interventions.csv\")\nglimpse(cochrane_dataset) #quick look at the data\n\n# Remove columns that we do not study\ncochrane_dataset &lt;- cochrane_dataset %&gt;%\n  select(-c(\n    \"Cochrane.Review.ID\", # this is equal to the last part of the DOI, bringing no further information\n    \"Source\", # always \"Cochrane Database of Systematic Reviews\"\n    \"Issue\", # integer between 1-12, representing the month of publication\n    \"Publisher\", # always \"John Wiley & Sons, Ltd\"\n    \"ISSN\", # always \"1465-1858\"\n    # Items that can be interesting for further analysis, but that we remove for now to make data lighter\n    \"Author.s.\", \n    \"Keywords\", \n    \"Cochrane.Review.Group.Code\" # 66 different review groups (\"Childhood Cancer\", \"Stroke\"...)\n    ))\n\n# The dataset only reports the latest version of each SR, but we will study every version of each SR\n# the dataset only reports the DOI of the latest version (e.g. \"10.1002/14651858.CD009730.pub3\")\n# each version DOI of a same SR only differs by the \".pubN\" suffix\n# we create a DOI.unique which will identify each SR, whatever the version, by removing the \".pubN\" suffix\ncochrane_dataset$DOI.unique &lt;- gsub(\"\\\\.pub\\\\d+\", \"\", cochrane_dataset$DOI)\n\n# See the remaning items: \"Title\", \"Year\", \"Abstract\", \"DOI\", \"URL\", \"DOI.unique\"\nglimpse(cochrane_dataset)\n\n# Reorder columns, rename URL to make it explicit it relates to a particular version\ncochrane_dataset &lt;- cochrane_dataset %&gt;%\n  select(c(Year, DOI.unique, URL_version = URL, Title, Abstract, DOI))\n\n\n\n\nGet all versions of reviews\nThe downloaded database only reports the latest version of each systematic review. But we want data on all published versions, to study changes between past and current versions on a same research question, and to see the evolution of Cochrane database content through time.\nThe history of versions is accessible on each systematic review webpage, below the title (go for instance on this one to see). The history of versions webpage (see here for instance) then presents a table with all versions and their unique DOI.\nBelow is the code to\n\ngo on the history of versions webpage of a given Systematic Review\nget the reported data for each of its particular versions (Publication date, Title, Stage, Authors, URL)\nreiterate of the 8976 reviews of the database (as of January 22, 2025).\n\n\nScrap versions updates dataCompute versions numbers\n\n\n\n\nCode\n# Functions used for extracting the versions data during scrapping:\n\n# Function to check if a string of characters is a date; returns TRUE/FALSE\nis_date &lt;- function(string) {\n  !is.na(as.Date(string, format = \"%Y %b %d\"))\n}\n\n# Function to clean an extracted table and standardize it to correct format\nclean_table &lt;- function(table, DOI.unique){\n  \n  # Checks if there actually was a version history version to download\n  if (!is.na(table)) {\n    \n    # Convert the table to a data frame\n    table_data &lt;- xml2::xml_find_all(table, \".//tr\") %&gt;%\n      purrr::map_df(~ {\n        cells &lt;- xml_find_all(.x, \".//th | .//td\")\n        setNames(as.list(xml_text(cells)), xml_name(cells))\n      })\n    \n    # Use the first row as column names and remove first row (now used as column names)\n    colnames(table_data) &lt;- table_data[1, ]\n    table_data &lt;- table_data[-1, ]\n    \n    # Remove unnamed columns (those with blank or NA names) and \n    # clean the remaining ones (remove extra spaces)\n    table_data &lt;- table_data[, !is.na(colnames(table_data)) & colnames(table_data) != \"\"]\n    colnames(table_data) &lt;- trimws(colnames(table_data)) # Base R method to remove leading/trailing spaces\n    \n    # Sanitize text to escape quotation marks\n    # otherwise problems in columns delimitations, when text contains ' \" '\n    table_data &lt;- table_data %&gt;%\n      mutate(across(everything(), ~ gsub('\"', '\"\"', .))) # Escape double quotes for  CSV compatibility\n    #escapes double quotation marks (\") by doubling them (\"\"), which is the correct way to handle quotes in CSV files\n    \n    # Remove rows where \"Title\" item is empty\n    table_data &lt;- table_data %&gt;% filter(Title != \"\")\n    \n    # Fill Version column NAs with the previous non-NA value\n    table_data &lt;- table_data %&gt;% fill(Version, .direction = \"down\")\n    \n    # Extract version events data into a revisions table, and correctly name their column names\n    revisions &lt;- table_data %&gt;% filter(Published==\"\") %&gt;% select(-Published)\n    colnames(revisions) &lt;-c(\"Revision_date\", \"Event\", \"Description\", \"Version\") \n    \n    # Remove lines where no date in the Revision_date column of revisions tables\n    revisions &lt;- revisions %&gt;% filter(sapply(Revision_date, is_date))\n    \n    # Remove rows where Publication date is empty in the table_data\n    table_data &lt;- table_data %&gt;% filter(Published != \"\")\n    \n    # now join the 2 tables for full description of all revision events for each update\n    full_data &lt;- left_join(table_data, revisions, by=\"Version\")\n    \n    # now adds the unique DOI ID of the Systematic Review\n    full_data$DOI.unique &lt;- DOI.unique\n    \n    # Return the processed Systematic Review versions data\n    return(full_data)\n    \n    } \n  \n  # Return NULL if no table is not found\n  else {\n    return(NULL) \n    }\n}\n\n\nUncomment the code below to scrap the versions DOIs/URLs (takes several hours). I scrapped the version history data on January 22, 2025.\n\n\nCode\n# # Path to the CSV file where the versions data of processed systematic reviews are stored\n# # if does not exist yet, will be created in the following\n# csv_file &lt;- \"output_data/versions/SR_versions_with_events.csv\"\n# \n# # Listing Systematic Reviews (identified by DOI.unique) whose versions data have not been extracted yet\n# # Identified by checking which DOI.unique from cochrane_dataset (the downloaded cochrane databse) are not present in the csv_file storing results\n# # Here 9000 &gt; to the number of systematic reviews (8,976), so it will scrap all the reviews; you can decrease the number\n# DOIs.unique.unprocessed &lt;- f_get_urls_to_process(csv_file, cochrane_dataset, 9000, \"DOI.unique\")\n# cat(length(DOIs.unique.unprocessed), \" systematic reviews left to process, out of\\n\",\n#     nrow(cochrane_dataset),  \" systematic reviews in the database (as of January 22, 2025)\", sep=\"\")\n#\n# # Process each DOI to get version history and append the result to the CSV file\n# for (DOI.unique in DOIs.unique.unprocessed) {\n# \n#   # URL linking to review version history\n#   url &lt;- paste0(\"https://www.cochranelibrary.com/cdsr/doi/\", DOI.unique, \"/information#versionTable\")\n# \n#   # Get html webpage content\n#   doc &lt;- f_get_html_page(url)\n# \n#   # Extract the raw table data, identified with the html ID versionTableContent\n#   table_versions &lt;- xml_find_first(doc, \"//table[@id='versionTableContent']\")\n# \n#   # Clean the extracted table\n#   cleaned_table_versions &lt;- clean_table(table_versions, DOI.unique)\n#   \n#   # Prints the processed systematic review\n#   cat(\"Systematic Review\", print(DOI.unique), \"processed\")\n#   \n#   # Add the results to the csv storing results (if there acutally was a table)\n#   if (!is.null(cleaned_table_versions)) {\n#     # Append to CSV file (and write col names only if does not exist yet)\n#     write_csv(cleaned_table_versions, file = csv_file, append = TRUE, col_names = !file.exists(csv_file))\n#   }\n# }\n\n\nBelow is an example of the history of version data displayed for a particular review (here){target=“_blank”}).\n\n\nCode\n# Read the pre-scrapped data\nversions_with_events &lt;- read_csv(\"output_data/versions/SR_versions_with_events.csv\")\n\n# Only select columns that are reported on the webpage, for SR \"10.1002/14651858.CD006919\" \ntemp &lt;- versions_with_events %&gt;%\n  filter(DOI.unique == \"10.1002/14651858.CD006919\") %&gt;%\n  select(c(\"Published\", \"Title\", \"Stage\", \"Authors\", \"Version\")) %&gt;%\n  distinct() # Because or processed data includes the \"show revisions\" data, not displayed here\n\n# Display the table\ndatatable(temp, rownames = F)\n\n\nLE CAS DES 34 NON PROCESSED ?\n\n\nCode\ncsv_file &lt;- \"output_data/versions/SR_versions_with_events.csv\"\n\n# Transform date string to a date object\nversions_with_events$Published &lt;- as.Date(versions_with_events$Published, format = \"%Y %b %d\")\n\n# number of reviews unprocessed for some reason\nDOIs.unique.unprocessed &lt;- f_get_urls_to_process(csv_file, cochrane_dataset, 16000, \"DOI.unique\")\nlength(DOIs.unique.unprocessed)\n\n\n\n\nWe now want to attribute a version number (V1, V2, V3…) to each particular version. Based on the table above, we can see that before the first version presenting results, there can be one or several Protocols. We code below attributes a version number to each version, not counting protocols.\nNUMEROTER LES PROTOCOLES !\n\n\nCode\n# Get the reviews versions, based on their URL suffix: empty, .pub2, .pub3, etc... If there have been n protocols with an associated URL, the version number is N - n, with N the number in the URL suffix pubN (N=1 if the URL is empty).\n\n# get a table with each line a unique occurence of version (no multiple lines due to multiple Events on a same version)\nversions &lt;- versions_with_events %&gt;% \n  select(Published, Stage, Version, DOI.unique) %&gt;% \n  distinct() # Because of multiple events in 1 unique version\n\n# if the stage is a protocol, indicate it, leave empty otherwise, ready to be filled by version number\nversions &lt;- versions %&gt;% \n  mutate(version_stage = case_when(Stage == \"Protocol\" ~ \"Protocol\",T~NA))\n\n# function to extract the N in URL suffix pubN, used to determine the version\nextract_pubN_number &lt;- function(version) {\n  if (str_detect(version, \"\\\\.pub\\\\d+$\")) {\n    as.integer(str_extract(version, \"\\\\d+$\"))\n  } else {\n    1\n  }\n}\n\n# fill actual version number, not counting protocol as a version number\n# - protocol rows keep \"Protocol\"\n# - non-protocol rows get assigned \"V\" followed by the extracted number minus the number of protocols for that DOI.unique\nversions &lt;- versions %&gt;%\n  group_by(DOI.unique) %&gt;%\n  mutate(\n    # Count the number of protocol rows for this DOI.unique\n    protocol_count = sum(Stage == \"Protocol\"),\n    # extract version number (or get default of 1)\n    version_number = sapply(Version, extract_pubN_number),\n    # For non-protocol rows (version_stage is NA) assign \"V\" followed by\n    # version_number adjusted by subtracting protocol_count.\n    # Protocol rows keep \"Protocol\".\n    version_stage = case_when(\n      !is.na(version_stage) ~ version_stage,\n      is.na(version_stage) ~ paste0(\"V\", version_number - protocol_count),\n      TRUE ~ version_stage\n    )\n  ) %&gt;%\n  select(-version_number, -protocol_count) %&gt;% \n  ungroup()\n\n# Order the versions: Protocol, V1, V2, V3...\nversions$version_stage &lt;- factor(\n  versions$version_stage, \n  levels = c(\"Protocol\", paste0(\"V\", 1:9))\n  )\n\n# now add version number to the main file\nversions_with_events$version_stage &lt;- \n  versions$version_stage[match(versions_with_events$Version, versions$Version)]\n\n# save the file with all the versions and their URL for each review\nf_save_csv_files(versions, \"output_data/versions/\", \"SR_versions.csv\")\n\n\n\n\n\nFrom now we will only focus on versions with results; we thus exclude the versions which are only protocols.\n\n\nCode\nversions_without_protocols &lt;- versions %&gt;%\n  filter(version_stage!=\"Protocol\")\n\ncat(\n  nrow(versions_without_protocols), \"unique versions, excluding protocols\\n\",\n  nrow(versions)-nrow(versions_without_protocols), \"versions that are protocols\"\n)\n\n\nThere are 15343 unique versions, excluding protocols; and 7011 versions that are protocols.\n\n\nScrap open access data\nBelow are the items that are open acess for every review, and for which you can completely reproduce the data scrapping results.\n\nAbstractsPlain language summary\n\n\nFor each abstract of each version, we extract the following contents:\n\nwhether it is a withdrawn version (in this case the abstract title is ”\nReason for withdrawal from publication”)\nthe abstract subsections (background, methods etc)\nthe text content\n\nUncomment the code below to scrap the abstract content of every versions (takes several hours). I scrapped the version history data on Feb 1, 2025.\n\n\nCode\n# # The principle is the same as in \"Scrap versions URL\"; see this section code if you want more details.\n# \n# # Path to the CSV file\n# csv_file &lt;- \"output_data/abstracts/SR_abstracts_with_sections.csv\"\n# versions_to_process &lt;- f_get_urls_to_process(csv_file, versions_without_protocols, 16000, \"Version\")\n# length(versions_to_process)\n# \n# # Process each version and append the result to the CSV file\n# for (version_url in versions_to_process) {\n# \n#   # Clean the URL by removing the \"https://doi.org/\" part\n#   url &lt;- sub(\"https://doi.org/\", \"\", version_url)\n# \n#   # Construct the full URL for the review version history\n#   url &lt;- paste0(\"https://www.cochranelibrary.com/cdsr/doi/\", url)\n# \n#   # Get the html code of the webpage page\n#   doc &lt;- f_get_html_page(url)\n# \n#   # Get the title of the Abstract section (in purple on the page)\n#   section_title &lt;- doc %&gt;%\n#     html_node(\"section.abstract\") %&gt;%\n#     html_nodes(\"h2.title\") %&gt;%\n#     html_text2()\n# \n#   # Extract the abstract sections\n#   sections &lt;- doc %&gt;%\n#     html_node(\"div.abstract.full_abstract\") %&gt;%# get abstract from its class ID\n#     html_nodes(\"section\") # Extract all &lt;section&gt; nodes\n# \n#   # Check if sections exist\n#   if (length(sections) &gt; 0) {\n#     # Extract section titles and their corresponding text\n#     df &lt;- tibble(\n#       Abstract_title = section_title,\n#       Section = sections %&gt;%\n#         html_node(\"h3.title\") %&gt;%\n#         html_text(trim = TRUE) %&gt;%\n#         replace(is.na(.), \"Unnamed Section\"), # Replace missing titles with \"Unnamed Section\"\n#       Text = sections %&gt;% html_node(\"p\") %&gt;% html_text2() # Extract section content\n#     )\n#   }\n#   # If no sections exist, extract the whole abstract text\n#   else {\n#     full_text &lt;- doc %&gt;%\n#       html_node(\"div.abstract.full_abstract\") %&gt;%\n#       html_text2() # Extract the full abstract text\n# \n#     # Create a tibble with \"Unnamed Section\"\n#     df &lt;- tibble(\n#       Abstract_title = section_title,\n#       Section = \"Unnamed Section\",\n#       Text = full_text\n#     )\n#   }\n# \n#   # Get metadata associated with the version URL\n#   metadata &lt;- versions_without_protocols %&gt;% filter(Version == version_url)\n# \n#   # Add metadata information\n#   df &lt;- df %&gt;%\n#     mutate(\n#       Published = metadata$Published,\n#       Version = metadata$Version,\n#       DOI.unique = metadata$DOI.unique,\n#       version_stage = metadata$version_stage\n#     )\n# \n#   # Rearrange columns so metadata columns come first\n#   df &lt;- df %&gt;%\n#     select(Published, Version, DOI.unique, version_stage, everything())\n# \n#   # Append to CSV file (and write col names only if does not exist yet)\n#   write_csv(df, file = csv_file, append = TRUE, col_names = !file.exists(csv_file))\n#   glimpse(df) # shows the collected data\n# }\n\n\n\n\nCode\ncsv_file &lt;- \"output_data/abstracts/SR_abstracts_with_sections.csv\"\n\n# Check no more versions to process\ncat(\n  \"\\n\", length(f_get_urls_to_process(csv_file, versions_without_protocols, 16000, \"Version\")), \n  \"versions left to process\"\n)\n\n# Glimpse into the SR_abstracts_with_sections.csv content\nglimpse(read_csv(csv_file))\n\n\n\n\nFor each version, we extract the Plain Language Summary content. If there is no Plain Language Summary, the content is simply “NA”.\nUncomment the code below to scrap the abstract content of every versions (takes several hours). I scrapped the version history data on January 27, 2025\n\n\nCode\n# # Path to the CSV file\n# csv_file &lt;- \"output_data/plain_language_summary/SR_plain_language_summary.csv\"\n# \n# versions_to_process &lt;- f_get_urls_to_process(csv_file, versions_without_protocols, 16000, \"Version\")\n# length(versions_to_process)\n# # Process each version and append the result to the CSV file\n# for (version_url in versions_to_process) {\n# \n#   # Clean the URL by removing the \"https://doi.org/\" part\n#   url &lt;- sub(\"https://doi.org/\", \"\", version_url)\n# \n#   # Construct the full URL for the review version history\n#   url &lt;- paste0(\"https://www.cochranelibrary.com/cdsr/doi/\", url)\n# \n#   # Get the html code of the webpage page\n#   doc &lt;- f_get_html_page(url)\n# \n#   # Extract the Plain Language Summary text from the page\n#   pls &lt;- doc %&gt;%\n#     html_node(\"div.abstract.abstract_plainLanguageSummary\") %&gt;%\n#     html_text2()\n# \n#   # Retrieve the row(s) from versions_without_protocols for this version_url\n#   version_data &lt;- versions_without_protocols %&gt;% filter(Version == version_url)\n# \n#   # Add the plain language summary as a new column\n#   version_data &lt;- version_data %&gt;%\n#     mutate(plain_languag_summary = pls)\n# \n#   # Append to CSV file (and write col names only if does not exist yet)\n#   write_csv(version_data, file = csv_file, append = TRUE, col_names = !file.exists(csv_file))\n#   glimpse(version_data) # shows the collected data\n# }\n\n\n\n\nCode\ncsv_file &lt;- \"output_data/plain_language_summary/SR_plain_language_summary.csv\"\n\n# Check no more versions to process\ncat(\n  \"\\n\", length(f_get_urls_to_process(csv_file, versions_without_protocols, 16000, \"Version\")), \n  \"versions left to process\"\n)\n\n# Glimpse into the SR_abstracts_with_sections.csv content\nglimpse(read_csv(csv_file))\n\n\n\n\n\n\n\nScrap data behind paywalls\nBelow are the Cochrane systematic reviews content that are not always open access, and might me behind paywalls. If you are not connected to a network which can access paying content, you will only get a subset of the results.\n\nTBD Author’s conclusionsSummary Of Findings\n\n\nsection class authorsConclusions\n\n\nCode\n# # Path to the CSV file\n# csv_file &lt;- \"output_data/SR_authors_conclusion.csv\"\n# \n# # Check if the CSV file exists and load processed Versions\n# if (file.exists(csv_file)) {\n#   processed_data &lt;- read_csv(csv_file)\n#   processed_versions &lt;- unique(processed_data$Version)\n# } else {\n#   processed_versions &lt;- character(0)\n# }\n# \n# # Identify unprocessed Versions\n# versions_to_process &lt;- setdiff(versions_without_protocols$Version, processed_versions)\n# \n# #only get a subset of the unprocessed version\n# versions_to_process &lt;- head(versions_to_process, 1000)\n# \n# # Process each version and append the result to the CSV file\n# for (version_url in versions_to_process) {\n#   # Clean the URL by removing the \"https://doi.org/\" part\n#   version &lt;- sub(\"https://doi.org/\", \"\", version_url)\n# \n#   # Construct the full URL for the review version history\n#   url &lt;- paste0(\"https://www.cochranelibrary.com/cdsr/doi/\", version)\n# \n#   # Get the HTML page and parse it\n#   response &lt;- GET(url, user_agent(\"Mozilla/5.0\"))\n#   doc &lt;- content(response, as = \"parsed\", encoding = \"UTF-8\")\n# \n#   # Extract the abstract text from the page\n#   pls &lt;- doc %&gt;%\n#     html_node(\"div.abstract.abstract_plainLanguageSummary\") %&gt;%\n#     html_text2()\n# \n#   # Retrieve the row(s) from versions_without_protocols for this version_url\n#   version_data &lt;- versions_without_protocols %&gt;%\n#     filter(Version == version_url)\n# \n#   # Add the abstract_text as a new column\n#   version_data &lt;- version_data %&gt;%\n#     mutate(plain_languag_summary = pls)\n# \n#   # # Check if the file exists\n#   # if (file.exists(csv_file)) {\n#   #   # Append to the file without writing headers\n#   #   write_csv(version_data, file = csv_file, append = TRUE)\n#   # } else {\n#   #   # Write the file with headers\n#   #   write_csv(version_data, file = csv_file)\n#   # }\n# }\n\n\n\n\nThe extraction of the Summary Of Findings tables content operates in 2 steps, because the data formatting is not always the same and is more complex. First, we scrap and save the tables (represented by their html code) for each version. Second we extract the content by giving ChatGPT (chagtp-4o-mini) asking it to extract the information.\n\nScrap SOF sections contentExtract content with OpenAI API\n\n\nSummary Of Findings sections were generalized only starting 2008-2010. As a results, many reviews versions do not have such a section. If that is the case, we indicate “No summary of Findings” instead of the section centent.\nUncomment the code below to scrap the abstract content of every versions (takes several hours). I scrapped the Summary Of Findings tables on January 28-30, 2025.\n\n\nCode\n# csv_file &lt;- \"output_data/summary_of_findings/temporary_tables_html/SR_SOF_section.csv\"\n# \n# versions_to_process &lt;- f_get_urls_to_process(csv_file, versions_without_protocols, 16000, \"Version\")\n# length(versions_to_process)\n# \n# # Process each table version and append the result to the CSV file\n# for (version_url in versions_to_process) {\n# \n#   # Clean the URL by removing the \"https://doi.org/\" part\n#   url &lt;- sub(\"https://doi.org/\", \"\", version_url)\n# \n#   # Construct the full URL for the review version history\n#   url &lt;- paste0(\"https://www.cochranelibrary.com/cdsr/doi/\", url)\n# \n#   # Get the html code of the webpage page\n#   doc &lt;- f_get_html_page(url)\n#   \n#   # Extract the summary of findings section\n#   section_SOM &lt;- doc %&gt;%\n#     html_node(\"section.summaryOfFindings\") %&gt;%\n#     as.character()\n#   \n#     # Handle the case where the section is not found\n#     SOM_html_string &lt;- if (!is.na(section_SOM)) {\n#       section_SOM %&gt;% as.character()\n#     } else {\n#       \"No summary of Findings\"\n#     }\n#   \n#   # Create a data frame with version_url and the section\n#   data &lt;- data.frame(\n#     Version = version_url,\n#     SOM_html_string = SOM_html_string,\n#     stringsAsFactors = FALSE  # Ensure the strings are not converted to factors\n#     )\n#   glimpse(data)\n#   \n#   # Save (if file does not yet exist) or append (if exist) the data to the CSV file\n#   write_csv(data, file=csv_file, append = TRUE, col_names = !file.exists(csv_file))\n# \n# }\n\n\n\n\nCode\ncsv_file &lt;- \"output_data/summary_of_findings/temporary_tables_html/SR_SOF_section.csv\"\n\n# Check no more versions to process\ncat(\n  \"\\n\", length(f_get_urls_to_process(csv_file, versions_without_protocols, 16000, \"Version\")), \n  \"versions left to process\"\n)\n\n# Glimpse into the SR_abstracts_with_sections.csv content\nglimpse(read_csv(csv_file))\n\n# Get only versions which contain a SOF\nversions_with_SOM &lt;- read_csv(csv_file) %&gt;% filter(SOM_html_string != \"No summary of Findings\")\n\n\nOut of the 15343 systematic reviews versions, only 6525 have a summary of findings section.\n\n\nThen, for each version Summary of Findings, we give the html tables to chagtp-4o-mini through OpenAI API, and ask it to extract some core information:\n\nthe title of the summary of findings table\nthe PICO (population, intervention, comparison, outcome)\nthe certainty of evidence\n\n\n\nCode\nif (Sys.getenv(\"OPENAI_API_KEY\") == \"\") {\n  print(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n}\n# Uncomment and remplace to enter your API key\n#Sys.setenv(OPENAI_API_KEY = \"sk-XXXXX\")\n\n\nUncomment the code below to extract the summary of findings tables content using chatgpt API. You will need an OpenAI API key. This is very long (about 40 hours, began on Feb 2, ended on Feb 4).\n\n\nCode\n# # Retrieve the list of version URLs to process\n# csv_file &lt;- \"output_data/summary_of_findings/SR_SOF_confidence.csv\"\n# versions_to_process &lt;- f_get_urls_to_process(csv_file, versions_with_SOM, 7000, \"Version\")\n# cat(\"Number of versions to process:\", length(versions_to_process), \"\\n\")\n# cat(\"Total unique versions:\", length(unique(versions_with_SOM$Version)), \"\\n\")\n# \n# # Iterate over each version URL\n# for (version_url in versions_to_process) {\n# \n#   # Retrieve the row(s) from versions_without_protocols for this version_url\n#   version_data &lt;- versions_without_protocols %&gt;% filter(Version == version_url)\n# \n#   # Extract metadata values\n#   published_date &lt;- version_data$Published[1]\n#   doi_unique &lt;- version_data$DOI.unique[1]\n#   version_stage &lt;- version_data$version_stage[1]\n# \n#   # Extract the Summary of Findings section (as HTML)\n#   SOM_section_html &lt;- versions_with_SOM %&gt;%\n#     filter(Version == version_url) %&gt;%\n#     pull(SOM_html_string)\n# \n#   # Parse the SOM HTML content\n#   html_doc &lt;- read_html(SOM_section_html)\n# \n#   # Extract all table nodes (assuming tables are contained within &lt;div class=\"table\"&gt; elements)\n#   tables &lt;- html_doc %&gt;% html_nodes(\"div.table\")\n# \n#   # Check if any tables were found\n#   if(length(tables) == 0){\n#     warning(paste(\"No tables found for Version:\", version_url))\n#     next  # Skip to the next iteration if no tables are found\n#   }\n# \n#   # Extract HTML content for each table (each table is assumed to have a &lt;table&gt; node within the div)\n#   table_htmls &lt;- sapply(tables, function(tbl) {\n#     node &lt;- tbl %&gt;% html_node(\"table\")\n#     if (!is.na(node)) as.character(node) else \"\"\n#   })\n# \n#   # Combine the HTML for all tables using a separator to delineate them\n#   combined_tables_html &lt;- paste(table_htmls, collapse = \"\\n\\n-----\\n\\n\")\n# \n#   # Create the prompt for OpenAI for the current SOM section containing multiple tables\n#   prompt &lt;- paste(\n#     \"I have the following Cochrane Summary of Findings section (as HTML), containing multiple tables. Each table contains multiple outcomes with various details.\",\n#     \"For each outcome, please extract the following information as CSV (one row per outcome, one column per variable; fill missing data with 'NA'):\",\n#     \"\\n- SOM_title: the table title (generally the first line) describing what is evaluated.\",\n#     \"\\n- patient_population: population (and possibly subpopulations) of interest.\",\n#     \"\\n- settings: e.g., inpatient hospital, primary care in Europe, etc.\",\n#     \"\\n- intervention: the experimental intervention.\",\n#     \"\\n- comparison: the comparator intervention (including no specific intervention).\",\n#     \"\\n- outcome: outcome measured.\",\n#     \"\\n- outcome_additional_info: any additional information about the outcome (e.g., scale, follow-up, etc.).\",\n#     \"\\n- outcome_reported: 'yes' if associated certainty of evidence is reported, otherwise 'no'.\",\n#     \"\\n- certainty_of_evidence: Certainty of Evidence (omit symbols like ⊕ or ⊝).\",\n#     \"  Possible values: 'high', 'moderate', 'low', 'very low', 'NA', or if unclear for you, 'unable to attribute'.\",\n#     \"\\n\\nBelow is the combined HTML content for the tables:\",\n#     \"\\n\\n\", combined_tables_html,\n#     \"\\n\\nDo not include any Markdown formatting, code blocks, or additional text. Just provide the raw CSV data, so that I can process it after.\",\n#     sep = \"\"\n#   )\n# \n#   # Make the Chat Completion request (one request per SOM section)\n#   response &lt;- tryCatch(\n#     {\n#       create_chat_completion(\n#         model = \"gpt-4o-mini\",  # or \"gpt-4\" if available\n#         messages = list(\n#           list(role = \"system\", content = \"You are a helpful medical reviewer assistant.\"),\n#           list(role = \"user\",   content = prompt)\n#         ),\n#         temperature = 0\n#       )\n#     },\n#     error = function(e) {\n#       warning(paste(\"OpenAI API request failed for Version:\", version_url, \"with error:\", e$message))\n#       return(NULL)\n#     }\n#   )\n# \n#   # Inform about the response and token usage\n#   if (!is.null(response)) {\n#     cat(\"Response generated for Version:\", version_url, \"\\n\",\n#         response$usage$total_tokens, \"tokens used\\n\")\n#   }\n# \n#   # Skip to next iteration if API call failed\n#   if (is.null(response)) next\n# \n#   # Extract the CSV output from the response\n#   csv_output &lt;- tryCatch(\n#     {\n#       response$choices$message.content\n#     },\n#     error = function(e) {\n#       warning(paste(\"Failed to extract CSV content for Version:\", version_url))\n#       return(NULL)\n#     }\n#   )\n# \n#   # Skip if CSV output is NULL\n#   if (is.null(csv_output)) next\n# \n#   # Parse the CSV string into a data frame\n#   parsed_csv &lt;- tryCatch(\n#     {\n#       read_csv(csv_output, show_col_types = FALSE)\n#     },\n#     error = function(e) {\n#       warning(paste(\"Failed to parse CSV for Version:\", version_url))\n#       return(NULL)\n#     }\n#   )\n# \n#   # Skip if parsing failed\n#   if (is.null(parsed_csv)) next\n# \n#   # Add additional metadata columns to parsed_csv\n#   parsed_csv &lt;- parsed_csv %&gt;%\n#     mutate(\n#       Published = published_date,\n#       Version = version_url,\n#       DOI.unique = doi_unique,\n#       version_stage = version_stage\n#     ) %&gt;%\n#     # Reorder columns to have metadata first\n#     select(Published, Version, DOI.unique, version_stage, everything())\n# \n#   # Save (if file does not yet exist) or append (if exists) the data to the CSV file\n#   write_csv(parsed_csv, file = csv_file, append = TRUE, col_names = !file.exists(csv_file))\n# \n#   cat(\"Successfully processed Version:\", version_url, \"\\n\")\n# }"
  }
]